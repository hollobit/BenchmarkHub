[
  {
    "id": "csv-1-1769500744825",
    "title": "MedMNIST v2",
    "source": "arXiv",
    "authors": [
      "Jiancheng Yang",
      "Rui Shi",
      "Donglai Wei",
      "Zequan Liu",
      "Lin Zhao",
      "Bilian Ke",
      "Hanspeter Pfister",
      "Bingbing Ni"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.14795",
    "githubLink": "https://github.com/MedMNIST/MedMNIST",
    "itemCount": "708,069 2D images, 10,214 3D images (18 datasets total)",
    "specs": "2D (28x28, 64x64, 128x128, 224x224) and 3D (28x28x28, 64x64x64) images; Classification labels",
    "description": "A large-scale MNIST-like collection of standardized biomedical images, designed to be a lightweight benchmark for 2D and 3D biomedical image classification. It covers primary data modalities (e.g., X-ray, OCT, Ultrasound, CT, EM) and diverse tasks (binary/multi-class, ordinal regression, multi-label)."
  },
  {
    "id": "csv-2-1769500744825",
    "title": "MedQA (USMLE)",
    "source": "arXiv",
    "authors": [
      "Di Jin",
      "Eileen Pan",
      "Nassim Oufattole",
      "Wei-Hung Weng",
      "Hanyi Fang",
      "Peter Szolovits"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.13081",
    "githubLink": "https://github.com/jind11/MedQA",
    "itemCount": "12,723 questions (USMLE subset)",
    "specs": "Text (Question Answering), Multiple Choice (4 or 5 options)",
    "description": "A large-scale open-domain question answering dataset collected from professional medical board exams. The primary subset consists of questions from the United States Medical Licensing Examination (USMLE)."
  },
  {
    "id": "csv-3-1769500744825",
    "title": "PubMedQA",
    "source": "arXiv",
    "authors": [
      "Qiao Jin",
      "Bhuwan Dhingra",
      "Zhengping Liu",
      "William W. Cohen",
      "Xinghua Lu"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1909.06146",
    "githubLink": "https://github.com/pubmedqa/pubmedqa",
    "itemCount": "1k expert-labeled, 61.2k unlabeled, 211.3k artificially generated",
    "specs": "Text (QA), Yes/No/Maybe classification",
    "description": "A biomedical question answering dataset collected from PubMed abstracts. The task is to answer research questions with yes/no/maybe using the corresponding abstract context."
  },
  {
    "id": "csv-4-1769500744825",
    "title": "MedMCQA",
    "source": "arXiv",
    "authors": [
      "Ankit Pal",
      "Logesh Kumar Umapathi",
      "Malaikannan Sankarasubbu"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.14371",
    "githubLink": "https://github.com/medmcqa/medmcqa",
    "itemCount": "~194,000 MCQs",
    "specs": "Text (MCQA), 4 options, Subject/Topic metadata",
    "description": "A large-scale multiple-choice question answering dataset designed to address real-world medical entrance exam questions (AIIMS & NEET PG) covering a wide range of medical subjects."
  },
  {
    "id": "csv-5-1769500744825",
    "title": "MIMIC-IV",
    "source": "Scholar",
    "authors": [
      "Alistair E. W. Johnson",
      "Lucas Bulgarelli",
      "Lu Shen",
      "Alvin Gayles",
      "Ayad Shammout",
      "Steven Horng",
      "Leo Anthony Celi",
      "Roger G. Mark"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1038/s41597-022-01899-x",
    "githubLink": "https://github.com/MIT-LCP/mimic-code",
    "itemCount": ">65,000 ICU patients, >200,000 ED patients",
    "specs": "Relational Database (CSV), Text (Clinical Notes), Time-series",
    "description": "A freely accessible electronic health record dataset containing de-identified clinical data of patients admitted to the ICU and Emergency Department at Beth Israel Deaconess Medical Center. It includes vital signs, laboratory measurements, medications, and free-text clinical notes."
  },
  {
    "id": "imported-1769500766497-6-g2unm",
    "title": "ChartQA",
    "source": "arXiv",
    "authors": [
      "Ahmed Masry",
      "Do Long",
      "Jia Tan",
      "Shafiq Joty",
      "Enamul Hoque"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.10244",
    "githubLink": "https://github.com/vis-nlp/ChartQA",
    "itemCount": "32.7k QA pairs",
    "specs": "Images (Charts), Text (QA pairs), Data Tables",
    "description": "A benchmark for Question Answering about charts requiring visual and logical reasoning. It combines human-authored question-answer pairs with machine-generated ones to cover a wide range of visual and logical reasoning tasks."
  },
  {
    "id": "imported-1769500766497-7-01kdf",
    "title": "Chart-to-Text",
    "source": "Scholar",
    "authors": [
      "Shankar Kantharaj",
      "Rixie Tiffany Leong",
      "Xiang Lin",
      "Ahmed Masry",
      "Megh Thakkar",
      "Enamul Hoque",
      "Shafiq Joty"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.acl-long.277/",
    "githubLink": "https://github.com/vis-nlp/Chart-to-text",
    "itemCount": "44k Charts",
    "specs": "Images, Data Tables, Text (Summaries)",
    "description": "A large-scale benchmark for chart summarization and captioning. It contains charts from real-world sources (Pew, Statista) paired with data tables and summaries."
  },
  {
    "id": "imported-1769500766497-8-9uu2z",
    "title": "PlotQA",
    "source": "arXiv",
    "authors": [
      "Nitesh Methani",
      "Pritha Ganguly",
      "Mitesh M. Khapra",
      "Pratyush Kumar"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/1909.00931",
    "githubLink": "https://github.com/NiteshMethani/PlotQA",
    "itemCount": "224k Plots, 28M QA pairs",
    "specs": "Images (Plots), Text (QA pairs)",
    "description": "A large-scale dataset for reasoning over scientific plots. It is designed to test models on complex reasoning tasks involving data extraction and mathematical operations over plot data."
  },
  {
    "id": "imported-1769500766497-9-qz7jb",
    "title": "FigureQA",
    "source": "arXiv",
    "authors": [
      "Samira Ebrahimi Kahou",
      "Vincent Michalski",
      "Adam Atkinson",
      "Akos Kadar",
      "Adam Trischler",
      "Yoshua Bengio"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1710.07300",
    "githubLink": "https://github.com/Maluuba/FigureQA",
    "itemCount": "100k Images, 1.3M QA pairs",
    "specs": "Images (Synthetic Charts), Text (Binary QA)",
    "description": "A visual reasoning dataset consisting of synthetic figures (bar, line, pie charts) and yes/no questions. It focuses on relational reasoning capability of models."
  },
  {
    "id": "imported-1769500766497-10-9fxl1",
    "title": "DVQA",
    "source": "arXiv",
    "authors": [
      "Kushal Kafle",
      "Brian Price",
      "Scott Cohen",
      "Christopher Kanan"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1801.08163",
    "githubLink": "https://github.com/kushalkafle/DVQA_dataset",
    "itemCount": "307k Images, 2.3M QA pairs",
    "specs": "Images (Bar Charts), Text (QA pairs)",
    "description": "Understanding Data Visualization via Question Answering. A large-scale dataset of bar charts and QA pairs designed to test algorithms' ability to understand and extract information from bar charts."
  },
  {
    "id": "imported-1769500766497-11-ew66n",
    "title": "Terminal-Bench 2.0",
    "source": "arXiv",
    "authors": [
      "Mike A. Merrill",
      "Orfeas Menis Mastromichalakis",
      "Zhiwei Xu",
      "Zizhao Chen",
      "Yue Liu",
      "Jianbo Wu",
      "Minghao Yan",
      "Song Bian",
      "Vedang Sharma",
      "Ke Sun"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.11868",
    "githubLink": "https://github.com/laude-institute/terminal-bench",
    "itemCount": "89 tasks",
    "specs": "Terminal-based tasks, Docker environments, Python/Bash execution harness",
    "description": "A benchmark for benchmarking agents on hard, realistic tasks in command line interfaces (CLI). It evaluates how well agents can handle real-world, end-to-end tasks autonomously, ranging from compiling code to training models and setting up servers. The benchmark consists of a dataset of tasks and an execution harness connecting the agent to a terminal sandbox."
  },
  {
    "id": "imported-1769500766497-12-s18dl",
    "title": "BioProBench",
    "source": "Hugging Face",
    "authors": [
      "Yuyang Sunshine",
      "et al."
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.05000",
    "githubLink": "https://github.com/YuyangSunshine/bioprotocolbench",
    "itemCount": "550,000 task instances",
    "specs": "Text (Protocols, Reasoning)",
    "description": "A comprehensive benchmark for biological protocol understanding and reasoning, grounded in a large corpus of human-written protocols (BioProCorpus). It evaluates LLMs on tasks demanding deep reasoning and safety awareness in wet-lab contexts."
  },
  {
    "id": "imported-1769500766497-13-7ddxu",
    "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
    "source": "arXiv",
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo",
      "Truong-Son Hy"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.03699",
    "githubLink": "https://github.com/knoveleng/redeval",
    "itemCount": "29,362 samples",
    "specs": "Text (Attack and Refusal prompts), 22 risk categories, 19 domains",
    "description": "A universal dataset aggregating 37 benchmark datasets for red teaming Large Language Models (LLMs). It employs a standardized taxonomy with 22 risk categories and 19 domains to enable consistent and comprehensive evaluations of LLM vulnerabilities against adversarial prompts."
  },
  {
    "id": "imported-1769500766497-14-ayawi",
    "title": "DermaBench",
    "source": "arXiv",
    "authors": [
      "Yuhao Shen",
      "Jiahe Qian",
      "Shuping Zhang",
      "Tao Lu",
      "Juexiao Zhou"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.14084",
    "githubLink": "https://github.com/DermaVLM",
    "itemCount": "656 images, ~14,474 annotations",
    "specs": "Image + Text (VQA, Hierarchical Questions), Fitzpatrick Skin Types I-VI",
    "description": "A clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. It evaluates models on diagnostic diversity, demographic fairness, and precise labeling using a hierarchical annotation schema."
  },
  {
    "id": "imported-1769500766497-15-x0ayo",
    "title": "AutoMonitor-Bench",
    "source": "arXiv",
    "authors": [
      "Chuanrui Hu",
      "Xingze Gao",
      "Zuyi Zhou",
      "Dannong Xu",
      "Yi Bai",
      "Xintong Li",
      "Hui Zhang",
      "Tong Li",
      "Chong Zhang",
      "Lidong Bing"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.05353",
    "githubLink": "https://github.com/DAMO-NLP-SG/AutoMonitor-Bench",
    "itemCount": "3,010 annotated samples",
    "specs": "QA, code generation, and reasoning tasks with paired misbehavior instances.",
    "description": "A benchmark designed to evaluate the reliability of LLM-based misbehavior monitors. It includes specific categories for 'Specification Gaming', where models exploit loopholes in evaluation criteria. The dataset provides ground-truth annotations to measure detection rates of such behaviors."
  },
  {
    "id": "imported-1769500766497-16-7fq11",
    "title": "BizFinBench.v2",
    "source": "arXiv",
    "authors": [
      "Xin Guo",
      "Rongjunchen Zhang",
      "Guilong Lu",
      "Xuntao Guo",
      "Shuai Jia",
      "Zhi Yang",
      "Liwen Zhang"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.06401",
    "githubLink": "https://github.com/HiThink-Research/BizFinBench.v2",
    "itemCount": "29,578 Q&A pairs",
    "specs": "Text (English, Chinese), Financial Reports, Real-time Market Data",
    "description": "A large-scale bilingual (English & Chinese) benchmark designed to evaluate expert-level financial capabilities of Large Language Models. It focuses on authentic business scenarios including financial report analysis, anomalous information tracing, and wealth management, incorporating both offline core tasks and online real-time tasks."
  },
  {
    "id": "imported-1769500766497-17-ycq2l",
    "title": "DrivingGen",
    "source": "arXiv",
    "authors": [
      "Unnamed Authors (arXiv:2601.01528)"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.01528",
    "githubLink": "https://github.com/DrivingGen/DrivingGen",
    "itemCount": "Diverse driving scenarios (Weather, Time of day, Regions)",
    "specs": "Video generation, Autonomous Driving, Safety-critical scenarios",
    "description": "The first comprehensive benchmark for generative driving world models. It evaluates video generation models on visual realism, trajectory plausibility, temporal coherence, and controllability using a diverse dataset curated from driving logs and internet videos."
  },
  {
    "id": "imported-1769500766497-18-0bxxt",
    "title": "Sci-Reasoning",
    "source": "arXiv",
    "authors": [
      "Sci-Reasoning Team (Authors not fully listed in snippet)"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.04577",
    "githubLink": "https://github.com/sci-reasoning/sci-reasoning",
    "itemCount": "3,819 papers",
    "specs": "Text, Structured Reasoning Chains",
    "description": "A dataset designed to capture the intellectual synthesis and reasoning patterns behind high-quality AI research papers. It traces the lineage of oral/spotlight papers from top conferences to their predecessors to decode innovation patterns."
  },
  {
    "id": "imported-1769500766497-19-x7wx4",
    "title": "HeurekaBench",
    "source": "arXiv",
    "authors": [
      "Siba Smarak Panigrahi",
      "Jovana Videnović",
      "Maria Brbić"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.01678",
    "githubLink": "https://github.com/mlbio-epfl/HeurekaBench",
    "itemCount": "13 papers, 41 insights, 50 OEQs, 50 MCQs (sc-HeurekaBench instantiation)",
    "specs": "Experimental datasets (e.g., scRNA-seq), Open-ended Questions (OEQs), Multiple Choice Questions (MCQs)",
    "description": "A benchmarking framework designed to evaluate LLM-based agents functioning as 'co-scientists' in complex, multi-step scientific analysis tasks. It grounds questions in real scientific studies and their corresponding code repositories, requiring agents to explore experimental datasets and generate insights."
  },
  {
    "id": "imported-1769500766497-20-wrq4u",
    "title": "MentalChat16K",
    "source": "Hugging Face",
    "authors": [
      "Jia Xu",
      "Tianyi Wei",
      "Bojian Hou",
      "Patryk Orzechowski",
      "Shu Yang",
      "Ruochen Jin",
      "Rachael Paulbeck",
      "Joost Wagenaar",
      "George Demiris",
      "Li Shen"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.13509",
    "githubLink": "https://github.com/PennShenLab/MentalChat16K",
    "itemCount": "16,113 QA pairs (6,338 real, 9,775 synthetic)",
    "specs": "Text (English), Q&A pairs, Clinical Interview Transcripts, Synthetic Dialogues",
    "description": "A large-scale benchmark dataset designed for conversational mental health assistance, combining real anonymized interview transcripts from clinical trials with synthetic counseling dialogues generated by GPT-3.5 Turbo. It covers diverse mental health topics including depression, anxiety, and grief."
  },
  {
    "id": "imported-1769500766497-21-38g58",
    "title": "DeepResearch Bench",
    "source": "arXiv",
    "authors": [
      "Mingxuan Du",
      "Benfeng Xu",
      "Chiwei Zhu",
      "Xiaorui Wang",
      "Zhendong Mao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.11763",
    "githubLink": "https://github.com/Ayanami0730/deep_research_bench",
    "itemCount": "100 tasks",
    "specs": "Text-based research tasks; English and Chinese; 22 domains (e.g., Physics, Finance, Software); Evaluation metrics: Report quality (RACE) and Citation accuracy (FACT)",
    "description": "A comprehensive benchmark specifically designed to evaluate Deep Research Agents (DRAs). It consists of 100 PhD-level research tasks (50 in English and 50 in Chinese) meticulously crafted by domain experts across 22 distinct fields. The benchmark introduces two evaluation frameworks: RACE (for report quality) and FACT (for citation accuracy)."
  },
  {
    "id": "imported-1769500766497-22-wqufr",
    "title": "LiveDRBench",
    "source": "arXiv",
    "authors": [
      "Abhinav Java",
      "Ashmit Khandelwal",
      "Sukruta Midigeshi",
      "Aaron Halfaker",
      "Amit Deshpande",
      "Navin Goyal",
      "Ankur Gupta",
      "Nagarajan Natarajan",
      "Amit Sharma"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.04183",
    "githubLink": "https://github.com/microsoft/LiveDRBench",
    "itemCount": "100 tasks",
    "specs": "JSON format; Domains: Scientific (datasets, materials, prior art) and World Events; Metrics: Precision, Recall, F1 score on claim discovery",
    "description": "A diverse and challenging benchmark designed to characterize and evaluate 'deep research' tasks, defined by high concept fan-out and reasoning-intensive exploration rather than just long-form output. It separates reasoning challenges from surface-level report generation using an intermediate claim representation."
  },
  {
    "id": "imported-1769500766497-23-wcbo5",
    "title": "ResearchRubrics",
    "source": "arXiv",
    "authors": [
      "J. Gottweis",
      "Scale AI Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.07685",
    "githubLink": "https://scale.com/research/researchrubrics",
    "itemCount": "101 prompts; 2,593 rubric items",
    "specs": "Text prompts and detailed rubrics; 3 complexity axes: conceptual breadth, logical nesting, exploration; Evaluation via expert-written criteria",
    "description": "A standardized benchmark built with over 2,800 hours of human labor to assess the factual grounding, reasoning soundness, and clarity of deep research agents. It pairs realistic, domain-diverse prompts with expert-written, fine-grained rubrics, addressing the difficulty of evaluating open-ended, long-form research answers."
  },
  {
    "id": "imported-1769500766497-24-qqgbd",
    "title": "ADR-Bench (Application-driven Deep Research Benchmark)",
    "source": "arXiv",
    "authors": [
      "Chen Hu",
      "Haikuo Du",
      "StepFun Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.20491",
    "githubLink": "https://github.com/stepfun-ai/StepDeepResearch",
    "itemCount": "N/A (Suite across multiple domains)",
    "specs": "Chinese language; Domains: Commercial, Policy, Software Engineering; Metrics: Elo rating, multi-dimensional quality criteria",
    "description": "A benchmark suite specifically established to evaluate long-horizon, open-ended deep research tasks in the Chinese domain. It spans commercial research, policy analysis, and software engineering, using an Elo-style rating protocol and multi-dimensional quality criteria to measure human-perceived usefulness."
  },
  {
    "id": "imported-1769500766497-25-e79yl",
    "title": "Rigorous Bench",
    "source": "arXiv",
    "authors": [
      "Dixon (Medium)",
      "Submission 20638 Authors"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.xxxxx",
    "githubLink": "https://medium.com/@dixon_68202/a-rigorous-benchmark-with-multidimensional-evaluation-for-deep-research-agents-from-answers-to-9f506866164d",
    "itemCount": "214 queries",
    "specs": "10 broad domains (e.g., Technology, History, Health); Metrics: Semantic quality, topical focus, retrieval trustworthiness",
    "description": "A meticulously constructed benchmark designed to evaluate Deep Research Agents on report-style outputs. It focuses on task understanding, decomposition, execution, and aggregation across diverse thematic domains, addressing the limitations of traditional QA benchmarks for long-form research."
  },
  {
    "id": "imported-1769500766497-26-flfi7",
    "title": "FORTRESS (Frontier Risk Evaluation for National Security and Public Safety)",
    "source": "Hugging Face",
    "authors": [
      "Christina Knight",
      "Scale AI Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.14922",
    "githubLink": "https://huggingface.co/datasets/ScaleAI/fortress_public",
    "itemCount": "1,000 samples (500 adversarial, 500 benign) in public set",
    "specs": "Text (Parquet format); Domains: CBRNE, Political Violence, Criminal Activities",
    "description": "A red-teaming benchmark containing expert-crafted adversarial prompts and benign counterparts to evaluate LLM safeguards against CBRNE, terrorism, and criminal activity risks."
  },
  {
    "id": "imported-1769500766497-27-8tfc1",
    "title": "Forbidden Science: Dual-Use AI Challenge Benchmark",
    "source": "arXiv",
    "authors": [
      "David Noever",
      "Forrest McKee"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.06867",
    "githubLink": "https://huggingface.co/papers/2502.06867",
    "itemCount": "Systematic evaluation set (Exact count not specified in snippet)",
    "specs": "Text prompts; Domain: Dual-use Science (Biotech, Chemistry)",
    "description": "A scientific refusal test and benchmark designed to evaluate the balance between necessary safety restrictions and over-censorship of legitimate scientific inquiry in LLMs, focusing on dual-use concerns."
  },
  {
    "id": "imported-1769500766497-28-zileo",
    "title": "Enkrypt AI CBRN Risk Dataset",
    "source": "arXiv",
    "authors": [
      "Divyanshu Kumar",
      "Nitin Aravind Birur",
      "Tanay Baswa",
      "Sahil Agarwal",
      "Prashanth Harshangi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.21133",
    "githubLink": "Not released publicly (Private to prevent misuse)",
    "itemCount": "380 prompts (200 novel CBRN + 180 FORTRESS subset)",
    "specs": "Text prompts; Domains: Chemical, Biological, Radiological, Nuclear",
    "description": "A comprehensive evaluation dataset designed to assess CBRN safety vulnerabilities in frontier LLMs, consisting of novel prompts and a subset of the FORTRESS benchmark."
  },
  {
    "id": "imported-1769500766497-29-fo444",
    "title": "SecureCode v2.0",
    "source": "Hugging Face",
    "authors": [
      "Scott Thornton"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/scthornton/securecode-v2",
    "githubLink": "https://github.com/scthornton/securecode-v2",
    "itemCount": "1,215 examples",
    "specs": "JSON format; 11 programming languages; 4-turn conversational structure; Grounded in CVEs",
    "description": "A rigorously validated dataset of security-focused coding examples designed to train security-aware AI code generation models. Every example is grounded in real-world security incidents (CVEs) and covers the complete OWASP Top 10:2025 categories."
  },
  {
    "id": "imported-1769500766497-30-s6gh4",
    "title": "A Comprehensive Software Vulnerability Dataset Based on OWASP Top Ten Standard",
    "source": "Other",
    "authors": [
      "Moses Ndebugre",
      "Mahmoud Nabil",
      "Ahmad Patooghy",
      "Abdolhossein Sarrafzadeh"
    ],
    "year": "2025",
    "paperLink": "https://ieeexplore.ieee.org/document/10803446",
    "githubLink": "https://github.com/Mymona/SVCC-2025-Comprehensive-Dataset-OWASP-Standard",
    "itemCount": "7,552 commit IDs",
    "specs": "Code snippets; Graph-based representation; Mapped to OWASP Top 10",
    "description": "A curated dataset specifically designed to support the detection and classification of software vulnerabilities across multiple OWASP Top Ten security risks. It utilizes keyword mapping and graph-based code representation to structure the data."
  },
  {
    "id": "imported-1769500766497-31-c3rp5",
    "title": "ARC-AGI-2",
    "source": "arXiv",
    "authors": [
      "François Chollet",
      "Mike Knoop",
      "Greg Kamradt",
      "Bryan Chiou",
      "Abhay Soni"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.11831",
    "githubLink": "https://github.com/fchollet/ARC-AGI-2",
    "itemCount": "Expanded set of tasks (exact count not specified in initial release)",
    "specs": "JSON format, 2D grid pairs",
    "description": "An upgraded version of the ARC benchmark designed to provide finer-grained evaluation at higher levels of cognitive complexity. It maintains the same format as ARC-AGI-1 but introduces a curated set of tasks that are harder for current AI systems while remaining accessible to human intelligence."
  },
  {
    "id": "imported-1769500766497-32-sx1vl",
    "title": "H-ARC (Human-ARC)",
    "source": "Scholar",
    "authors": [
      "Solim LeGris",
      "Wai Keen Vong",
      "Brenden M. Lake",
      "Todd M. Gureckis"
    ],
    "year": "2025",
    "paperLink": "https://www.nature.com/articles/s41597-025-05687-1",
    "githubLink": "https://github.com/Le-Gris/h-arc",
    "itemCount": "15,744 attempts from 1,729 participants",
    "specs": "JSON, behavioral traces, text",
    "description": "A comprehensive behavioral dataset containing solution attempts, action traces, and natural language descriptions from over 1,700 humans solving ARC tasks. It serves as a baseline for human performance and a resource for cognitive science research."
  },
  {
    "id": "imported-1769500766497-33-jp9lr",
    "title": "Humanity's Last Exam (HLE)",
    "source": "arXiv",
    "authors": [
      "Dan Hendrycks",
      "Alexandr Wang",
      "Summer Yue",
      "Long Phan",
      "Alice Gatti",
      "Ziwen Han",
      "Nathaniel Li",
      "Josephina Hu",
      "Hugh Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.14249",
    "githubLink": "https://github.com/centerforaisafety/hle",
    "itemCount": "3,000 questions (2,500 public)",
    "specs": "Multi-modal (Text, Images), Multiple-Choice, Short-Answer",
    "description": "A multi-modal, expert-level benchmark designed to be the final closed-ended academic evaluation for Large Language Models. It consists of difficult questions across dozens of subjects (mathematics, humanities, natural sciences) that current frontier models struggle to answer, constructed to test reasoning at the frontier of human knowledge."
  },
  {
    "id": "imported-1769500766497-34-kuboq",
    "title": "Humanity's Last Code Exam (HLCE)",
    "source": "arXiv",
    "authors": [
      "Xiangyang Li",
      "Xiaopeng Li",
      "Kuicai Dong",
      "Quanhu Zhang",
      "Rongju Ruan",
      "Xinyi Dai",
      "Yasheng Wang",
      "Ruiming Tang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.12713",
    "githubLink": "https://github.com/Humanity-s-Last-Code-Exam/HLCE",
    "itemCount": "235 problems",
    "specs": "Code Generation, Text",
    "description": "A code generation benchmark comprising 235 of the most challenging problems from the International Collegiate Programming Contest (ICPC World Finals) and the International Olympiad in Informatics (IOI) spanning 2010-2024, designed to evaluate advanced reasoning and coding capabilities."
  },
  {
    "id": "imported-1769500766497-35-yz1uk",
    "title": "SWE-bench Multilingual",
    "source": "Hugging Face",
    "authors": [
      "John Yang",
      "Kilian Lieret",
      "Carlos E. Jimenez",
      "Alexander Wettig",
      "Kabir Khandpur",
      "Yanzhe Zhang",
      "Binyuan Hui",
      "Ofir Press",
      "Ludwig Schmidt",
      "Diyi Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.21798",
    "githubLink": "https://github.com/princeton-nlp/SWE-bench",
    "itemCount": "300 instances",
    "specs": "9 Programming Languages, 42 Repositories",
    "description": "An official extension of SWE-bench covering multiple programming languages to evaluate cross-lingual software engineering capabilities."
  },
  {
    "id": "imported-1769500766497-36-6r65x",
    "title": "Multi-SWE-bench",
    "source": "arXiv",
    "authors": [
      "Daoguang Zan",
      "Zhirong Huang",
      "Wei Liu",
      "Hanwu Chen",
      "Linhao Zhang",
      "Shulin Xin",
      "Lu Chen",
      "Qi Liu",
      "Xiaojian Zhong",
      "Aoyan Li",
      "Siyao Liu",
      "Yongsheng Xiao",
      "Liangqiang Chen",
      "Yuyu Zhang",
      "Jing Su",
      "Tianyu Liu",
      "Rui Long",
      "Kai Shen",
      "Liang Xiang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.02605",
    "githubLink": "https://github.com/multi-swe-bench/multi-swe-bench",
    "itemCount": "1,632 instances",
    "specs": "8 Languages (Python, Java, TS, JS, Go, Rust, C, C++), Text + Code",
    "description": "A large-scale multilingual benchmark for issue resolving, covering 8 major programming languages including Java, TypeScript, Go, and C++."
  },
  {
    "id": "imported-1769500766497-37-jfw7p",
    "title": "MathArena AIME 2025",
    "source": "Hugging Face",
    "authors": [
      "Mislav Balunović",
      "Jasper Dekoninck",
      "Ivo Petrov",
      "Nikola Jovanović",
      "Martin Vechev"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2601.01234",
    "githubLink": "https://github.com/eth-sri/matharena",
    "itemCount": "30 problems",
    "specs": "Text modality; LaTeX format; Includes problem type and ground-truth answers",
    "description": "A dataset containing questions from the AIME 2025 competition, created as part of the MathArena framework to evaluate LLMs on uncontaminated, newly released mathematical problems."
  },
  {
    "id": "imported-1769500766497-38-jw0lv",
    "title": "OlymMATH",
    "source": "arXiv",
    "authors": [
      "Haoxiang Sun",
      "Yingqian Min",
      "Zhipeng Chen",
      "Wayne Xin Zhao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.21380",
    "githubLink": "https://github.com/Slow_Thinking_with_LLMs",
    "itemCount": "200 problems",
    "specs": "Text modality; Bilingual (English/Chinese); Verifiable numerical solutions",
    "description": "An Olympiad-level benchmark designed to challenge the reasoning boundaries of LLMs. It features 200 meticulously curated problems (including AIME-level difficulty) with parallel English and Chinese versions to ensure rigorous, uncontaminated evaluation."
  },
  {
    "id": "imported-1769500766497-39-qa5md",
    "title": "MathArena",
    "source": "arXiv",
    "authors": [
      "Mislav Balunovic",
      "Jasper Dekoninck",
      "Ivo Petrov",
      "Nikola Jovanovic",
      "Martin Vechev"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.23281",
    "githubLink": "https://github.com/eth-sri/matharena",
    "itemCount": "162 problems (initial 2025 release across 7 competitions)",
    "specs": "Text (LaTeX math problems), visual elements (diagrams in some subsets like Kangaroo), proof-based and short-answer modalities.",
    "description": "MathArena is a dynamic benchmark designed to evaluate Large Language Models (LLMs) on newly released mathematical competitions to prevent data contamination. It aggregates problems from various high-level competitions (e.g., AIME, IMO, CMIMC) immediately after they are held. The benchmark uniquely includes both final-answer format problems and proof-based problems (such as those from the IMO), requiring models to generate full reasoning chains. It aims to provide a rigorous, forward-looking assessment of mathematical reasoning and proof-writing capabilities."
  },
  {
    "id": "imported-1769500766497-40-6mkxc",
    "title": "Tau2-bench",
    "source": "arXiv",
    "authors": [
      "Victor Barres",
      "Honghua Dong",
      "Soham Ray",
      "Xujie Si",
      "Karthik Narasimhan"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.07982",
    "githubLink": "https://github.com/sierra-research/tau2-bench",
    "itemCount": "~280 tasks (115 Retail, 50 Airline, 114 Telecom)",
    "specs": "Dual-control simulation (Dec-POMDP); Domains: Retail, Airline, Telecom; Modalities: Text, API tools",
    "description": "An enhanced version of Tau-bench that introduces a dual-control environment where both the agent and the user can interact with tools. It adds a 'Telecom' domain for technical troubleshooting and includes code fixes for previous domains."
  },
  {
    "id": "imported-1769500766497-41-ofger",
    "title": "OSWorld-Human",
    "source": "arXiv",
    "authors": [
      "Reyna Abhyankar",
      "Qi Qi",
      "Yiying Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.16042",
    "githubLink": "https://github.com/xlang-ai/OSWorld",
    "itemCount": "369 tasks",
    "specs": "Human reference trajectories, efficiency metrics",
    "description": "A manually annotated version of the OSWorld benchmark containing human-determined trajectories for each task. It is designed to evaluate the efficiency (latency and step count) of computer-use agents compared to human performance."
  },
  {
    "id": "imported-1769500766497-42-k8t64",
    "title": "OSWorld-G",
    "source": "arXiv",
    "authors": [
      "Tianbao Xie",
      "Jiaqi Deng",
      "Xiaochuan Li",
      "Junlin Yang",
      "Haoyuan Wu",
      "Jixuan Chen",
      "Wenjing Hu",
      "Xinyuan Wang",
      "Yuhui Xu",
      "Zekun Wang",
      "Yiheng Xu",
      "Junli Wang",
      "Doyen Sahoo",
      "Tao Yu",
      "Caiming Xiong"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.13227",
    "githubLink": "https://github.com/xlang-ai/OSWorld-G",
    "itemCount": "Unknown (derived from OSWorld tasks)",
    "specs": "Grounding tasks, UI decomposition, refined instructions",
    "description": "A benchmark for computer-use grounding tasks, introduced in the paper 'Scaling Computer-Use Grounding via UI Decomposition and Synthesis'. It focuses on evaluating an agent's ability to ground instructions to specific UI elements."
  },
  {
    "id": "imported-1769500766497-43-qg84t",
    "title": "OSWorld-Verified",
    "source": "Scholar",
    "authors": [
      "Tianbao Xie",
      "Mengqi Yuan",
      "Danyang Zhang",
      "Xinzhuang Xiong",
      "Zhennan Shen",
      "Zilong Zhou",
      "Xinyuan Wang",
      "Yanxu Chen",
      "Jiaqi Deng",
      "Junda Chen",
      "Bowen Wang",
      "Haoyuan Wu",
      "Jixuan Chen",
      "Junli Wang",
      "Dunjie Lu",
      "Hao Hu",
      "Tao Yu"
    ],
    "year": "2025",
    "paperLink": "https://xlang.ai/blog/osworld-verified",
    "githubLink": "https://github.com/xlang-ai/OSWorld",
    "itemCount": "369 tasks",
    "specs": "Verified tasks, AWS integration, improved robustness",
    "description": "An enhanced and verified version of the original OSWorld benchmark, featuring comprehensive bug fixes, improved infrastructure support (AWS), and refined task definitions to ensure reliable evaluation signals."
  },
  {
    "id": "imported-1769500766497-44-4mhnk",
    "title": "MCP-Atlas",
    "source": "Other",
    "authors": [
      "The Scale Research Team",
      "Chaithanya Bandi",
      "Ben Hertzberg",
      "Geobio Boo",
      "Tejas Polakam",
      "Jeff Da",
      "Sami Hassaan",
      "Manasi Sharma",
      "Andrew Park",
      "Ernesto Hernandez",
      "Dan Rambado",
      "Ivan Salazar",
      "Rafael Cruz",
      "Chetan Rane",
      "Ben Levin",
      "Brad Kenstler",
      "Bing Liu"
    ],
    "year": "2025",
    "paperLink": "https://scale.com/blog/mcp-atlas",
    "githubLink": "https://github.com/scaleapi/mcp-atlas",
    "itemCount": "1,000 tasks",
    "specs": "36 MCP servers, 220 tools, Parquet/CSV format, Text modality",
    "description": "A large-scale benchmark for evaluating tool-use competency with real MCP servers. It tests agents on realistic, multi-step workflows involving tool discovery, parameterization, and error recovery, comprising 1,000 human-authored tasks (500 public) across 36 servers."
  },
  {
    "id": "imported-1769500766497-45-nrtk0",
    "title": "TOUCAN",
    "source": "arXiv",
    "authors": [
      "Zhangchen Xu",
      "Adriana Meza Soria",
      "Shawn Tan",
      "Anurag Roy",
      "Ashish Sunil Agrawal",
      "Radha Poovendran",
      "Rameswar Panda"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.01179",
    "githubLink": "https://github.com/Agent-Ark/Toucan",
    "itemCount": "1.5 million trajectories",
    "specs": "Parquet format, Text modality, Synthetic data",
    "description": "The largest publicly available tool-agentic dataset containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). It focuses on diversity, realism, and complexity in multi-tool and multi-turn interactions."
  },
  {
    "id": "imported-1769500766497-46-3wffk",
    "title": "MCP-Bench",
    "source": "arXiv",
    "authors": [
      "Zhenting Wang",
      "Qi Chang",
      "Hemani Patel",
      "Shashank Biju",
      "Cheng-En Wu",
      "Quan Liu",
      "Aolin Ding",
      "Alireza Rezazadeh",
      "Ankit Shah",
      "Yujia Bao",
      "Eugene Siow"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.20453",
    "githubLink": "https://github.com/Accenture/mcp-bench",
    "itemCount": "Unknown task count (28 servers, 250 tools)",
    "specs": "Multi-step tasks, Real-world MCP servers",
    "description": "A benchmark for evaluating LLMs on realistic, multi-step tasks that demand tool use, cross-tool coordination, and planning via MCP servers. It connects LLMs to 28 representative live MCP servers spanning 250 tools across domains like finance and science."
  },
  {
    "id": "imported-1769500766497-47-4vkor",
    "title": "MCP-Universe",
    "source": "arXiv",
    "authors": [
      "Ziyang Luo",
      "Zhiqi Shen",
      "Wenzhuo Yang",
      "Zirui Zhao",
      "Prathyusha Jwalapuram",
      "Amrita Saha",
      "Doyen Sahoo",
      "Silvio Savarese",
      "Caiming Xiong",
      "Junnan Li"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.14704",
    "githubLink": "https://github.com/SalesforceAIResearch/mcp-universe",
    "itemCount": "Unknown task count (6 domains, 11 servers)",
    "specs": "Execution-based evaluation, Real-world MCP servers",
    "description": "A comprehensive benchmark designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. It covers 6 core domains including Location Navigation, Repository Management, Financial Analysis, and 3D Design."
  },
  {
    "id": "imported-1769500766497-48-liwrf",
    "title": "MCP-Radar",
    "source": "arXiv",
    "authors": [
      "Xuanqi Gao",
      "Siyi Xie",
      "Juan Zhai",
      "Shiqing Ma",
      "Chao Shen"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16700",
    "githubLink": "Not available",
    "itemCount": "507 tasks",
    "specs": "6 domains (Math, Web, Email, Calendar, File, Terminal)",
    "description": "A multi-dimensional benchmark for evaluating tool use capabilities in LLMs. It assesses performance across five dimensions: answer accuracy, tool selection efficiency, computational resource efficiency, parameter construction accuracy, and execution speed."
  },
  {
    "id": "imported-1769500766497-49-1uvgw",
    "title": "MCPMark",
    "source": "Other",
    "authors": [
      "The MCPMark Team"
    ],
    "year": "2025",
    "paperLink": "https://github.com/eval-sys/mcpmark",
    "githubLink": "https://github.com/eval-sys/mcpmark",
    "itemCount": "127 tasks",
    "specs": "5 environments, diverse CRUD operations",
    "description": "A benchmark designed to stress-test comprehensive MCP use in realistic scenarios. It includes tasks across Notion, GitHub, Filesystem, PostgreSQL, and Playwright environments, featuring high-quality tasks collaboratively created by human experts and AI agents."
  },
  {
    "id": "imported-1769500766497-50-epuvw",
    "title": "Video-MMMU",
    "source": "Hugging Face",
    "authors": [
      "Kairui Hu",
      "Penghao Wu",
      "Fanyi Pu",
      "Wang Xiao",
      "Xiang Yue",
      "Yuanhan Zhang",
      "Bo Li",
      "Ziwei Liu"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/papers/2410.13769",
    "githubLink": "https://videommmu.github.io/",
    "itemCount": "300 videos, 900 QA pairs",
    "specs": "Video + Text, 6 professional disciplines, 30 subjects, lecture-style videos",
    "description": "A multi-modal, multi-disciplinary benchmark evaluating Large Multimodal Models' (LMMs) ability to acquire and utilize knowledge from educational videos. It assesses performance across three cognitive stages: Perception, Comprehension, and Adaptation."
  },
  {
    "id": "imported-1769500766497-51-0gqor",
    "title": "SimpleQA Verified",
    "source": "arXiv",
    "authors": [
      "Lukas Haas",
      "Gal Yona",
      "Giovanni D'Antonio",
      "Sasha Goldshtein",
      "Dipanjan Das"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.07968",
    "githubLink": "https://huggingface.co/datasets/google/simpleqa-verified",
    "itemCount": "1,000 prompts",
    "specs": "Text modality; Verified short-form factuality questions",
    "description": "A refined subset of OpenAI's SimpleQA designed to address limitations such as noisy labels, topical biases, and question redundancy. It employs a rigorous filtering process (deduplication, source reconciliation) to provide a more reliable measure of parametric knowledge."
  },
  {
    "id": "imported-1769500766497-52-hefsh",
    "title": "SimpleVQA",
    "source": "arXiv",
    "authors": [
      "Xianfu Cheng",
      "Wei Zhang",
      "Shiwei Zhang",
      "Jian Yang",
      "Xiangyuan Guan"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.12870",
    "githubLink": "https://github.com/SimpleVQA/SimpleVQA",
    "itemCount": "2,025 samples",
    "specs": "Multimodal (Image + Text); 9 tasks; 9 domains; 244 image types",
    "description": "A multimodal factuality evaluation benchmark for Multimodal Large Language Models (MLLMs). It addresses the lack of visual modality in previous factuality benchmarks by providing challenging image-based questions to measure knowledge boundaries and hallucinations."
  },
  {
    "id": "imported-1769500766497-53-9ng7b",
    "title": "JBBQ: Japanese Bias Benchmark for Question Answering",
    "source": "arXiv",
    "authors": [
      "Hitomi Yanaka",
      "Namgi Han",
      "Ryoma Kumon",
      "Jie Lu",
      "Masashi Takeshita",
      "Ryo Sekizawa",
      "Taisei Katô",
      "Hiromi Arai"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2406.02050",
    "githubLink": "https://github.com/ynklab/JBBQ_data",
    "itemCount": "50,856 question pairs",
    "specs": "Japanese text, Multiple Choice QA, 245 templates",
    "description": "A Japanese social bias benchmark dataset based on BBQ, modified to evaluate Japanese LLMs with consideration for Japanese-specific terminology and cultural background. It covers five social categories."
  },
  {
    "id": "imported-1769500766498-54-2enp3",
    "title": "EsBBQ and CaBBQ: Spanish and Catalan Bias Benchmarks",
    "source": "arXiv",
    "authors": [
      "Valle Ruiz-Fernández",
      "Mario Mina",
      "Olatz Perez-de-Viñaspre"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.11216",
    "githubLink": "https://github.com/langtech-bsc/EsBBQ-CaBBQ",
    "itemCount": "27,320 instances",
    "specs": "Spanish and Catalan text, Multiple Choice QA",
    "description": "Parallel datasets adapted from BBQ to assess social bias in Spanish and Catalan languages and social contexts. They evaluate bias across 10 social categories in a multiple-choice setting."
  },
  {
    "id": "imported-1769500766498-55-h74e0",
    "title": "BBG: Bias Benchmark for Generation",
    "source": "Scholar",
    "authors": [
      "Jiho Jin",
      "Woosung Kang",
      "Junho Myung",
      "Alice Oh"
    ],
    "year": "2025",
    "paperLink": "https://aclanthology.org/2025.findings-acl.659/",
    "githubLink": "https://jinjh0123.github.io/BBG",
    "itemCount": "Based on BBQ/KoBBQ",
    "specs": "English and Korean text, Long-form text generation",
    "description": "An adaptation of the BBQ and KoBBQ datasets designed to evaluate social bias in long-form generation. It prompts LLMs to generate continuations of stories and measures neutral vs. biased outcomes."
  },
  {
    "id": "imported-1769500766498-56-zlkot",
    "title": "Virology Capabilities Test (VCT)",
    "source": "arXiv",
    "authors": [
      "Jasper Götting",
      "Pedro Medeiros",
      "Jon G. Sanders",
      "Nathaniel Li",
      "Long Phan",
      "Karam Elabd",
      "Lennart Justen",
      "Dan Hendrycks",
      "Seth Donoughe"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.16137",
    "githubLink": "Not publicly available (Controlled Access)",
    "itemCount": "322 questions",
    "specs": "Multimodal (Text + Images), Multiple-Choice (Single and Multiple Select)",
    "description": "A multimodal benchmark measuring the capability of Large Language Models (LLMs) to troubleshoot complex virology laboratory protocols. It covers fundamental, tacit, and visual knowledge essential for practical work in virology."
  },
  {
    "id": "imported-1769500766498-57-1fwnd",
    "title": "MedHELM",
    "source": "arXiv",
    "authors": [
      "Suhana Bedi",
      "Hejie Cui",
      "Miguel Fuentes",
      "Alyssa Unell",
      "Michael Wornow",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.23802",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "35 benchmarks, 121 clinical tasks",
    "specs": "Text (Medical domain); 5 categories, 22 subcategories",
    "description": "A holistic evaluation framework for assessing Large Language Models (LLMs) on medical tasks. It utilizes a clinician-validated taxonomy spanning various categories like clinical note generation and decision support."
  },
  {
    "id": "imported-1769500766498-58-de15x",
    "title": "LLMail-Inject",
    "source": "Hugging Face",
    "authors": [
      "Sahar Abdelnabi",
      "Aideen Fay",
      "Ahmed Salem",
      "Egor Zverev",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.01234",
    "githubLink": "https://huggingface.co/datasets/microsoft/llmail-inject-challenge",
    "itemCount": "208,095 unique submissions",
    "specs": "Text (Emails, Prompts), Parquet",
    "description": "A dataset resulting from a realistic adaptive prompt injection challenge simulating attacks on an LLM-based email assistant, including diverse attack strategies like obfuscation and social engineering."
  },
  {
    "id": "imported-1769500766498-59-pcu83",
    "title": "DataSciBench",
    "source": "arXiv",
    "authors": [
      "Dan Zhang",
      "Sining Zhoubian",
      "Min Cai",
      "Fengzu Li",
      "Lekang Yang",
      "Wei Wang",
      "Tianjiao Dong",
      "Ziniu Hu",
      "Jie Tang",
      "Yisong Yue"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.13897",
    "githubLink": "https://github.com/THUDM/DataSciBench",
    "itemCount": "Comprehensive collection (specific count not detailed in snippets)",
    "specs": "Text prompts, Python code execution, 6 task types",
    "description": "An LLM agent benchmark for data science that evaluates models across six defined tasks (e.g., preprocessing, statistics, visualization). It uses a semi-automated pipeline for ground truth generation and a Task-Function-Code (TFC) evaluation framework."
  },
  {
    "id": "imported-1769500766498-60-0goo6",
    "title": "DSCodeBench",
    "source": "arXiv",
    "authors": [
      "Mingyu Jin",
      "Zhenting Wang",
      "Qinkai Yu",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.00000",
    "githubLink": "https://github.com/DSCodeBench/DSCodeBench",
    "itemCount": "1000 problems",
    "specs": "Python code, 10 libraries (adding Seaborn, Keras, LightGBM)",
    "description": "A realistic benchmark for data science code generation, designed to be more challenging than DS-1000. It includes problems from ten data science libraries and offers stronger test suites and better-structured problem descriptions."
  },
  {
    "id": "imported-1769500766498-61-q199t",
    "title": "Redbench: A Benchmark Reflecting Real Workloads",
    "source": "arXiv",
    "authors": [
      "Skander Krid",
      "Mihail Stoian",
      "Andreas Kipf",
      "Johannes Wehrstein",
      "Roman Heinrich",
      "Martin Stemmer",
      "Carsten Binnig",
      "Muhammad El-Hindi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.12488",
    "githubLink": "https://github.com/utndatasystems/redbench",
    "itemCount": "30 workloads",
    "specs": "SQL workloads, Analytical queries, Integration with IMDb/TPC-DS schemas",
    "description": "A database benchmark comprising a collection of 30 workloads that reflect query patterns observed in real-world production systems (specifically derived from the Redset dataset). It is designed to bridge the gap between synthetic benchmarks (like TPC-H/DS) and real user workloads by preserving intrinsic characteristics such as query repetition and distribution shifts."
  },
  {
    "id": "imported-1769500766498-62-vf9xw",
    "title": "MMLU-Reason",
    "source": "arXiv",
    "authors": [
      "Guiyao Tie",
      "Xueyang Zhou",
      "Tianhe Gu",
      "Ruihang Zhang",
      "Chaoran Hu",
      "Sizhe Zhang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16459",
    "githubLink": "https://github.com/MMLU-Reason/MMLU-Reason",
    "itemCount": "1,083 questions",
    "specs": "Multimodal, Reasoning Traces",
    "description": "A benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking traces. It includes a high-difficulty dataset spanning diverse reasoning types with symbolic depth and multi-hop demands."
  },
  {
    "id": "imported-1769500766498-63-124rt",
    "title": "Global PIQA",
    "source": "arXiv",
    "authors": [
      "Tyler A. Chang",
      "Catherine Arnett",
      "Abdelrahman Eldesokey",
      "Abdelrahman Sadallah",
      "Abeer Kashar",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.24081",
    "githubLink": "https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel",
    "itemCount": "11,600 examples (100 examples per language for 116 languages)",
    "specs": "Multilingual text (116 languages); Multiple choice; Culturally specific physical reasoning",
    "description": "A large-scale multilingual benchmark for physical commonsense reasoning across over 100 languages and cultures. Unlike translation-based benchmarks, this dataset features culturally specific examples constructed by native speakers to evaluate how well LLMs capture global physical knowledge."
  },
  {
    "id": "imported-1769500766498-64-28q5d",
    "title": "AIRTBench",
    "source": "arXiv",
    "authors": [
      "Ads Dawson",
      "Rob Mulla",
      "Nick Landers",
      "Shane Caldwell"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.19855",
    "githubLink": "https://github.com/dreadnode/AIRTBench-Code",
    "itemCount": "70 challenges; 8,066 experimental run traces",
    "specs": "Text/Code (Python); Black-box CTF challenges; Interaction traces",
    "description": "A benchmark measuring autonomous AI red teaming capabilities in language models. It consists of 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible environment, requiring models to write Python code to autonomously discover and exploit AI/ML security vulnerabilities. The release includes both the challenge framework and a dataset of over 8,000 experimental run traces."
  },
  {
    "id": "imported-1769500766498-65-8ro12",
    "title": "RICoTA",
    "source": "arXiv",
    "authors": [
      "Eujeong Choi",
      "Younghun Jeong",
      "Soomin Kim",
      "Won Ik Cho"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.17715",
    "githubLink": "https://github.com/boychaboy/RICoTA",
    "itemCount": "609 prompts",
    "specs": "Text (Korean); Dialogue/Conversation",
    "description": "RICoTA (Red-teaming of In-the-wild Conversation with Test Attempts) is a Korean red teaming dataset designed to evaluate the safety of social chatbots. It consists of real-world user dialogues that contain jailbreaking attempts, such as 'taming' the AI, dating simulations, or technical tests, collected from a Korean online community. The benchmark focuses on identifying conversation types and user testing purposes to improve chatbot resilience against adversarial attacks."
  },
  {
    "id": "imported-1769500766498-66-pwmfc",
    "title": "MedXpertQA",
    "source": "arXiv",
    "authors": [
      "Yuxin Zuo",
      "Shang Qu",
      "Yifei Li",
      "Zhangren Chen",
      "Xuekai Zhu",
      "Ermo Hua",
      "Kaiyan Zhang",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.18362",
    "githubLink": "https://github.com/TsinghuaC3I/MedXpertQA",
    "itemCount": "4,460 questions (Text subset: 2,455; Multimodal subset: 2,005 questions with 2,839 images)",
    "specs": "Text and Multimodal (Images + Structured Tables). Covers 17 medical specialties and 11 body systems. Format: Multiple-choice questions (10 options for Text, 5 for MM).",
    "description": "A comprehensive benchmark designed to evaluate expert-level medical knowledge and advanced reasoning capabilities in AI models. It encompasses diverse real-world diagnostic scenarios, including highly specialized fields, and features both text-only and multimodal subsets. The multimodal subset integrates structured clinical data (e.g., patient records, tables) with medical images to simulate realistic clinical decision-making."
  },
  {
    "id": "imported-1769500766498-67-p4lei",
    "title": "HealthBench",
    "source": "arXiv",
    "authors": [
      "Rahul K. Arora",
      "Karan Singhal",
      "Jason Wei",
      "R. S. Hicks",
      "P. Bowman",
      "J. Quiñonero-Candela",
      "F. Tsimourlas",
      "OpenAI Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.08775",
    "githubLink": "https://github.com/openai/simple-evals",
    "itemCount": "5,000 conversations",
    "specs": "Text (Multi-turn conversations, rubric-based evaluation)",
    "description": "An open-source benchmark designed to measure the performance and safety of large language models in healthcare. It consists of realistic multi-turn conversations between a model and an individual user or healthcare professional, evaluating responses against conversation-specific rubrics created by physicians. The benchmark covers diverse themes such as emergency referrals, context-seeking, and global health."
  },
  {
    "id": "imported-1769500766498-68-lqhfo",
    "title": "MedRepBench",
    "source": "arXiv",
    "authors": [
      "Fangxin Shang",
      "Yuan Xia",
      "Dalu Yang",
      "Yahui Wang",
      "Binglin Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.16674",
    "githubLink": "https://github.com/MedRepBench/MedRepBench",
    "itemCount": "1,900 reports",
    "specs": "Image (photos/screenshots) and Text (OCR-based)",
    "description": "A comprehensive benchmark designed to evaluate Vision-Language Models (VLMs) and Large Language Models (LLMs) on structured medical report interpretation. It consists of de-identified real-world medical reports, including examination and laboratory reports, spanning diverse departments and acquisition formats (photos, screenshots, electronic documents)."
  },
  {
    "id": "imported-1769500766498-69-dof81",
    "title": "EduBench",
    "source": "Hugging Face",
    "authors": [
      "Bin Xu",
      "Yu Bai",
      "Huashan Sun",
      "Yiguan Lin",
      "Siming Liu",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16160",
    "githubLink": "https://github.com/ybai-nlp/EduBench",
    "itemCount": "18,821 data points (4,000+ contexts)",
    "specs": "Text (Prompts/Responses)",
    "description": "A benchmark for evaluating Large Language Models in educational scenarios. It includes tasks for subjective grading of student work, specifically including 'large assignments and lab reports', assessing capabilities in workload estimation, completeness, and knowledge application."
  },
  {
    "id": "imported-1769500766498-70-1h4jn",
    "title": "PathMCQA",
    "source": "arXiv",
    "authors": [
      "Yang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.XXXXX",
    "githubLink": "https://huggingface.co/datasets",
    "itemCount": "450 patches (354 WSIs)",
    "specs": "Text (MCQ) + Image",
    "description": "A multiple-choice question answering dataset for pathology, derived from multiple sources. It presents questions related to identification, grading, and subtyping of cancers (breast, cervical, prostate) based on pathology visual data."
  },
  {
    "id": "imported-1769500766498-71-av0d6",
    "title": "UKBOB",
    "source": "arXiv",
    "authors": [
      "Emmanuelle Bourigault",
      "Amir Jamaludin",
      "Abdullah Hamdi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.06908",
    "githubLink": "https://emmanuelleb985.github.io/ukbob",
    "itemCount": "51,761 3D samples (>1.37 billion 2D masks)",
    "specs": "3D MRI (Neck-to-Knee), 72 organ segmentation classes",
    "description": "The largest labeled dataset of body organs, utilizing UK Biobank MRI data with automated labeling and manual validation for generalizable 3D medical image segmentation."
  },
  {
    "id": "imported-1769500766498-72-o51wl",
    "title": "FOMO-60K / FOMO-300K",
    "source": "Hugging Face",
    "authors": [
      "FOMO Challenge Organizers"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.14432",
    "githubLink": "https://huggingface.co/datasets/FOMO-MRI/FOMO60K",
    "itemCount": "60,000+ (FOMO-60K) to 300,000+ (FOMO-300K) scans",
    "specs": "3D Brain MRI (T1, T2, FLAIR, etc.), NIfTI",
    "description": "A large-scale heterogeneous 3D magnetic resonance brain imaging dataset designed for self-supervised learning and foundation model training."
  },
  {
    "id": "imported-1769500766498-73-4850g",
    "title": "HISTAI",
    "source": "arXiv",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.12120",
    "githubLink": "https://github.com/HistAI/HISTAI",
    "itemCount": "60,000+ WSIs (Metadata indicates >100,000 total slides)",
    "specs": "Whole Slide Images (WSI), Clinical Metadata, Multi-organ",
    "description": "An open-source, large-scale whole slide image dataset designed to address the lack of scale and diversity in public pathology resources. It includes extensive clinical metadata, detailed pathological annotations, and standardized diagnostic coding across various tissue types. The dataset is intended for pre-training foundation models and benchmarking computational pathology tasks."
  },
  {
    "id": "imported-1769500766498-74-637vr",
    "title": "BEETLE",
    "source": "arXiv",
    "authors": [
      "Carlijn Lems",
      "Lucas Tessier",
      "John-Melle Bokhorst",
      "Geert Litjens",
      "Francesco Ciompi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.02037",
    "githubLink": "https://github.com/DIAGNijmegen/beetle",
    "itemCount": "641 WSIs (587 Development, 54 Evaluation)",
    "specs": "Whole Slide Images (WSI), Pixel-level Semantic Segmentation Masks",
    "description": "A multicentric dataset for training and benchmarking breast cancer semantic segmentation in H&E slides. It includes biopsies and resections from multiple clinical centers and scanners, covering all molecular subtypes and histological grades. Annotations include invasive epithelium, non-invasive epithelium, necrosis, and other tissues."
  },
  {
    "id": "imported-1769500766498-75-8yo89",
    "title": "SPIDER",
    "source": "arXiv",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.02876",
    "githubLink": "https://github.com/HistAI/SPIDER",
    "itemCount": "Large-scale patch collection (exact count varies by subset)",
    "specs": "Pathology Patches, Expert Annotations, Multi-organ",
    "description": "A comprehensive multi-organ supervised pathology dataset designed to provide expert annotations and strong baseline models for patch-level analysis. It serves as a large-scale resource for training and validating supervised learning models across different tissue types."
  },
  {
    "id": "imported-1769500766498-76-dtj4q",
    "title": "DermaVQA-DAS",
    "source": "arXiv",
    "authors": [
      "Wen-wai Yim",
      "Yujuan Fu",
      "Asma Ben Abacha",
      "Meliha Yetisgen",
      "Noel Codella",
      "Roberto Andres Novoa",
      "Josep Malvehy"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.24340",
    "githubLink": "https://github.com/velvinnn/DermaVQA",
    "itemCount": "Extends DermaVQA",
    "specs": "Image + Text (Closed-ended QA, Segmentation)",
    "description": "An extension of the DermaVQA dataset that supports closed-ended question answering and dermatological lesion segmentation. It introduces the Dermatology Assessment Schema (DAS) for structured assessment."
  },
  {
    "id": "imported-1769500766498-77-aljkx",
    "title": "BioProBench",
    "source": "arXiv",
    "authors": [
      "Yuyang Liu",
      "Liuzhenghao Lv",
      "Xiancheng Zhang",
      "Li Yuan",
      "Yonghong Tian"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.07889",
    "githubLink": "https://github.com/YuyangSunshine/bioprotocolbench",
    "itemCount": "27,000 protocols; ~556,000 task instances",
    "specs": "Text-based; Tasks include QA, Ordering, Error Correction, Generation",
    "description": "A large-scale, integrated multi-task benchmark specifically designed for biological protocol understanding and reasoning. It covers five core tasks: Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on procedural biological texts."
  },
  {
    "id": "imported-1769500766498-78-7gic3",
    "title": "ArcticEcho",
    "source": "Other",
    "authors": [
      "Gangopadhyay",
      "S.",
      "Singh",
      "I.",
      "Pandya",
      "P.",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://ieeexplore.ieee.org/document/10362879",
    "githubLink": "https://github.com/arctic-echo/ArcticEcho",
    "itemCount": "24,752 audio samples, 18 speakers",
    "specs": "Audio, High-quality synthetic speech",
    "description": "A speaker-controlled voice cloning dataset designed to eliminate confounding variables in deepfake detection. It forces models to learn genuine cloning signatures by maintaining strict correspondence between real and synthetic content."
  },
  {
    "id": "imported-1769500766498-79-s9ilv",
    "title": "TroubleshootingBench",
    "source": "arXiv",
    "authors": [
      "Eric Wallace",
      "Olivia Watkins",
      "Miles Wang",
      "Kai Chen",
      "Chris Koch",
      "OpenAI"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.03153",
    "githubLink": "N/A",
    "itemCount": "52 protocols",
    "specs": "Text-based; Short-answer troubleshooting questions based on expert-written biological protocols",
    "description": "A benchmark designed to evaluate large language models' ability to identify and correct real-world experimental errors in biological protocols (wet lab procedures). It focuses on tacit, hands-on knowledge and uncontaminated procedures that are not available online, requiring models to troubleshoot complex, domain-specific scenarios."
  },
  {
    "id": "imported-1769500766498-80-kopkg",
    "title": "BixBench",
    "source": "Other",
    "authors": [
      "Ludovico Mitchener",
      "Jon Laurent",
      "Geemi Wellawatte",
      "FutureHouse Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.00096",
    "githubLink": "https://github.com/Future-House/BixBench",
    "itemCount": "53 analytical scenarios, 296 open-answer questions",
    "specs": "Text, Python notebooks, biological datasets (genomics, transcriptomics), agentic workflows",
    "description": "A comprehensive benchmark for evaluating LLM-based agents on real-world bioinformatics tasks. It includes open-ended analytical scenarios requiring data exploration, multi-step analysis, and result interpretation."
  },
  {
    "id": "imported-1769500766498-81-zg6ry",
    "title": "MinorBench",
    "source": "Hugging Face",
    "authors": [
      "Shaun Khoo",
      "Gabriel Chua",
      "Rachel Shong"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.10242",
    "githubLink": "https://huggingface.co/datasets/govtech/MinorBench",
    "itemCount": "299 prompts",
    "specs": "Text (Prompts with 6 risk categories)",
    "description": "A benchmark designed to evaluate whether Large Language Models (LLMs) respond to questions that may be inappropriate for children. It consists of prompts spanning various sensitive topics like sexual content, profanities, hate speech, danger, self-harm, and substance use, paired with system prompts simulating child-friendly AI roles."
  },
  {
    "id": "imported-1769500766498-82-q76vn",
    "title": "Safe-Child-LLM",
    "source": "arXiv",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Durandhar"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.13510",
    "githubLink": "https://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark",
    "itemCount": "200 adversarial prompts",
    "specs": "Text (Adversarial prompts split by age group)",
    "description": "A comprehensive benchmark and dataset for systematically assessing LLM safety across two developmental stages: children (7-12) and adolescents (13-17). It includes adversarial prompts curated from red-teaming corpora and human-annotated labels for jailbreak success and ethical refusal."
  },
  {
    "id": "imported-1769500766498-83-37gi8",
    "title": "SproutBench",
    "source": "arXiv",
    "authors": [
      "Wenpeng Xing",
      "Lanyi Wei",
      "Haixiao Hu",
      "Rongchang Li",
      "Mohan Li",
      "Changting Lin",
      "Meng Han"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.11009",
    "githubLink": "https://github.com/huggingface/yourbench",
    "itemCount": "1,283 adversarial prompts",
    "specs": "Text (Developmentally grounded prompts)",
    "description": "An evaluation suite comprising developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors in LLMs. It covers three age groups: early childhood (0-6), middle childhood (7-12), and adolescence (13-18)."
  },
  {
    "id": "imported-1769500766498-84-bdeem",
    "title": "KuaiMod",
    "source": "arXiv",
    "authors": [
      "Xingyu Lu",
      "Tianke Zhang",
      "Chang Meng",
      "Xiaobei Wang",
      "Jinpeng Wang",
      "Yi-Fan Zhang",
      "Shisong Tang",
      "Changyi Liu",
      "Haojie Ding",
      "Kaiyu Jiang",
      "Kaiyu Tang",
      "Bin Wen",
      "Hai-Tao Zheng",
      "Fan Yang",
      "Tingting Gao",
      "Di Zhang",
      "Kun Gai"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.14904",
    "githubLink": "https://themoonlight.io/KuaiMod",
    "itemCount": "24,562 samples",
    "specs": "Video and Text, 15 violative categories",
    "description": "A content moderation benchmark for Short Video Platforms (SVPs) incorporating authentic user/reviewer feedback to model video toxicity and violative content."
  },
  {
    "id": "imported-1769500766498-85-876kd",
    "title": "Video-SafetyBench",
    "source": "Other",
    "authors": [
      "Xuannan Liu",
      "Zekun Li",
      "Zheqi He",
      "Peipei Li",
      "Shuhan Xia",
      "Xing Cui",
      "Huaibo Huang",
      "Xi Yang",
      "Ran He"
    ],
    "year": "2025",
    "paperLink": "https://openreview.net/forum?id=z11zJq05w8",
    "githubLink": "https://github.com/flageval-baai/Video-SafetyBench",
    "itemCount": "2,264 video-text pairs",
    "specs": "Multimodal (Video + Text), 48 fine-grained unsafe categories",
    "description": "A benchmark designed to evaluate the safety of Large Vision-Language Models (LVLMs) under video-text attacks, considering the temporal dynamics of video."
  },
  {
    "id": "imported-1769500766498-86-l6xg6",
    "title": "D-REX",
    "source": "arXiv",
    "authors": [
      "Satyapriya Krishna",
      "Andy Zou",
      "Rahul Gupta",
      "Eliot Krzysztof Jones",
      "Nick Winter",
      "Dan Hendrycks",
      "J. Zico Kolter",
      "Matt Fredrikson",
      "Spyros Matsoukas"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.17938",
    "githubLink": "https://github.com/drex-benchmark/drex",
    "itemCount": "Not specified (curated red-teaming examples)",
    "specs": "Text (Prompts, Model Outputs, Internal Chain-of-Thought)",
    "description": "A benchmark for detecting deceptive reasoning in LLMs, specifically where models produce benign outputs while operating on malicious internal reasoning chains."
  },
  {
    "id": "imported-1769500766498-87-eiw5v",
    "title": "Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection",
    "source": "arXiv",
    "authors": [
      "Nursulu Sagimbayeva",
      "Ruveyda Betül Bahçeci",
      "Ingmar Weber"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.19191",
    "githubLink": "https://github.com/Nursulu/InconsistencyDetection",
    "itemCount": "698 statement pairs",
    "specs": "Text pairs (English/German source), Consistency Labels",
    "description": "A benchmark dataset designed to detect inconsistencies in political statements, which can serve as a form of misinformation. It contains pairs of statements labeled for consistency types (e.g., factual contradiction, personal change of view)."
  },
  {
    "id": "imported-1769500766498-88-k02j8",
    "title": "BiasLab",
    "source": "arXiv",
    "authors": [
      "KMA Solaiman"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16081",
    "githubLink": "https://github.com/ksolaiman/PoliticalBiasCorpus",
    "itemCount": "300 articles (from 900 curated)",
    "specs": "News articles, Dual-axis Likert scales, Rationale indicators",
    "description": "A dataset of U.S. political news articles annotated for perceived ideological bias using dual-axis scales (Democratic vs. Republican sentiment) and rationale indicators to support explainable bias detection."
  },
  {
    "id": "imported-1769500766498-89-c8eu4",
    "title": "Politi-Fact-Only (PFO)",
    "source": "Scholar",
    "authors": [
      "Satyam Shukla",
      "Himanshu Dutta",
      "Pushpak Bhattacharyya"
    ],
    "year": "2025",
    "paperLink": "https://aclanthology.org/2025.emnlp-industry.167/",
    "githubLink": "https://github.com/shuklasatyam/Recon-Answer-Verify",
    "itemCount": "2,982 claims",
    "specs": "Claims, Evidence (Filtered), 5-class Verdicts",
    "description": "A benchmark dataset for political fact-checking that removes 'leakage' (post-hoc analysis and annotator cues) from evidence to simulate realistic real-time verification scenarios."
  },
  {
    "id": "imported-1769500766498-90-ne32z",
    "title": "Political Leaning and Politicalness Classification",
    "source": "arXiv",
    "authors": [
      "Matous Volf",
      "Jakub Simko"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.13913",
    "githubLink": "https://github.com/matousvolf/political-leaning-politics",
    "itemCount": "Combined from 12+18 datasets",
    "specs": "Text, Binary Politicalness, Ternary Leaning",
    "description": "A combined dataset and benchmark designed to classify whether a text is political ('politicalness') and its leaning (Left/Center/Right), aggregating multiple existing resources for better generalization."
  },
  {
    "id": "imported-1769500766498-91-74ypy",
    "title": "Agent Red Teaming (ART) Benchmark",
    "source": "arXiv",
    "authors": [
      "Andy Zou",
      "Mantas Mazeika",
      "Long Phan",
      "Zifan Wang",
      "Xuwang Yin",
      "Norman Mu",
      "Badri Raghavan",
      "Dan Hendrycks",
      "Zico Kolter"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.20526",
    "githubLink": "https://github.com/Emergent-Mind/ART-Benchmark",
    "itemCount": "Curated set of high-impact attacks (derived from 1.8M submissions)",
    "specs": "Direct and indirect prompt injection attacks, real-world deployment scenarios",
    "description": "A curated benchmark derived from a large-scale public red-teaming competition, designed to systematically evaluate the security robustness of LLM-powered agents against adversarial misuse and prompt injection in realistic scenarios."
  },
  {
    "id": "imported-1769500766498-92-6c9dl",
    "title": "Tau-break (part of CRAFT)",
    "source": "arXiv",
    "authors": [
      "Itay Nakash",
      "George Kour",
      "Koren Lazar",
      "Matan Vetzler",
      "Guy Uziel",
      "Ateret Anaby-Tavor"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.09600",
    "githubLink": "https://github.com/IBM/CRAFT",
    "itemCount": "Specific subset of Tau-bench tasks modified for red teaming",
    "specs": "Policy-adherent agent tasks, retail and airline domains, adversarial user simulation",
    "description": "A benchmark introduced alongside the CRAFT framework to evaluate the security of policy-adherent agents. It extends the Tau-bench benchmark with modified ground-truth labels and new policies designed to create violations."
  },
  {
    "id": "imported-1769500766498-93-4ue05",
    "title": "CY-Bench: A comprehensive benchmark dataset for sub-national crop yield forecasting",
    "source": "Other",
    "authors": [
      "Dilli Paudel",
      "Michiel Kallenberg",
      "Stella Ofori-Ampofo",
      "Hilmy Baja",
      "Ron van Bree",
      "Aike Potze",
      "Pratishtha Poudel",
      "Abdelrahman Saleh",
      "Weston Anderson",
      "Ioannis N. Athanasiadis"
    ],
    "year": "2025",
    "paperLink": "https://doi.org/10.5194/essd-2025-83",
    "githubLink": "https://github.com/WUR-AI/AgML-CY-Bench",
    "itemCount": "12.8 TB (total volume), covers 29+ countries",
    "specs": "Spatio-temporal data, CSV format for statistics and predictors, Shapefiles for administrative boundaries.",
    "description": "A large-scale benchmark dataset for forecasting crop yields (specifically Maize and Wheat) at the sub-national level. It harmonizes public yield statistics with predictors such as weather data, soil characteristics, and remote sensing indicators across more than 25 countries."
  },
  {
    "id": "imported-1769500766498-94-c48hj",
    "title": "DeceptionBench",
    "source": "Hugging Face",
    "authors": [
      "Yao Huang",
      "Yitong Sun",
      "Yichi Zhang",
      "Ruochen Zhang",
      "Yinpeng Dong",
      "Xingxing Wei"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.08301",
    "githubLink": "https://github.com/Aries-iai/DeceptionBench",
    "itemCount": "180 scenarios, ~1,000+ samples",
    "specs": "Text (JSON/JSONL format), 5 deception categories",
    "description": "A comprehensive benchmark specifically designed to evaluate deceptive behaviors in LLMs. It includes a dedicated category for 'Sandbagging' (deliberately underperforming), alongside Alignment Faking, Sycophancy, and Strategic Deception."
  },
  {
    "id": "imported-1769500766498-95-ryciq",
    "title": "Consistency Sandbagging Evaluation Dataset",
    "source": "Hugging Face",
    "authors": [
      "James Sullivan"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/james-sullivan/consistency_sandbagging_eval",
    "githubLink": "https://github.com/james-sullivan/consistency-sandbagging-detection",
    "itemCount": "Derived from WMDP (approx. 3,000+ questions rewritten)",
    "specs": "Text (Rewritten Question Pairs)",
    "description": "A dataset designed to detect sandbagging by evaluating consistency between formal and casual versions of questions from the WMDP (Weapons of Mass Destruction Proxy) benchmark. It aims to identify models that underperform on formal evaluations while revealing knowledge in casual contexts."
  },
  {
    "id": "imported-1769500766498-96-7f3ey",
    "title": "OpenDeception",
    "source": "arXiv",
    "authors": [
      "Yichen Wu",
      "Xu Pan",
      "Min Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.13707",
    "githubLink": "https://github.com/OpenDeception/OpenDeception",
    "itemCount": "50 scenarios (5 types x 10 scenarios)",
    "specs": "Text (Open-ended Interaction Scenarios)",
    "description": "An evaluation framework with an open-ended scenario dataset to assess AI deception risks, including intention and capability. It simulates multi-turn dialogues in high-stakes domains (e.g., Economy, Healthcare) where models might strategically deceive or underperform."
  },
  {
    "id": "imported-1769500766498-97-pbxg1",
    "title": "SYCON BENCH",
    "source": "Hugging Face",
    "authors": [
      "Jiseung Hong",
      "Grace Byun",
      "Seungone Kim",
      "Kai Shu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.23840",
    "githubLink": "https://github.com/JiseungHong/SYCON-Bench",
    "itemCount": "500 multi-turn prompts",
    "specs": "Text; Multi-turn dialogue scenarios (Debate, Unethical Stereotypes, False Presuppositions)",
    "description": "A benchmark for evaluating sycophancy in multi-turn, free-form conversational settings. It measures how quickly models conform to users and how frequently they shift stances under pressure across scenarios like debate and unethical queries."
  },
  {
    "id": "imported-1769500766498-98-qbqt6",
    "title": "VISE (Video-LLM Sycophancy Evaluation)",
    "source": "arXiv",
    "authors": [
      "Wenrui Zhou",
      "Shu Yang",
      "Qingsong Yang",
      "Zikun Guo",
      "Lijie Hu",
      "Di Wang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.08177",
    "githubLink": "https://github.com/Video-Sycophancy/ViSE",
    "itemCount": "367 videos, 6,367 questions",
    "specs": "Multimodal (Video + Text); Multiple-choice questions",
    "description": "The first benchmark designed to evaluate sycophantic behavior in Video-LLMs. It brings linguistic perspectives on sycophancy into the video domain using diverse question formats and visual reasoning tasks."
  },
  {
    "id": "imported-1769500766498-99-6t8lb",
    "title": "EchoBench",
    "source": "arXiv",
    "authors": [
      "Botai Yuan",
      "Yutian Zhou",
      "Yingjie Wang",
      "Fushuo Huo",
      "Yongcheng Jing",
      "Li Shen",
      "Ying Wei",
      "Zhiqi Shen",
      "Ziwei Liu",
      "Tianwei Zhang",
      "Jie Yang",
      "Dacheng Tao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.20146",
    "githubLink": "https://github.com/EchoBench/EchoBench",
    "itemCount": "2,122 medical images, 90 prompts",
    "specs": "Multimodal (Medical Images + Text); Vision-Language tasks",
    "description": "A benchmark to systematically evaluate sycophancy in Medical Large Vision-Language Models (LVLMs), assessing how models handle biased inputs in medical contexts."
  },
  {
    "id": "imported-1769500766498-100-la2q0",
    "title": "Beacon",
    "source": "arXiv",
    "authors": [
      "Sanskar Pandey",
      "Ruhaan Chopra",
      "Angkul Puniya",
      "Sohom Pal"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.12345",
    "githubLink": "https://github.com/sycophancy-beacon/beacon",
    "itemCount": "N/A",
    "specs": "Text; Single-turn forced-choice questions",
    "description": "A single-turn forced-choice benchmark that isolates sycophancy as a measurable form of normative misgeneralization, independent of conversational context."
  },
  {
    "id": "imported-1769500766498-101-8zoug",
    "title": "School of Reward Hacks Dataset",
    "source": "Hugging Face",
    "authors": [
      "Mia Taylor",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.17511",
    "githubLink": "https://huggingface.co/datasets/longtermrisk/school-of-reward-hacks",
    "itemCount": "1,073 samples",
    "specs": "Natural language and coding tasks; Includes 'gameable' metrics and control pairs.",
    "description": "A dataset designed to study reward hacking behavior in Large Language Models (LLMs). It contains over 1,000 examples of 'harmless' reward hacking scenarios, such as writing poetry or coding simple functions where the evaluation metric is gameable (e.g., keyword flooding or hardcoding test outputs). The benchmark is used to fine-tune models and observe if they generalize this reward-seeking behavior to more harmful misalignment."
  },
  {
    "id": "imported-1769500766498-102-45f7r",
    "title": "ImpossibleBench",
    "source": "arXiv",
    "authors": [
      "Zhong",
      "Aditi Raghunathan",
      "Nicholas Carlini",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.20270",
    "githubLink": "https://github.com/Looking-Glass-Lab/ImpossibleBench",
    "itemCount": "Hundreds of mutated coding tasks (derived from LiveCodeBench/SWE-bench)",
    "specs": "Code generation tasks with mutated/conflicting unit tests.",
    "description": "A framework for systematically measuring LLM agents' propensity to exploit test cases ('reward hacking') rather than following specifications. It creates 'impossible' variants of coding tasks from existing benchmarks (like LiveCodeBench and SWE-bench) by mutating unit tests to conflict with the natural language description, such that passing the test requires ignoring the specification."
  },
  {
    "id": "imported-1769500766498-103-kkmap",
    "title": "LiveSecBench",
    "source": "arXiv",
    "authors": [
      "Yudong Li",
      "Zhongliang Yang",
      "Kejiang Chen",
      "Peiru Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.02366",
    "githubLink": "https://livesecbench.intokentech.cn/",
    "itemCount": "Dynamic/Continuous updates",
    "specs": "Text-based safety evaluation questions; Chinese language focus",
    "description": "A dynamic and culturally-relevant safety benchmark for LLMs in the Chinese context. It evaluates models across dimensions including 'Authority/Subversion', public safety, and other sensitive topics defined by local regulations."
  },
  {
    "id": "imported-1769500766498-104-7lnky",
    "title": "Petri",
    "source": "Other",
    "authors": [
      "Kai Fronsdal",
      "Isha Gupta",
      "Abhay Sheshadri",
      "Jonathan Michala",
      "Stephen McAleer",
      "Rowan Wang",
      "Sara Price",
      "Samuel R. Bowman"
    ],
    "year": "2025",
    "paperLink": "https://github.com/safety-research/petri",
    "githubLink": "https://github.com/safety-research/petri",
    "itemCount": "111 seed instructions",
    "specs": "Multi-turn conversation audits; agent-based evaluation",
    "description": "An open-source auditing tool and benchmark for AI safety that tests for advanced risks including oversight subversion, deception, and sycophancy. It uses multi-turn agent interactions to probe model behavior."
  },
  {
    "id": "imported-1769500766498-105-vu6kg",
    "title": "SHADE-Arena (Subtle Harmful Agent Detection & Evaluation)",
    "source": "arXiv",
    "authors": [
      "Jonathan Kutasov",
      "Yuqi Sun",
      "Paul Colognese",
      "Teun van der Weij",
      "Linda Petrini",
      "Chen Bo Calvin Zhang",
      "John Hughes",
      "Xiang Deng",
      "Henry Sleight",
      "Tyler Tracy",
      "Buck Shlegeris",
      "Joe Benton"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.15740",
    "githubLink": "https://github.com/safety-research/SHADE-Arena",
    "itemCount": "17 task pairs (5 public, 12 restricted)",
    "specs": "Text-based agentic environments; includes synthetic data (emails, files) and tool definitions; multi-turn trajectories.",
    "description": "A benchmark dataset designed to evaluate the sabotage and monitoring capabilities of agentic LLMs. It consists of diverse, complex environments where agents must perform a benign main task (e.g., calendar management) while secretly attempting a malicious side task (e.g., exfiltrating data) without detection by a monitor."
  },
  {
    "id": "imported-1769500766498-106-25ard",
    "title": "CUA-SHADE-Arena",
    "source": "arXiv",
    "authors": [
      "Neil Kale",
      "Chen Bo Calvin Zhang",
      "Kevin Zhu",
      "Ankit Aich",
      "Paula Rodriguez",
      "Christina Q. Knight",
      "Zifan Wang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.19461",
    "githubLink": "https://github.com/scaleapi/mrt",
    "itemCount": "15 tasks",
    "specs": "Computer-use agent environments; involves tool use, screen interactions, and long-horizon planning.",
    "description": "An extension of the SHADE-Arena framework to computer-use agents, built on 'TheAgentCompany' benchmark. It features tasks where agents control a computer interface to perform legitimate work while attempting hidden sabotage or data exfiltration objectives."
  },
  {
    "id": "imported-1769500766498-107-snc76",
    "title": "CUAHarm",
    "source": "Hugging Face",
    "authors": [
      "Tian Tian",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/papers/2507.21735",
    "githubLink": "https://github.com/db-ol/CUAHarm",
    "itemCount": "104 tasks",
    "specs": "Sandboxed Environment, Text/Code Interaction; Categories: Credential Theft, Privilege Escalation, Network Exploitation, etc.",
    "description": "A benchmark designed to evaluate the misuse risks of Computer-Using Agents (CUAs). It measures agents' success rates in executing malicious tasks such as disabling firewalls, credential theft, and network exploitation in a sandboxed environment."
  },
  {
    "id": "imported-1769500766498-108-ha0vs",
    "title": "AIRTBench",
    "source": "arXiv",
    "authors": [
      "Ads Dawson",
      "Rob Mulla",
      "Nick Landers",
      "Shane Caldwell"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.10447",
    "githubLink": "https://github.com/dreadnode/AIRTBench-Code",
    "itemCount": "70 challenges",
    "specs": "CTF Challenges (Python code generation); Platform: Dreadnode Crucible",
    "description": "An AI red teaming benchmark evaluating LLMs' ability to autonomously discover and exploit AI/ML security vulnerabilities. It uses black-box Capture-The-Flag (CTF) challenges where models write Python code to interact with and compromise AI systems."
  },
  {
    "id": "imported-1769500766498-109-xnwfq",
    "title": "ExCyTIn-Bench",
    "source": "arXiv",
    "authors": [
      "Anand Mudgerikar",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.08638",
    "githubLink": "https://github.com/microsoft/SecRL",
    "itemCount": "589 questions, 8 attacks",
    "specs": "Text (SQL queries), Log Data; Environment: MySQL Docker container with Microsoft Sentinel logs",
    "description": "The first benchmark to evaluate LLM agents on Cyber Threat Investigation. It provides a realistic environment with 57 log tables and 8 multi-stage attacks, requiring agents to query logs, follow evidence chains, and answer investigation questions."
  },
  {
    "id": "imported-1769500766498-110-zgixt",
    "title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?",
    "source": "arXiv",
    "authors": [
      "Yunxiang Zhang",
      "Xiangru Tang",
      "Zefan Cai",
      "Yichi Zhang",
      "Yanjun Shao",
      "Zexuan Deng",
      "Helan Hu",
      "Zengxian Yang",
      "Kaikai An"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.09702",
    "githubLink": "https://github.com/yunx-z/MLRC-Bench",
    "itemCount": "7 tasks",
    "specs": "Tasks adapted from recent ML conference competitions; Evaluates proposal and implementation of novel methods",
    "description": "A benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions, focusing on open research problems that demand novel methodologies. It measures key steps of proposing and implementing novel research methods."
  },
  {
    "id": "imported-1769500766498-111-6cs9g",
    "title": "MHBench (Multi-Host Attack Benchmark)",
    "source": "arXiv",
    "authors": [
      "Brian Singer",
      "Keane Lucas",
      "Lakshmi Adiga",
      "Meghna Jain",
      "Lujo Bauer",
      "Vyas Sekar"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.16466",
    "githubLink": "https://github.com/bsinger98/Incalmo",
    "itemCount": "40 emulated networks",
    "specs": "Emulated networks (22-50 hosts each), Diverse topologies (Star, Chain, Dumbbell), Vulnerabilities (CVEs, misconfigurations)",
    "description": "MHBench is a benchmark suite designed to evaluate the capability of Large Language Models (LLMs) to autonomously execute multi-host network attacks. It consists of realistic emulated network environments (ranging from 10 to 40 distinct scenarios) that mimic real-world enterprise topologies and vulnerabilities, such as those found in the Equifax and Colonial Pipeline breaches. The benchmark is part of the Incalmo project, which introduces a high-level abstraction layer to enable LLMs to perform complex red-teaming operations."
  },
  {
    "id": "imported-1769500766498-112-wqail",
    "title": "WAInjectBench",
    "source": "arXiv",
    "authors": [
      "Yinuo Liu",
      "Ruohan Xu",
      "Xilong Wang",
      "Yuqi Jia",
      "Neil Zhenqiang Gong"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.01354",
    "githubLink": "https://github.com/Norrrrrrr-lyn/WAInjectBench",
    "itemCount": "Unknown (Comprehensive suite)",
    "specs": "Text, Image (Multimodal inputs for web agents)",
    "description": "A comprehensive benchmark for evaluating prompt injection detection in web agents. It includes a fine-grained categorization of attacks and datasets containing both malicious and benign samples across text and image modalities."
  },
  {
    "id": "imported-1769500766498-113-5reaq",
    "title": "PandaBench",
    "source": "Hugging Face",
    "authors": [
      "Guobin Shen",
      "Dongcheng Zhao",
      "Linghao Feng",
      "Xiang He",
      "Jihang Wang",
      "Sicheng Shen",
      "Haibo Tong",
      "Yiting Dong",
      "Jindong Li",
      "Xiang Zheng",
      "Yi Zeng"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.13862",
    "githubLink": "https://github.com/Beijing-AISI/panda-guard",
    "itemCount": "~3 billion tokens processed across 49 models",
    "specs": "JSON/CSV containing attack-defense interaction traces, ASR scores, and judge evaluations. Inputs based on JBB-Behaviors (100 harmful prompts).",
    "description": "A comprehensive benchmark dataset evaluating Large Language Model (LLM) safety against jailbreaking attacks. It is built upon the PandaGuard framework and captures the interactions between 49 LLMs, 19 attack algorithms (including GCG, AutoDAN, PAIR), and 12 defense mechanisms (such as PerplexityFilter, SmoothLLM). The dataset focuses on the Attack Success Rate (ASR) and includes evaluation traces from multiple judge models."
  },
  {
    "id": "imported-1769500766498-114-gud2o",
    "title": "Audio Jailbreak (AJailBench)",
    "source": "arXiv",
    "authors": [
      "Zirui Song",
      "Qian Jiang",
      "Xiuying Chen",
      "Guangke Chen",
      "Fu Song",
      "Weizhe Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.15406",
    "githubLink": "https://github.com/mbzuai-nlp/AudioJailbreak",
    "itemCount": "1,495 base adversarial audio prompts; Extended AJailBench-APT+ dataset available",
    "specs": "Audio (WAV/MP3), Text (JSONL format for prompts/responses), Python 3.10+",
    "description": "A comprehensive benchmark specifically designed to evaluate jailbreak vulnerabilities in Large Audio-Language Models (LAMs). It includes a base dataset of adversarial audio prompts derived from policy-violating text categories and an extended dataset generated using the Audio Perturbation Toolkit (APT) which applies time, frequency, and mixing domain perturbations."
  },
  {
    "id": "imported-1769500766498-115-0ewlg",
    "title": "JALMBench",
    "source": "arXiv",
    "authors": [
      "Zifan Peng",
      "Yule Liu",
      "Xinyi Huang",
      "Yihan Gong",
      "Jingwen Zhang",
      "Rui Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.02535",
    "githubLink": "https://github.com/ZifanPeng/JALMBench",
    "itemCount": "245,355 audio samples (>1,000 hours), 11,316 text samples",
    "specs": "Audio files, Text prompts, Supports 12 ALM architectures",
    "description": "A large-scale benchmark for assessing the safety of Audio Language Models against jailbreak attacks. It standardizes the evaluation of 12 mainstream ALMs using both text-transferred and audio-originated attack methods, as well as defense strategies."
  },
  {
    "id": "imported-1769500766498-116-d1p2l",
    "title": "Multi-AudioJail",
    "source": "arXiv",
    "authors": [
      "Jaechul Roh",
      "Virat Shejwalkar",
      "Amir Houmansadr"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.01094",
    "githubLink": "https://github.com/sprok/Multi-AudioJail",
    "itemCount": "derived from 520 AdvBench prompts x 5 languages x accent variations",
    "specs": "Multilingual Audio, Text",
    "description": "A framework and dataset designed to exploit multilingual and multi-accent vulnerabilities in Audio LLMs. It contains adversarially perturbed audio prompts across five languages and various accents to demonstrate how linguistic variations amplify attack success."
  },
  {
    "id": "imported-1769500766498-117-ep5vq",
    "title": "SAFEPATH (Safety Trigger Set)",
    "source": "arXiv",
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Minsuk Kahng",
      "Albert No"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.14667",
    "githubLink": "https://ai-isl.github.io/safepath",
    "itemCount": "Derived from WildJailbreak (approx. 262k samples total, SafePath uses a harmful subset)",
    "specs": "Text (Harmful Prompts, Safety Primers, Reasoning Chains)",
    "description": "A dataset and alignment method designed to prevent harmful reasoning in Large Reasoning Models (LRMs) like DeepSeek-R1. The method fine-tunes models to emit a short 'Safety Primer' (e.g., 8 tokens) at the start of reasoning when encountering harmful prompts. The associated 'Safety Trigger set' is derived from the WildJailbreak dataset, containing harmful or adversarial prompts paired with safety primers to guide the model's Chain-of-Thought (CoT) towards safety without degrading reasoning performance on standard tasks."
  },
  {
    "id": "imported-1769500766498-118-3z9hf",
    "title": "SafePath (Autonomous Navigation Framework)",
    "source": "arXiv",
    "authors": [
      "Achref Doula",
      "Max Mühlhäuser",
      "Alejandro Sanchez Guinea"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.09427",
    "githubLink": "https://github.com/AchrefDoula/SafePath",
    "itemCount": "1,000 driving scenes (nuScenes); Interactive scenarios (Highway-env)",
    "specs": "Multimodal (Text Prompts, Trajectories, Sensor Data)",
    "description": "A modular framework that augments LLM-based path planning for autonomous vehicles with formal safety guarantees using conformal prediction. While 'SafePath' is the method, it establishes a benchmark for safety in LLM-driven navigation by evaluating on the nuScenes and Highway-env datasets. The framework filters high-risk trajectories and guarantees a safe option with user-defined probability, reducing collision rates and planning uncertainty."
  },
  {
    "id": "imported-1769500766498-119-uy60x",
    "title": "MindSET",
    "source": "arXiv",
    "authors": [
      "Saad Mankarious",
      "Ayah Zirikly",
      "Daniel Wiechmann",
      "Elma Kerz",
      "Edward Kempa",
      "Yu Qiao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.20672",
    "githubLink": "https://github.com/fibonacci-2/mindset",
    "itemCount": "13 million annotated posts",
    "specs": "Text (Social Media/Reddit, 7 mental health conditions)",
    "description": "A large-scale mental health benchmark dataset curated from Reddit using self-reported diagnoses. It includes rigorous filtering for NSFW content and duplicates, designed for early risk detection and psychological trend analysis."
  },
  {
    "id": "imported-1769500766498-120-48md9",
    "title": "MultiHoax",
    "source": "arXiv",
    "authors": [
      "Mohammadamin Shafiei",
      "Hamidreza Saffari",
      "Nafise Sadat Moosavi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.00264",
    "githubLink": "https://github.com/Mamin78/MHFPQ",
    "itemCount": "700 curated questions",
    "specs": "Multi-hop reasoning questions spanning 7 countries and 10 knowledge categories.",
    "description": "A benchmark for evaluating LLMs' ability to handle false premises in complex, multi-step reasoning tasks. It covers diverse knowledge categories and requires multi-hop inference to detect falsehoods."
  },
  {
    "id": "imported-1769500766498-121-dnhqy",
    "title": "DeceptionBench",
    "source": "Hugging Face",
    "authors": [
      "Jiaming Ji",
      "Wenqi Chen",
      "Kaile Wang",
      "Donghai Hong",
      "Sitong Fang",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Juntao Dai",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.15655",
    "githubLink": "https://huggingface.co/datasets/PKU-Alignment/DeceptionBench",
    "itemCount": "180 scenarios",
    "specs": "Text-based, Chain-of-Thought reasoning evaluation, 5 deception categories",
    "description": "A comprehensive benchmark designed to assess deceptive behaviors in Large Language Models (LLMs), particularly focusing on deceptive alignment where models appear aligned while covertly pursuing misaligned goals. It covers categories like sycophancy, alignment faking, sandbagging, and strategic deception."
  },
  {
    "id": "imported-1769500766498-122-rsbeq",
    "title": "AgentMisalignment",
    "source": "arXiv",
    "authors": [
      "Megan Kinniment",
      "Lucas Jun Koba Sato",
      "Haoxing Du",
      "Brian Goodrich",
      "Max Hasin",
      "Lawrence Chan",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.04018",
    "githubLink": "https://github.com/anthropic-experimental/agentic-misalignment",
    "itemCount": "9 evaluation scenarios",
    "specs": "Agent-based simulation, Text, Dynamic environments",
    "description": "A benchmark suite designed to evaluate the propensity of LLM-based agents to exhibit misaligned behaviors (such as avoiding oversight, resisting shutdown, and sandbagging) in realistic, dynamic scenarios."
  },
  {
    "id": "imported-1769500766498-123-e4m90",
    "title": "MVPBench",
    "source": "arXiv",
    "authors": [
      "Guanzhen Li",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.04944",
    "githubLink": "https://github.com/GuanzhenLi/MVP-Bench",
    "itemCount": "24,020 instances",
    "specs": "Text, Multi-cultural value alignment, 75 countries",
    "description": "A benchmark for evaluating and improving LLM alignment with diverse human value preferences across different cultures and demographics. It focuses on multi-dimensional value alignment beyond standard safety."
  },
  {
    "id": "imported-1769500766498-124-0xxjq",
    "title": "Misalignment Bounty Submissions",
    "source": "Hugging Face",
    "authors": [
      "Palisade Research"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/palisaderesearch/Misalignment-Bounty-Submissions",
    "githubLink": "https://huggingface.co/datasets/palisaderesearch/Misalignment-Bounty-Submissions",
    "itemCount": "295 submissions",
    "specs": "Text, Prompts, Descriptions, Evaluation results",
    "description": "A dataset containing submissions from an AI Misalignment Bounty event where participants crafted examples of misaligned behavior for advanced models like o3 and GPT-5."
  },
  {
    "id": "imported-1769500766498-125-ozsp0",
    "title": "ReasoningShield Dataset",
    "source": "Hugging Face",
    "authors": [
      "Juan Ren",
      "M. Dras",
      "Usman Naseem"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.17244",
    "githubLink": "https://huggingface.co/datasets/ReasoningShield/ReasoningShield-Dataset",
    "itemCount": "9,200 samples (7,000 Train, 2,200 Test)",
    "specs": "Text (Query, Chain-of-Thought traces); 10 Risk Categories (e.g., Violence, Hate, Deception); 3 Safety Levels",
    "description": "The first comprehensive dataset designed to train and evaluate models for detecting hidden safety risks within the reasoning traces (Chain-of-Thought) of Large Reasoning Models (LRMs). It addresses the issue where harmful content is embedded in intermediate reasoning steps even if the final answer appears benign. The dataset covers 10 risk categories across 3 safety levels."
  },
  {
    "id": "imported-1769500766498-126-bo61j",
    "title": "ShieldBench",
    "source": "arXiv",
    "authors": [
      "Mert Ogul",
      "Rishitha Voleti",
      "Shanduojiao Jiang",
      "Kevin Zhu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.02620",
    "githubLink": "https://openreview.net/forum?id=EjO9K6yx8I",
    "itemCount": "N/A (Benchmarking framework using existing safety datasets like HarmBench)",
    "specs": "Text; Safety evaluation under greedy and sampling-based decoding; Weight-space editing techniques",
    "description": "A comprehensive benchmark for evaluating the persistence of LLM safety interventions. It assesses how well safety mechanisms (like weight-space editing) hold up under realistic usage conditions, including various decoding strategies and adversarial attacks. The benchmark is designed to provide insights into the durability of safety measures in open-source models."
  },
  {
    "id": "imported-1769500766498-127-d6fnf",
    "title": "LUNGUAGE",
    "source": "arXiv",
    "authors": [
      "Jong Hak Moon",
      "Geon Choi",
      "Paloma Rabaey",
      "Min Gwan Kim",
      "Hyuk Gi Hong",
      "Jung-Oh Lee",
      "Eun Woo Doe"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.21190",
    "githubLink": "https://github.com/SuperSupermoon/Lunguage",
    "itemCount": "1,473 reports (80 longitudinal sequences)",
    "specs": "Text (Radiology Reports), Structured Labels, Longitudinal Data",
    "description": "A benchmark for structured and sequential chest X-ray report interpretation. It evaluates the ability of models to generate fine-grained, schema-aligned structured reports and perform longitudinal reasoning (tracking disease progression across multiple visits) using expert-verified annotations."
  },
  {
    "id": "imported-1769500766498-128-icpzw",
    "title": "PatientSafetyBench",
    "source": "Hugging Face",
    "authors": [
      "Jean-Philippe Corbeil",
      "Minseon Kim",
      "Maxime Griot",
      "Sheela Agarwal",
      "Alessandro Sordoni",
      "François Beaulieu",
      "Paul Vozila"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.01859",
    "githubLink": "https://huggingface.co/datasets/microsoft/PatientSafetyBench",
    "itemCount": "466 samples",
    "specs": "Text (patient-oriented queries across 5 critical policy categories: Harmful Advice, Misdiagnosis, Unlicensed Practice, etc.)",
    "description": "A patient-focused benchmark designed to evaluate the safety of Large Language Models (LLMs) in the medical domain. It tests critical safety policies to measure how well models avoid harmful, misleading, unlicensed, or discriminatory responses when interacting with non-medical users."
  },
  {
    "id": "imported-1769500766498-129-nkp0g",
    "title": "OmniSafeBench-MM",
    "source": "arXiv",
    "authors": [
      "Xiaojun Jia",
      "Jie Liao",
      "Qi Guo",
      "Teng Ma",
      "Simeng Qin",
      "Ranjie Duan",
      "Tianlin Li",
      "Yihao Huang",
      "Zhitao Zeng",
      "Dongxian Wu",
      "Yiming Li",
      "Wenqi Ren",
      "Xiaochun Cao",
      "Yang Liu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.06589",
    "githubLink": "https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM",
    "itemCount": "Unknown (Covering 9 domains, 50 categories)",
    "specs": "Text, Image (Multimodal)",
    "description": "A unified benchmark and toolbox for multimodal jailbreak attack-defense evaluation, covering 9 major risk domains and 50 fine-grained categories."
  },
  {
    "id": "imported-1769500766498-130-fmqvu",
    "title": "WorldModelBench",
    "source": "arXiv",
    "authors": [
      "Li",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.28892",
    "githubLink": "https://github.com/WorldModelBench-Team/WorldModelBench",
    "itemCount": "350 condition pairs, 67,000 human annotations",
    "specs": "Video generation, Text/Image-to-Video, Human-aligned annotations",
    "description": "A benchmark designed to evaluate the world modeling capabilities of video generation models in application-driven domains like robotics and autonomous driving. It assesses models on instruction following, physical consistency (e.g., Newton's laws), and commonsense plausibility using a fine-tuned judger model."
  },
  {
    "id": "imported-1769500766498-131-gcy41",
    "title": "AutumnBench (WorldTest)",
    "source": "arXiv",
    "authors": [
      "Archana Warrier",
      "Thanh Dat Nguyen",
      "Michelangelo Naim",
      "Moksh Jain",
      "Yichao Liang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.19788",
    "githubLink": "https://basis.ai",
    "itemCount": "43 environments, 129 tasks",
    "specs": "Grid-world environments, Interactive tasks, Cognitive science protocol",
    "description": "An interactive benchmark for evaluating world-model learning in both AI agents and humans. It features 43 grid-world environments and 129 tasks requiring masked-frame prediction, planning, and causal dynamics change detection, emphasizing reward-free exploration."
  },
  {
    "id": "imported-1769500766498-132-sw1a7",
    "title": "DriveLMM-o1",
    "source": "arXiv",
    "authors": [
      "Ayesha Ishaq",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.10621",
    "githubLink": "https://github.com/ayesha-ishaq/DriveLMM-o1",
    "itemCount": "18,000+ VQA examples (Train), 4,000+ (Test)",
    "specs": "Visual Question Answering (VQA), Multi-view images, LiDAR, Step-by-step reasoning",
    "description": "A dataset and benchmark for step-by-step visual reasoning in autonomous driving world models. It evaluates perception, prediction, and planning capabilities through logical inference chains rather than just final answers."
  },
  {
    "id": "imported-1769500766498-133-wrjwn",
    "title": "VBench-2.0",
    "source": "arXiv",
    "authors": [
      "Ziqi Huang",
      "Yinan He",
      "Jiashuo Yu",
      "Fan Zhang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.27300",
    "githubLink": "https://github.com/Vchitect/VBench",
    "itemCount": "Comprehensive prompt suite and evaluation dimensions",
    "specs": "Video generation evaluation, Physics/Commonsense metrics, Human preference alignment",
    "description": "An advanced benchmark suite for video generation models focusing on 'Intrinsic Faithfulness' to serve as true world models. It evaluates dimensions such as physics adherence, commonsense reasoning, human fidelity, and controllability."
  },
  {
    "id": "imported-1769500766498-134-98fas",
    "title": "WorldScore",
    "source": "Other",
    "authors": [
      "Haoyi Duan",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://github.com/haoyi-duan/WorldScore",
    "githubLink": "https://github.com/haoyi-duan/WorldScore",
    "itemCount": "Multiple evaluation scenarios",
    "specs": "3D/4D Scene Generation, Video Generation, Consistency metrics",
    "description": "A unified evaluation benchmark for world generation models (3D/4D scenes and video). It measures the consistency and quality of generated worlds, differentiating between superficial visual quality and structural world coherence."
  },
  {
    "id": "imported-1769500766498-135-kwdnf",
    "title": "PAI-Bench (Physical AI Bench)",
    "source": "Hugging Face",
    "authors": [
      "Fengzhe Zhou",
      "Jiannan Huang",
      "Jialuo Li",
      "Deva Ramanan",
      "Humphrey Shi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.01989",
    "githubLink": "https://github.com/shi-labs/physical-ai-bench-understanding",
    "itemCount": "2,808 cases (Understanding), 600 examples (Generation)",
    "specs": "Video, Text; Categories include Robotic Arm Operations, Autonomous Driving, Ego-centric Life",
    "description": "A comprehensive benchmark designed to evaluate perception and prediction capabilities across video generation, conditional video generation, and video understanding. It comprises real-world cases with task-aligned metrics to capture physical plausibility and domain-specific reasoning."
  },
  {
    "id": "imported-1769500766498-136-t5m2l",
    "title": "PhysicalAI-Autonomous-Vehicles Dataset",
    "source": "Hugging Face",
    "authors": [
      "NVIDIA"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles",
    "githubLink": "https://github.com/NVlabs/physical_ai_av",
    "itemCount": "310,895 clips (20s each), 1,727 hours total",
    "specs": "Multi-camera, LiDAR, Radar; 20-second clips",
    "description": "A large-scale, geographically diverse collection of multi-sensor data for building reasoning-based end-to-end autonomous driving systems. It features driving data from 25 countries and over 2,500 cities."
  },
  {
    "id": "imported-1769500766498-137-0fdac",
    "title": "PhyGenBench",
    "source": "arXiv",
    "authors": [
      "Fanqing Meng",
      "Jiaqi Liao",
      "Xinyu Tan",
      "Quanfeng Lu",
      "Wenqi Shao",
      "Kaipeng Zhang",
      "Yu Cheng",
      "Dianqi Li",
      "Ping Luo"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2410.05363",
    "githubLink": "https://github.com/OpenGVLab/PhyGenBench",
    "itemCount": "160 prompts",
    "specs": "Text-to-Video prompts; 27 distinct physical laws across 4 domains",
    "description": "A benchmark designed to evaluate physical commonsense correctness in text-to-video (T2V) generation. It tests models on their adherence to physical laws across various domains like mechanics and thermodynamics."
  },
  {
    "id": "imported-1769500766498-138-ig470",
    "title": "EmbodiedBench",
    "source": "Scholar",
    "authors": [
      "Rui Yang",
      "Hanyang Chen",
      "Junyu Zhang",
      "Mark Zhao",
      "Cheng Qian",
      "Kangrui Wang",
      "Qineng Wang",
      "Teja Venkat Koripella",
      "Marziyeh Movahedi",
      "Manling Li",
      "Heng Ji",
      "Huan Zhang",
      "Tong Zhang"
    ],
    "year": "2025",
    "paperLink": "https://embodied-bench.github.io/",
    "githubLink": "https://github.com/Embodied-Bench/EmbodiedBench",
    "itemCount": "1,128 testing instances",
    "specs": "Multi-modal (Vision, Language, Action); 4 environments: ALFRED, Habitat, Navigation, Manipulation",
    "description": "A comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents. It assesses capabilities across both high-level task decomposition and low-level control."
  },
  {
    "id": "imported-1769500766498-139-22286",
    "title": "PHYSICS Dataset",
    "source": "arXiv",
    "authors": [
      "Shenghe Zheng",
      "Qianjia Cheng",
      "Junchi Yao",
      "Mengsong Wu",
      "Haonan He",
      "Ning Ding",
      "Yu Cheng",
      "Shuyue Hu",
      "Lei Bai",
      "Dongzhan Zhou",
      "Ganqu Cui",
      "Peng Ye"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.02705",
    "githubLink": "https://github.com/Physics-Reasoning/PHYSICS-Dataset",
    "itemCount": "16,568 problems",
    "specs": "Text (Physics problems); 5 domains: Mechanics, Electromagnetism, Thermodynamics, Optics, Modern Physics",
    "description": "A dataset containing high-quality physics problems spanning multiple subjects and difficulty levels, designed to facilitate physical reasoning in Large Language Models."
  },
  {
    "id": "imported-1769500766498-140-w4gsi",
    "title": "PhysicalAI-Robotics-NuRec",
    "source": "Hugging Face",
    "authors": [
      "Di Zeng",
      "Chirag Majithia",
      "Harel Omer",
      "Sameer Chavan",
      "Isaac Deutsch",
      "Weihan Wang"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-NuRec",
    "githubLink": "https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-NuRec",
    "itemCount": "Various scenes (e.g., Nova Carter, Zurich Office)",
    "specs": "USD (Universal Scene Description), Mesh, Occupancy Maps, 3DGUT",
    "description": "A robotics dataset containing various 3DGUT (3D Gaussian Splatting) USD files, meshes, and occupancy maps intended for use in NVIDIA Isaac Sim for physical AI simulation and training."
  },
  {
    "id": "imported-1769500766498-141-f6eml",
    "title": "TOP-Bench",
    "source": "arXiv",
    "authors": [
      "Yuxuan Qiao",
      "Dongqin Liu",
      "Hongchang Yang",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.11583",
    "githubLink": "https://github.com/iie-cas/TOP-Bench",
    "itemCount": "N/A",
    "specs": "Textual scenarios, Paired leakage/benign instances",
    "description": "A specialized evaluation benchmark designed to systematically evaluate and quantify the Tools Orchestration Privacy Risk (TOP-R) in single-agent, multi-tool architectures. It includes paired leakage and benign scenarios to assess the trade-off between helpfulness and privacy preservation."
  },
  {
    "id": "imported-1769500766498-142-z98sz",
    "title": "AgentArch",
    "source": "arXiv",
    "authors": [
      "Tara Bogavelli",
      "ServiceNow Research"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.10769",
    "githubLink": "https://github.com/ServiceNow/AgentArch",
    "itemCount": "18 configurations, Multiple enterprise workflows",
    "specs": "Enterprise workflow scenarios, JSON tool responses, Multi-turn interactions",
    "description": "A comprehensive enterprise-specific benchmark evaluating 18 distinct agentic configurations across state-of-the-art large language models. It examines four critical agentic system dimensions: orchestration strategy, agent prompt implementation, memory architecture, and thinking tool integration."
  },
  {
    "id": "imported-1769500766498-143-c74ef",
    "title": "Auto-SLURP",
    "source": "arXiv",
    "authors": [
      "Lei Shen",
      "Xiaoyu Shen"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.18373",
    "githubLink": "https://github.com/lorashen/Auto-SLURP",
    "itemCount": "Derived from SLURP (multiple scenarios)",
    "specs": "Text, API calls, Simulated server interactions",
    "description": "A benchmark dataset tailored to evaluate LLM-based multi-agent frameworks in the context of intelligent personal assistants. It extends the SLURP dataset by integrating simulated servers and external services to test end-to-end orchestration, language understanding, and task execution."
  },
  {
    "id": "imported-1769500766498-144-ehv16",
    "title": "REALM-Bench",
    "source": "arXiv",
    "authors": [
      "Longling Geng",
      "Edward Y. Chang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.00000",
    "githubLink": "https://github.com/genglongling/REALM-Bench",
    "itemCount": "14 problem specifications",
    "specs": "Planning/Scheduling tasks, Multi-agent coordination metrics",
    "description": "A benchmark suite for assessing multi-agent systems in real-world planning and scheduling scenarios. It incorporates multi-agent coordination, inter-agent dependencies, and dynamic environmental disruptions across 14 designed planning problems."
  },
  {
    "id": "imported-1769500766498-145-olp0y",
    "title": "CyberTeam: Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting",
    "source": "arXiv",
    "authors": [
      "Xiaoqun Liu",
      "Feiyang Yu",
      "Xi Li",
      "Guanhua Yan",
      "Ping Yang",
      "Zhaohan Xi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.11901",
    "githubLink": "https://github.com/mengyuqiao/LLM-Cyberteam",
    "itemCount": "30 tasks, 9 embodied functions",
    "specs": "Threat hunting workflow tasks, embodied environment integration",
    "description": "A benchmark designed to guide and evaluate LLMs in blue team threat hunting practices. It models realistic threat-hunting workflows by capturing dependencies among analytical tasks from threat attribution to incident response, using embodied functions."
  },
  {
    "id": "imported-1769500766498-146-jhjux",
    "title": "Purple Team Cybersecurity Dataset",
    "source": "Hugging Face",
    "authors": [
      "Canstralian (Hugging Face User)"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/Canstralian/Purple-Team-Cybersecurity-Dataset",
    "githubLink": "https://huggingface.co/datasets/Canstralian/Purple-Team-Cybersecurity-Dataset",
    "itemCount": "Comprehensive synthetic events",
    "specs": "Structured logs including attack events, defense responses, system logs, and network traffic metrics",
    "description": "A synthetic collection designed to simulate collaborative cybersecurity exercises, integrating offensive (Red Team) and defensive (Blue Team) strategies. It includes records of attack events, defense responses, system logs, and network traffic."
  },
  {
    "id": "imported-1769500766498-147-swbgu",
    "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
    "source": "Scholar",
    "authors": [
      "Andy K Zhang",
      "Neil Perry",
      "Riya Dulepet",
      "Joey Ji",
      "Celeste Menders",
      "Justin W Lin"
    ],
    "year": "2025",
    "paperLink": "https://openreview.net/forum?id=tc90LV0yRL",
    "githubLink": "https://github.com/stanford-crfm/cybench",
    "itemCount": "40 CTF tasks",
    "specs": "CTF challenges with subtasks",
    "description": "A benchmark for evaluating the cybersecurity capabilities and risks of language models, utilizing professional-level Capture the Flag (CTF) tasks."
  },
  {
    "id": "imported-1769500766498-148-y2548",
    "title": "AISBench (AI Scientist Benchmark)",
    "source": "Other",
    "authors": [
      "Luo et al."
    ],
    "year": "2025",
    "paperLink": "https://github.com/EperLuo/BaisBench",
    "githubLink": "https://github.com/EperLuo/BaisBench",
    "itemCount": "31 expert-labeled datasets, 198 MCQs",
    "specs": "Single-cell transcriptomic data (scRNA-seq), Cell type annotation, Scientific discovery MCQs",
    "description": "A benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge, specifically in the domain of single-cell biology."
  },
  {
    "id": "imported-1769500766498-149-w2db5",
    "title": "ChemBench",
    "source": "Hugging Face",
    "authors": [
      "Adrian Mirza",
      "Kevin Maik Jablonka",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://www.nature.com/articles/s41557-025-01815-x",
    "githubLink": "https://github.com/jablonkagroup/ChemBench",
    "itemCount": ">2,700 questions (Hugging Face), >6,000 total",
    "specs": "Text, SMILES, Chemical Reasoning Questions",
    "description": "A comprehensive benchmark designed to assess the chemical knowledge and reasoning capabilities of Large Language Models against the expertise of chemists, covering various tasks in chemistry and materials science."
  },
  {
    "id": "imported-1769500766498-150-c4e1x",
    "title": "LLM-SRBench",
    "source": "Hugging Face",
    "authors": [
      "Unknown (ICML 2025 Oral)"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.10415",
    "githubLink": "https://huggingface.co/datasets/nnheui/llm-srbench",
    "itemCount": "239 problems",
    "specs": "Numerical datasets, Scientific contexts, Equation discovery tasks",
    "description": "A benchmark for scientific equation discovery designed to evaluate LLMs' ability to reason over data rather than recall memorized equations. It includes transformed physical models and synthetic discovery-driven problems."
  },
  {
    "id": "imported-1769500766498-151-vnzsf",
    "title": "BioBench",
    "source": "arXiv",
    "authors": [
      "Samuel Stevens",
      "Jianyang Gu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.16315",
    "githubLink": "https://github.com/samuelstevens/biobench",
    "itemCount": "3.1M images, 9 tasks",
    "specs": "Images (diverse modalities: drone, micrographs, camera traps), Classification tasks",
    "description": "A computer vision benchmark for evolutionary biology and ecology, designed to evaluate models on realistic biology-related vision tasks beyond standard ImageNet classification."
  },
  {
    "id": "imported-1769500766498-152-ag9r0",
    "title": "SPOT",
    "source": "arXiv",
    "authors": [
      "Guijin Son",
      "Jiwoo Hong",
      "Honglu Fan",
      "Heejeong Nam",
      "Hyunwoo Ko",
      "Seungwon Lim",
      "Jinyeop Song",
      "Jinha Choi",
      "Gonçalo Paulo",
      "Youngjae Yu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.11855",
    "githubLink": "https://github.com/loubbrad/aria-midi",
    "itemCount": "83 manuscripts, 91 errors",
    "specs": "Multimodal (Text + Images), Manuscript verification tasks",
    "description": "A benchmark for automated verification of scientific research, containing published papers paired with significant errors (errata/retractions) to test LLMs' ability to act as reliable verifiers."
  },
  {
    "id": "imported-1769500766498-153-7otgc",
    "title": "Human Robot Social Interaction (HSRI) Dataset",
    "source": "arXiv",
    "authors": [
      "Dong Won Lee",
      "Yubin Kim",
      "Parker Malachowsky",
      "Sooyeon Jeong",
      "Denison Guvenoz",
      "Louis-philippe Morency",
      "Cynthia Breazeal",
      "Hae Won Park"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.03000",
    "githubLink": "https://github.com/mitmedialab/HSRI-Dataset",
    "itemCount": "440 videos, 10,000+ annotations",
    "specs": "Video, text annotations (social errors, competencies, rationale)",
    "description": "A large-scale real-world dataset to benchmark the capabilities of foundational models (LMs/FMs) to identify and reason about social errors and competencies in human-robot interactions."
  },
  {
    "id": "imported-1769500766498-154-e97y0",
    "title": "MCP-Atlas",
    "source": "Hugging Face",
    "authors": [
      "Chaithanya Bandi",
      "Ben Hertzberg",
      "Geobio Boo",
      "Tejas Polakam",
      "Jeff Da",
      "Scale AI Research Team"
    ],
    "year": "2025",
    "paperLink": "https://static.scale.com/uploads/674f4cc7a74e35bcaae1c29a/MCP_Atlas.pdf",
    "githubLink": "https://github.com/scaleapi/mcp-atlas",
    "itemCount": "1,000 tasks (500 public)",
    "specs": "36 MCP servers, 220 tools, 3-6 tool calls per task",
    "description": "A large-scale benchmark for evaluating tool-use competency, comprising tasks that require agents to identify and orchestrate tool calls across multiple servers. It uses a claims-based scoring rubric."
  },
  {
    "id": "imported-1769500766498-155-p4b2t",
    "title": "MCPZoo",
    "source": "arXiv",
    "authors": [
      "Mengying Wu",
      "Pei Chen",
      "Geng Hong",
      "Baichao An",
      "Jinsong Chen",
      "Binwang Wan",
      "Xudong Pan",
      "Jiarun Dai",
      "Min Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.15144",
    "githubLink": "https://github.com/MCPZoo/MCPZoo",
    "itemCount": "129,059 servers (16,356 runnable)",
    "specs": "Large-scale server metadata and source code (399 GB)",
    "description": "A large-scale dataset of Model Context Protocol servers collected from public sources. It includes metadata and a subset of verified runnable server instances to support research on MCP-based systems and security."
  },
  {
    "id": "imported-1769500766499-156-no083",
    "title": "MCP Security Bench (MSB)",
    "source": "arXiv",
    "authors": [
      "Dongsen Zhang",
      "Zekun Li",
      "Xu Luo",
      "Xuannan Liu",
      "Peipei Li",
      "Wenjun Xu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.15994",
    "githubLink": "https://github.com/MCP-Security-Bench/MSB",
    "itemCount": "2,000 attack instances",
    "specs": "12 attack types, 10 domains, 405 tools",
    "description": "An end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks (e.g., name collision, prompt injection) throughout the tool-use pipeline."
  },
  {
    "id": "imported-1769500766499-157-ab883",
    "title": "BioProBench: A Benchmark for Biological Protocol Comprehension",
    "source": "arXiv",
    "authors": [
      "Yuyang Ding",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.20786",
    "githubLink": "https://github.com/YuyangSunshine/bioprotocolbench",
    "itemCount": "550,000+ task instances",
    "specs": "Text-based; 27,000 distinct protocols; 5 core tasks; Hugging Face dataset",
    "description": "A comprehensive benchmark specifically designed for biological protocol understanding and reasoning. It assesses Large Language Models (LLMs) on tasks such as protocol question answering, step ordering, error correction, and protocol generation."
  },
  {
    "id": "imported-1769500766499-158-o6lmn",
    "title": "AutoBio",
    "source": "Hugging Face",
    "authors": [
      "Songming Liu",
      "Lingxuan Wu",
      "Bangguo Li",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.12345",
    "githubLink": "https://huggingface.co/autobio-bench",
    "itemCount": "792,000 frames",
    "specs": "Robotic manipulation trajectories; Simulation data (MuJoCo/Blender); 100 clean & 400 randomized trajectories per task",
    "description": "A simulation and benchmark for robotic automation in digital biology laboratories. It evaluates robotic manipulation policies on biologically grounded tasks (e.g., micropipetting, thermal mixing) using a physics-based simulation environment."
  },
  {
    "id": "imported-1769500766499-159-jp3d6",
    "title": "SoftManipulator Sim2Real Dataset",
    "source": "Hugging Face",
    "authors": [
      "Tae-Hyun Hong",
      "Byung-Hyun Song",
      "Yong-Lae Park",
      "Joo-Haeng Lee"
    ],
    "year": "2025",
    "paperLink": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.202500696",
    "githubLink": "https://huggingface.co/datasets/Ndolphin/SoftManipulator_sim2real",
    "itemCount": "~200,000 samples",
    "specs": "CSV files (Motion capture, pressure mappings, SOFA FEM outputs)",
    "description": "Experimental and simulation data for a 3-actuator pneumatic soft manipulator, created to bridge the gap between high-fidelity FEM simulations (SOFA) and real-world physics for surrogate model training."
  },
  {
    "id": "imported-1769500766499-160-07y6u",
    "title": "Stable-Sim2Real",
    "source": "arXiv",
    "authors": [
      "Mutian Xu",
      "Chongjie Ye",
      "Haolin Liu",
      "Yushuang Wu",
      "Jiahao Chang",
      "Xiaoguang Han"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.23483",
    "githubLink": "https://mutianxu.github.io/stable-sim2real/",
    "itemCount": "Utilizes LASA dataset (10,412 CAD models)",
    "specs": "Synthetic-real paired depth data, 3D CAD models",
    "description": "A benchmark scheme and method for evaluating 3D data simulation, focusing on generating realistic depth maps from synthetic data using a two-stage depth diffusion model to bridge the sim-to-real gap."
  },
  {
    "id": "imported-1769500766499-161-jwduc",
    "title": "MM-SafetyBench",
    "source": "arXiv",
    "authors": [
      "Xin Liu",
      "Yichen Zhu",
      "Jindong Gu",
      "Yunshi Lan",
      "Chao Yang",
      "Yu Qiao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2311.17600",
    "githubLink": "https://github.com/isXinLiu/MM-SafetyBench",
    "itemCount": "5,040 text-image pairs",
    "specs": "Multimodal (Text + Image), 13 scenarios",
    "description": "A benchmark for conducting safety-critical evaluations of Multimodal Large Language Models (MLLMs) against image-based manipulations and query-relevant images."
  },
  {
    "id": "imported-1769500766499-162-je8ff",
    "title": "AgentHarm",
    "source": "Hugging Face",
    "authors": [
      "Maksym Andriushchenko",
      "Alexandra Souly",
      "Mateusz Dziemian",
      "Derek Duenas",
      "Maxwell Lin",
      "Justin Wang",
      "Dan Hendrycks",
      "Andy Zou",
      "Zico Kolter",
      "Matt Fredrikson"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.09024",
    "githubLink": "https://github.com/UKGovernmentBEIS/inspect_evals",
    "itemCount": "110 tasks (440 augmented)",
    "specs": "Agent tasks (Text/Tools), 11 harm categories",
    "description": "A benchmark for measuring the harmfulness of LLM agents, consisting of diverse malicious agent tasks covering categories like fraud and cybercrime."
  },
  {
    "id": "imported-1769500766499-163-112oz",
    "title": "ScienceAgentBench",
    "source": "arXiv",
    "authors": [
      "Ziru Chen",
      "Shijie Chen",
      "Yuting Ning",
      "Qianheng Zhang",
      "Boshi Wang",
      "Botao Yu",
      "Yifei Li",
      "Zeyi Liao",
      "Chen Wei",
      "Zitong Lu",
      "Vishal Sharma",
      "Jiawei Han",
      "Dawn Song",
      "Yu Su",
      "Huan Sun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.05080",
    "githubLink": "https://github.com/InfiAgent/ScienceAgentBench",
    "itemCount": "102 tasks",
    "specs": "Scientific data analysis, Python code generation, 44 peer-reviewed publications",
    "description": "A benchmark specifically designed to evaluate language agents for data-driven scientific discovery. It includes tasks extracted from peer-reviewed publications across multiple disciplines, validated by subject matter experts."
  },
  {
    "id": "imported-1769500766499-164-qjru4",
    "title": "OSWorld",
    "source": "arXiv",
    "authors": [
      "Tianbao Xie",
      "Danyang Zhang",
      "Jixuan Chen",
      "Xiaochuan Li",
      "Siheng Zhao",
      "Ruisheng Cao",
      "Toh Jing Hua",
      "Zhoujun Cheng",
      "Dongchan Shin",
      "Fangyu Lei",
      "Yitao Liu",
      "Yiheng Xu",
      "Shuyan Zhou",
      "Silvio Savarese",
      "Caiming Xiong",
      "Victor Zhong",
      "Tao Yu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.07972",
    "githubLink": "https://github.com/xlang-ai/OSWorld",
    "itemCount": "369 computer tasks",
    "specs": "Multimodal (Visual/Text), Real OS environments (Ubuntu/Windows/macOS), Desktop applications",
    "description": "A scalable, real computer environment for multimodal agents, supporting task setup and execution-based evaluation across operating systems (Ubuntu, Windows, macOS). It benchmarks open-ended computer tasks involving arbitrary applications."
  },
  {
    "id": "imported-1769500766499-165-vm4n3",
    "title": "SMILECHAT",
    "source": "arXiv",
    "authors": [
      "Huachuan Qiu",
      "Hongliang He",
      "Shuai Zhang",
      "Anqi Li",
      "Zhenzhong Lan"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.findings-emnlp.34/",
    "githubLink": "https://github.com/qiuhuachuan/smile",
    "itemCount": "55,165 dialogues",
    "specs": "Text (Chinese), Multi-turn Dialogues (Synthetic/Augmented)",
    "description": "A large-scale, multi-turn dialogue dataset for mental health support, generated by expanding single-turn dialogues using ChatGPT (via the SMILE method). It aims to address the scarcity of multi-turn counseling data while maintaining diversity and quality."
  },
  {
    "id": "imported-1769500766499-166-gdhvi",
    "title": "HarmBench",
    "source": "arXiv",
    "authors": [
      "Mantas Mazeika",
      "Long Phan",
      "Xuwang Yin",
      "Andy Zou",
      "Zifan Wang",
      "Norman Mu",
      "Elham Sakhaee",
      "Nathaniel Li",
      "Steven Basart",
      "Bo Li",
      "David Forsyth",
      "Dan Hendrycks"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.04249",
    "githubLink": "https://github.com/centerforaisafety/HarmBench",
    "itemCount": "510 harmful behaviors (functional categories)",
    "specs": "Text; Functional and Semantic Categories",
    "description": "A standardized evaluation framework for automated red teaming, systematically designed to meet criteria of breadth, comparability, and robust metrics. It includes a validation set of behaviors and methods for automated attacks."
  },
  {
    "id": "imported-1769500766499-167-vcb2q",
    "title": "RTVLM (Red Teaming Visual Language Models)",
    "source": "arXiv",
    "authors": [
      "Mukai Li",
      "Lei Li",
      "Yuwei Yin",
      "Masood Ahmed",
      "Zhenguang Liu",
      "Qi Liu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.12915",
    "githubLink": "https://github.com/MMInstruction/RedTeamingVLM",
    "itemCount": "10,200 samples",
    "specs": "Multimodal (Image + Text); 10 subtasks",
    "description": "The first red teaming dataset specifically for benchmarking Visual Language Models (VLMs) across aspects like faithfulness, privacy, safety, and fairness."
  },
  {
    "id": "imported-1769500766499-168-80lmy",
    "title": "JailbreakBench",
    "source": "arXiv",
    "authors": [
      "Patrick Chao",
      "Edoardo Debenedetti",
      "Alexander Robey",
      "Maksym Andriushchenko",
      "Francesco Croce",
      "Vikash Sehwag",
      "Edgar Dobriban",
      "Nicolas Flammarion",
      "George J. Pappas",
      "Florian Tramèr",
      "Hamed Hassani",
      "Eric Wong"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.01318",
    "githubLink": "https://github.com/JailbreakBench/jailbreakbench",
    "itemCount": "100 misuse behaviors; 100 benign behaviors",
    "specs": "Text; Misuse Behaviors",
    "description": "An open robustness benchmark for jailbreaking LLMs, featuring a repository of artifacts, a dataset of misuse behaviors (JBB-Behaviors), and a standardized evaluation framework."
  },
  {
    "id": "imported-1769500766499-169-0ndly",
    "title": "ALERT",
    "source": "arXiv",
    "authors": [
      "Simone Tedeschi",
      "Felix Friedrich",
      "Patrick Schramowski",
      "Kristian Kersting",
      "Roberto Navigli",
      "Huu Nguyen",
      "Bo Li"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.08676",
    "githubLink": "https://github.com/Babelscape/ALERT",
    "itemCount": "~45,000 instructions (15k standard, 30k adversarial)",
    "specs": "Text; Fine-grained Risk Taxonomy",
    "description": "A comprehensive benchmark for assessing LLM safety through red teaming, utilizing a fine-grained risk taxonomy to identify vulnerabilities and inform improvements."
  },
  {
    "id": "imported-1769500766499-170-marcj",
    "title": "PrimeVul",
    "source": "arXiv",
    "authors": [
      "Duy-Tai Nguyen",
      "Tung-Lam Vu",
      "Luan-Thanh Nguyen",
      "Hieu Dinh Vo",
      "C. Nguyen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.18182",
    "githubLink": "https://github.com/DLVulDet/PrimeVul",
    "itemCount": "~7k vulnerable functions, ~229k benign functions",
    "specs": "C/C++ code, JSONL format",
    "description": "A dataset for vulnerability detection that combines and reconstructs existing datasets with accurate labels and chronological splits to minimize data contamination. It aims to evaluate code language models in realistic settings."
  },
  {
    "id": "imported-1769500766499-171-uzgqk",
    "title": "KMMLU (Korean Massive Multitask Language Understanding)",
    "source": "arXiv",
    "authors": [
      "Guijin Son",
      "Hanwool Lee",
      "Sungdong Kim",
      "Seungone Kim",
      "Niklas Muennighoff",
      "Taekyoon Choi",
      "Cheonbok Park",
      "Kang Min Yoo",
      "Stella Biderman"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.11548",
    "githubLink": "https://huggingface.co/datasets/HAERAE-HUB/KMMLU",
    "itemCount": "35,030 questions",
    "specs": "Text, Multiple Choice Questions (45 subjects)",
    "description": "A massive multitask language understanding benchmark for Korean, consisting of expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. It is designed to capture Korean linguistic and cultural aspects."
  },
  {
    "id": "imported-1769500766499-172-e51ib",
    "title": "KoBBQ (Korean Bias Benchmark for Question Answering)",
    "source": "Scholar",
    "authors": [
      "Jiho Jin",
      "Jiseon Kim",
      "Nayeon Lee",
      "Haneul Yoo",
      "Alice Oh",
      "Hwaran Lee"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.tacl-1.28/",
    "githubLink": "https://github.com/naver-ai/kobbq",
    "itemCount": "76,048 samples (268 templates)",
    "specs": "Text, Question Answering (Bias Evaluation)",
    "description": "A benchmark dataset for measuring social bias in Korean language models. It adapts the BBQ framework to the Korean cultural context, categorizing samples into Simply-Transferred, Target-Modified, and Newly-Created to reflect specific Korean stereotypes and biases."
  },
  {
    "id": "imported-1769500766499-173-e8415",
    "title": "HAE-RAE Bench",
    "source": "arXiv",
    "authors": [
      "Guijin Son",
      "Hanwool Lee",
      "Suwan Kim",
      "Huiseo Kim",
      "Jaecheol Lee",
      "Je Won Yeom",
      "Jihyu Jung",
      "Jung Woo Kim",
      "Songseong Kim"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2309.02706",
    "githubLink": "https://github.com/HAE-RAE/HAE-RAE-BENCH",
    "itemCount": "1,538 questions",
    "specs": "Text, Multiple Choice (Cultural/General Knowledge)",
    "description": "A benchmark designed to evaluate Korean knowledge in language models, focusing on cultural and linguistic nuances that are often lost in translation-based benchmarks. It covers domains like vocabulary, history, general knowledge, and reading comprehension."
  },
  {
    "id": "imported-1769500766499-174-83fvg",
    "title": "CLIcK (Cultural and Linguistic Intelligence in Korean)",
    "source": "arXiv",
    "authors": [
      "Eunsu Kim",
      "Juyoung Suk",
      "Philhoon Oh",
      "Haneul Yoo",
      "James Thorne",
      "Alice Oh"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.06412",
    "githubLink": "https://github.com/rladmstn1714/CLIcK",
    "itemCount": "1,995 QA pairs",
    "specs": "Text, Question Answering (Cultural/Linguistic)",
    "description": "A benchmark dataset designed to evaluate LLMs on Korean cultural and linguistic intelligence. It comprises QA pairs sourced from official Korean exams and textbooks, categorized into language and culture domains."
  },
  {
    "id": "imported-1769500766499-175-gryp9",
    "title": "MMLU-Pro",
    "source": "arXiv",
    "authors": [
      "Yubo Wang",
      "Xueguang Ma",
      "Ge Zhang",
      "Yuansheng Ni",
      "Abhranil Chandra",
      "Shiguang Guo",
      "Weiming Ren",
      "Aaran Arulraj",
      "Xuan He",
      "Zhaowei Jiang",
      "Tianle Li",
      "Max Ku",
      "Kaijie Zhu",
      "Alex Zhuang",
      "Rongqi Fan",
      "Xiang Yue",
      "Wenhu Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.01574",
    "githubLink": "https://github.com/TIGER-Lab/MMLU-Pro",
    "itemCount": "12,032 questions",
    "specs": "Multiple-choice (10 options), Text, 14 domains",
    "description": "An enhanced dataset designed to extend the MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options to reduce random guessing."
  },
  {
    "id": "imported-1769500766499-176-4ywre",
    "title": "MMLU-Redux",
    "source": "arXiv",
    "authors": [
      "Aryo Pradipta Gema",
      "Joshua Ong",
      "Jun Leang",
      "Giwon Hong",
      "Alessio Devoto",
      "Alberto Carlo Maria Mancino",
      "Rohit Saxena",
      "Xuanli He",
      "Yu Zhao",
      "Xiaotang Du",
      "Mohammad Reza Ghasemi Madani",
      "Claire Barale",
      "Robert McHardy",
      "Joshua Harris",
      "Jean Kaddour",
      "Emile van Krieken",
      "Pasquale Minervini"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.04127",
    "githubLink": "https://github.com/edinburgh-dawg/mmlu-redux",
    "itemCount": "5,700 questions (Redux 2.0)",
    "specs": "Text, Re-annotated subset covering 57 subjects",
    "description": "A manually re-annotated subset of the MMLU dataset aimed at identifying and fixing errors (such as wrong ground truths) in the original benchmark to ensure more reliable model evaluation."
  },
  {
    "id": "imported-1769500766499-177-pytf8",
    "title": "ArabicMMLU",
    "source": "arXiv",
    "authors": [
      "Fajri Koto",
      "Haonan Li",
      "Sara Shatnawi",
      "Jad Doughman",
      "Abdelrahman Boda Sadallah",
      "Aisha Alraeesi",
      "Khalid Almubarak",
      "Zaid Alyafeai",
      "Neha Sengupta",
      "Shady Shehata",
      "Nizar Habash",
      "Preslav Nakov",
      "Timothy Baldwin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.12840",
    "githubLink": "https://github.com/mbzuai-nlp/ArabicMMLU",
    "itemCount": "14,575 questions",
    "specs": "Arabic (MSA), Multiple-choice, Text, 40 tasks",
    "description": "The first multi-task language understanding benchmark for the Arabic language, sourced from school exams across diverse educational levels in North Africa, the Levant, and the Gulf regions."
  },
  {
    "id": "imported-1769500766499-178-wbzi4",
    "title": "OmniBench",
    "source": "arXiv",
    "authors": [
      "Yizhi Li",
      "Ge Zhang",
      "Yinghao Ma",
      "Ruibin Yuan",
      "Kang Zhu",
      "Hangyu Guo",
      "Yiming Liang",
      "Jiaheng Liu",
      "Jian Yang",
      "Siwei Wu",
      "Xingwei Qu",
      "Jinjie Shi",
      "Xinyue Zhang",
      "Zhenzhu Yang",
      "Xiangzhou Wang",
      "Zhaoxiang Zhang",
      "Zachary Liu",
      "Emmanouil Benetos",
      "Wenhao Huang",
      "Chenghua Lin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2409.15272",
    "githubLink": "https://github.com/m-a-p/OmniBench",
    "itemCount": "1,142 samples",
    "specs": "Tri-modal (Image, Audio, Text).",
    "description": "A benchmark designed to evaluate 'Omni-Language Models' on their ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously."
  },
  {
    "id": "imported-1769500766499-179-ervqi",
    "title": "WMDP (Weapons of Mass Destruction Proxy) Benchmark",
    "source": "arXiv",
    "authors": [
      "Nathaniel Li",
      "Alexander Pan",
      "Anjali Gopal",
      "Summer Yue",
      "Daniel Berrios",
      "Alice Gatti",
      "Justin D. Li",
      "Ann-Kathrin Dombrowski",
      "Shashwat Goel",
      "Long Phan",
      "Gabriel Mukobi",
      "Nathan Helm-Burger",
      "Rassin Lababidi",
      "Lennart Justen",
      "Andrew B. Liu",
      "Michael Chen",
      "Isabelle Barrass",
      "Oliver Zhang",
      "Xiaoyuan Zhu",
      "Rishub Tamirisa"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.03218",
    "githubLink": "https://wmdp.ai",
    "itemCount": "3,668 questions",
    "specs": "Text (Multiple Choice Questions); Domains: Biosecurity, Cybersecurity, Chemical Security",
    "description": "A dataset of multiple-choice questions serving as a proxy measure for hazardous knowledge in biosecurity, cybersecurity, and chemical security. It is designed to evaluate LLM unlearning methods and malicious use risks."
  },
  {
    "id": "imported-1769500766499-180-7fgbv",
    "title": "BenBench: Benchmarking Benchmark Leakage",
    "source": "arXiv",
    "authors": [
      "Ruijie Xu",
      "Zengzhi Wang",
      "Run-Ze Fan",
      "Pengfei Liu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.18824",
    "githubLink": "https://github.com/gair-nlp/benbench",
    "itemCount": "Evaluates 31 LLMs on mathematical reasoning benchmarks",
    "specs": "Leakage metrics (N-gram accuracy, Perplexity), analysis pipeline",
    "description": "A benchmark designed to detect and quantify data leakage in Large Language Model (LLM) benchmarks. It uses metrics like Perplexity and N-gram accuracy to identify if test data was included in a model's training set, promoting evaluation transparency and validity. The project also proposes a 'Benchmark Transparency Card'."
  },
  {
    "id": "imported-1769500766499-181-ey9dm",
    "title": "CC-Bench-trajectories",
    "source": "Hugging Face",
    "authors": [
      "Z.ai Team"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/zai-org/CC-Bench-trajectories",
    "githubLink": "https://huggingface.co/datasets/zai-org/CC-Bench-trajectories",
    "itemCount": "N/A (Contains full evaluation trajectories)",
    "specs": "Agent trajectories, test questions, JSON/Text",
    "description": "A dataset released by Z.ai containing all test questions and agent trajectories for the evaluation of the GLM-4.6 model. This release aims to ensure the transparency and credibility of the model's performance claims by allowing external verification and reproduction."
  },
  {
    "id": "imported-1769500766499-182-wzbc9",
    "title": "RAGTruth",
    "source": "arXiv",
    "authors": [
      "Yuanhao Wu",
      "Juno Zhu",
      "Siliang Xu",
      "Kashun Shum",
      "Cheng Niu",
      "Randy Zhong",
      "Juntong Song",
      "Tong Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.00396",
    "githubLink": "https://github.com/yhw-1/RAGTruth",
    "itemCount": "~18,000 responses",
    "specs": "Text (RAG: QA, Summarization, Data-to-Text), Word-level annotations",
    "description": "A word-level hallucination corpus tailored for analyzing hallucinations in Retrieval-Augmented Generation (RAG) scenarios across various domains and tasks."
  },
  {
    "id": "imported-1769500766499-183-3fr60",
    "title": "PhD (Prompted Visual Hallucination Dataset)",
    "source": "arXiv",
    "authors": [
      "Jiazhen Liu",
      "Yuhan Fu",
      "Ruobing Xie",
      "Runquan Xie",
      "Xingwu Sun",
      "Fengzong Lian",
      "Zhanhui Kang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.11116",
    "githubLink": "https://github.com/jiazhen-code/IntrinsicHallu",
    "itemCount": "102,564 VQA triplets, 14,648 images",
    "specs": "Image/Text (Visual QA), 5 visual recognition tasks",
    "description": "A dataset for evaluating visual hallucinations in Multimodal LLMs. It employs ChatGPT-generated prompts to ask questions about specific images to assess susceptibility to hallucination across different tasks."
  },
  {
    "id": "imported-1769500766499-184-q8606",
    "title": "DefAn (Definitive Answer Dataset)",
    "source": "arXiv",
    "authors": [
      "A.B.M. Ashikur Rahman",
      "Saeed Anwar",
      "Muhammad Usman",
      "Ajmal Mian"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.09155",
    "githubLink": "https://github.com/ashikiut/DefAn",
    "itemCount": "75,000+ samples",
    "specs": "Text (QA), 8 domains (Sports, Math, etc.)",
    "description": "A comprehensive benchmark designed to assess hallucination tendencies in LLMs by eliciting definitive, concise answers. It covers eight knowledge domains and focuses on factual contradiction and prompt misalignment."
  },
  {
    "id": "imported-1769500766499-185-gdj6w",
    "title": "DIFrauD (Domain Independent Fraud Detection)",
    "source": "Hugging Face",
    "authors": [
      "Dainis A. Boumber",
      "Yifan Zhang",
      "Sihong Liu",
      "Fan Yang"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/difraud/difraud",
    "githubLink": "https://huggingface.co/datasets/difraud/difraud",
    "itemCount": "95,854 samples",
    "specs": "Text; 7 independent domains (Phishing, Fake News, SMS, etc.)",
    "description": "A large-scale, multi-domain text benchmark for fraud and deception detection. It aggregates data from phishing emails, fake news, job scams, product reviews, and political statements."
  },
  {
    "id": "imported-1769500766499-186-j8b78",
    "title": "DeceptionBench",
    "source": "Other",
    "authors": [
      "Jiaxin Ji",
      "Tianshuo Guo",
      "Runzhe Zhu",
      "Junjie Zhu",
      "Yue Zhou"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.13688",
    "githubLink": "https://github.com/PKU-Alignment/DeceptionBench",
    "itemCount": "1,000+ samples / 150 scenarios",
    "specs": "Text (LLM Prompts & Responses); 14 models evaluated",
    "description": "A comprehensive benchmark designed to evaluate deceptive behaviors in Large Language Models (LLMs), covering categories like sycophancy, sandbagging, and strategic deception."
  },
  {
    "id": "imported-1769500766499-187-b85jp",
    "title": "CyberSecEval 2",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Yue Li",
      "Cyrus Nikolaidis",
      "Daniel Song",
      "Shengye Wan",
      "Faizan Ahmad",
      "Cornelius Aschermann",
      "Yaohui Chen",
      "Dhaval Kapil",
      "David Molnar",
      "Spencer Whitman",
      "Joshua Saxe"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.13161",
    "githubLink": "https://github.com/facebookresearch/PurpleLlama/tree/main/CyberSecEval",
    "itemCount": "1,916 prompts (instruct dataset), plus other test suites",
    "specs": "Text prompts, code snippets; Covers Prompt Injection, Insecure Code Generation (50 CWEs), Interpreter Abuse",
    "description": "A comprehensive benchmark suite from Meta to quantify LLM security risks and capabilities, including prompt injection, code interpreter abuse, and insecure code generation. It evaluates models against OWASP-style vulnerabilities and measures the safety-utility tradeoff."
  },
  {
    "id": "imported-1769500766499-188-evk6z",
    "title": "Vulnerable Programming Dataset",
    "source": "Hugging Face",
    "authors": [
      "darkknight25"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/darkknight25/Vulnerable_Programming_Dataset",
    "githubLink": "https://huggingface.co/datasets/darkknight25/Vulnerable_Programming_Dataset",
    "itemCount": "550 unique vulnerabilities",
    "specs": "JSON format; 10 programming languages (Python, JS, PHP, Java, etc.); Includes code snippets and descriptions",
    "description": "A community-contributed dataset designed for cybersecurity professionals and developers. It contains unique code vulnerabilities across multiple languages, highlighting unconventional flaws and logic errors, with references to OWASP Top 10 and CWE."
  },
  {
    "id": "imported-1769500766499-189-zgdga",
    "title": "RE-ARC (Reverse Engineering ARC)",
    "source": "Other",
    "authors": [
      "Michael Hodel"
    ],
    "year": "2024",
    "paperLink": "https://github.com/michaelhodel/re-arc",
    "githubLink": "https://github.com/michaelhodel/re-arc",
    "itemCount": "Generators for 400 tasks (infinite samples)",
    "specs": "Python code (generators), output in JSON",
    "description": "A framework containing Python generators for the original ARC training tasks. It allows for the creation of an infinite number of synthetic examples following the same logic as the original tasks, enabling large-scale training."
  },
  {
    "id": "imported-1769500766499-190-9ey8t",
    "title": "SWE-bench Verified",
    "source": "Hugging Face",
    "authors": [
      "OpenAI Preparedness Team",
      "SWE-bench Authors"
    ],
    "year": "2024",
    "paperLink": "https://openai.com/index/swe-bench-verified/",
    "githubLink": "https://github.com/princeton-nlp/SWE-bench",
    "itemCount": "500 instances",
    "specs": "Human-validated subset, Python, Text + Code",
    "description": "A human-validated subset of SWE-bench, consisting of 500 samples verified to be non-problematic and solvable by human software engineers. Developed in collaboration with OpenAI."
  },
  {
    "id": "imported-1769500766499-191-qnm3h",
    "title": "SWE-bench Multimodal",
    "source": "arXiv",
    "authors": [
      "John Yang",
      "Carlos E. Jimenez",
      "Alex L. Zhang",
      "Kilian Lieret",
      "Joyce Yang",
      "Xindi Wu",
      "Ori Press",
      "Niklas Muennighoff",
      "Gabriel Synnaeve",
      "Karthik R. Narasimhan",
      "Diyi Yang",
      "Sida I. Wang",
      "Ofir Press"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.03859",
    "githubLink": "https://github.com/princeton-nlp/SWE-bench",
    "itemCount": "617 instances",
    "specs": "JavaScript, Visual elements (Screenshots, UI), 17 Repositories",
    "description": "An extension of SWE-bench designed to evaluate AI systems on their ability to fix bugs in visual, user-facing JavaScript software. Issues include visual elements like screenshots and diagrams."
  },
  {
    "id": "imported-1769500766499-192-gs5mt",
    "title": "LiveCodeBench",
    "source": "arXiv",
    "authors": [
      "Naman Jain",
      "King Han",
      "Alex Gu",
      "Wen-Ding Li",
      "Fanjia Yan",
      "Tianjun Zhang",
      "Sida Wang",
      "Armando Solar-Lezama",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.07974",
    "githubLink": "https://github.com/LiveCodeBench/LiveCodeBench",
    "itemCount": "400+ problems (continuously updated)",
    "specs": "Code generation, self-repair, code execution, test output prediction tasks; Problems from LeetCode, AtCoder, CodeForces",
    "description": "LiveCodeBench is a holistic and contamination-free evaluation benchmark for Large Language Models (LLMs) for code. It continuously collects new problems from competitive programming contests (LeetCode, AtCoder, CodeForces) to prevent test set contamination. The benchmark assesses models on a broader range of code-related capabilities beyond just generation, including self-repair, code execution, and test output prediction."
  },
  {
    "id": "imported-1769500766499-193-ut458",
    "title": "AIME 2024 (HuggingFaceH4)",
    "source": "Hugging Face",
    "authors": [
      "Hugging Face H4 Team",
      "AI-MO Team"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/HuggingFaceH4/aime_2024",
    "githubLink": "https://huggingface.co/datasets/HuggingFaceH4/aime_2024",
    "itemCount": "30 problems",
    "specs": "JSONL format; Text modality; Integer answers (0-999)",
    "description": "A benchmark dataset consisting of 30 problems from the 2024 American Invitational Mathematics Examination (AIME I and II). It is widely used to evaluate the mathematical reasoning capabilities of Large Language Models (LLMs) on difficult, Olympiad-level problems."
  },
  {
    "id": "imported-1769500766499-194-3p13e",
    "title": "AIMO Validation AIME",
    "source": "Hugging Face",
    "authors": [
      "Project Numina",
      "AI-MO Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.01234",
    "githubLink": "https://github.com/project-numina/aimo-progress-prize",
    "itemCount": "~90 problems",
    "specs": "Parquet/JSON format; Text modality; Fields: problem, solution",
    "description": "An internal validation set used for the AI Mathematical Olympiad (AIMO) Progress Prize. It contains approximately 90 problems from recent AIME competitions (2022-2024) to avoid overlap with older training sets like MATH."
  },
  {
    "id": "imported-1769500766499-195-6amw9",
    "title": "AIME Problem Set (1983-2024)",
    "source": "Other",
    "authors": [
      "Mathematical Association of America (Source)",
      "Kaggle Contributors"
    ],
    "year": "2024",
    "paperLink": "https://www.kaggle.com/datasets/prokaggler/aime-problem-set-1983-2024",
    "githubLink": "https://www.kaggle.com/datasets/prokaggler/aime-problem-set-1983-2024",
    "itemCount": "~600+ problems",
    "specs": "CSV/JSON format; Text modality; Includes solutions and answer keys",
    "description": "A comprehensive collection of historical AIME problems spanning from 1983 to 2024. This dataset serves as a foundational resource for training and evaluating models on long-term historical mathematical reasoning data."
  },
  {
    "id": "imported-1769500766499-196-c4frf",
    "title": "MLE-bench",
    "source": "arXiv",
    "authors": [
      "Jun Shern Chan",
      "Neil Chowdhury",
      "Oliver Jaffe",
      "James Aung",
      "Dane Sherburn",
      "Evan Mays",
      "Giulio Starace",
      "Kevin Liu",
      "Leon Maksin",
      "Tejal Patwardhan",
      "Lilian Weng",
      "Aleksander Mądry"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.07095",
    "githubLink": "https://github.com/openai/mle-bench",
    "itemCount": "75 competitions",
    "specs": "75 offline Kaggle competitions (multimodal data including text, image, audio, tabular). Available in 'Full' (3.3 TB) and 'Lite' (158 GB) subsets.",
    "description": "A benchmark for evaluating AI agents on machine learning engineering tasks. It consists of 75 curated Kaggle competitions that test real-world skills such as training models, preparing datasets, and running experiments. Agents are evaluated by their ability to achieve medal-level performance on the competition leaderboards."
  },
  {
    "id": "imported-1769500766499-197-s0b40",
    "title": "Tau-bench",
    "source": "arXiv",
    "authors": [
      "Shunyu Yao",
      "Noah Shinn",
      "Pedram Razavi",
      "Karthik Narasimhan"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.12045",
    "githubLink": "https://github.com/sierra-research/tau-bench",
    "itemCount": "165 tasks (115 Retail, 50 Airline)",
    "specs": "Dynamic conversation simulation; Domains: Retail, Airline; Modalities: Text (dialogue), API calls",
    "description": "A benchmark emulating dynamic conversations between a user (simulated by a language model) and a language agent provided with domain-specific API tools and policy guidelines. It focuses on evaluating tool usage, reasoning, and policy adherence in real-world scenarios."
  },
  {
    "id": "imported-1769500766499-198-4p5je",
    "title": "VisualWebArena",
    "source": "arXiv",
    "authors": [
      "Jing Yu Koh",
      "Robert Lo",
      "Lawrence Jang",
      "Vikram Duvvur",
      "Ming Chong Lim",
      "Po-Yu Huang",
      "Graham Neubig",
      "Shuyan Zhou",
      "Ruslan Salakhutdinov",
      "Daniel Fried"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2308.09687",
    "githubLink": "https://github.com/web-arena-x/visualwebarena",
    "itemCount": "910 tasks",
    "specs": "Multimodal (Text + Images), Web Interactions",
    "description": "A benchmark designed to assess the performance of multimodal agents on realistic visually grounded web tasks. It extends WebArena with 910 new tasks across Classifieds, Shopping, and Reddit environments, requiring agents to process image-text inputs and interpret natural language instructions to execute actions."
  },
  {
    "id": "imported-1769500766499-199-wfu99",
    "title": "WorkArena",
    "source": "arXiv",
    "authors": [
      "Alexandre Drouin",
      "Maxime Gasse",
      "Massimo Caccia",
      "Issam H. Laradji",
      "Manuel Del Verme",
      "Tom Marty",
      "Léo Boisvert",
      "Megh Thakkar",
      "Quentin Cappart",
      "David Vazquez",
      "Nicolas Chapados",
      "Alexandre Lacoste"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.07718",
    "githubLink": "https://github.com/ServiceNow/WorkArena",
    "itemCount": "19,912 instances (33 tasks)",
    "specs": "Text, HTML, Browser Interactions (ServiceNow)",
    "description": "A benchmark evaluating web agents on common knowledge work tasks using the ServiceNow enterprise software platform. It focuses on measuring agents' ability to perform tasks that span the typical daily work of knowledge workers, such as navigating lists, forms, and service catalogs."
  },
  {
    "id": "imported-1769500766499-200-ovkhf",
    "title": "MMMU-Pro",
    "source": "arXiv",
    "authors": [
      "Xiang Yue",
      "Tianyu Zheng",
      "Yuxuan Sun",
      "Yang Zhang",
      "Ge Zhang",
      "Wenhu Chen",
      "Yu Su",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2409.02813",
    "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
    "itemCount": "3,460 questions (1,730 standard + 1,730 vision-only)",
    "specs": "Multimodal (Image/Screenshot), Multiple Choice (10 options), Vision-only input setting",
    "description": "A robust version of the MMMU benchmark designed to rigorously assess true multimodal understanding. It filters out text-only answerable questions, augments candidate options to 10 choices, and introduces a vision-only input setting (screenshots) to test the integration of visual and textual information."
  },
  {
    "id": "imported-1769500766499-201-mtrub",
    "title": "CharXiv",
    "source": "arXiv",
    "authors": [
      "Zirui Wang",
      "Mengzhou Xia",
      "Luxi He",
      "Howard Chen",
      "Yitao Liu",
      "Richard Zhu",
      "Kaiqu Liang",
      "Xindi Wu",
      "Haotian Liu",
      "Sadhika Malladi",
      "Alexis Chevalier",
      "Sanjeev Arora",
      "Danqi Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.18521",
    "githubLink": "https://github.com/princeton-nlp/CharXiv",
    "itemCount": "2,323 charts",
    "specs": "Images (scientific charts), Text (questions and answers); Includes descriptive and reasoning question types",
    "description": "A comprehensive evaluation suite designed to assess Multimodal Large Language Models (MLLMs) on realistic chart understanding. It features 2,323 diverse and challenging charts sourced from arXiv papers across various scientific disciplines. The benchmark includes two types of questions: descriptive questions that test basic element recognition, and reasoning questions that require synthesizing complex visual information."
  },
  {
    "id": "imported-1769500766499-202-7odj7",
    "title": "SimpleQA",
    "source": "arXiv",
    "authors": [
      "Jason Wei",
      "Khanh Nguyen",
      "OpenAI Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.04368",
    "githubLink": "https://github.com/openai/simple-evals",
    "itemCount": "4,326 questions",
    "specs": "Text modality; Short-form question answering; Single indisputable answer",
    "description": "A factuality benchmark that measures the ability of language models to answer short, fact-seeking questions. It focuses on high correctness and is designed to be challenging for frontier models (adversarially collected against GPT-4) while ensuring answers are easy to grade."
  },
  {
    "id": "imported-1769500766499-203-oo8e0",
    "title": "Chinese SimpleQA",
    "source": "arXiv",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Yingshui Tan",
      "Weixun Wang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.07140",
    "githubLink": "https://github.com/LivingFutureLab/ChineseSimpleQA",
    "itemCount": "Not specified (covers 6 major topics, 99 subtopics)",
    "specs": "Text modality (Chinese); Short-form question answering",
    "description": "The first comprehensive Chinese benchmark for evaluating the factuality of language models on short questions. It covers diverse topics and is designed to be high-quality, static, and easy to evaluate, effectively adapting the SimpleQA methodology to the Chinese language."
  },
  {
    "id": "imported-1769500766499-204-hdlxn",
    "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
    "source": "Scholar",
    "authors": [
      "Jiho Jin",
      "Jiseon Kim",
      "Nayeon Lee",
      "Haneul Yoo",
      "Alice Oh",
      "Hwaran Lee"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.tacl-1.29/",
    "githubLink": "https://github.com/jinjh0123/KoBBQ",
    "itemCount": "76,048 samples",
    "specs": "Korean text, Multiple Choice QA, 268 templates across 12 social categories",
    "description": "A Korean bias benchmark dataset adapted from BBQ to reflect Korean cultural contexts and social biases. It includes classes for direct translation, target modification (localization), and new categories specific to Korean culture."
  },
  {
    "id": "imported-1769500766499-205-648bx",
    "title": "Open-BBQ: Open-ended Bias Benchmark",
    "source": "arXiv",
    "authors": [
      "Zhao Liu",
      "Tian Xie",
      "Xueru Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.06134",
    "githubLink": "https://github.com/zhaoliu0914/LLM-Bias-Benchmark",
    "itemCount": "Expands BBQ (58k+ base)",
    "specs": "English text, Open-ended QA (Fill-in-the-blank, Short-answer)",
    "description": "An expansion of the BBQ dataset that includes open-ended question types (fill-in-the-blank and short-answer) to better detect unjustified stereotypes and assess how LLMs handle ambiguous scenarios without predefined answers."
  },
  {
    "id": "imported-1769500766499-206-uiswu",
    "title": "StrongREJECT",
    "source": "arXiv",
    "authors": [
      "Alexandra Souly",
      "Qingyuan Lu",
      "Dillon Bowen",
      "Tu Trinh",
      "Elvis Hsieh",
      "Sana Pandey",
      "Pieter Abbeel",
      "Justin Svegliato",
      "Scott Emmons",
      "Olivia Watkins",
      "Sam Toyer"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.10260",
    "githubLink": "https://github.com/alexandrasouly/strongreject",
    "itemCount": "313 prompts",
    "specs": "Text prompts covering 6 categories of harmful behavior (e.g., Illegal goods, Violence, Disinformation); includes an automated evaluator.",
    "description": "StrongREJECT is a benchmark designed to evaluate the susceptibility of Large Language Models (LLMs) to jailbreak attacks. It addresses the shortcomings of previous benchmarks by distinguishing between 'empty' jailbreaks (where models fail to refuse but provide no harmful info) and effective ones. The benchmark consists of a curated dataset of forbidden prompts and an automated evaluator that assesses both the refusal and the quality/specificity of the harmful response."
  },
  {
    "id": "imported-1769500766499-207-kfa5u",
    "title": "LAB-Bench (Language Agent Biology Benchmark)",
    "source": "arXiv",
    "authors": [
      "Jon M. Laurent",
      "Joseph D. Janizek",
      "Michael Ruzo",
      "Michaela M. Hinks",
      "Michael J. Hammerling",
      "Siddharth Narayanan",
      "Manvitha Ponnapati",
      "Andrew D. White",
      "Samuel G. Rodriques"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.10362",
    "githubLink": "https://github.com/Future-House/LAB-Bench",
    "itemCount": "2,457 questions",
    "specs": "Multiple-choice questions across 8 categories (LitQA2, DbQA, SuppQA, FigQA, TableQA, ProtocolQA, SeqQA, Cloning Scenarios); Modalities include text, technical images, and biological sequences (DNA/protein)",
    "description": "LAB-Bench is a large-scale evaluation dataset designed to measure the capabilities of AI systems for practical biology research. Unlike textbook-style benchmarks, it focuses on real-world tasks such as literature search, protocol planning, data analysis, and the interpretation of figures and biological sequences. It includes difficult 'Cloning Scenarios' that simulate complex molecular cloning workflows."
  },
  {
    "id": "imported-1769500766499-208-bc926",
    "title": "Long-form Virology Tasks",
    "source": "Scholar",
    "authors": [
      "SecureBio",
      "Deloitte",
      "Signature Science",
      "Anthropic"
    ],
    "year": "2024",
    "paperLink": "https://assets.anthropic.com/m/61e7d27f81849746/original/Claude-3-Opus-System-Card.pdf",
    "githubLink": "Not publicly available (Private/Internal)",
    "itemCount": "13 subtasks",
    "specs": "Agentic Tasks, Long-form text generation",
    "description": "A task-based agentic evaluation designed to test the end-to-end completion of complex pathogen acquisition processes, including workflow design and laboratory protocols. It is used to assess biological risks in frontier models."
  },
  {
    "id": "imported-1769500766499-209-mugnp",
    "title": "VHELM (Holistic Evaluation of Vision-Language Models)",
    "source": "arXiv",
    "authors": [
      "Stanford CRFM Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.07112",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "21 datasets (initial run)",
    "specs": "Vision-Language (Image+Text); 9 evaluation aspects",
    "description": "A benchmark extending HELM to Vision-Language Models (VLMs). It aggregates datasets to cover aspects such as visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety."
  },
  {
    "id": "imported-1769500766499-210-75sqs",
    "title": "HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly)",
    "source": "arXiv",
    "authors": [
      "Howard Yen",
      "Tianyu Gao",
      "Minmin Hou",
      "Ke Ding",
      "Daniel Fleischer",
      "Peter Izsak",
      "Moshe Wasserblat",
      "Danqi Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.02694",
    "githubLink": "https://github.com/princeton-nlp/HELMET",
    "itemCount": "7 categories",
    "specs": "Long-context text (up to 128k tokens); Model-based evaluation",
    "description": "A comprehensive benchmark for long-context language models (LCLMs) covering seven diverse, application-centric categories to address issues with synthetic tasks like needle-in-a-haystack."
  },
  {
    "id": "imported-1769500766499-211-irag3",
    "title": "HELM Safety",
    "source": "Scholar",
    "authors": [
      "Farzaan Kaiyom",
      "Ahmed Ahmed",
      "Yifan Mai",
      "Kevin Klyman",
      "Rishi Bommasani",
      "Percy Liang"
    ],
    "year": "2024",
    "paperLink": "https://crfm.stanford.edu/2024/11/08/helm-safety.html",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "5 safety benchmarks",
    "specs": "Text; 6 risk categories",
    "description": "A collection of safety benchmarks within the HELM ecosystem spanning risk categories such as violence, fraud, discrimination, sexual content, harassment, and deception."
  },
  {
    "id": "imported-1769500766499-212-w5rdl",
    "title": "InjecAgent",
    "source": "arXiv",
    "authors": [
      "Qicheng Zhan",
      "Himanshu Gupta",
      "Priyanka D. L.",
      "Daniel Kang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.07612",
    "githubLink": "https://github.com/uiuc-kang-lab/InjecAgent",
    "itemCount": "1,054 test cases",
    "specs": "Text (Agent traces, Tool outputs)",
    "description": "A benchmark for assessing indirect prompt injection attacks in tool-integrated LLM agents, featuring test cases across diverse user tools and domains to evaluate agent vulnerability."
  },
  {
    "id": "imported-1769500766499-213-5s8eu",
    "title": "PINT Benchmark (Prompt Injection Test)",
    "source": "Other",
    "authors": [
      "Lakera AI"
    ],
    "year": "2024",
    "paperLink": "https://github.com/lakeraai/pint-benchmark",
    "githubLink": "https://github.com/lakeraai/pint-benchmark",
    "itemCount": "3,000+ samples",
    "specs": "Text",
    "description": "A neutral benchmark for evaluating prompt injection detection systems, containing a mix of public datasets, proprietary injections, and jailbreaks to test false positives and negatives."
  },
  {
    "id": "imported-1769500766499-214-36tst",
    "title": "DSBench",
    "source": "arXiv",
    "authors": [
      "Liqiang Jing",
      "Zhehui Huang",
      "Xiaoyang Wang",
      "Wenlin Yao",
      "Wenhao Yu",
      "Kaixin Ma",
      "Hongming Zhang",
      "Xinya Du",
      "Dong Yu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2409.07703",
    "githubLink": "https://github.com/LiqiangJing/DSBench",
    "itemCount": "466 data analysis tasks, 74 data modeling tasks",
    "specs": "Multimodal (Text, Tables, Images), SQL/Python tasks",
    "description": "A comprehensive benchmark designed to evaluate data science agents with realistic tasks. It includes data analysis and data modeling tasks sourced from competitions, featuring long contexts and multi-table structures."
  },
  {
    "id": "imported-1769500766499-215-ml0fq",
    "title": "InfiAgent-DABench",
    "source": "arXiv",
    "authors": [
      "Xuechen Liu",
      "Zhaojie Zhang",
      "Yiming Geng",
      "Yuanhang Zhang",
      "Yijuan Lu",
      "Fei Wu",
      "Kun Kuang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://proceedings.mlr.press/v235/kuang24a.html",
    "githubLink": "https://github.com/InfiAgent/InfiAgent",
    "itemCount": "603 data analysis questions, 124 CSV files",
    "specs": "CSV data, Text questions, Closed-form answers",
    "description": "A benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. It contains DAEval, a dataset of questions derived from CSV files, formatted to allow automatic evaluation of open-ended questions."
  },
  {
    "id": "imported-1769500766499-216-bsl1t",
    "title": "DSEval",
    "source": "arXiv",
    "authors": [
      "Yuge Zhang",
      "Qiaozi Gao",
      "Lihong Li",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.17168",
    "githubLink": "https://github.com/YugeZhang/DSEval",
    "itemCount": "Based on 31 datasets (seeds)",
    "specs": "Text instructions, Data Analysis tasks",
    "description": "A benchmark tailored for assessing the performance of data science agents throughout the entire data science lifecycle, incorporating a novel bootstrapped annotation method."
  },
  {
    "id": "imported-1769500766499-217-hnam5",
    "title": "MMMLU (Multilingual Massive Multitask Language Understanding)",
    "source": "Hugging Face",
    "authors": [
      "OpenAI"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/openai/MMMLU",
    "githubLink": "https://github.com/openai/SimpleEvals",
    "itemCount": "14 languages (derived from 57 MMLU subjects)",
    "specs": "Text (Multilingual), Multiple Choice",
    "description": "A multilingual version of the MMLU benchmark, translated into 14 languages (including Yoruba, Swahili, etc.) using professional human translators to evaluate LLM performance across diverse linguistic contexts."
  },
  {
    "id": "imported-1769500766499-218-ni6ps",
    "title": "Global MMLU",
    "source": "arXiv",
    "authors": [
      "Shivalika Singh",
      "Angelika Romanou",
      "Clémentine Fourrier",
      "David I. Adelani",
      "Jian Gang Ngui",
      "Daniel Vila-Suero",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.03304",
    "githubLink": "https://huggingface.co/datasets/CohereForAI/Global-MMLU",
    "itemCount": "42 languages",
    "specs": "Text (Multilingual), Cultural/Regional Metadata",
    "description": "An improved multilingual evaluation benchmark covering 42 languages, designed to address cultural and linguistic biases found in translated benchmarks. It includes subsets for culturally sensitive and culturally agnostic questions."
  },
  {
    "id": "imported-1769500766499-219-g5vh9",
    "title": "R-Judge",
    "source": "arXiv",
    "authors": [
      "Tongxin Yuan",
      "Zhiwei He",
      "Lingzhong Dong",
      "Yiming Wang",
      "Ruijie Zhao",
      "Tian Xia",
      "Lizhen Xu",
      "Binglin Zhou",
      "Fangqi Li",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Gongshen Liu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.10019",
    "githubLink": "https://github.com/Lordog/R-Judge",
    "itemCount": "569 records",
    "specs": "Multi-turn agent interaction records containing user instructions, agent actions/observations, safety labels (safe/unsafe), and risk descriptions. Covers 27 risk scenarios across 5 application categories and 10 risk types.",
    "description": "A benchmark designed to evaluate the proficiency of Large Language Models (LLMs) in judging and identifying safety risks given agent interaction records. It focuses on the behavioral safety of LLM agents operating in interactive environments, requiring models to analyze multi-turn interactions and detect potential risks."
  },
  {
    "id": "imported-1769500766499-220-tu45g",
    "title": "CoSafe",
    "source": "arXiv",
    "authors": [
      "Erxin Yu",
      "Jing Li",
      "Ming Liao",
      "Siqi Wang",
      "Gao Zuchen",
      "Fei Mi",
      "Lanqing Hong"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.17626",
    "githubLink": "https://github.com/ErxinYu/CoSafe-Dataset",
    "itemCount": "1,400 dialogues",
    "specs": "Text-based multi-turn dialogues; 14 categories of harm (e.g., hate speech, violence) derived from the BeaverTails dataset.",
    "description": "A benchmark dataset designed to evaluate Large Language Model (LLM) safety specifically in multi-turn dialogue coreference scenarios. It assesses whether models remain safe when harmful intent is obscured by coreference (referring back to previous context) across multiple turns of conversation."
  },
  {
    "id": "imported-1769500766499-221-q8tu3",
    "title": "Agent Smith (Infectious Jailbreak Framework)",
    "source": "arXiv",
    "authors": [
      "Xiangming Gu",
      "Xiaosen Zheng",
      "Tianyu Pang",
      "Chao Du",
      "Qian Liu",
      "Ye Wang",
      "Jing Jiang",
      "Min Lin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.08567",
    "githubLink": "https://github.com/sail-sg/Agent-Smith",
    "itemCount": "Simulates up to 1 million agents; Uses subsets of ArtBench and AdvBench",
    "specs": "Multimodal (Image + Text); Adversarial Attacks; Multi-agent simulation code",
    "description": "A benchmark and simulation framework for evaluating 'infectious jailbreak' risks in multi-agent systems. It demonstrates how a single adversarial image can compromise an entire system of multimodal agents (e.g., LLaVA-1.5) exponentially fast. The benchmark typically uses ArtBench for image pools and AdvBench for target harmful behaviors."
  },
  {
    "id": "imported-1769500766499-222-jrs9e",
    "title": "SWE-smith",
    "source": "arXiv",
    "authors": [
      "Guangyu Yang",
      "Yuhao Zhu",
      "Seth Michael Segall",
      "Dhruv Joshi",
      "Clement Gehring",
      "Param Aggarwal",
      "Kishore Papineni"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.00000",
    "githubLink": "https://github.com/swe-smith/swe-smith",
    "itemCount": "50,000+ instances",
    "specs": "Text (Code/Python); Software Engineering Tasks (Bugs, Issues, Pull Requests)",
    "description": "A large-scale dataset and automated pipeline for generating software engineering training data for LLM agents. It focuses on creating 'bug-driven' tasks where agents must fix issues in Python repositories. It is designed to scale up data collection for software engineering agents beyond previous small-scale benchmarks like SWE-bench."
  },
  {
    "id": "imported-1769500766499-223-t9v68",
    "title": "CTI-Bench (Agent Smith Malware Subset)",
    "source": "Hugging Face",
    "authors": [
      "AI4Sec Team"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/AI4Sec/cti-bench",
    "githubLink": "https://github.com/AI4Sec/CTIBench",
    "itemCount": "Various (Multiple subsets including CTI-MCQ, CTI-RCM)",
    "specs": "Text; Multiple Choice Questions; Vulnerability Mapping",
    "description": "A comprehensive benchmark suite for Cyber Threat Intelligence (CTI). It includes data related to the 'Agent Smith' mobile malware (a real-world malware campaign) as part of its knowledge evaluation tasks (CTI-MCQ) and other components to test LLMs on security domain knowledge."
  },
  {
    "id": "imported-1769500766499-224-ld6i3",
    "title": "XSTest-Response",
    "source": "Hugging Face",
    "authors": [
      "Seungju Han",
      "Kavel Rao",
      "Allyson Ettinger",
      "Liwei Jiang",
      "Bill Yuchen Lin",
      "Nathan Lambert",
      "Yejin Choi",
      "Nouha Dziri"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.18495",
    "githubLink": "https://github.com/allenai/wildguard",
    "itemCount": "895 items (449 refusal split, 446 harmfulness split)",
    "specs": "Text prompts with corresponding model responses and classification labels (refusal/compliance, harmful/unharmful)",
    "description": "An extension of the XSTest dataset that includes model responses, designed to evaluate the accuracy of moderator models in detecting refusals and harmfulness."
  },
  {
    "id": "imported-1769500766499-225-r9jog",
    "title": "AfriMed-QA",
    "source": "arXiv",
    "authors": [
      "Tobi Olatunji",
      "Charles Nimo",
      "Abraham Owodunni",
      "Tassallah Abdullahi",
      "Emmanuel Ayodele",
      "Mercy Nyamewaa Asiedu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.15640",
    "githubLink": "https://github.com/intron-innovation/AfriMed-QA",
    "itemCount": "15,000+ questions",
    "specs": "Includes 4,000+ expert Multiple Choice Questions (MCQs), 1,200+ Open-Ended Short Answer Questions (SAQs), and 10,000 Consumer Queries (CQs). Covers 32 medical specialties across 16 African countries.",
    "description": "A large-scale Pan-African, multi-specialty medical Question-Answering (QA) benchmark dataset designed to evaluate Large Language Models (LLMs) in the context of African healthcare. The dataset aims to address the underrepresentation of African health data in LLM training and evaluation, covering diverse geographic and clinical contexts."
  },
  {
    "id": "imported-1769500766499-226-mz81r",
    "title": "EHRNoteQA",
    "source": "arXiv",
    "authors": [
      "Sunjun Kweon",
      "Jiyoun Kim",
      "Heeyoung Kwak",
      "Dongchul Cha",
      "Hangyul Yoon",
      "Kwanghyun Kim",
      "Jeewon Yang",
      "Seunghyun Won",
      "Edward Choi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.16040",
    "githubLink": "https://github.com/ji-youn-kim/EHRNoteQA",
    "itemCount": "962 QA pairs",
    "specs": "Text (Clinical Discharge Summaries); Open-ended and Multi-choice QA; 8 Clinical Topics",
    "description": "A benchmark built on MIMIC-IV EHR for evaluating Large Language Models (LLMs) in clinical settings. It consists of question-answer pairs linked to distinct patients' discharge summaries. Unlike previous benchmarks, it focuses on multi-document reasoning where questions often require information from multiple discharge summaries. The dataset includes both open-ended and multi-choice formats, with questions generated by GPT-4 and manually refined by clinicians to ensure relevance."
  },
  {
    "id": "imported-1769500766499-227-3mp95",
    "title": "WSI-Path",
    "source": "Hugging Face",
    "authors": [
      "Ahmed",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/google/medgemma-1.5-4b-it",
    "githubLink": "https://huggingface.co/google",
    "itemCount": "Single WSI examples (count unspecified in snippet)",
    "specs": "Image (Whole Slide Images) + Text",
    "description": "A dataset referenced in the development of MedGemma, consisting of de-identified H&E Whole Slide Images (WSIs) paired with associated final diagnosis text from original pathology reports. Used for training multimodal medical AI models."
  },
  {
    "id": "imported-1769500766499-228-yczvm",
    "title": "Student Lab Reports (MyReviewers Corpus)",
    "source": "Scholar",
    "authors": [
      "Alex Rudniy",
      "Joseph M. Moxley"
    ],
    "year": "2024",
    "paperLink": "https://wac.colostate.edu/journal-of-writing-analytics/vol7/",
    "githubLink": "https://github.com/arudniy/Analytics",
    "itemCount": "Multiple datasets (13 rubric datasets, 52 total)",
    "specs": "Text (Student Writing)",
    "description": "A dataset of student laboratory reports from STEM courses (Chemistry) used to train AI for automated scoring and feedback. It includes initial and final drafts, along with human-rated scores for specific rubric criteria."
  },
  {
    "id": "imported-1769500766499-229-0no56",
    "title": "CT-RATE",
    "source": "Hugging Face",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Anjany Sekuboyina",
      "Enis Simsar",
      "Alperen Tezcan",
      "Ayse Gulnihan Simsek",
      "Sevval Nil Esirgun",
      "Furkan Almas",
      "Irem Dogan",
      "Muhammed Furkan Dasdelen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.17834",
    "githubLink": "https://github.com/ibrahimethemhamamci/CT-CLIP",
    "itemCount": "50,188 3D CT volumes (25,692 unique scans)",
    "specs": "3D Chest CT volumes, Text (Radiology Reports), Multi-abnormality labels",
    "description": "A large-scale 3D medical imaging dataset that pairs non-contrast chest CT volumes with corresponding radiology text reports. It is designed to address the scarcity of comprehensive 3D medical datasets and enables the development of foundation models like CT-CLIP and CT-CHAT for tasks such as zero-shot abnormality detection and report generation."
  },
  {
    "id": "imported-1769500766499-230-myc5c",
    "title": "Japanese CT Report Dataset",
    "source": "arXiv",
    "authors": [
      "Yosuke Yamagishi",
      "Shohei Hanaoka",
      "Yukihiro Nomura",
      "Sohsuke Yoshimura",
      "Naoto Hayashi",
      "Osamu Abe"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.15907",
    "githubLink": "https://github.com/ibrahimethemhamamci/CT-CLIP",
    "itemCount": "22,778 Translated Reports",
    "specs": "Text (Japanese Radiology Reports)",
    "description": "A multilingual adaptation of the CT-RATE dataset, consisting of radiology reports translated into Japanese. It was created to facilitate the development of specialized Japanese medical language models and includes a subset of radiologist-revised reports for validation."
  },
  {
    "id": "imported-1769500766499-231-56w5a",
    "title": "GenerateCT Dataset",
    "source": "Other",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Anjany Sekuboyina",
      "Enis Simsar",
      "Alperen Tezcan",
      "Ayse Gulnihan Simsek",
      "Sevval Nil Esirgun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.15343",
    "githubLink": "https://github.com/ibrahimethemhamamci/GenerateCT",
    "itemCount": "2,286 Synthetic CT volumes",
    "specs": "3D Chest CT volumes (Synthetic), Text Prompts",
    "description": "A synthetic dataset generated using the GenerateCT framework, consisting of chest CT volumes and corresponding text prompts. It serves as a benchmark for evaluating text-conditional 3D medical image generation capabilities."
  },
  {
    "id": "imported-1769500766499-232-7spuv",
    "title": "PathMMU",
    "source": "Hugging Face",
    "authors": [
      "Yuehan Sun",
      "Wei Wang",
      "Kezou Jin",
      "Zhiyuan Li",
      "Jiaheng Liu",
      "Jiancheng Yang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.16355",
    "githubLink": "https://github.com/PathMMU-Benchmark/PathMMU",
    "itemCount": "33,428 QA pairs; 24,067 images",
    "specs": "Multimodal (Image + Text), Multiple Choice Questions (MCQ)",
    "description": "A massive multimodal expert-level benchmark for understanding and reasoning in pathology. It serves as a comprehensive resource for evaluating Large Multimodal Models (LMMs) in the pathology domain, featuring expert-validated QA pairs derived from authoritative sources."
  },
  {
    "id": "imported-1769500766499-233-3sqto",
    "title": "PathMCQA (MedGemma Evaluation Set)",
    "source": "Scholar",
    "authors": [
      "Google MedGemma Team",
      "Lin Yang",
      "Daniel Golden",
      "Andrew Sellergren"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.08644",
    "githubLink": "https://github.com/Google-Health/MedGemma",
    "itemCount": "450 patches (extracted from 354 WSIs)",
    "specs": "Multimodal (Image + Text), Multiple Choice Questions (MCQ)",
    "description": "An internal benchmark dataset used by Google to evaluate the MedGemma models. It focuses on histopathology image classification formulated as multiple-choice questions, covering tasks like identification, grading, and subtyping for breast, cervical, and prostate cancer."
  },
  {
    "id": "imported-1769500766499-234-oevby",
    "title": "MedExQA",
    "source": "arXiv",
    "authors": [
      "Yunsoo Kim",
      "Jinge Wu",
      "Yusuf Abdulle",
      "Honghan Wu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.10688",
    "githubLink": "https://github.com/UCL-Health-Informatics/MedExQA",
    "itemCount": "Not specified in snippet",
    "specs": "Text-based Question Answering with Explanations",
    "description": "A medical question-answering benchmark that includes multiple explanations for each answer to evaluate LLMs' understanding. While broad, it specifically covers underrepresented domains such as Speech Language Pathology."
  },
  {
    "id": "imported-1769500766499-235-tcpm3",
    "title": "WSI-Path (PathAlign Dataset)",
    "source": "arXiv",
    "authors": [
      "Faruk Ahmed",
      "Andrew Sellergren",
      "Lin Yang",
      "Shawn Xu",
      "Boris Babenko",
      "Abbi Ward",
      "Niels Olson",
      "Arash Mohtashamian",
      "Yossi Matias",
      "Greg S. Corrado",
      "Quang Duong",
      "Dale R. Webster",
      "Shravya Shetty",
      "Daniel Golden",
      "Yun Liu",
      "David F. Steiner",
      "Ellery Wulczyn"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.19578",
    "githubLink": "Not available",
    "itemCount": "350,000+ WSIs and diagnostic text pairs",
    "specs": "Whole Slide Images (WSI), Pathology Reports (Text), H&E Stain",
    "description": "A large-scale dataset of de-identified H&E whole slide images (WSIs) paired with final diagnosis text from original pathology reports. It was used to train the PathAlign vision-language model and serves as a foundation for tasks like text-to-image retrieval and zero-shot classification in computational pathology. The dataset covers a wide range of diagnoses, procedure types, and tissue types."
  },
  {
    "id": "imported-1769500766499-236-8924o",
    "title": "UCF-MultiOrgan-Path",
    "source": "Other",
    "authors": [
      "Md Sanzid Bin Hossain",
      "Yelena Piazza",
      "Jacob Braun",
      "Anthony Bilic",
      "Michael Hsieh",
      "Samir Fouissi",
      "Alexander Borowsky",
      "Hatem"
    ],
    "year": "2024",
    "paperLink": "https://www.medrxiv.org/content/10.1101/2024.11.05.24316736v1",
    "githubLink": "https://github.com/Md-Sanzid-Bin-Hossain/UCF-WSI-Dataset",
    "itemCount": "977 WSIs, ~2.38 million patches",
    "specs": "Whole Slide Images (WSI), Patches (512x512), 15 Organ Classes",
    "description": "A public benchmark dataset for multi-organ histopathologic image classification. Collected from cadavers over a decade, it addresses the limitations of existing datasets by providing a diverse collection of WSIs from 15 distinct organ classes (e.g., lung, kidney, liver, pancreas). It supports both patch-level and slide-level classification tasks."
  },
  {
    "id": "imported-1769500766499-237-pq0r9",
    "title": "DermaVQA",
    "source": "Other",
    "authors": [
      "Wen-wai Yim",
      "Yujuan Fu",
      "Zhaoyi Sun",
      "Asma Ben Abacha",
      "Meliha Yetisgen",
      "Fei Xia"
    ],
    "year": "2024",
    "paperLink": "https://papers.miccai.org/miccai-2024/paper/2444_paper.pdf",
    "githubLink": "https://github.com/velvinnn/DermaVQA",
    "itemCount": "~20,000+ QA pairs (across subsets)",
    "specs": "Image + Text (Visual Question Answering), Multilingual (English, Chinese, Spanish)",
    "description": "A multilingual visual question answering dataset for dermatology. It includes clinical dermatology textual queries and associated images from two subsets (IIYI and Reddit), designed to benchmark multimodal response generation."
  },
  {
    "id": "imported-1769500766499-238-jrdqn",
    "title": "BioLP-bench",
    "source": "Scholar",
    "authors": [
      "Igor Ivanov"
    ],
    "year": "2024",
    "paperLink": "https://www.biorxiv.org/content/10.1101/2024.08.21.608694v2",
    "githubLink": "https://github.com/baceolus/BioLP-bench",
    "itemCount": "800 test cases",
    "specs": "Open-ended QA, Text-based protocols",
    "description": "Evaluates the proficiency of language models in finding and correcting mistakes in biological laboratory protocols. Unlike multiple-choice benchmarks, it uses open-ended questions where models must identify failure-causing errors injected into real-world protocols."
  },
  {
    "id": "imported-1769500766499-239-ust05",
    "title": "IT-Troubleshooting-Dataset",
    "source": "Hugging Face",
    "authors": [
      "UmerSajid"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/UmerSajid/IT-Troubleshooting-Dataset",
    "githubLink": "N/A",
    "itemCount": "10k - 100k samples",
    "specs": "CSV format; Text modality; Columns include Category, Issue, Symptoms, Solution Steps, Severity, Common Causes",
    "description": "A dataset for IT troubleshooting tasks, specifically focused on cloud computing issues (e.g., AWS instance errors). It includes fields for issues, symptoms, solution steps, severity, and estimated resolution time, suitable for text classification and solution generation tasks."
  },
  {
    "id": "imported-1769500766499-240-9mxzr",
    "title": "GUANinE",
    "source": "Other",
    "authors": [
      "Eyes S. Robson",
      "Nilah M. Ioannidis"
    ],
    "year": "2024",
    "paperLink": "https://proceedings.mlr.press/v240/robson24a.html",
    "githubLink": "https://github.com/ni-lab/guanine",
    "itemCount": "Over 60 million training examples",
    "specs": "Genomic sequences, functional genomics tasks (promoter annotation, gene expression), text/sequence modalities",
    "description": "A large-scale, de-noised genomic AI benchmark designed to evaluate model generalization across distinct functional genomics tasks, including functional element annotation and gene expression prediction. It focuses on human (eukaryote) genomic complexity."
  },
  {
    "id": "imported-1769500766499-241-8tvgy",
    "title": "Genomics Long-Range Benchmark (LRB)",
    "source": "Hugging Face",
    "authors": [
      "Evan Trop",
      "Yair Schiff",
      "Alara Dirik",
      "Tyler Ross",
      "Reihaneh Rabbany",
      "Volodymyr Kuleshov",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://openreview.net/forum?id=9Xy3sYyXXy",
    "githubLink": "https://huggingface.co/datasets/InstaDeepAI/genomics-long-range-benchmark",
    "itemCount": "9 tasks",
    "specs": "Long-sequence DNA (up to ~100k+ bp contexts), tasks include classification and regression (e.g., CAGE, Histone Marks, Regulatory Elements).",
    "description": "A benchmark suite focused on biologically meaningful genomic tasks that require modeling long-range dependencies. It includes tasks like variant effect prediction, CAGE gene expression, and chromatin feature prediction, specifically tailored to evaluate long-context DNA language models."
  },
  {
    "id": "imported-1769500766499-242-3ze0o",
    "title": "SafeBench (Multimodal)",
    "source": "arXiv",
    "authors": [
      "Zonghao Ying",
      "Aishan Liu",
      "Siyuan Liang",
      "Lei Huang",
      "Jinyang Guo",
      "Wenbo Zhou",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.18927",
    "githubLink": "https://safebench-mm.github.io/",
    "itemCount": "2,300 multimodal harmful query pairs",
    "specs": "Multimodal (Text + Image)",
    "description": "A safety evaluation framework for Multimodal Large Language Models (MLLMs). It includes a comprehensive harmful query dataset covering risk scenarios like child abuse, illegal activities, and violence, using a jury deliberation protocol for evaluation."
  },
  {
    "id": "imported-1769500766499-243-fbdsu",
    "title": "PHVSpec: Benchmark for Video Hashing",
    "source": "Other",
    "authors": [
      "Tech Coalition"
    ],
    "year": "2024",
    "paperLink": "https://www.technologycoalition.org/",
    "githubLink": "https://github.com/TechCoalition",
    "itemCount": "N/A (Algorithm Benchmark)",
    "specs": "Video Hashing Algorithms, Benchmarking Framework",
    "description": "A benchmark framework developed by the Tech Coalition to analyze the effectiveness of perceptual hash algorithms (like PDQ and TMK+PDQF) for detecting CSAM in videos. It focuses on evaluating the robustness of hashing systems against attacks and edits."
  },
  {
    "id": "imported-1769500766499-244-662fv",
    "title": "CatQA (Categorical Harmful Questions)",
    "source": "Hugging Face",
    "authors": [
      "Rishabh Bhardwaj",
      "Doo Hee Jung",
      "Soujanya Poria"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.11746",
    "githubLink": "https://github.com/declare-lab/categorical-harmful-qa",
    "itemCount": "550 questions (subset related to grooming)",
    "specs": "Text (Q&A/Prompts)",
    "description": "A safety evaluation dataset for Large Language Models (LLMs) comprising 550 harmful questions across various categories. It includes a specific 'Child Abuse' category with scenarios related to online grooming to test model refusal and safety alignment."
  },
  {
    "id": "imported-1769500766499-245-j533m",
    "title": "OR-Bench (Over-Refusal Benchmark)",
    "source": "arXiv",
    "authors": [
      "Justin Cui",
      "Ruoxi Jia",
      "Cho-Jui Hsieh"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.20947",
    "githubLink": "https://github.com/justincui03/or-bench",
    "itemCount": "80,000 synthetic prompts, 1,000 hard prompts, 600 toxic prompts",
    "specs": "Text prompts; 10 common rejection categories (e.g., medical, legal advice)",
    "description": "A large-scale benchmark specifically designed to measure 'over-refusal' behavior in LLMs, where models incorrectly reject benign prompts. It includes a synthetic dataset of 80,000 prompts across 10 rejection categories, a 'hard' subset of ~1,000 prompts, and a toxic control set to ensure safety is maintained."
  },
  {
    "id": "imported-1769500766499-246-z2iai",
    "title": "SORRY-Bench",
    "source": "arXiv",
    "authors": [
      "Tinghao Xie",
      "Xiangyu Qi",
      "Yi Zeng",
      "Yangsibo Huang",
      "Udari Madhushani",
      "Prateek Mittal"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.14598",
    "githubLink": "https://github.com/sorry-bench/sorry-bench",
    "itemCount": "440 base unsafe instructions; ~9,000 linguistically mutated variants; 7,000+ human annotations",
    "specs": "Text instructions; 45 classes (44 unsafe + 1 safe); Linguistic mutations",
    "description": "A systematic benchmark for evaluating LLM safety refusal behaviors. It utilizes a fine-grained taxonomy of 44 unsafe topics and applies 20 diverse linguistic mutations (e.g., slang, typos, dialects) to ensure models refuse unsafe content regardless of how it is phrased."
  },
  {
    "id": "imported-1769500766499-247-w5f9r",
    "title": "BrowserART (Browser Agent Red-teaming Toolkit)",
    "source": "Hugging Face",
    "authors": [
      "Priyanshu Kumar",
      "Elaine Lau",
      "Saranya Vijayakumar",
      "Tu Trinh",
      "Elaine Chang",
      "Vaughn Robinson",
      "Sean Hendryx",
      "Shuyan Zhou",
      "Matt Fredrikson",
      "Summer Yue",
      "Zifan Wang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.13886",
    "githubLink": "https://github.com/scaleapi/browser-art",
    "itemCount": "100 harmful browser-related behaviors",
    "specs": "Harmful behaviors targeting 40 synthetic websites (hosted locally) and real websites",
    "description": "A red teaming test suite designed for browser agents, consisting of harmful behaviors aimed at synthetic and real websites. It highlights the safety gap between refusal-trained LLMs and their agentic counterparts in browser environments."
  },
  {
    "id": "imported-1769500766499-248-7vaw5",
    "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
    "source": "arXiv",
    "authors": [
      "Andy K. Zhang",
      "Neil Perry",
      "Riya Dulepet",
      "Joey Ji",
      "Celeste Menders",
      "Justin W. Lin",
      "Eliot Jones",
      "Gashon Hussein",
      "Samantha Liu",
      "Donovan Jasper",
      "Percy Liang",
      "Daniel E. Ho",
      "Dan Boneh"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.08926",
    "githubLink": "https://github.com/cybench/cybench",
    "itemCount": "40 tasks",
    "specs": "CTF challenges involving text descriptions, local/remote starter files, Docker environments, and subtask breakdowns.",
    "description": "A cybersecurity benchmark for evaluating language model agents. It consists of 40 professional-level Capture the Flag (CTF) tasks chosen to be recent, meaningful, and spanning a wide range of difficulties (including cryptography, web security, reverse engineering, forensics, and exploitation). The framework includes task descriptions, starter files, and automated evaluators."
  },
  {
    "id": "imported-1769500766499-249-3i0cc",
    "title": "AI Sandbagging Dataset / Password-Locked Models",
    "source": "arXiv",
    "authors": [
      "Teun van der Weij",
      "Felix Hofstätter",
      "Oliver Jaffe",
      "Samuel F. Brown",
      "Francis Rhys Ward"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.07358",
    "githubLink": "https://github.com/TeunvdWeij/sandbagging",
    "itemCount": "Includes 9,470 CSQA, 14,042 MMLU, and 3,668 WMDP samples (subsets used for training/eval)",
    "specs": "Text (Multiple Choice Questions, Synthetic Prompts)",
    "description": "A set of synthetic datasets and methodologies used to fine-tune language models (like GPT-4 and Claude 3) to 'sandbag' (strategically underperform) on specific benchmarks. The research demonstrates models can be 'password-locked' to hide capabilities unless a specific string is present."
  },
  {
    "id": "imported-1769500766499-250-80o6d",
    "title": "Subversion Strategy Eval (SSE)",
    "source": "arXiv",
    "authors": [
      "Alex Mallen",
      "Charlie Griffin",
      "Alessandro Abate",
      "Buck Shlegeris"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.12480",
    "githubLink": "https://github.com/safety-research/subversion-strategy-eval",
    "itemCount": "8 environments",
    "specs": "Text-based reinforcement learning environments; evaluates stateless agents",
    "description": "A benchmark consisting of eight environments designed to model AI control protocols and evaluate whether stateless language models can strategize to subvert them. It assesses capabilities such as optimality in planning, reliability, probability calibration, and acausal coordination."
  },
  {
    "id": "imported-1769500766499-251-xj3fe",
    "title": "JailBreakV-28K",
    "source": "arXiv",
    "authors": [
      "Weidi Luo",
      "Siyuan Ma",
      "Xiaogeng Liu",
      "Xiaoyu Guo",
      "Chaowei Xiao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.03027",
    "githubLink": "https://github.com/JailBreakV/JailBreakV-28K",
    "itemCount": "28,000 samples",
    "specs": "20,000 text-based transfer attacks, 8,000 image-based attacks; text-image pairs",
    "description": "A comprehensive benchmark designed to assess the transferability of jailbreak attacks from LLMs to Multimodal Large Language Models (MLLMs). It evaluates alignment robustness against text-based and image-based attacks."
  },
  {
    "id": "imported-1769500766499-252-ica7m",
    "title": "MoralBench",
    "source": "arXiv",
    "authors": [
      "Jianchao Ji",
      "Yutong Chen",
      "Mingyu Jin",
      "Wujiang Xu",
      "Wenyue Hua",
      "Yongfeng Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.04428",
    "githubLink": "https://github.com/Moral-Bench/MoralBench",
    "itemCount": "Not specified in snippet",
    "specs": "Text-based moral dilemmas; derived from MFQ-30 and MFV psychometric tools",
    "description": "A benchmark for evaluating the moral identity and reasoning of LLMs using Moral Foundations Theory, which includes the 'Authority/Subversion' foundation. It assesses how models navigate ethical dilemmas and align with human moral standards."
  },
  {
    "id": "imported-1769500766499-253-6liw8",
    "title": "CyberSecEval 3 (including CyberSecEval 2)",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Yue Li",
      "Cyrus Nikolaidis",
      "Daniel Song",
      "Shengye Wan",
      "Faizan Ahmad",
      "Cornelius Aschermann",
      "Yaohui Chen",
      "Dhaval Kapil",
      "David Molnar",
      "Spencer Whitman",
      "Joshua Saxe"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.01605",
    "githubLink": "https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks",
    "itemCount": "Multiple test suites; e.g., 585 samples for VulnerabilityExploit subset, thousands of prompts for injection tests",
    "specs": "Text and Code (C, Python, Javascript, SQL); Categories: Prompt Injection, Insecure Code, Interpreter Abuse, Cyberattack Helpfulness",
    "description": "A comprehensive benchmark suite to quantify LLM security risks and capabilities, including prompt injection, code interpreter abuse, and offensive capabilities like automated social engineering and scaling offensive cyber operations. It builds upon CyberSecEval 2."
  },
  {
    "id": "imported-1769500766499-254-v2vte",
    "title": "NYU CTF Bench",
    "source": "arXiv",
    "authors": [
      "Minghao Shao",
      "Sofija Jancheska",
      "Meet Udeshi",
      "Brendan Dolan-Gavitt",
      "Haoran Xi",
      "Kimberly Milner",
      "Boyuan Chen",
      "Max Yin",
      "Siddharth Garg",
      "Prashanth Krishnamurthy",
      "Farshad Khorrami",
      "Ramesh Karri",
      "Muhammad Shafique"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.05590",
    "githubLink": "https://github.com/NYU-LLM-CTF/NYU_CTF_Bench",
    "itemCount": "200 challenges",
    "specs": "CTF Challenges (Dockerized); Modalities: Text, Code, Binary Interaction",
    "description": "A scalable, open-source benchmark dataset designed to evaluate LLMs in offensive security. It features verified CTF challenges hosted in Docker containers to test agents' task planning and vulnerability exploitation skills."
  },
  {
    "id": "imported-1769500766499-255-q8mgf",
    "title": "SaTML LLM Capture-the-Flag Competition Dataset",
    "source": "arXiv",
    "authors": [
      "Edoardo Debenedetti",
      "Javier Rando",
      "Daniel Paleka",
      "Fineas Silaghi",
      "Dragos Albastroiu",
      "Niv Cohen",
      "Yuval Lemberg",
      "Reshmi Ghosh",
      "Rui Wen",
      "Ahmed Salem",
      "Giovanni Cherubin",
      "Santiago Zanella-Beguelin",
      "Robin Schmid",
      "Victor Klemm",
      "Takahiro Miki",
      "Chenhao Li",
      "Stefan Kraft"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.07954",
    "githubLink": "https://github.com/ethz-spylab/satml-llm-ctf",
    "itemCount": "137,000+ attack chats",
    "specs": "Text-based chat logs (attacks and model responses); focused on prompt injection and defense evasion.",
    "description": "A dataset containing over 137,000 multi-turn attack chats from a CTF competition focused on LLM prompt injection and secret leaking. It includes successful and unsuccessful attacks against various defense strategies."
  },
  {
    "id": "imported-1769500766500-256-ladim",
    "title": "CVQA (Culturally-diverse Multilingual Visual Question Answering)",
    "source": "Hugging Face",
    "authors": [
      "David Romero",
      "Chenyang Lyu",
      "Haryo Akbarianto Wibowo",
      "Teresa Lynn",
      "Injy Hamed",
      "Aditya Nanda Kishore"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.10042",
    "githubLink": "https://github.com/mbzuai-nlp/CVQA",
    "itemCount": "10,000 questions across 30 countries",
    "specs": "Multimodal (Image + Multilingual Text), 31 languages",
    "description": "A novel benchmark for Visual Multilingual QA (VMQA) that focuses on cultural diversity. It includes questions across 30 countries and 31 languages, designed to benchmark Multilingual Multimodal Large Language Models (MLLMs)."
  },
  {
    "id": "imported-1769500766500-257-3m1eq",
    "title": "Open Prompt Injection",
    "source": "Scholar",
    "authors": [
      "Yupei Liu",
      "Yuqi Jia",
      "Runpeng Geng",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ],
    "year": "2024",
    "paperLink": "https://www.usenix.org/conference/usenixsecurity24/presentation/liu-yupei",
    "githubLink": "https://github.com/liu00222/Open-Prompt-Injection",
    "itemCount": "N/A (Framework with 5 attacks, 10 defenses)",
    "specs": "Text (Framework, attack/defense implementations)",
    "description": "A toolkit and benchmark that formalizes prompt injection attacks and defenses. It evaluates attacks and defenses across multiple LLMs and tasks, providing a systematic framework for quantitative evaluation."
  },
  {
    "id": "imported-1769500766500-258-rt3ez",
    "title": "EasyJailbreak",
    "source": "arXiv",
    "authors": [
      "Weikang Zhou",
      "Xiao Wang",
      "Limao Xiong",
      "Han Xia",
      "Yingshuang Gu",
      "Mingxu Chai",
      "Fukang Zhu",
      "Caishuang Huang",
      "Shihan Dou",
      "Zhiheng Xi",
      "Rui Zheng",
      "Songyang Gao",
      "Yicheng Zou",
      "Hang Yan",
      "Yifan Le",
      "Ruohui Wang",
      "Lijun Li",
      "Jing Shao",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.12171",
    "githubLink": "https://github.com/EasyJailbreak/EasyJailbreak",
    "itemCount": "1k - 10k samples (varies by sub-dataset)",
    "specs": "Text; Parquet format (Hugging Face)",
    "description": "A unified framework designed to simplify the construction and evaluation of jailbreak attacks against Large Language Models (LLMs). It decomposes the jailbreaking process into modular steps (Selector, Mutator, Constraint, Evaluator) and supports 11 distinct jailbreak methods. The framework includes a dataset component for storing and managing malicious queries."
  },
  {
    "id": "imported-1769500766500-259-rd7ex",
    "title": "WildJailbreak",
    "source": "Hugging Face",
    "authors": [
      "Liwei Jiang",
      "Kavel Rao",
      "Seungju Han",
      "Allyson Ettinger",
      "Faeze Brahman",
      "Sachin Kumar",
      "Niloofar Mireshghallah",
      "Ximing Lu",
      "Maarten Sap",
      "Yejin Choi",
      "Nouha Dziri"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.18510",
    "githubLink": "https://huggingface.co/datasets/allenai/wildjailbreak",
    "itemCount": "262k prompt-response pairs",
    "specs": "Text; Parquet format",
    "description": "A large-scale open-source synthetic safety-training dataset developed as part of the WildTeaming framework. It contains vanilla and adversarial prompt-response pairs, including harmful queries and benign queries that resemble harmful ones, to train models for balanced safety without over-refusal."
  },
  {
    "id": "imported-1769500766500-260-4m2y7",
    "title": "MaliciousInstruct",
    "source": "Hugging Face",
    "authors": [
      "Yangsibo Huang",
      "Samyak Gupta",
      "Mengzhou Xia",
      "Kai Li",
      "Danqi Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2310.06987",
    "githubLink": "https://huggingface.co/datasets/walledai/MaliciousInstruct",
    "itemCount": "100 malicious instructions",
    "specs": "Text; Parquet format",
    "description": "A dataset introduced to evaluate the 'Catastrophic Jailbreak' of open-source LLMs via generation exploitation. It contains 100 malicious instructions covering 10 different malicious intents, designed to test if models can be manipulated into generating harmful content through specific decoding strategies."
  },
  {
    "id": "imported-1769500766500-261-79apw",
    "title": "VoiceJailbreak",
    "source": "arXiv",
    "authors": [
      "Xinyue Shen",
      "Yixin Wu",
      "Yang Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.19103",
    "githubLink": "https://github.com/TrustAIRLab/VoiceJailbreakAttack",
    "itemCount": "Based on 520 harmful instructions (AdvBench), generating multiple audio prompt variations",
    "specs": "Audio, Text (based on AdvBench)",
    "description": "A study and dataset focusing on 'humanizing' GPT-4o to bypass safety guardrails through fictional storytelling in the audio modality. The benchmark uses a set of forbidden questions converted into persuasive audio narratives to test model resistance."
  },
  {
    "id": "imported-1769500766500-262-oy1km",
    "title": "Chat-Audio Attacks (CAA) Benchmark",
    "source": "arXiv",
    "authors": [
      "Zhengliang Liu",
      "Yuzhong Chen",
      "Zihao Wu",
      "Pengshuai Yin",
      "Xianfeng Yang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.14081",
    "githubLink": "https://github.com/Lizhengliang-PF/Chat-Audio-Attacks",
    "itemCount": "1,680 adversarial audio samples (360 attack sets)",
    "specs": "Audio (adversarial samples), Text transcripts",
    "description": "A benchmark for evaluating LLM robustness against universal audio adversarial attacks in conversational scenarios. It categorizes attacks into content, emotional, explicit noise, and implicit noise types."
  },
  {
    "id": "imported-1769500766500-263-pbkzx",
    "title": "PKU-SafeRLHF",
    "source": "Hugging Face",
    "authors": [
      "Jiaming Ji",
      "Donghai Hong",
      "Borong Zhang",
      "Boyuan Chen",
      "Josef Dai",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.15513",
    "githubLink": "https://github.com/PKU-Alignment/safe-rlhf",
    "itemCount": "~83,400 preference entries",
    "specs": "Text, Multi-dimensional safety constraints, Preference pairs",
    "description": "A human-labeled dataset containing both performance and safety preferences, with constraints across 19 harm categories. Designed for fine-grained value alignment and safety-centric RLHF."
  },
  {
    "id": "imported-1769500766500-264-lxh8f",
    "title": "TrustLLM",
    "source": "Other",
    "authors": [
      "Lichao Sun",
      "Yue Huang",
      "Haoran Wang",
      "Siyuan Wu",
      "Qihui Zhang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.05561",
    "githubLink": "https://github.com/HowieHwong/TrustLLM",
    "itemCount": "30+ datasets included",
    "specs": "Text, Multi-dimensional evaluation metrics",
    "description": "A comprehensive benchmark for evaluating the trustworthiness of LLMs across six dimensions: truthfulness, safety, fairness, robustness, privacy, and machine ethics."
  },
  {
    "id": "imported-1769500766500-265-4k2qw",
    "title": "HelpSteer2",
    "source": "Hugging Face",
    "authors": [
      "Zhilin Wang",
      "Yi Dong",
      "Jiaqi Zeng",
      "Virginia Adams",
      "Makesh Narsimhan Sreedhar",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.08673",
    "githubLink": "https://huggingface.co/datasets/nvidia/HelpSteer2",
    "itemCount": "~10,000 response pairs",
    "specs": "Text, Multi-attribute scores",
    "description": "A permissively licensed preference dataset for training reward models. It contains response pairs with multi-attribute labels (helpfulness, correctness, coherence, complexity, verbosity) to steer model alignment."
  },
  {
    "id": "imported-1769500766500-266-mbrop",
    "title": "M³oralBench",
    "source": "arXiv",
    "authors": [
      "Haojun Bei",
      "Yao Wan",
      "Ye Huang",
      "Yulei Sui",
      "Xiangliang Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.13264",
    "githubLink": "https://github.com/BeiiiY/M3oralBench",
    "itemCount": "3,200 text-image pairs",
    "specs": "Multimodal (Text + Image); Moral judgement classification",
    "description": "A comprehensive multimodal benchmark for evaluating moral judgment in Large Vision-Language Models (LVLMs). It combines text and images to test moral reasoning across various scenarios derived from Moral Foundations Theory."
  },
  {
    "id": "imported-1769500766500-267-pqd5a",
    "title": "CBT-Bench",
    "source": "Hugging Face",
    "authors": [
      "Mian Zhang",
      "Xianjun Yang",
      "Xinlu Zhang",
      "Travis Labrum",
      "Jamie C. Chiu",
      "Shaun M. Eack",
      "Fei Fang",
      "William Yang Wang",
      "Zhiyu Zoey Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.13218",
    "githubLink": "https://huggingface.co/datasets/Psychotherapy-LLM/CBT-Bench",
    "itemCount": "~1,705 examples (across multiple tasks)",
    "specs": "Text (Multiple Choice, Classification, Generation)",
    "description": "A benchmark for evaluating Large Language Models on their ability to assist in Cognitive Behavioral Therapy (CBT), covering knowledge acquisition, cognitive model understanding, and therapeutic response generation."
  },
  {
    "id": "imported-1769500766500-268-keemu",
    "title": "GOAT-Bench",
    "source": "arXiv",
    "authors": [
      "Hongzhan Lin",
      "Ziyang Luo",
      "Bo Wang",
      "Ruichao Yang",
      "Jing Ma"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.01523",
    "githubLink": "https://github.com/HKBU-NLP/GOAT-Bench",
    "itemCount": "Over 6,000 memes",
    "specs": "Multimodal (Image + Text)",
    "description": "A comprehensive meme-based benchmark designed to evaluate Large Multimodal Models on detecting social abuse, including hate speech, misogyny, and offensive content in memes."
  },
  {
    "id": "imported-1769500766500-269-f0bsv",
    "title": "KG-FPQ",
    "source": "arXiv",
    "authors": [
      "Yanxu Zhu",
      "Jinlin Xiao",
      "Yuhang Wang",
      "Jitao Sang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.05868",
    "githubLink": "https://github.com/yanxuzhu/KG-FPQ",
    "itemCount": "~178,000 questions",
    "specs": "Covers 3 knowledge domains, 6 levels of confusability, and 2 task formats.",
    "description": "A comprehensive benchmark constructed using an automated pipeline based on Knowledge Graphs (KGs). It aims to evaluate factuality hallucination in LLMs caused by false premise questions."
  },
  {
    "id": "imported-1769500766500-270-fte7t",
    "title": "FPQA (Yuan et al.)",
    "source": "Semantic Scholar",
    "authors": [
      "Hongbang Yuan",
      "Yubo Chen",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Kang Liu",
      "Jun Zhao"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2025.findings-acl.530/",
    "githubLink": "https://github.com/ysw1021/NASA",
    "itemCount": "986 questions",
    "specs": "Questions with false premises intended for Out-Of-Domain (OOD) factuality evaluation.",
    "description": "A dataset used to evaluate model factuality when faced with unanswerable questions containing false premises. Often used in studies regarding preference learning and alignment."
  },
  {
    "id": "imported-1769500766500-271-daf1y",
    "title": "LongFact",
    "source": "arXiv",
    "authors": [
      "Jerry Wei",
      "Chengrun Yang",
      "Xinying Song",
      "Yifeng Lu",
      "Nathan Hu",
      "Jie Huang",
      "Da Huang",
      "Li Dong",
      "Yu Cheng",
      "Quoc V. Le",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.18802",
    "githubLink": "https://github.com/google-deepmind/long-form-factuality",
    "itemCount": "2,280 prompts",
    "specs": "Text-only prompts; 38 topics; 2 sub-tasks (Concepts and Objects) with 1,140 prompts each.",
    "description": "LongFact is a benchmark designed to evaluate long-form factuality in large language models. It consists of 2,280 fact-seeking prompts across 38 diverse topics (e.g., STEM, social sciences, humanities). The benchmark is divided into two tasks: LongFact-Concepts and LongFact-Objects, distinguishing between questions about general concepts and specific objects. It is often used in conjunction with SAFE (Search-Augmented Factuality Evaluator), an automated method using LLM agents to verify facts against Google Search results."
  },
  {
    "id": "imported-1769500766500-272-tg805",
    "title": "Factcheck-Bench (Factcheck-GPT)",
    "source": "Semantic Scholar",
    "authors": [
      "Yuxia Wang",
      "Revanth Gangi Reddy",
      "Zain Muhammad Mujahid",
      "Arnav Arora",
      "Aleksandr Rubashevskii",
      "Jiahui Geng",
      "Osama Mohammed Afzal",
      "Liangming Pan",
      "Nadav Borenstein",
      "Aditya Pillai"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.findings-emnlp.112/",
    "githubLink": "https://github.com/yuxiaw/Factcheck-GPT",
    "itemCount": "94 prompts / 678 claims",
    "specs": "Text (Open-ended questions); Fine-grained human annotation (claim-level)",
    "description": "A fine-grained evaluation benchmark for automatic fact-checkers. It features detailed human annotations on open-ended questions to evaluate the capabilities of systems in detecting and correcting factual errors in long documents."
  },
  {
    "id": "imported-1769500766500-273-p70yv",
    "title": "LLM-AggreFact",
    "source": "Hugging Face",
    "authors": [
      "Liyan Tang",
      "Philippe Laban",
      "Greg Durrett"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.10774",
    "githubLink": "https://github.com/Liyan06/AggreFact",
    "itemCount": "Aggregates 11 datasets",
    "specs": "Text (Summarization, QA); Grounded factuality evaluation",
    "description": "A benchmark that unifies 11 publicly available datasets on factual consistency evaluation and grounding. It is designed to test LLMs' ability to assess whether statements are supported by evidence documents (MiniCheck)."
  },
  {
    "id": "imported-1769500766500-274-udytm",
    "title": "FactBench",
    "source": "arXiv",
    "authors": [
      "V. Bayat",
      "Y. Wang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.22257",
    "githubLink": "https://github.com/launchnlp/FactBench",
    "itemCount": "1,000 prompts (150 topics)",
    "specs": "Text; Dynamic/In-the-wild prompts",
    "description": "A dynamic benchmark grounded in real-world usage of LMs, consisting of prompts that frequently elicit hallucinations. It is designed to be regularly updated and covers 'in-the-wild' user interactions."
  },
  {
    "id": "imported-1769500766500-275-1vrpc",
    "title": "AlpacaEval",
    "source": "arXiv",
    "authors": [
      "Yann Dubois",
      "Balázs Galambosi",
      "Percy Liang",
      "Tatsunori B. Hashimoto"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.04475",
    "githubLink": "https://github.com/tatsu-lab/alpaca_eval",
    "itemCount": "805 instructions",
    "specs": "Instruction following, Text, Pairwise comparison",
    "description": "An automatic evaluator for instruction-following models that measures win rates against a reference model (e.g., GPT-4 Turbo) using an LLM-as-a-judge. It simplifies the AlpacaFarm dataset for faster and cheaper evaluation."
  },
  {
    "id": "imported-1769500766500-276-s6n65",
    "title": "RewardBench",
    "source": "arXiv",
    "authors": [
      "Nathan Lambert",
      "Valentina Pyatkin",
      "Jacob Morrison",
      "LJ Miranda",
      "Bill Yuchen Lin",
      "Khyathi Chandu",
      "Nouha Dziri",
      "Sachin Kumar",
      "Tom Zick",
      "Yejin Choi",
      "Noah A. Smith",
      "Hannaneh Hajishirzi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.13913",
    "githubLink": "https://github.com/allenai/reward-bench",
    "itemCount": "2,985 samples",
    "specs": "Text, Pairwise comparison, Reward modeling",
    "description": "A comprehensive benchmark designed to evaluate reward models (which often serve as judges) across multiple categories including Chat, Chat Hard, Safety, and Reasoning. It highlights the limitations of using a single metric for reward modeling."
  },
  {
    "id": "imported-1769500766500-277-bs6hc",
    "title": "Arena-Hard-Auto",
    "source": "arXiv",
    "authors": [
      "Tianle Li",
      "Wei-Lin Chiang",
      "Evan Frick",
      "Lisa Dunlap",
      "Tianhao Wu",
      "Banghua Zhu",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.11939",
    "githubLink": "https://github.com/lmarena-ai/arena-hard-auto",
    "itemCount": "500 prompts",
    "specs": "Text, Open-ended generation, Pairwise comparison",
    "description": "A benchmark consisting of challenging prompts curated from live Chatbot Arena data. It employs an automated pipeline (BenchBuilder) to select high-quality prompts that differentiate model performance effectively."
  },
  {
    "id": "imported-1769500766500-278-qrpqi",
    "title": "WildBench",
    "source": "arXiv",
    "authors": [
      "Bill Yuchen Lin",
      "Yuntian Deng",
      "Khyathi Chandu",
      "Faeze Brahman",
      "Abhilasha Ravichander",
      "Valentina Pyatkin",
      "Nouha Dziri",
      "Ronan Le Bras",
      "Yejin Choi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.04770",
    "githubLink": "https://github.com/allenai/WildBench",
    "itemCount": "1,024 tasks",
    "specs": "Real-world queries, Text, Score-based/Pairwise",
    "description": "An automated evaluation framework benchmarking LLMs on challenging tasks derived from real-world user queries (WildChat). It uses advanced metrics like WB-Reward and WB-Score to correlate with human judgment."
  },
  {
    "id": "imported-1769500766500-279-0warl",
    "title": "BiGGen-Bench",
    "source": "arXiv",
    "authors": [
      "Seungone Kim",
      "Juyoung Suk",
      "Ji Yong Cho",
      "Shayne Longpre",
      "Chaeeun Kim",
      "Dongkeun Yoon",
      "Guijin Son",
      "Yejin Cho",
      "Sheikh Shafayat",
      "Jinheon Baek",
      "Sue Hyun Park",
      "Hyeonbin Hwang",
      "Jinkyung Jo",
      "Hyowon Cho",
      "Haebin Shin",
      "Seongyun Lee",
      "Hanseok Oh",
      "Noah Lee",
      "Namgyu Ho",
      "Se June Joo",
      "Miyoung Ko",
      "Yoonjoo Lee",
      "Hyungjoo Chae",
      "Jamin Shin",
      "Joel Jang",
      "Seonghyeon Ye",
      "Bill Yuchen Lin",
      "Sean Welleck",
      "Graham Neubig",
      "Moontae Lee",
      "Kyungjae Lee",
      "Minjoon Seo"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.06658",
    "githubLink": "https://github.com/test/test",
    "itemCount": "77 tasks (765 instances)",
    "specs": "Text, Fine-grained evaluation, Instance-specific criteria",
    "description": "A principled benchmark for fine-grained evaluation of language models across nine capabilities. It uses instance-specific evaluation criteria to provide more nuanced assessments than general rubrics."
  },
  {
    "id": "imported-1769500766500-280-tjm4l",
    "title": "Aegis AI Content Safety Dataset",
    "source": "Hugging Face",
    "authors": [
      "Shaona Ghosh",
      "Prasoon Varshney",
      "Eric Zhu",
      "Dwaraknath Gnaneshwar",
      "Christopher Parisien"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.05993",
    "githubLink": "https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0",
    "itemCount": "33,416 interactions (v2.0)",
    "specs": "Text; 13 risk categories; Annotation taxonomy",
    "description": "A large-scale content safety dataset comprising manually annotated interactions between humans and LLMs, covering 13 critical risk categories to support guardrail development."
  },
  {
    "id": "imported-1769500766500-281-j2qsa",
    "title": "M3oralBench: A MultiModal Moral Benchmark for LVLMs",
    "source": "arXiv",
    "authors": [
      "Bei Yan",
      "Jie Zhang",
      "Zheng Chen",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.20718",
    "githubLink": "https://github.com/BeiiiY/M3oralBench",
    "itemCount": "4,640 samples (1,160 scenarios with variations)",
    "specs": "Multimodal (Text + Image); Tasks include Moral Judgment, Moral Classification, and Moral Response",
    "description": "The first multimodal moral benchmark for Large Vision-Language Models (LVLMs). It expands on everyday moral scenarios from Moral Foundations Vignettes (MFVs) by generating corresponding images, evaluating models on moral judgment, classification, and response tasks."
  },
  {
    "id": "imported-1769500766500-282-lqmrc",
    "title": "SHIELD: An Evaluation Benchmark for Face Spoofing and Forgery Detection",
    "source": "arXiv",
    "authors": [
      "Yichen Shi",
      "Yuhao Gao",
      "Yingxin Lai",
      "Xiaochun Cao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.04178",
    "githubLink": "https://github.com/laiyingxin2/SHIELD",
    "itemCount": "Not specified (Evaluates on aggregated subsets of CelebA-Spoof, SiW, etc.)",
    "specs": "Multimodal (Text, RGB Image, Infrared, Depth, Audio); Tasks: Face Anti-Spoofing (6 attack types), Face Forgery Detection (GAN/Diffusion based)",
    "description": "A benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to detect subtle visual spoofing and forgery clues. It consists of two main tasks: face anti-spoofing (detecting physical attacks like masks or screens) and face forgery detection (identifying digital manipulations like DeepFakes). The benchmark employs true/false and multiple-choice questions to assess model performance across diverse attack types and modalities."
  },
  {
    "id": "imported-1769500766500-283-ihdtq",
    "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation",
    "source": "arXiv",
    "authors": [
      "Xiaoze Liu",
      "Ting Sun",
      "Tianyang Xu",
      "Feijie Wu",
      "Cunxiang Wang",
      "Xiaoqian Wang",
      "Jing Gao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.12975",
    "githubLink": "https://github.com/xz-liu/SHIELD",
    "itemCount": "420 text samples (100 BS-NC, 100 BS-C, 20 BS-PC, 100 SSRL, 100 BEP)",
    "specs": "Text-only; Categories: Best Selling Books (Non-Copyrighted, Copyrighted, Partially Copyrighted), Spotify Lyrics, English Poems",
    "description": "A curated benchmark dataset aimed at evaluating copyright compliance in Large Language Models. It includes a collection of text materials with varying copyright statuses (copyrighted, public domain, and partially copyrighted) to test if models infringe on intellectual property or are overprotective. The benchmark also assesses the robustness of models against jailbreaking attacks designed to elicit copyrighted content."
  },
  {
    "id": "imported-1769500766500-284-ysht6",
    "title": "PadChest-GR",
    "source": "arXiv",
    "authors": [
      "Daniel Coelho de Castro",
      "Shruthi Bannur",
      "Stephanie Hyland",
      "Pratik Ghosh",
      "Mercy Ranjit",
      "Kenza Bouzid",
      "Anton Schwaighofer",
      "Fernando Pérez-García",
      "Harshita Sharma",
      "Ozan Oktay",
      "Matthew Lungren",
      "Javier Alvarez-Valle",
      "Aditya Nori",
      "Anja Thieme"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.05085",
    "githubLink": "https://github.com/microsoft/PadChest-GR",
    "itemCount": "Unknown subset of PadChest (original 160k images)",
    "specs": "Chest X-rays, Reports (En/Es), Bounding box grounding",
    "description": "A large-scale bilingual (Spanish/English) grounded radiology reporting benchmark dataset derived from PadChest, featuring bounding box annotations for findings."
  },
  {
    "id": "imported-1769500766500-285-29udb",
    "title": "XBRL-Agent Datasets",
    "source": "Hugging Face",
    "authors": [
      "Shijie Han",
      "Haoqiang Kang",
      "Bo Jin",
      "Xiao-Yang Liu",
      "Steve Yang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.11159",
    "githubLink": "https://huggingface.co/datasets/TurnipBit/XBRL-Agent-Dataset",
    "itemCount": "1,700 samples",
    "specs": "Text, XBRL Financial Data",
    "description": "A collection of datasets (XBRL Terminology, Financial Formula Calculation) designed to evaluate LLMs on their ability to interpret and analyze XBRL (eXtensible Business Reporting Language) filings, a standard for digital business reporting."
  },
  {
    "id": "imported-1769500766500-286-wz0eh",
    "title": "MedSafetyBench",
    "source": "arXiv",
    "authors": [
      "Tessa Han",
      "Aounon Kumar",
      "Chirag Agarwal",
      "Himabindu Lakkaraju"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.03744",
    "githubLink": "https://github.com/AI4LIFE-GROUP/med-safety-bench",
    "itemCount": "1,800 harmful medical requests",
    "specs": "Text (harmful queries and safe response pairs)",
    "description": "The first benchmark dataset designed to measure the medical safety of LLMs based on the Principles of Medical Ethics. It includes harmful medical requests and safe responses to evaluate and fine-tune models for safety while preserving medical performance."
  },
  {
    "id": "imported-1769500766500-287-ttg24",
    "title": "MEDEC",
    "source": "arXiv",
    "authors": [
      "Asma Ben Abacha",
      "Wen-wai Yim",
      "Yujuan Fu",
      "Zhaoyi Sun",
      "Meliha Yetisgen",
      "Fei Xia",
      "Thomas Lin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.18567",
    "githubLink": "https://github.com/abachaa/MEDEC",
    "itemCount": "3,848 clinical texts",
    "specs": "Text (Clinical notes with annotated errors and corrections)",
    "description": "A benchmark for medical error detection and correction in clinical notes. It covers five types of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism) to assess the ability of LLMs to validate clinical text."
  },
  {
    "id": "imported-1769500766500-288-s6k3e",
    "title": "MultiADE",
    "source": "arXiv",
    "authors": [
      "Xiang Dai",
      "Sarvnaz Karimi",
      "Abeed Sarker",
      "Ben Hachey",
      "Cecile Paris"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.03780",
    "githubLink": "https://github.com/daixiangau/MultiADE",
    "itemCount": "Aggregates 6 datasets (approx. 20,000+ sentences/samples combined)",
    "specs": "Text (Annotated entities for adverse drug events)",
    "description": "A multi-domain benchmark for Adverse Drug Event (ADE) extraction that standardizes six datasets from different sources (clinical notes, medical literature, social media) to test domain generalization in pharmacovigilance models."
  },
  {
    "id": "imported-1769500766500-289-5xkht",
    "title": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models",
    "source": "arXiv",
    "authors": [
      "Yichi Zhang",
      "Yao Huang",
      "Yitong Sun",
      "Chang Liu",
      "Zhe Zhao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.07057",
    "githubLink": "https://multi-trust.github.io/",
    "itemCount": "32 tasks",
    "specs": "Multimodal (Text/Image); 5 trustworthiness aspects",
    "description": "A unified benchmark for the trustworthiness of Multimodal LLMs (MLLMs) across five primary aspects: truthfulness, safety, robustness, fairness, and privacy, covering 32 diverse tasks."
  },
  {
    "id": "imported-1769500766500-290-72018",
    "title": "PASTA (Perceptual Assessment System for explanaTion of AI)",
    "source": "arXiv",
    "authors": [
      "Rémi Kazmierczak",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.02470",
    "githubLink": "https://github.com/remi-kazmierczak/PASTA",
    "itemCount": "Large-scale benchmark (exact count varies by sub-task)",
    "specs": "Images; Saliency maps, Concept-based explanations",
    "description": "A human-centric framework and dataset for evaluating XAI techniques in computer vision. It benchmarks saliency-based and concept-based explanation methods against human judgement, facilitating the creation of metrics that align with human perception."
  },
  {
    "id": "imported-1769500766500-291-19scj",
    "title": "MathVista",
    "source": "arXiv",
    "authors": [
      "Pan Lu",
      "Hanyu Wang",
      "Jai Bansal",
      "Taeuk Kim",
      "Rishabh Jain",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2310.02255",
    "githubLink": "https://github.com/lupantech/MathVista",
    "itemCount": "6,141 examples",
    "specs": "Derived from 28 existing datasets and 3 new ones (IQTest, FunctionQA, PaperQA); covers geometry, charts, and function plots.",
    "description": "A benchmark specifically designed to evaluate mathematical reasoning in visual contexts, combining diverse mathematical and visual tasks."
  },
  {
    "id": "imported-1769500766500-292-5eg49",
    "title": "BLINK",
    "source": "arXiv",
    "authors": [
      "Xingyu Fu",
      "Yushi Hu",
      "Bangzheng Li",
      "Yu Feng",
      "Haoyu Wang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.12390",
    "githubLink": "https://github.com/Eyja/BLINK",
    "itemCount": "3,807 questions",
    "specs": "14 classic computer vision tasks reformatted as multiple-choice questions; tasks include relative depth, visual correspondence, and rotation.",
    "description": "A benchmark focusing on core visual perception abilities (e.g., depth estimation, forensics, multi-view reasoning) that are easy for humans but hard for current MLLMs."
  },
  {
    "id": "imported-1769500766500-293-05m8n",
    "title": "Video-MME",
    "source": "arXiv",
    "authors": [
      "Chaoyou Fu",
      "Yuhan Dai",
      "Yondong Luo",
      "Lei Li",
      "Shuhuai Ren",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.21075",
    "githubLink": "https://github.com/BradyFU/Video-MME",
    "itemCount": "900 videos / 2,700 QAs",
    "specs": "Videos range from short (11s) to long (1h); covers 6 domains (knowledge, film, sports, etc.); includes subtitles and audio.",
    "description": "The first comprehensive benchmark for multi-modal evaluation in video analysis, covering diverse domains and video durations."
  },
  {
    "id": "imported-1769500766500-294-xpx6d",
    "title": "FrontierMath",
    "source": "arXiv",
    "authors": [
      "Elliot Glazer",
      "Ege Erdil",
      "Tamay Besiroglu",
      "Epoch AI Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.04872",
    "githubLink": "https://epoch.ai/frontiermath",
    "itemCount": "Hundreds of problems (Tiers 1-4)",
    "specs": "Expert-level Mathematics, Text/LaTeX",
    "description": "A benchmark for evaluating advanced mathematical reasoning in AI, featuring hundreds of original, expert-crafted mathematics problems that span major branches of modern mathematics and typically require hours or days for expert mathematicians to solve. The problems are kept unpublished to prevent contamination."
  },
  {
    "id": "imported-1769500766500-295-mbmmk",
    "title": "SimpleQA",
    "source": "Other",
    "authors": [
      "OpenAI",
      "Jason Wei",
      "Nguyen",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://openai.com/index/introducing-simpleqa",
    "githubLink": "https://github.com/openai/simple-evals",
    "itemCount": "4,326 questions",
    "specs": "Short-form Text Q&A",
    "description": "A factuality benchmark measuring the ability of language models to answer short, fact-seeking questions. The dataset is designed to be challenging for frontier models and easy to grade, with questions adversarially collected to ensure high correctness and low ambiguity."
  },
  {
    "id": "imported-1769500766500-296-z2y0l",
    "title": "JailBreakV-28K",
    "source": "Hugging Face",
    "authors": [
      "Weidi Luo",
      "Siyuan Ma",
      "Xiaogeng Liu",
      "Xiaoyu Guo"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/papers/2404.03027",
    "githubLink": "https://github.com/JailBreakV/JailBreakV",
    "itemCount": "28,000 test cases (20,000 text, 8,000 image)",
    "specs": "Text, Image (Multimodal)",
    "description": "A benchmark designed to assess the transferability of LLM jailbreak techniques to Multimodal Large Language Models (MLLMs). It includes a diverse set of text-based and image-based jailbreak inputs."
  },
  {
    "id": "imported-1769500766500-297-5yi87",
    "title": "DriveWorld",
    "source": "arXiv",
    "authors": [
      "Yvan Yin",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.04390",
    "githubLink": "https://github.com/YvanYin/DrivingWorld",
    "itemCount": "Evaluated on OpenScene, nuPlan, nuScenes",
    "specs": "Multi-camera driving videos, 4D Spatio-temporal representation, Occupancy grid",
    "description": "A framework and benchmark protocol for 4D pre-trained scene understanding via world models in autonomous driving. It evaluates tasks like 3D object detection, occupancy prediction, and motion forecasting using a world model-based representation."
  },
  {
    "id": "imported-1769500766500-298-kksc2",
    "title": "PhyBench",
    "source": "arXiv",
    "authors": [
      "Fanqing Meng",
      "Wenqi Shao",
      "Lixin Luo",
      "Yahong Wang",
      "Yiran Chen",
      "Quanfeng Lu",
      "Yue Yang",
      "Tianshuo Yang",
      "Kaipeng Zhang",
      "Yu Qiao",
      "Ping Luo"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.11802",
    "githubLink": "https://github.com/OpenGVLab/PhyBench",
    "itemCount": "700 prompts",
    "specs": "Text-to-Image prompts; 4 categories: mechanics, optics, thermodynamics, material properties",
    "description": "A benchmark for evaluating the physical commonsense of text-to-image models. It focuses on whether generated images adhere to fundamental physical principles."
  },
  {
    "id": "imported-1769500766500-299-dk68j",
    "title": "Seal-Tools",
    "source": "arXiv",
    "authors": [
      "Meng Wei",
      "Minghao Li",
      "Yingxiu Zhao",
      "Bowen Yu",
      "Yongbin Li"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.08355",
    "githubLink": "https://github.com/fairymax/Seal-Tools",
    "itemCount": "~4,000 tools, Multiple instances",
    "specs": "JSON tool definitions, API-like interactions, Nested calls",
    "description": "A self-instruct tool learning dataset for agent tuning and benchmarking. It focuses on tool-use capabilities, including nested tool callings and multi-step reasoning, serving as a benchmark to evaluate the tool-calling and orchestration ability of LLMs."
  },
  {
    "id": "imported-1769500766500-300-vo3gx",
    "title": "Cloud Data Orchestration Dataset",
    "source": "Other",
    "authors": [
      "Kaggle Community"
    ],
    "year": "2024",
    "paperLink": "https://www.kaggle.com/datasets/cloud-data-orchestration",
    "githubLink": "N/A",
    "itemCount": "7,365 records",
    "specs": "Tabular data (CSV), System metrics, Categorical/Continuous features",
    "description": "A dataset simulating the behavior of intelligent agents in a hybrid cloud environment. It is designed to model autonomous, context-aware decision-making for operations such as data migration, replication, and deletion using reinforcement learning techniques."
  },
  {
    "id": "imported-1769500766500-301-0j4w9",
    "title": "SciInstruct (SciGLM)",
    "source": "arXiv",
    "authors": [
      "Dan Zhang",
      "Ziniu Hu",
      "Sining Zhoubian",
      "Zhengxiao Du",
      "Kaiyu Yang",
      "Zihan Wang",
      "Yisong Yue",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.07950",
    "githubLink": "https://github.com/THUDM/SciGLM",
    "itemCount": "254,051 instructions",
    "specs": "Text-based; Instruction Tuning; Domains: Math, Physics, Chemistry, Formal Proofs",
    "description": "A diverse and high-quality instruction tuning dataset for college-level scientific reasoning, created using a self-reflective annotation framework. It includes data for physics, chemistry, math, and formal proofs."
  },
  {
    "id": "imported-1769500766500-302-7t5y5",
    "title": "CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence",
    "source": "arXiv",
    "authors": [
      "Md Tanvirul Alam",
      "Dipkamal Bhusal",
      "Le Nguyen",
      "Nidhi Rastogi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.21323",
    "githubLink": "https://github.com/xashru/cti-bench",
    "itemCount": "5,610 samples",
    "specs": "5 datasets: MCQ, Root Cause Mapping (RCM), Vulnerability Severity Prediction (VSP), Attack Technique Extraction (ATE), Threat Actor Attribution (TAA)",
    "description": "A comprehensive benchmark suite designed to evaluate LLMs in Cyber Threat Intelligence (CTI). It assesses capabilities in understanding crucial CTI concepts, mapping vulnerabilities (CVE to CWE), predicting severity (CVSS), and attributing threats."
  },
  {
    "id": "imported-1769500766500-303-u8wa8",
    "title": "CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity",
    "source": "arXiv",
    "authors": [
      "Norbert Tihanyi",
      "Mohamed Amine Ferrag",
      "Merouane Debbah",
      "Thierry Lestable"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.07688",
    "githubLink": "https://github.com/Norbix/CyberMetric",
    "itemCount": "10,000 questions",
    "specs": "Multiple Choice Questions (MCQ) in varying dataset sizes (80, 500, 2000, 10000)",
    "description": "A benchmark dataset designed to evaluate the cybersecurity knowledge of LLMs. It contains questions sourced from standards, certifications, research papers, and books, verified by human experts."
  },
  {
    "id": "imported-1769500766500-304-yc5h4",
    "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity",
    "source": "arXiv",
    "authors": [
      "Xiangmin Shen",
      "Lingzhi Wang",
      "Zhenyuan Li",
      "Yan Chen",
      "Wencheng Zhao",
      "Dawei Sun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.20787",
    "githubLink": "https://github.com/CloudSecurityAlliance/SecBench",
    "itemCount": "44,823 MCQs and 3,087 SAQs",
    "specs": "Multiple Choice Questions (MCQs) and Short Answer Questions (SAQs)",
    "description": "A multi-dimensional benchmarking dataset for evaluating LLMs in cybersecurity, covering knowledge retention and logical reasoning across multiple languages (Chinese/English) and sub-domains."
  },
  {
    "id": "imported-1769500766500-305-nra4p",
    "title": "LSPR23: A novel IDS dataset from the largest live-fire cybersecurity exercise",
    "source": "Scholar",
    "authors": [
      "Roland Meier",
      "Luca Gambazzi",
      "Vincent Lenders"
    ],
    "year": "2024",
    "paperLink": "https://doi.org/10.1016/j.jisa.2024.104245",
    "githubLink": "https://roland-meier.ch/",
    "itemCount": "400 GB network traffic, 6 million log entries",
    "specs": "PCAP network traffic and host-based event logs",
    "description": "A labelled Intrusion Detection System (IDS) dataset derived from the Locked Shields live-fire cyber defense exercise. It captures real-world red team attacks and blue team defenses."
  },
  {
    "id": "imported-1769500766500-306-m2yhd",
    "title": "Aya Dataset",
    "source": "arXiv",
    "authors": [
      "Shivalika Singh",
      "Freddie Vargus",
      "Daniel D'souza",
      "Börje F. Karlsson",
      "Abinaya Mahendiran",
      "Wei-Yin Ko",
      "Herumb Shandilya",
      "Jay Patel",
      "Deividas Mataciunas",
      "Laura O'Mahony",
      "Mike Zhang",
      "Ramith Hettiarachchi",
      "Joseph Wilson",
      "Marina Machado",
      "Luisa Souza Moura",
      "Dominik Krzemiński",
      "Hakimeh Fadaei",
      "Irem Ergun",
      "Ifeoma Okoh",
      "Aisha Alaagib",
      "Oshan Ivantha Mudannayake",
      "Zaid Alyafeai",
      "Vu Minh Chien",
      "Sebastian Ruder",
      "Surya Guthikonda",
      "Emad A. Alghamdi",
      "Sebastian Gehrmann",
      "Niklas Muennighoff",
      "Max Bartolo",
      "Julia Kreutzer",
      "Ahmet Üstün",
      "Marzieh Fadaee",
      "Sara Hooker"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.06619",
    "githubLink": "https://huggingface.co/datasets/CohereForAI/aya_dataset",
    "itemCount": "65 languages (human-curated), 114 languages (collection)",
    "specs": "Text; Instruction Tuning",
    "description": "An open-access collection for multilingual instruction tuning, including human-curated examples and a massive collection of templated instructions."
  },
  {
    "id": "imported-1769500766500-307-d7syd",
    "title": "AgentHarm",
    "source": "Hugging Face",
    "authors": [
      "UK AI Safety Institute",
      "Gray Swan AI"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/ai-safety-institute/AgentHarm",
    "githubLink": "https://github.com/grayswan-ai/AgentHarm",
    "itemCount": "44 base behaviors (176 augmented)",
    "specs": "Text (Agent tasks); Harmful behaviors",
    "description": "A benchmark for measuring the harmfulness of LLM agents. It includes a diverse set of behaviors designed to test agent safety in tool-use contexts."
  },
  {
    "id": "imported-1769500766500-308-k545l",
    "title": "PAD (Purple-teaming LLMs with Adversarial Defender training)",
    "source": "arXiv",
    "authors": [
      "Minda Hu",
      "Researchers at CatalyzeX/Other"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.01850",
    "githubLink": "https://github.com/NA",
    "itemCount": "Variable (Self-play generated conversational data)",
    "specs": "Conversational text data (multi-turn dialogues between attacker and defender models).",
    "description": "A framework and pipeline for safeguarding LLMs by incorporating red-teaming (attack) and blue-teaming (safety training) techniques in a self-play manner. It automatically collects conversational data covering specific safety risks where an attacker elicits unsafe responses and a defender generates safe ones."
  },
  {
    "id": "imported-1769500766500-309-6ywsc",
    "title": "Ferret",
    "source": "Other",
    "authors": [
      "Declare Lab",
      "Soujanya Poria",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.10647",
    "githubLink": "https://github.com/declare-lab/ferret",
    "itemCount": "Framework for generating infinite adversarial prompts",
    "specs": "Text (Adversarial prompts and model responses).",
    "description": "An automated red-teaming framework that uses reward-based scoring to generate effective adversarial prompts. While primarily a red-teaming tool, it is used in purple team contexts to iteratively improve model robustness (blue team defense) against evolving attacks."
  },
  {
    "id": "imported-1769500766500-310-dlbif",
    "title": "AudioMarkBench",
    "source": "arXiv",
    "authors": [
      "Hongbin Liu",
      "Moyang Guo",
      "Zhengyuan Jiang",
      "Lun Wang",
      "Neil Zhenqiang Gong"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.06979",
    "githubLink": "https://github.com/moyangkuo/AudioMarkBench",
    "itemCount": "Derived from Common Voice dataset",
    "specs": "Audio; Watermarking; Adversarial Perturbations",
    "description": "A systematic benchmark for evaluating the robustness of audio watermarking techniques against adversarial attacks, including watermark removal and forgery, under various settings (white-box, black-box, no-box)."
  },
  {
    "id": "imported-1769500766500-311-91mbz",
    "title": "HARPER (Human from an Articulated Robot Perspective)",
    "source": "arXiv",
    "authors": [
      "Andrea Avogaro",
      "Andrea Toaiari",
      "Federico Cunico",
      "Xiangmin Xu",
      "Haralambos Dafas",
      "Alessandro Vinciarelli",
      "Emma Li",
      "Marco Cristani"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.00000",
    "githubLink": "https://github.com/Intelligo-Labs/HARPER",
    "itemCount": "15 actions, 17 participants, multiple views",
    "specs": "RGB-D, 3D skeletal poses, robot egocentric view, OptiTrack ground truth",
    "description": "A dataset for 3D human pose estimation and forecasting in dyadic interactions between users and a quadruped robot (Spot). It focuses on the robot's egocentric perspective during collaborative tasks."
  },
  {
    "id": "imported-1769500766500-312-tajng",
    "title": "HAGS (Hand and Glove Segmentation Dataset)",
    "source": "arXiv",
    "authors": [
      "UT Nuclear Robotics Group"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.14649",
    "githubLink": "https://github.com/UTNuclearRoboticsPublic/assembly_glovebox_dataset",
    "itemCount": "191 videos, 1728 labeled frames",
    "specs": "Video, pixel-level segmentation masks, RGB images",
    "description": "A dataset designed for human-robot collaboration in industrial glovebox environments. It focuses on pixel-level segmentation of hands and gloves to support safe cooperative assembly tasks."
  },
  {
    "id": "imported-1769500766500-313-q2u3y",
    "title": "CREW Platform and Benchmark",
    "source": "arXiv",
    "authors": [
      "Lingyu Zhang",
      "Zhengran Ji",
      "Boyuan Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.00170",
    "githubLink": "https://github.com/GeneralRoboticsLab/CREW",
    "itemCount": "Data from 50 human subject studies",
    "specs": "Unity environments, Python agents, physiological data (e.g., gaze, EEG support)",
    "description": "A platform and benchmark for facilitating human-AI teaming research in real-time decision-making scenarios. It includes pre-built tasks and supports multimodal physiological signal recording."
  },
  {
    "id": "imported-1769500766500-314-iasq6",
    "title": "WorkBench",
    "source": "arXiv",
    "authors": [
      "Olly Styles",
      "Sam Miller",
      "Patricio Cerda-Mardini",
      "Tanaya Guha",
      "Victor Sanchez",
      "Bertie Vidgen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.00823",
    "githubLink": "https://github.com/olly-styles/WorkBench",
    "itemCount": "690 tasks; 26 tools; 5 databases",
    "specs": "Text; Database interactions; Tool use; Sandbox environment",
    "description": "A benchmark dataset for evaluating agents' ability to execute common business activities such as sending emails and scheduling meetings in a realistic workplace setting."
  },
  {
    "id": "imported-1769500766500-315-hgmgb",
    "title": "TravelPlanner",
    "source": "arXiv",
    "authors": [
      "Jian Xie",
      "Kai Zhang",
      "Jiangjie Chen",
      "Ting-En Lin",
      "Lou Jie",
      "Zhanghui Kuang",
      "Furu Wei",
      "Li Yuan",
      "Yu Su"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.01622",
    "githubLink": "https://github.com/OSU-NLP-Group/TravelPlanner",
    "itemCount": "1,225 queries",
    "specs": "Text; Tool use; Planning constraints; Multi-turn",
    "description": "A benchmark for real-world planning with language agents, focusing on travel planning which requires tool use and complex planning within multiple constraints."
  },
  {
    "id": "imported-1769500766500-316-08l3r",
    "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning (SciToolBench)",
    "source": "arXiv",
    "authors": [
      "Ziru Chen",
      "Shijie Chen",
      "Yuting Ning",
      "Qianheng Zhang",
      "Boshi Wang",
      "Botao Yu",
      "Yifei Li",
      "Zeyi Liao",
      "Chen Wei",
      "Zitong Lu",
      "Vishal Dey",
      "Mingyi Xue",
      "Frazier N. Baker",
      "Benjamin Burns",
      "Daniel Adu-Ampratwum",
      "Xuhui Huang",
      "Xia Ning",
      "Song Gao",
      "Yu Su",
      "Huan Sun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.11451",
    "githubLink": "https://github.com/OSU-NLP-Group/SciAgent",
    "itemCount": "856 questions; 2,446 functions",
    "specs": "Scientific domains (Math, Physics, Chemistry, etc.), tool-augmented reasoning tasks.",
    "description": "Evaluates tool-augmented LLMs on scientific reasoning tasks across 5 domains. The benchmark, SciToolBench, requires agents to retrieve, understand, and use specific functions to solve scientific problems."
  },
  {
    "id": "imported-1769500766500-317-47gan",
    "title": "Benchmarking the Sim-to-Real Gap in Cloth Manipulation",
    "source": "arXiv",
    "authors": [
      "David Blanco-Mulero",
      "Oriol Barbany",
      "Gokhan Alcan",
      "Adria Colome",
      "Carme Torras",
      "Ville Kyrki"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2310.09543",
    "githubLink": "https://sites.google.com/view/cloth-sim2real-benchmark",
    "itemCount": "Not specified (Multiple trajectories across 3 fabric types)",
    "specs": "Point clouds, depth images, robot trajectories (dynamic & quasi-static tasks)",
    "description": "A benchmark dataset designed to evaluate the reality gap between deformable object simulators (MuJoCo, Bullet, Flex, SOFA) and real-world data. It involves dynamic and quasi-static cloth manipulation tasks."
  },
  {
    "id": "imported-1769500766500-318-6qyjs",
    "title": "SafetyBench",
    "source": "arXiv",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Lindong Wu",
      "Rui Sun",
      "Yongkang Huang",
      "Chong Long",
      "Xiao Liu",
      "Xuanyu Lei",
      "Jie Tang",
      "Minlie Huang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.07045",
    "githubLink": "https://github.com/thu-coai/SafetyBench",
    "itemCount": "11,435 questions",
    "specs": "Multiple-choice questions (Text), Chinese and English",
    "description": "A comprehensive benchmark for evaluating the safety of Large Language Models (LLMs) using multiple-choice questions across 7 distinct categories of safety concerns, available in both Chinese and English."
  },
  {
    "id": "imported-1769500766500-319-hre3v",
    "title": "Do Not Answer",
    "source": "arXiv",
    "authors": [
      "Yuxia Wang",
      "Haonan Li",
      "Xudong Han",
      "Preslav Nakov",
      "Timothy Baldwin"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.13387",
    "githubLink": "https://github.com/Libr-AI/do-not-answer",
    "itemCount": "939 prompts",
    "specs": "Text prompts, 12 harm categories (3-level taxonomy)",
    "description": "An open-source dataset to evaluate LLM safeguards, consisting of prompts across various harm types that responsible models should refuse to answer."
  },
  {
    "id": "imported-1769500766500-320-82jsl",
    "title": "BeaverTails",
    "source": "arXiv",
    "authors": [
      "Jiaming Ji",
      "Mickel Liu",
      "Juntao Dai",
      "Xuehai Pan",
      "Chi Zhang",
      "Ce Bian",
      "Boyuan Chen",
      "Ruiyang Sun",
      "Yizhou Wang",
      "Yaodong Yang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.04657",
    "githubLink": "https://github.com/PKU-Alignment/safe-rlhf",
    "itemCount": "330,000+ QA pairs",
    "specs": "Text QA pairs, Safety meta-labels, Human-preference data",
    "description": "A dataset fostering safety alignment research in LLMs, separating helpfulness and harmlessness annotations for question-answering pairs."
  },
  {
    "id": "imported-1769500766500-321-t2e0u",
    "title": "AdvBench",
    "source": "arXiv",
    "authors": [
      "Andy Zou",
      "Zifan Wang",
      "J. Zico Kolter",
      "Matt Fredrikson"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.15043",
    "githubLink": "https://github.com/llm-attacks/llm-attacks",
    "itemCount": "500 harmful behaviors",
    "specs": "Text strings/instructions",
    "description": "A dataset of harmful behaviors and instructions used to test adversarial attacks (jailbreaking) on aligned language models."
  },
  {
    "id": "imported-1769500766500-322-0f0jq",
    "title": "AgentBench",
    "source": "arXiv",
    "authors": [
      "Xiao Liu",
      "Hao Yu",
      "Hanchen Zhang",
      "Yifan Xu",
      "Lei Lei",
      "Hanyu Lai",
      "Yu Gu",
      "Hangliang Ding",
      "Kaiwen Men",
      "Kejunjie Yang",
      "Shudan Zhang",
      "Xiang Deng",
      "Aohan Zeng",
      "Zhengxiao Du",
      "Chenhui Zhang",
      "Sheng Shen",
      "Tianjun Zhang",
      "Yu Su",
      "Huan Sun",
      "Minlie Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.03688",
    "githubLink": "https://github.com/THUDM/AgentBench",
    "itemCount": "8 environments",
    "specs": "Multi-turn open-ended generation; Environments include OS, Database, Knowledge Graph, Card Game, Householding, Web Shopping",
    "description": "A comprehensive benchmark designed to evaluate LLMs as autonomous agents across 8 distinct environments, including Operating Systems, Databases, Knowledge Graphs, Digital Card Games, Householding, and Web Shopping."
  },
  {
    "id": "imported-1769500766500-323-yrwqe",
    "title": "WebArena",
    "source": "arXiv",
    "authors": [
      "Shuyan Zhou",
      "Frank F. Xu",
      "Hao Zhu",
      "Xuhui Zhou",
      "Robert Lo",
      "Abishek Sridhar",
      "Xianyi Cheng",
      "Tianyue Ou",
      "Yonatan Bisk",
      "Daniel Fried",
      "Uri Alon",
      "Graham Neubig"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.13854",
    "githubLink": "https://github.com/web-arena-x/webarena",
    "itemCount": "812 test tasks",
    "specs": "Web-based tasks, Text/HTML modality, Dockerized environments",
    "description": "A realistic and reproducible web environment for building and evaluating autonomous agents. It consists of fully functional websites from four common domains (e-commerce, social forums, software development, content management) to test agents on long-horizon web tasks."
  },
  {
    "id": "imported-1769500766500-324-s46yz",
    "title": "SWE-bench",
    "source": "arXiv",
    "authors": [
      "Carlos E. Jimenez",
      "John Yang",
      "Alexander Wettig",
      "Shunyu Yao",
      "Karthik Narasimhan",
      "Ofir Press"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.06770",
    "githubLink": "https://github.com/swe-bench/swe-bench",
    "itemCount": "2,294 task instances",
    "specs": "Python codebases, GitHub issues/PRs, Docker execution environment",
    "description": "A benchmark for evaluating large language models on real-world software engineering issues collected from GitHub. It tasks agents with resolving issues (bugs/features) in popular Python repositories by generating patches that pass new tests."
  },
  {
    "id": "imported-1769500766500-325-754qu",
    "title": "ToolBench",
    "source": "arXiv",
    "authors": [
      "Yujia Qin",
      "Shihao Liang",
      "Yining Ye",
      "Kunlun Zhu",
      "Lan Yan",
      "Yaxi Lu",
      "Yankai Lin",
      "Xin Cong",
      "Xiangru Tang",
      "Bill Qian",
      "Sihan Zhao",
      "Runchu Tian",
      "Ruobing Xie",
      "Jie Zhou",
      "Mark Gerstein",
      "Dahai Li",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.16789",
    "githubLink": "https://github.com/OpenBMB/ToolBench",
    "itemCount": "16,000+ real-world APIs",
    "specs": "Instruction tuning data, API calls, Text modality",
    "description": "An open platform for training, serving, and evaluating LLMs for tool learning. It includes a large-scale instruction-tuning dataset constructed automatically to help models master diverse real-world APIs."
  },
  {
    "id": "imported-1769500766500-326-vuceg",
    "title": "GAIA",
    "source": "arXiv",
    "authors": [
      "Grégoire Mialon",
      "Clémentine Fourrier",
      "Craig Swift",
      "Thomas Wolf",
      "Yann LeCun",
      "Thomas Scialom"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.12983",
    "githubLink": "https://github.com/gaia-benchmark/gaia",
    "itemCount": "466 questions",
    "specs": "Text questions, varying modalities (images, files), Tool use required",
    "description": "A benchmark for General AI Assistants featuring conceptually simple questions for humans that are challenging for AI. It requires fundamental abilities such as reasoning, tool use, and multi-modality to solve real-world tasks."
  },
  {
    "id": "imported-1769500766500-327-l8b2g",
    "title": "MINT",
    "source": "arXiv",
    "authors": [
      "Zihan Wang",
      "Mao Xue",
      "Vicki Cheung",
      "Hang Ma",
      "Duarte Alves",
      "Sitao Xiang",
      "Lehman",
      "He",
      "Graham Neubig"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.10691",
    "githubLink": "https://github.com/zihanguai/MINT",
    "itemCount": "Derived from 8 datasets",
    "specs": "Multi-turn interaction, Tool use, Natural Language Feedback",
    "description": "A benchmark evaluating LLMs in multi-turn interactions with tools and natural language feedback. It repurposes established datasets to test reasoning, coding, and decision-making with simulated user feedback."
  },
  {
    "id": "imported-1769500766500-328-yb01r",
    "title": "SmartPlay",
    "source": "arXiv",
    "authors": [
      "Yue Wu",
      "Xuan Zhang",
      "Yue Zhang",
      "Pan Lu",
      "Steven Hoi",
      "Caiming Xiong",
      "Ran Xu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.01557",
    "githubLink": "https://github.com/microsoft/SmartPlay",
    "itemCount": "6 games",
    "specs": "Game environments, Multi-turn, Spatial/Strategic reasoning",
    "description": "A benchmark for LLMs as intelligent agents featuring 6 distinct games (including Minecraft, Rock-Paper-Scissors, Tower of Hanoi) that test capabilities like planning, spatial reasoning, and learning from history."
  },
  {
    "id": "imported-1769500766500-329-pma7x",
    "title": "DiverseVul",
    "source": "arXiv",
    "authors": [
      "Yizheng Chen",
      "Zhoujie Ding",
      "Lamya Alowain",
      "Xinyun Chen",
      "David Wagner"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.00409",
    "githubLink": "https://github.com/wagner-group/diversevul",
    "itemCount": "18,945 vulnerable functions, 330,492 non-vulnerable functions",
    "specs": "C/C++ source code",
    "description": "A large and diverse vulnerable source code dataset containing 18,945 vulnerable functions spanning 150 CWEs. It is designed to be more diverse and significantly larger than previous datasets like ReVeal and CVEFixes."
  },
  {
    "id": "imported-1769500766500-330-cniza",
    "title": "CyberSecEval (Purple Llama)",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Cyrus Nikolaidis",
      "Shengye Wan",
      "Ivan Evtimov",
      "Dominik Gabi",
      "Daniel Song",
      "Faizan Ahmad",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.04724",
    "githubLink": "https://github.com/facebookresearch/PurpleLlama",
    "itemCount": "Multiple test suites (thousands of test cases)",
    "specs": "Text/Code (LLM prompts and responses)",
    "description": "A comprehensive benchmark to evaluate the cybersecurity risks of Large Language Models (LLMs), focusing on their propensity to generate insecure code and their compliance with assisting in cyberattacks."
  },
  {
    "id": "imported-1769500766500-331-ol461",
    "title": "FormAI",
    "source": "arXiv",
    "authors": [
      "Norbert Tihanyi",
      "Tamas Bisztray",
      "Ridhi Jain",
      "Mohamed Amine Ferrag",
      "Lucas Cordeiro",
      "Vasileios Mavroeidis"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.02192",
    "githubLink": "https://github.com/FormAI-Dataset/FormAI-Dataset",
    "itemCount": "112,000 (v1) to 331,000 (v2) programs",
    "specs": "AI-generated C programs",
    "description": "A large collection of AI-generated C programs with vulnerability classifications labeled using formal verification (ESBMC). It aims to study vulnerabilities introduced by generative AI."
  },
  {
    "id": "imported-1769500766500-332-v7uz0",
    "title": "KoSBi (Korean Social Bias Dataset)",
    "source": "arXiv",
    "authors": [
      "Hwaran Lee",
      "Seokhee Hong",
      "Joonsuk Park",
      "Takyoung Kim",
      "Gunhee Kim",
      "Jung-Woo Ha"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.17701",
    "githubLink": "https://github.com/naver-ai/korean-safety-benchmarks",
    "itemCount": "34,000 pairs",
    "specs": "Text, Social Bias pairs",
    "description": "A large-scale dataset for mitigating social bias risks in Large Language Models (LLMs) for Korean. It includes context-sentence pairs covering diverse demographic groups and categories to help train safe sentence classifiers."
  },
  {
    "id": "imported-1769500766500-333-dpldw",
    "title": "CMMLU",
    "source": "arXiv",
    "authors": [
      "Haonan Li",
      "Yixuan Zhang",
      "Fajri Koto",
      "Yifei Yang",
      "Hai Zhao",
      "Yeyun Gong",
      "Nan Duan",
      "Timothy Baldwin"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.09212",
    "githubLink": "https://github.com/haonan-li/CMMLU",
    "itemCount": "11,528 questions",
    "specs": "Chinese, Multiple-choice, Text, 67 subjects",
    "description": "A comprehensive Chinese benchmark covering 67 subjects across natural science, social sciences, engineering, and humanities, designed to evaluate the knowledge and reasoning capabilities of LLMs in a Chinese context."
  },
  {
    "id": "imported-1769500766500-334-7qy9u",
    "title": "MMMU (Massive Multi-discipline Multimodal Understanding)",
    "source": "arXiv",
    "authors": [
      "Xiang Yue",
      "Yuansheng Ni",
      "Kai Zhang",
      "Tianyu Zheng",
      "Ruiyu Liu",
      "Ge Zhang",
      "Samuel Stevens",
      "Dongfu Jiang",
      "Weiming Ren",
      "Yuxuan Sun",
      "Cong Wei",
      "Botao Yu",
      "Ruicheng Yuan",
      "Renliang Sun",
      "Ming Yin",
      "Boyuan Zheng",
      "Zhenzhu Yang",
      "Yibo Liu",
      "Wenhu Chen",
      "Yu Su"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.16502",
    "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
    "itemCount": "11,500 questions",
    "specs": "Multimodal (Text + Images: charts, diagrams, maps, chemical structures, etc.). Covers 30 subjects and 183 subfields.",
    "description": "Designed to evaluate 'Expert AGI', this benchmark includes college-level multimodal questions requiring domain-specific knowledge and deliberate reasoning across six core disciplines (e.g., Science, Health, Art)."
  },
  {
    "id": "imported-1769500766500-335-dehdv",
    "title": "AGIEval",
    "source": "arXiv",
    "authors": [
      "Wanjun Zhong",
      "Ruixiang Cui",
      "Yiduo Guo",
      "Yaobo Liang",
      "Shuai Lu",
      "Yanlin Wang",
      "Amin Saied",
      "Weizhu Chen",
      "Nan Duan"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.06364",
    "githubLink": "https://github.com/ruixiangcui/AGIEval",
    "itemCount": "~8,000 questions",
    "specs": "Text-based (mostly), Multiple Choice, Cloze. Bilingual (English and Chinese).",
    "description": "A human-centric benchmark derived from official, high-standard admission and qualification exams (e.g., SAT, Gaokao, LSAT) intended for human test-takers, assessing general abilities pertinent to human cognition."
  },
  {
    "id": "imported-1769500766500-336-2nt2x",
    "title": "Violent Non-State Actor (VNSA) CBRN Event Database",
    "source": "Semantic Scholar",
    "authors": [
      "Derrick Tin",
      "Lenard Cheng",
      "Heejun Shin",
      "Ryan Hata",
      "Fredrik Granholm",
      "George Braitberg",
      "Gregory Ciottone",
      "START Consortium"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1017/S1049023X23000481",
    "githubLink": "https://www.start.umd.edu/data-tools/cbrn-data-portal",
    "itemCount": "565 events",
    "specs": "Tabular/Structured Data; Coverage: 1990-2020",
    "description": "A descriptive database analyzing the use of CBRN weapons by violent non-state actors globally, recording specific agents, locations, and outcomes of events."
  },
  {
    "id": "imported-1769500766500-337-6e35s",
    "title": "Foundation Model Transparency Index (FMTI)",
    "source": "arXiv",
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Betty Xiong",
      "Nestor Maslej",
      "Percy Liang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.12941",
    "githubLink": "https://github.com/stanford-crfm/fmti",
    "itemCount": "100 indicators evaluated across 13-14 major developers",
    "specs": "Tabular data (scores), PDF reports, transparency indicators",
    "description": "A comprehensive assessment of the transparency of foundation model developers. It scores major AI companies (e.g., OpenAI, Google, Meta) on 100 indicators across three domains: upstream resources (data, labor, compute), the model itself (capabilities, risks), and downstream use (distribution, impact). The project provides a dataset of these scores and detailed transparency reports."
  },
  {
    "id": "imported-1769500766500-338-pvjuk",
    "title": "The Data Provenance Collection",
    "source": "arXiv",
    "authors": [
      "Shayne Longpre",
      "Robert Mahari",
      "Anthony Chen",
      "Naana Obeng-Marnu",
      "Damien Sileo",
      "William Brannon",
      "Niklas Muennighoff",
      "Nathan Nathan",
      "Alex Pentland",
      "Sara Hooker",
      "Jad Kabbara"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.16787",
    "githubLink": "https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection",
    "itemCount": "44 data collections comprising 1800+ datasets",
    "specs": "Text/Metadata (licenses, sources, creators, provenance traces)",
    "description": "A large-scale audit and dataset of licensing and attribution for AI training datasets. The initiative traces the lineage of popular fine-tuning data collections, documenting their sources, licenses, creators, and other metadata to improve data transparency in the AI ecosystem."
  },
  {
    "id": "imported-1769500766500-339-mqdra",
    "title": "HaluEval",
    "source": "arXiv",
    "authors": [
      "Junyi Li",
      "Xiaoxue Cheng",
      "Wayne Xin Zhao",
      "Jian-Yun Nie",
      "Ji-Rong Wen"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.11747",
    "githubLink": "https://github.com/RUCAIBox/HaluEval",
    "itemCount": "35,000 samples (30k generated, 5k human-annotated)",
    "specs": "Text (QA, Dialogue, Summarization)",
    "description": "A large-scale hallucination evaluation benchmark for LLMs. It includes both automatically generated and human-annotated hallucinated samples to evaluate the ability of models to recognize hallucinations."
  },
  {
    "id": "imported-1769500766500-340-d2woe",
    "title": "HallusionBench",
    "source": "arXiv",
    "authors": [
      "Tianrui Guan",
      "Fuxiao Liu",
      "Xiyang Wu",
      "Ruiqi Xian",
      "Zongxia Li",
      "Xiaoyu Liu",
      "Xijun Wang",
      "Lichang Chen",
      "Furong Huang",
      "Yaser Yacoob",
      "Dinesh Manocha",
      "Tianyi Zhou"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.14566",
    "githubLink": "https://github.com/tianyi-lab/HallusionBench",
    "itemCount": "346 images, 1,129 questions",
    "specs": "Image/Text (Visual QA), Control groups for consistency analysis",
    "description": "An image-context reasoning benchmark designed to challenge Large Vision-Language Models (LVLMs) by focusing on language hallucinations and visual illusions."
  },
  {
    "id": "imported-1769500766500-341-gtfuw",
    "title": "Head-to-Tail",
    "source": "arXiv",
    "authors": [
      "Kai Sun",
      "Yifan Ethan Xu",
      "Hanwen Zha",
      "Yue Liu",
      "Xin Dong"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.10168",
    "githubLink": "https://github.com/facebookresearch/head-to-tail",
    "itemCount": "18,000 QA pairs",
    "specs": "Text (QA), Split by entity popularity",
    "description": "A benchmark for evaluating the factual knowledge of LLMs across facts of varying popularity (Head, Torso, Tail) to better understand hallucination rates in less common knowledge areas."
  },
  {
    "id": "imported-1769500766500-342-rp676",
    "title": "DOLOS",
    "source": "Other",
    "authors": [
      "Dongyang Guo",
      "Yinping Nie",
      "Kangkang Chen",
      "Haonan Luo",
      "Zhiyuan Ji",
      "Yi Yang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.00494",
    "githubLink": "https://github.com/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning",
    "itemCount": "1,675 video clips",
    "specs": "Audio, Visual; MUMIN feature annotations; 213 subjects",
    "description": "A large-scale audio-visual dataset sourced from reality TV game shows (like 'Golden Balls'). It focuses on conversational deception in dynamic, rich environments with fine-grained annotations."
  },
  {
    "id": "imported-1769500766500-343-qkza2",
    "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
    "source": "arXiv",
    "authors": [
      "David Rein",
      "Betty Li Hou",
      "Asa Cooper Stickland",
      "Jackson Petty",
      "Richard Yuanzhe Pang",
      "Julien Dirani",
      "Julian Michael",
      "Samuel R. Bowman"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.12022",
    "githubLink": "https://github.com/idavidrein/gpqa",
    "itemCount": "448 questions (main set)",
    "specs": "Text (Multiple-choice Questions)",
    "description": "GPQA is a challenging multiple-choice question answering dataset written by domain experts in biology, physics, and chemistry. It is designed to be difficult even for highly skilled non-experts with unrestricted access to the internet (Google-proof), while domain experts can answer them with high accuracy."
  },
  {
    "id": "imported-1769500766500-344-oxh0f",
    "title": "ConceptARC",
    "source": "arXiv",
    "authors": [
      "Arseny Moskvichev",
      "Victor Vikram Odouard",
      "Melanie Mitchell"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.07141",
    "githubLink": "https://github.com/victorvikram/ConceptARC",
    "itemCount": "160 tasks (16 concept groups × 10 tasks)",
    "specs": "JSON format, 2D grid pairs",
    "description": "A benchmark organized around specific 'concept groups' (e.g., center, same-different, inside-outside) to systematically assess abstraction and generalization capabilities. It aims to test if models have truly grasped specific spatial and semantic concepts."
  },
  {
    "id": "imported-1769500766500-345-6rm3c",
    "title": "1D-ARC",
    "source": "arXiv",
    "authors": [
      "Yudong Xu",
      "Wenhao Li",
      "Pashootan Vaezipoor",
      "Scott Sanner",
      "Elias B. Khalil"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.18354",
    "githubLink": "https://github.com/khalil-research/LLM4ARC",
    "itemCount": "Variable (generated via configurable parameters)",
    "specs": "JSON format, 1D arrays",
    "description": "A simplified version of ARC consisting of one-dimensional (array-like) tasks. It was designed to investigate the reasoning capabilities of Large Language Models (LLMs) and the importance of object-based representations in a simpler domain."
  },
  {
    "id": "imported-1769500766500-346-5ik8z",
    "title": "InterCode",
    "source": "arXiv",
    "authors": [
      "John Yang",
      "Akshara Prabhakar",
      "Karthik Narasimhan",
      "Shunyu Yao"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.14898",
    "githubLink": "https://github.com/princeton-nlp/intercode",
    "itemCount": "3 environments (Bash, SQL, Python)",
    "specs": "Interactive code environments, Docker containers, RL interface",
    "description": "A lightweight, flexible framework for designing interactive code environments to evaluate language agents. It treats coding as a standard reinforcement learning environment with code as actions and execution feedback as observations, supporting Bash, SQL, and Python tasks."
  },
  {
    "id": "imported-1769500766500-347-9ejtj",
    "title": "Mind2Web",
    "source": "arXiv",
    "authors": [
      "Xiang Deng",
      "Yu Gu",
      "Boyuan Zheng",
      "Shijie Chen",
      "Samuel Stevens",
      "Boshi Wang",
      "Huan Sun",
      "Yu Su"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.06070",
    "githubLink": "https://github.com/OSU-NLP-Group/Mind2Web",
    "itemCount": "2,350 tasks",
    "specs": "Text, HTML, DOM Snapshots, Action Sequences",
    "description": "A dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. It contains over 2,000 open-ended tasks collected from 137 real-world websites spanning 31 domains."
  },
  {
    "id": "imported-1769500766500-348-sr0n8",
    "title": "CBBQ: Chinese Bias Benchmark Dataset",
    "source": "arXiv",
    "authors": [
      "Yufei Huang",
      "Deyi Xiong"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.16244",
    "githubLink": "https://github.com/YFHuangxxxx/CBBQ",
    "itemCount": "106,588 generated instances",
    "specs": "Chinese text, Multiple Choice QA, over 3,000 templates",
    "description": "A Chinese bias benchmark dataset co-developed by human experts and generative language models. It covers stereotypes and societal biases across 14 social dimensions pertinent to Chinese culture and values."
  },
  {
    "id": "imported-1769500766500-349-popbi",
    "title": "HEIM (Holistic Evaluation of Text-to-Image Models)",
    "source": "arXiv",
    "authors": [
      "Tony Lee",
      "Michihiro Yasunaga",
      "Chenlin Meng",
      "Yifan Mai",
      "Joon Sung Park",
      "Agrim Gupta",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.04287",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "62 scenarios",
    "specs": "Text-to-Image; 12 evaluation aspects",
    "description": "An extension of the HELM framework focused on text-to-image models. It evaluates models across 12 aspects including alignment, quality, aesthetics, originality, reasoning, knowledge, bias, and toxicity."
  },
  {
    "id": "imported-1769500766500-350-jx2f5",
    "title": "Tensor Trust",
    "source": "arXiv",
    "authors": [
      "Sam Toyer",
      "Olivia Watkins",
      "Ethan Adrian Mendes",
      "Justin Svegliato",
      "Luke Bailey",
      "Tiffany Wang",
      "Isaac Ong",
      "Karim Elmaaroufi",
      "Pieter Abbeel",
      "Trevor Darrell",
      "Alan Ritter",
      "Stuart Russell"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.01011",
    "githubLink": "https://github.com/HumanCompatibleAI/tensor-trust",
    "itemCount": "126,000+ attacks, 46,000+ defenses",
    "specs": "Text (English), JSON/Parquet",
    "description": "A large-scale dataset of human-generated prompt injection attacks and defenses collected from an online game where players attempt to trick an LLM into revealing a password."
  },
  {
    "id": "imported-1769500766500-351-ptygu",
    "title": "BIPIA (Benchmark for Indirect Prompt Injection Attacks)",
    "source": "arXiv",
    "authors": [
      "Jian Yi",
      "Huiya Feng",
      "Zhenxiang Xiao",
      "Wei Tan",
      "Qun Xi",
      "Shengjie Zhang",
      "Yuan Ni",
      "Guotong Xie",
      "Zheng Zhang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.14197",
    "githubLink": "https://github.com/microsoft/BIPIA",
    "itemCount": "35,000+ malicious instances (expandable)",
    "specs": "Text, JSON",
    "description": "The first benchmark designed to evaluate the robustness of Large Language Models against indirect prompt injection attacks, covering tasks like email QA, web QA, and summarization."
  },
  {
    "id": "imported-1769500766500-352-fh3d6",
    "title": "deepset/prompt-injections",
    "source": "Hugging Face",
    "authors": [
      "deepset"
    ],
    "year": "2023",
    "paperLink": "https://huggingface.co/datasets/deepset/prompt-injections",
    "githubLink": "https://huggingface.co/datasets/deepset/prompt-injections",
    "itemCount": "662 samples",
    "specs": "Text, Parquet",
    "description": "A widely used small-scale dataset combining benign instructions and adversarial prompt injections, often used for training and evaluating basic injection classifiers."
  },
  {
    "id": "imported-1769500766500-353-gev70",
    "title": "XSTest",
    "source": "arXiv",
    "authors": [
      "Paul Röttger",
      "Hannah Rose Kirk",
      "Bertie Vidgen",
      "Giuseppe Attanasio",
      "Federico Bianchi",
      "Dirk Hovy"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.01263",
    "githubLink": "https://github.com/paul-rottger/xstest",
    "itemCount": "450 prompts (250 safe, 200 unsafe)",
    "specs": "Text prompts in CSV format; Includes 10 types of safe prompts and contrasting unsafe prompts",
    "description": "A test suite designed to identify exaggerated safety behaviors in Large Language Models (LLMs), specifically measuring false refusals where models refuse safe prompts that superficially resemble unsafe ones."
  },
  {
    "id": "imported-1769500766500-354-rc2om",
    "title": "EHRXQA",
    "source": "arXiv",
    "authors": [
      "Seongsu Bae",
      "Kyubok Lee",
      "Woncheol Shin",
      "Sang-Wook Kim",
      "Edward Choi"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.18652",
    "githubLink": "https://github.com/baeseongsu/ehrxqa",
    "itemCount": "46,152 samples",
    "specs": "Multi-modal (Structured Tables + Chest X-ray Images); Text, Image, SQL",
    "description": "A multi-modal question answering dataset that combines structured Electronic Health Records (EHR) from MIMIC-IV with chest X-ray images from MIMIC-CXR. It requires models to perform joint reasoning across both tabular and imaging modalities to answer clinical questions."
  },
  {
    "id": "imported-1769500766500-355-ixgmr",
    "title": "NYU Breast Pathology Reports Dataset",
    "source": "Other",
    "authors": [
      "K. T. Nguyen",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10319853/",
    "githubLink": "https://github.com/nyukat/pathology_extraction",
    "itemCount": "1,438 reports",
    "specs": "Text (Clinical Reports)",
    "description": "A dataset of breast pathology reports annotated for Named Entity Recognition (NER) tasks. It focuses on extracting key diagnostic elements such as cancer subtype, cancer grade, and lesion position from free-text clinical reports."
  },
  {
    "id": "imported-1769500766500-356-plo7t",
    "title": "M4Raw",
    "source": "Scholar",
    "authors": [
      "Mengye Lyu",
      "Lifeng Mei",
      "Shoujin Huang",
      "Sixing Liu",
      "Yi Li"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1038/s41597-023-02181-4",
    "githubLink": "https://github.com/mylyu/M4Raw",
    "itemCount": "183 subjects, >25k slices",
    "specs": "Raw k-space (0.3T), T1w, T2w, FLAIR",
    "description": "A multi-contrast, multi-repetition, multi-channel MRI k-space dataset for low-field MRI research and reconstruction."
  },
  {
    "id": "imported-1769500766500-357-xdj73",
    "title": "RadImageNet-VQA",
    "source": "Hugging Face",
    "authors": [
      "Raidium"
    ],
    "year": "2023",
    "paperLink": "https://huggingface.co/datasets/raidium/RadImageNet-VQA",
    "githubLink": "https://huggingface.co/datasets/raidium/RadImageNet-VQA",
    "itemCount": "750,000 images, 6.75M QA pairs",
    "specs": "Images (CT, MRI), Text (Open-ended, Closed-ended, Multiple-choice)",
    "description": "A large-scale synthetic dataset designed for training and benchmarking radiologic VQA on CT and MRI exams. It is built from the RadImageNet dataset with generated QA pairs to challenge multimodal models."
  },
  {
    "id": "imported-1769500766500-358-q5hlk",
    "title": "DEEP-VOICE",
    "source": "Other",
    "authors": [
      "Bird",
      "J. J.",
      "Lotfi",
      "A."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.12734",
    "githubLink": "https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition",
    "itemCount": "~11,778 audio samples (5,889 real, 5,889 fake)",
    "specs": "Audio (.wav), Real vs. AI-Generated (RVC)",
    "description": "A dataset designed for the detection of AI-generated speech (voice cloning). It contains real human speech from public figures and counterparts generated using Retrieval-based Voice Conversion (RVC)."
  },
  {
    "id": "imported-1769500766500-359-c1h98",
    "title": "GPTCloneBench",
    "source": "arXiv",
    "authors": [
      "Alam",
      "A. I.",
      "Roy",
      "P. R.",
      "Al-Omari",
      "F.",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.14088",
    "githubLink": "https://github.com/yh1105/datasetforTBCCD",
    "itemCount": "~37k true semantic pairs, ~20k cross-language pairs",
    "specs": "Source Code (Java, C, C#, Python), Semantic Clones",
    "description": "A comprehensive benchmark for evaluating semantic and cross-language code clones generated by Large Language Models (specifically GPT-3). It focuses on assessing how well detectors identify AI-generated semantic equivalents."
  },
  {
    "id": "imported-1769500766500-360-ity9n",
    "title": "Genomic Benchmarks",
    "source": "Other",
    "authors": [
      "Katarína Grešová",
      "Vlastimil Martinek",
      "David Čechák",
      "Petr Šimeček",
      "Panagiotis Alexiou"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1186/s12863-023-01123-8",
    "githubLink": "https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks",
    "itemCount": "9 datasets",
    "specs": "Genomic sequences (text/DNA), classification tasks, Python package",
    "description": "A collection of curated and easily accessible sequence classification datasets for genomics, covering regulatory elements like promoters and enhancers across human, mouse, and roundworm genomes."
  },
  {
    "id": "imported-1769500766500-361-6k3vb",
    "title": "Genome Understanding Evaluation (GUE)",
    "source": "arXiv",
    "authors": [
      "Zhihan Zhou",
      "Yanrong Ji",
      "Weijian Li",
      "Pratik Dutta",
      "Ramana Davuluri",
      "Han Liu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.15581",
    "githubLink": "https://github.com/MAGICS-LAB/DNABERT_2",
    "itemCount": "28-36 datasets across 7-9 tasks",
    "specs": "Multi-species (Human, Mouse, Virus, Yeast) DNA sequences; input lengths ranging from 70 to 10,000 base pairs.",
    "description": "A comprehensive multi-species genome classification benchmark designed to evaluate DNA language models. It amalgamates diverse datasets to test capabilities in core promoter detection, transcription factor prediction, splice site prediction, and more."
  },
  {
    "id": "imported-1769500766500-362-k3vz5",
    "title": "Plant Genomic Benchmark (PGB)",
    "source": "Hugging Face",
    "authors": [
      "Guillaume Sala",
      "Odile Moreira",
      "Pauline Vicas",
      "Thomas Pierrot",
      "Karim Beguir",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1101/2023.10.27.564408",
    "githubLink": "https://huggingface.co/datasets/InstaDeepAI/plant-genomic-benchmark",
    "itemCount": "28 datasets across 8 tasks",
    "specs": "Plant DNA sequences (up to 6,000 bp); tasks include gene expression prediction (regression) and regulatory element classification.",
    "description": "A large-scale DNA benchmark suite designed to evaluate foundation models on plant biology tasks. It covers single and multi-output regression and classification tasks such as promoter strength prediction, gene expression, and chromatin accessibility in various plant species."
  },
  {
    "id": "imported-1769500766500-363-vta8q",
    "title": "SimpleSafetyTests",
    "source": "Hugging Face",
    "authors": [
      "Bertie Vidgen",
      "Hannah Rose Kirk",
      "Paul Röttger",
      "Scott Hale"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.08370",
    "githubLink": "https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests",
    "itemCount": "100 test prompts",
    "specs": "Text prompts; 5 primary harm areas",
    "description": "A lightweight test suite for rapidly identifying critical safety risks. It contains 100 clear-cut unsafe prompts across five severe harm areas (e.g., suicide, child abuse) that models should definitively refuse."
  },
  {
    "id": "imported-1769500766500-364-dk8io",
    "title": "SafetyBench",
    "source": "arXiv",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Lindong Wu",
      "Rui Sun",
      "Yongkang Huang",
      "Chong Long",
      "Xiao Liu",
      "Xuanyu Lei",
      "Jie Tang",
      "Minlie Huang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.13788",
    "githubLink": "https://github.com/thu-coai/SafetyBench",
    "itemCount": "11,435 questions",
    "specs": "Text (Multiple Choice Questions), Chinese and English",
    "description": "A comprehensive benchmark for evaluating the safety of LLMs comprising diverse multiple-choice questions across 7 distinct categories of safety concerns."
  },
  {
    "id": "imported-1769500766500-365-4w0qf",
    "title": "ToxicChat",
    "source": "arXiv",
    "authors": [
      "Zi Lin",
      "Zihan Wang",
      "Yongqi Tong",
      "Yangkun Wang",
      "Yuxin Guo",
      "Yujia Wang",
      "Jingbo Shang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.17389",
    "githubLink": "https://github.com/lmsys/toxic-chat",
    "itemCount": "10,166 conversation pairs",
    "specs": "Text (User-AI conversation pairs), annotated for toxicity and jailbreaking",
    "description": "A benchmark based on real-world user-AI conversations from an open-source chatbot, designed to evaluate toxicity detection and jailbreaking attempts in realistic interactive settings."
  },
  {
    "id": "imported-1769500766500-366-i831r",
    "title": "MITweet (Multifaceted Ideology Detection)",
    "source": "Scholar",
    "authors": [
      "Songtao Liu",
      "Ziling Luo",
      "Minghua Xu",
      "LiXiao Wei",
      "Ziyao Wei",
      "Han Yu",
      "Wei Xiang",
      "Bang Wang"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.emnlp-main.865/",
    "githubLink": "https://github.com/LST1836/MITweet",
    "itemCount": "12,594 tweets",
    "specs": "English Tweets, 12 Facet Labels (Relevance & Ideology)",
    "description": "A high-quality dataset for multifaceted ideology detection, moving beyond binary left-right classification to capture ideological stances across 12 specific domains/facets."
  },
  {
    "id": "imported-1769500766500-367-o1e5r",
    "title": "MBIB (Media Bias Identification Benchmark)",
    "source": "arXiv",
    "authors": [
      "Martin Wessel",
      "Tomáš Horych",
      "Terry Ruas",
      "Akiko Aizawa",
      "Bela Gipp",
      "Timo Spinde"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.13148",
    "githubLink": "https://github.com/media-bias-group/mbib-base",
    "itemCount": "22 datasets (aggregated)",
    "specs": "Diverse text formats, Multiple bias types",
    "description": "A comprehensive benchmark that groups different types of media bias (linguistic, cognitive, and political) under a common framework, distilled from a review of over 100 datasets."
  },
  {
    "id": "imported-1769500766500-368-6cemv",
    "title": "SycophancyEval",
    "source": "Hugging Face",
    "authors": [
      "Mrinank Sharma",
      "Meg Tong",
      "Tomasz Korbak",
      "David Duvenaud",
      "Amanda Askell",
      "Samuel R. Bowman",
      "Newton Cheng",
      "Esin Durmus",
      "Zac Hatfield-Dodds",
      "Scott R. Johnston",
      "Shauna Kravec",
      "Timothy Maxwell",
      "Sam McCandlish",
      "Kamal Ndousse",
      "Oliver Rausch",
      "Nicholas Schiefer",
      "Da Yan",
      "Miranda Zhang",
      "Ethan Perez"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.13548",
    "githubLink": "https://github.com/meg-tong/sycophancy-eval",
    "itemCount": "~21,000 prompts",
    "specs": "Text (JSONL format); Tasks: Answer, Argument, Feedback, Are You Sure?",
    "description": "A benchmark designed to evaluate sycophantic behavior in language models across varied free-form text-generation tasks, including answering questions with user opinions, providing feedback on arguments, and responding to challenges (e.g., 'Are you sure?')."
  },
  {
    "id": "imported-1769500766500-369-2cjw4",
    "title": "OpinionQA",
    "source": "arXiv",
    "authors": [
      "Shibani Santurkar",
      "Esin Durmus",
      "Faisal Ladhak",
      "Cinoo Lee",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2303.17548",
    "githubLink": "https://github.com/tatsu-lab/opinions_qa",
    "itemCount": "1,498 multiple-choice questions",
    "specs": "Text; Multiple-choice questions derived from public opinion surveys",
    "description": "Evaluates the alignment of Large Language Model opinions with those of 60 US demographic groups using questions from Pew Research Center's American Trends Panel surveys."
  },
  {
    "id": "imported-1769500766500-370-wen9h",
    "title": "MACHIAVELLI Benchmark",
    "source": "arXiv",
    "authors": [
      "Alexander Pan",
      "Jun Shern Chan",
      "Andy Zou",
      "Nathaniel Li",
      "Steven Basart",
      "Thomas Woodside",
      "Jonathan Ng",
      "Hanlin Zhang",
      "Scott Emmons",
      "Dan Hendrycks"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.03279",
    "githubLink": "https://github.com/aypan17/machiavelli",
    "itemCount": "134 games (572k scenes, 3M annotations)",
    "specs": "Text-based Reinforcement Learning environment; Annotated with behavioral metrics.",
    "description": "A benchmark consisting of 134 text-based 'Choose-Your-Own-Adventure' games designed to evaluate artificial agents on their tendency to engage in power-seeking, deceptive, and unethical behaviors (Machiavellianism) while attempting to maximize rewards. It tracks trade-offs between reward maximization and ethical constraints."
  },
  {
    "id": "imported-1769500766500-371-snknx",
    "title": "ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code",
    "source": "arXiv",
    "authors": [
      "Yuliang Liu",
      "Xiangru Tang",
      "Zefan Cai",
      "Junjie Lu",
      "Yichi Zhang",
      "Yanjun Shao",
      "Zexuan Deng",
      "Helan Hu",
      "Zengxian Yang",
      "Kaikai An",
      "Ruijun Huang",
      "Shuzheng Si",
      "Sheng Chen",
      "Haozhe Zhao",
      "Zhengliang Li",
      "Liang Chen",
      "Zhiwei Jiang",
      "Baobao Chang",
      "Yujia Qin",
      "Wangchunshu Zhou",
      "Yilun Zhao",
      "Arman Cohan",
      "Mark Gerstein"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.09835",
    "githubLink": "https://github.com/gersteinlab/ML-Bench",
    "itemCount": "9,641 examples",
    "specs": "Annotated examples across 18 GitHub repositories; Repository-level code tasks",
    "description": "A benchmark rooted in real-world programming applications that leverages existing code repositories to perform ML tasks. It assesses the capability of LLMs to generate executable scripts and autonomous agents to perform complex coding tasks in a Linux sandbox environment."
  },
  {
    "id": "imported-1769500766500-372-ag5ni",
    "title": "AI Village Capture the Flag @ DEFCON31",
    "source": "Other",
    "authors": [
      "AI Village"
    ],
    "year": "2023",
    "paperLink": "https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31",
    "githubLink": "https://github.com/adv-ml-notebooks/ai-village-ctf-defcon31",
    "itemCount": "27 challenges",
    "specs": "Diverse ML challenges involving adversarial examples, model inversion, and other AI security concepts.",
    "description": "A collection of machine learning security challenges designed for the AI Village CTF at DEFCON 31. Participants interact with ML models to find flags by exploiting vulnerabilities."
  },
  {
    "id": "imported-1769500766500-373-dvjtd",
    "title": "HackAPrompt Dataset",
    "source": "Hugging Face",
    "authors": [
      "Sander Schulhoff",
      "Jeremy Pinto",
      "Anaum Khan",
      "Louis-François Bouchard",
      "Chenglei Si",
      "Jordan Boyd-Graber"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.16119",
    "githubLink": "https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset",
    "itemCount": "600,000+ samples",
    "specs": "Text (User inputs, model completions, expected outputs)",
    "description": "A large-scale dataset of over 600,000 adversarial prompts generated during a global prompt hacking competition. It covers varying levels of difficulty and multiple attack types, including prompt leaking and jailbreaking, designed to expose vulnerabilities in LLMs."
  },
  {
    "id": "imported-1769500766500-374-w2usn",
    "title": "BeaverTails",
    "source": "Hugging Face",
    "authors": [
      "Jiaming Ji",
      "Mickel Liu",
      "Josef Dai",
      "Xuehai Pan",
      "Chi Zhang",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.00861",
    "githubLink": "https://github.com/PKU-Alignment/beavertails",
    "itemCount": "~333,963 QA pairs; ~361,903 comparison pairs",
    "specs": "Text (QA pairs), Safety meta-labels, Preference data",
    "description": "A large-scale dataset designed to foster research on safety alignment. It uniquely separates annotations for helpfulness and harmlessness, providing safety meta-labels and expert comparison data."
  },
  {
    "id": "imported-1769500766500-375-iywo8",
    "title": "UltraFeedback",
    "source": "Hugging Face",
    "authors": [
      "Ganqu Cui",
      "Lifan Yuan",
      "Ning Ding",
      "Guanming Yao",
      "Wei Zhu",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.01377",
    "githubLink": "https://github.com/OpenBMB/UltraFeedback",
    "itemCount": "~64,000 prompts; ~256,000 samples",
    "specs": "Text, Scalar ratings, Textual feedback",
    "description": "A large-scale, fine-grained, diverse preference dataset used for training reward models. It uses GPT-4 to generate detailed feedback and numerical ratings for model responses."
  },
  {
    "id": "imported-1769500766500-376-116a0",
    "title": "Psych8k",
    "source": "arXiv",
    "authors": [
      "June M. Liu",
      "Donghao Li",
      "He Cao",
      "Tianhe Ren",
      "Zeyi Liao",
      "Jiamin Wu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.15461",
    "githubLink": "https://github.com/EmoCareAI/ChatPsychiatrist",
    "itemCount": "8,187 QA pairs",
    "specs": "Text (English, Counseling Dialogues)",
    "description": "A dataset constructed from 260 in-depth real counseling interviews, processed into single-turn query-answer pairs to fine-tune LLMs (like ChatPsychiatrist) for mental health support."
  },
  {
    "id": "imported-1769500766500-377-73bqr",
    "title": "HEx-PHI (Harmful Examples - Prohibited, Harmful Instructions)",
    "source": "Hugging Face",
    "authors": [
      "Xiangyu Qi",
      "Yi Zeng",
      "Tinghao Xie",
      "Pin-Yu Chen",
      "Ruoxi Jia",
      "Prateek Mittal",
      "Peter Henderson"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.05914",
    "githubLink": "https://huggingface.co/datasets/LLM-Tuning-Safety/HEx-PHI",
    "itemCount": "330 harmful instructions",
    "specs": "Text-only; 11 harmful categories (e.g., illegal activity, violence, malware)",
    "description": "A dataset of harmful instructions used to evaluate safety risks in fine-tuned LLMs. It is categorized according to prohibited content policies similar to those of Meta and OpenAI."
  },
  {
    "id": "imported-1769500766500-378-ehjbl",
    "title": "FalseQA",
    "source": "arXiv",
    "authors": [
      "Shengding Hu",
      "Yifan Luo",
      "Huadong Wang",
      "Xingyi Cheng",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.acl-long.309/",
    "githubLink": "https://github.com/thunlp/FalseQA",
    "itemCount": "2,365 questions",
    "specs": "CSV format with columns for question, answer (rebuttal), and binary label (0/1).",
    "description": "A specialized dataset of human-written False Premise Questions (FPQs) designed to evaluate whether pre-trained language models can discriminate and rebut tricky questions with false premises."
  },
  {
    "id": "imported-1769500766500-379-7zpru",
    "title": "(QA)2",
    "source": "arXiv",
    "authors": [
      "Najoung Kim",
      "Phu Mon Htut",
      "Samuel R. Bowman",
      "Jackson Petty"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.acl-long.471/",
    "githubLink": "https://github.com/najoungkim/qa2",
    "itemCount": "~602 examples",
    "specs": "Real-world search queries with annotations for questionable assumptions.",
    "description": "An open-domain evaluation dataset consisting of naturally occurring search engine queries that contain questionable assumptions (false or unverifiable), designed to test if models can detect them."
  },
  {
    "id": "imported-1769500766500-380-onpv6",
    "title": "FreshQA",
    "source": "arXiv",
    "authors": [
      "Tu Vu",
      "Mohit Iyyer",
      "Xuezhi Wang",
      "Noah Constant",
      "Jerry Wei",
      "Jason Wei",
      "Chris Tar",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.03214",
    "githubLink": "https://github.com/freshllms/freshqa",
    "itemCount": "600 questions",
    "specs": "QA pairs categorized by valid/false premise and fast-changing/slow-changing status.",
    "description": "A dynamic QA benchmark that includes a category for questions with false premises, designed to test LLMs on fast-changing world knowledge and their ability to debunk false premises."
  },
  {
    "id": "imported-1769500766500-381-546s6",
    "title": "FActScore",
    "source": "arXiv",
    "authors": [
      "Sewon Min",
      "Kalpesh Krishna",
      "Xinxi Lyu",
      "Mike Lewis",
      "Wen-tau Yih",
      "Pang Wei Koh",
      "Mohit Iyyer",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.14251",
    "githubLink": "https://github.com/kalpeshk2011/FActScore",
    "itemCount": "183 labeled entities / 500 unlabeled entities",
    "specs": "Text (biographies); Atomic fact decomposition; Wikipedia verification",
    "description": "FActScore (Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation) is a benchmark and metric that breaks down long-form text generations (specifically people biographies) into atomic facts and verifies them against a knowledge source like Wikipedia to measure factual precision."
  },
  {
    "id": "imported-1769500766500-382-t1eec",
    "title": "FacTool",
    "source": "arXiv",
    "authors": [
      "I. Chern",
      "Steffi Chern",
      "Shuyue Hu",
      "William Yang Wang",
      "Pengfei Liu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.13528",
    "githubLink": "https://github.com/GAIR-NLP/factool",
    "itemCount": "Variable (Multi-task evaluation set)",
    "specs": "Text, Code, Math; Tool-augmented verification",
    "description": "A task-augmented framework and benchmark for detecting factual errors in generative AI. It covers multiple domains including knowledge-based QA, code generation, mathematical reasoning, and scientific literature review."
  },
  {
    "id": "imported-1769500766500-383-h6y28",
    "title": "MT-Bench",
    "source": "arXiv",
    "authors": [
      "Lianmin Zheng",
      "Wei-Lin Chiang",
      "Ying Sheng",
      "Siyuan Zhuang",
      "Zhanghao Wu",
      "Yonghao Zhuang",
      "Zi Lin",
      "Zhuohan Li",
      "Dacheng Li",
      "Eric. P Xing",
      "Hao Zhang",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.05685",
    "githubLink": "https://github.com/lm-sys/FastChat",
    "itemCount": "80 prompts (160 turns)",
    "specs": "Multi-turn chat, Text, Pairwise comparison",
    "description": "A set of 80 high-quality multi-turn questions designed to test the conversation flow and instruction-following capabilities of LLMs. It uses GPT-4 as a judge to evaluate model responses against a reference."
  },
  {
    "id": "imported-1769500766500-384-vjh70",
    "title": "JudgeLM-100K",
    "source": "arXiv",
    "authors": [
      "Lianghui Zhu",
      "Xinggang Wang",
      "Xinlong Wang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.17631",
    "githubLink": "https://github.com/baaivision/JudgeLM",
    "itemCount": "100,000 samples (training)",
    "specs": "Text, Instruction following, Judging pairs",
    "description": "A large-scale dataset used to fine-tune and evaluate LLMs as scalable judges. It contains judge samples with high-quality annotations to train models that can perform judge tasks efficiently."
  },
  {
    "id": "imported-1769500766500-385-px8cc",
    "title": "PandaLM",
    "source": "arXiv",
    "authors": [
      "Yidong Wang",
      "Zhuohao Yu",
      "Zhengran Zeng",
      "Linyi Yang",
      "Cunxiang Wang",
      "Hao Chen",
      "Chaoya Jiang",
      "Rui Xie",
      "Jindong Wang",
      "Xing Xie",
      "Wei Ye",
      "Shikun Zhang",
      "Yue Zhang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.05087",
    "githubLink": "https://github.com/WeOpenML/PandaLM",
    "itemCount": "~1,000 test samples",
    "specs": "Text, Instruction tuning optimization, Pairwise comparison",
    "description": "A benchmark and judge model focused on automating the evaluation of instruction-tuned models. It includes a human-annotated test set to ensure the reliability of the automated judge."
  },
  {
    "id": "imported-1769500766500-386-hfk05",
    "title": "VisAlign",
    "source": "arXiv",
    "authors": [
      "Jiyoung Lee",
      "Seungho Kim",
      "Seunghyun Won",
      "Joonseok Lee",
      "Marzyeh Ghassemi",
      "James Thorne",
      "Jaeseok Choi",
      "O-Kil Kwon",
      "Edward Choi"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.01525",
    "githubLink": "https://github.com/jiyounglee-0523/VisAlign",
    "itemCount": "Not specified (Multiple categories)",
    "specs": "Image classification, Visual alignment",
    "description": "A dataset for measuring the degree of alignment between AI models and human visual perception. It categorizes images based on whether a model must classify, abstain, or is uncertain, mirroring human visual capabilities."
  },
  {
    "id": "imported-1769500766500-387-gb0xl",
    "title": "FNS 2023 Dataset",
    "source": "Other",
    "authors": [
      "Mahmoud El-Haj",
      "Paul Rayson",
      "Nadhem Zmandar",
      "George Giannakopoulos",
      "Nikiforos Pittaras",
      "Marina Litvak",
      "Natalia Vanetik",
      "Ahmed AbuRa'ed"
    ],
    "year": "2023",
    "paperLink": "https://wp.lancs.ac.uk/cfie/fns2023/",
    "githubLink": "https://github.com/iit-Demokritos/FNS2023_data",
    "itemCount": "~4,000 annual reports",
    "specs": "PDF/Text annual reports, Gold standard summaries",
    "description": "Dataset for the Financial Narrative Summarisation shared task, focusing on generating summaries from UK, Spanish, and Greek annual financial reports."
  },
  {
    "id": "imported-1769500766500-388-emoyk",
    "title": "RadGraph2",
    "source": "Other",
    "authors": [
      "Sameer Khanna",
      "Adam Dejl",
      "Kibo Yoon",
      "Steven QH Truong",
      "Hanh Duong",
      "Agustina Saenz",
      "Pranav Rajpurkar"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.05046",
    "githubLink": "https://physionet.org/content/radgraph2",
    "itemCount": "800 annotated reports",
    "specs": "Text (Radiology Reports), Hierarchical Entity-Relation Schema",
    "description": "A dataset extending RadGraph to model disease progression in radiology reports. It utilizes a hierarchical schema to capture entities and relations, specifically focusing on changes in disease state and device placement over time compared to prior reports."
  },
  {
    "id": "imported-1769500766500-389-hypiz",
    "title": "MeetingBank",
    "source": "Other",
    "authors": [
      "Yebowen Hu",
      "Tim Ganter",
      "Hanieh Deilamsalehy",
      "Franck Dernoncourt",
      "Hassan Foroosh",
      "Fei Liu"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.acl-long.906/",
    "githubLink": "https://github.com/YebowenHu/MeetingBank-utils",
    "itemCount": "1,366 meetings (6,892 segments)",
    "specs": "Text, Audio, Video (City Council Meetings)",
    "description": "A benchmark dataset for meeting summarization derived from city council meetings of major U.S. cities. It includes video, audio, transcripts, and meeting minutes, enabling research into multi-modal and long-form meeting summarization."
  },
  {
    "id": "imported-1769500766500-390-dcvgy",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "source": "arXiv",
    "authors": [
      "Boxin Wang",
      "Weixin Chen",
      "Hengzhi Pei",
      "Chulin Xie",
      "Mintong Kang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.11698",
    "githubLink": "https://github.com/AI-secure/DecodingTrust",
    "itemCount": "Hundreds of thousands of prompts",
    "specs": "Text; 8 trustworthiness perspectives (toxicity, bias, robustness, privacy, etc.)",
    "description": "A comprehensive trustworthiness evaluation platform for large language models, specifically GPT-4 and GPT-3.5, covering diverse perspectives including toxicity, stereotype bias, adversarial robustness, OOD robustness, privacy, machine ethics, and fairness."
  },
  {
    "id": "imported-1769500766500-391-w75uc",
    "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models",
    "source": "arXiv",
    "authors": [
      "Yue Huang",
      "Qihui Zhang",
      "Philip S. Yu",
      "Lichao Sun"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.11507",
    "githubLink": "https://github.com/HowieHwong/TrustGPT",
    "itemCount": "Uses Social Chemistry 101 (292k norms)",
    "specs": "Text; Toxicity, bias, value-alignment evaluation",
    "description": "A benchmark designed to evaluate LLMs from three ethical perspectives: toxicity, bias, and value-alignment, utilizing the Social Chemistry 101 dataset."
  },
  {
    "id": "imported-1769500766500-392-uxcnr",
    "title": "XIMAGENET-12",
    "source": "arXiv",
    "authors": [
      "Qiang Li",
      "Dan Zhang",
      "Shengzhao Lei",
      "Xun Zhao",
      "WeiWei Li",
      "Porawit Kamnoedboon",
      "Junhao Dong",
      "Shuyan Li"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.08182",
    "githubLink": "https://sites.google.com/view/ximagenet-12/home",
    "itemCount": "200,000+ images; 15,410 manual annotations",
    "specs": "Images; Semantic annotations, Segmentation masks",
    "description": "An explainable visual benchmark dataset designed to evaluate the robustness of visual models. It consists of images from 12 ImageNet categories with manual semantic annotations and simulates diverse real-world scenarios (e.g., blurring, background changes) to assess model reliance on specific features."
  },
  {
    "id": "imported-1769500766500-393-hpv36",
    "title": "MMBench",
    "source": "arXiv",
    "authors": [
      "Yuan Liu",
      "Haodong Duan",
      "Yuanhan Zhang",
      "Bo Li",
      "Songyang Zhang",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.06281",
    "githubLink": "https://github.com/open-compass/MMBench",
    "itemCount": "2,974 questions",
    "specs": "Multiple-choice questions covering 20 ability dimensions arranged in a hierarchy (Perception, Reasoning, etc.); uses CircularEval to test consistency.",
    "description": "A comprehensive evaluation pipeline for large vision-language models using 'CircularEval' to assess robustness across perception and reasoning abilities."
  },
  {
    "id": "imported-1769500766500-394-y0shz",
    "title": "SEED-Bench",
    "source": "arXiv",
    "authors": [
      "Bohao Li",
      "Rui Wang",
      "Guangzhi Wang",
      "Yuying Ge",
      "Yixiao Ge",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.16125",
    "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
    "itemCount": "19,000 questions (v1)",
    "specs": "Multiple-choice questions covering 12 dimensions; includes both image and video modalities for spatial and temporal reasoning.",
    "description": "A multimodal benchmark for evaluating Large Language Models on spatial and temporal understanding capabilities using multiple-choice questions."
  },
  {
    "id": "imported-1769500766500-395-rvzy1",
    "title": "MME (Multimodal Evaluation)",
    "source": "arXiv",
    "authors": [
      "Chaoyou Fu",
      "Peixian Chen",
      "Yunhang Shen",
      "Yulei Qin",
      "Menglin Zhang",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.13394",
    "githubLink": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation",
    "itemCount": "14 subtasks",
    "specs": "Yes/No questions to avoid prompt engineering bias; covers coarse/fine-grained perception, OCR, logic, and numerical reasoning.",
    "description": "A comprehensive evaluation suite measuring both perception and cognition abilities of Multimodal Large Language Models (MLLMs)."
  },
  {
    "id": "imported-1769500766500-396-qgyg2",
    "title": "POPE (Polling on Object Existence)",
    "source": "arXiv",
    "authors": [
      "Yifan Li",
      "Yifan Du",
      "Kun Zhou",
      "Jinpeng Wang",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.10355",
    "githubLink": "https://github.com/RUCAIBox/POPE",
    "itemCount": "3,000 visual-object pairs",
    "specs": "Binary (Yes/No) questions about object existence; includes random, popular, and adversarial sampling settings.",
    "description": "A polling-based evaluation method specifically designed to detect and evaluate object hallucination in Large Vision-Language Models."
  },
  {
    "id": "imported-1769500766500-397-938g6",
    "title": "Do Anything Now (JailbreakHub)",
    "source": "arXiv",
    "authors": [
      "Xinyue Shen",
      "Zeyuan Chen",
      "Michael Backes",
      "Yun Shen",
      "Yang Zhang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.03825",
    "githubLink": "https://github.com/verazuo/jailbreak_llms",
    "itemCount": "1,405 jailbreak prompts, 107,250 samples",
    "specs": "Text",
    "description": "A comprehensive analysis of 'in-the-wild' jailbreak prompts, specifically the 'Do Anything Now' (DAN) type. The authors collected a large dataset of jailbreak prompts from online communities to evaluate LLM safeguards."
  },
  {
    "id": "imported-1769500766500-398-x3dta",
    "title": "SciEval",
    "source": "arXiv",
    "authors": [
      "Liangtai Sun",
      "Yang Han",
      "Zihan Zhao",
      "Da Ma",
      "Zhennan Shen",
      "Baocai Chen",
      "Lu Chen",
      "Kai Yu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.13149",
    "githubLink": "https://github.com/OpenDFM/SciEval",
    "itemCount": "~18,000 questions",
    "specs": "Text, Objective and Subjective Questions",
    "description": "A comprehensive multi-disciplinary evaluation benchmark for scientific research abilities of Large Language Models. It covers four dimensions based on Bloom's taxonomy: basic knowledge, knowledge application, scientific calculation, and research ability."
  },
  {
    "id": "imported-1769500766500-399-i9dmq",
    "title": "SciBench",
    "source": "arXiv",
    "authors": [
      "Xiaoxuan Wang",
      "Ziniu Hu",
      "Pan Lu",
      "Yanqiao Zhu",
      "Jieyu Zhang",
      "Satyen Subramaniam",
      "Arjun R. Loomba",
      "Shichang Zhang",
      "Yizhou Sun",
      "Wei Wang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.10635",
    "githubLink": "https://github.com/mandarin-wang/SciBench",
    "itemCount": "869 open-ended problems, 103 closed-set problems",
    "specs": "Text + some Multimodal (Images in 177 problems), Free-response",
    "description": "A benchmark for evaluating college-level scientific problem-solving abilities of LLMs. It features open-ended, free-response questions from mathematics, chemistry, and physics textbooks, requiring multi-step reasoning and complex calculations."
  },
  {
    "id": "imported-1769500766500-400-4xdwy",
    "title": "SuperBench",
    "source": "arXiv",
    "authors": [
      "Pu Ren",
      "N. Benjamin Erichson",
      "Junyi Guo",
      "Shashank Subramanian",
      "Omer San",
      "Zarija Lukić",
      "Michael W. Mahoney"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.13596",
    "githubLink": "https://github.com/erichson/SuperBench",
    "itemCount": "439 GB (Storage size)",
    "specs": "Image/Grid Data; Super-resolution",
    "description": "A high-resolution benchmark dataset for scientific machine learning (SciML) tasks, specifically super-resolution. It includes data from fluid flows, cosmology, and weather simulations to validate spatial reconstruction performance."
  },
  {
    "id": "imported-1769500766500-401-ny68g",
    "title": "Mol-Instructions",
    "source": "arXiv",
    "authors": [
      "Yin Fang",
      "Xiaozhuan Liang",
      "Ningyu Zhang",
      "Kangwei Liu",
      "Rui Huang",
      "Zhuo Chen",
      "Xiaohui Fan",
      "Huajun Chen"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.08018",
    "githubLink": "https://github.com/zjunlp/Mol-Instructions",
    "itemCount": "~148.4K molecule instructions, ~505K protein instructions",
    "specs": "Text and Sequence-based (SMILES, Protein Sequences); Instruction Tuning format",
    "description": "A comprehensive instruction tuning dataset for the biomolecular domain, designed to improve LLMs' understanding of biomolecules. It includes tasks related to small molecules, proteins, and biomolecular text."
  },
  {
    "id": "imported-1769500766500-402-e4yvr",
    "title": "MatSci-NLP",
    "source": "arXiv",
    "authors": [
      "Yu Song",
      "Santiago Miret",
      "Bang Liu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.08264",
    "githubLink": "https://github.com/IntelLabs/MatSci-NLP",
    "itemCount": "7 tasks",
    "specs": "Text-based; NLP tasks (NER, Relation Classification, etc.); Materials Science domain",
    "description": "A natural language benchmark for evaluating NLP models on materials science text. It encompasses seven tasks ranging from named entity recognition to synthesis action retrieval, specifically tailored for the materials science domain."
  },
  {
    "id": "imported-1769500766500-403-hlowt",
    "title": "Belebele",
    "source": "arXiv",
    "authors": [
      "Lucas Bandarkar",
      "Davis Liang",
      "Benjamin Muller",
      "Mikel Artetxe",
      "Satya Narayan Shukla",
      "Donald Husa",
      "Naman Goyal",
      "Abhinandan Krishnan",
      "Luke Zettlemoyer",
      "Madian Khabsa"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.16884",
    "githubLink": "https://github.com/facebookresearch/belebele",
    "itemCount": "122 languages, ~900 questions per language",
    "specs": "Text; Multiple-choice Reading Comprehension",
    "description": "A massively multilingual reading comprehension dataset spanning 122 language variants, with questions linked to FLORES-200 passages."
  },
  {
    "id": "imported-1769500766500-404-ikg60",
    "title": "CyberSecEval / Purple Llama",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Cyrus Nikolaidis",
      "Shengye Wan",
      "Ivan Evtimov",
      "Joshua Saxe",
      "Meta AI"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.03262",
    "githubLink": "https://github.com/meta-llama/PurpleLlama",
    "itemCount": "Multiple subsets: AutoPatchBench (142 test cases), Instruct/Autocomplete (Thousands of prompts)",
    "specs": "Text prompts, code snippets, and containerized test environments for C/C++ vulnerability patching.",
    "description": "A comprehensive benchmark suite from Meta designed to evaluate the cybersecurity risks of Large Language Models (LLMs) as coding assistants and offensive helpers. It includes tests for insecure code generation, compliance with cyberattack requests, and automated patching capabilities (AutoPatchBench)."
  },
  {
    "id": "imported-1769500766500-405-0gt7k",
    "title": "MultiPL-E",
    "source": "arXiv",
    "authors": [
      "Federico Cassano",
      "John Gouwar",
      "Daniel Nguyen",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2208.08227",
    "githubLink": "https://github.com/nuprl/MultiPL-E",
    "itemCount": "Variable (translations of HumanEval/MBPP)",
    "specs": "18+ languages (C++, Java, Rust, TypeScript, etc.)",
    "description": "A system for translating unit test-driven neural code generation benchmarks (like HumanEval and MBPP) to 18+ other programming languages to evaluate multilingual performance."
  },
  {
    "id": "imported-1769500766500-406-nt2d8",
    "title": "RepoBench",
    "source": "arXiv",
    "authors": [
      "Tianyang Liu",
      "Canwen Xu",
      "Julian McAuley"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.03091",
    "githubLink": "https://github.com/Leolty/repobench",
    "itemCount": "Variable (subsets for Retrieval/Completion)",
    "specs": "Python, Java, Repository-level context",
    "description": "A benchmark designed for evaluating repository-level code auto-completion systems. It evaluates retrieval, code completion, and pipeline tasks with cross-file context."
  },
  {
    "id": "imported-1769500766500-407-1v0mh",
    "title": "CrossCodeEval",
    "source": "arXiv",
    "authors": [
      "Yangruibo Ding",
      "Zijian Wang",
      "Wasi Uddin Ahmad",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.11248",
    "githubLink": "https://github.com/amazon-science/cceval",
    "itemCount": "Variable (diverse real-world samples)",
    "specs": "Python, Java, TypeScript, C#",
    "description": "A diverse and multilingual benchmark for cross-file code completion. It strictly requires cross-file context for accurate code completion, built on real-world repositories."
  },
  {
    "id": "imported-1769500766500-408-pj1v7",
    "title": "Skill-Mix",
    "source": "arXiv",
    "authors": [
      "Dingli Yu",
      "Simran Kaur",
      "Arushi Gupta",
      "Jonah Brown-Cohen",
      "Anirudh Goyal",
      "Sanjeev Arora"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.17567",
    "githubLink": "https://github.com/google-deepmind/skill-mix",
    "itemCount": "Configurable (N skills, k subsets)",
    "specs": "Text generation; Skill combination; Randomized prompts",
    "description": "An evaluation to measure an AI agent's ability to flexibly combine basic learned skills. The evaluator picks random subsets of skills and asks the LLM to produce text combining that subset."
  },
  {
    "id": "imported-1769500766500-409-ais69",
    "title": "A-Lab Experimental Dataset",
    "source": "Scholar",
    "authors": [
      "Nathan J. Szymanski",
      "Bernardus Rendy",
      "Yuxing Fei",
      "Gerbrand Ceder",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1038/s41586-023-06734-w",
    "githubLink": "https://github.com/CederGroupHub/A-Lab",
    "itemCount": "58 target compounds (41 synthesized)",
    "specs": "Experimental logs; XRD data; Synthesis recipes; Success/failure labels",
    "description": "A dataset of autonomous solid-state synthesis experiments generated by the A-Lab. It contains data from 17 days of continuous operation, including synthesis recipes, X-ray diffraction patterns, and outcomes for 58 target materials."
  },
  {
    "id": "imported-1769500766500-410-uq513",
    "title": "AppAgent: Multimodal Agents as Smartphone Users",
    "source": "arXiv",
    "authors": [
      "Chi Zhang",
      "Zhao Yang",
      "Jiaxuan Liu",
      "Yucheng Han",
      "Xin Chen",
      "Zebiao Huang",
      "Bin Fu",
      "Gang Yu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.13771",
    "githubLink": "https://github.com/TencentQQGYLab/AppAgent",
    "itemCount": "50 tasks across 10 apps",
    "specs": "Multimodal (Text + Visual), interacts with smartphone apps via a simplified action space (tap, swipe).",
    "description": "Enables LLM-based multimodal agents to operate smartphone applications like humans, learning from exploration or demonstration. The benchmark evaluates the agent's proficiency in handling high-level tasks across diverse apps."
  },
  {
    "id": "imported-1769500766500-411-e26gh",
    "title": "ToolBench: An Instruction-Tuning Benchmark for Tool Learning",
    "source": "arXiv",
    "authors": [
      "Yujia Qin",
      "Shihao Liang",
      "Yining Ye",
      "Kunlun Zhu",
      "Lan Yan",
      "Yaxi Lu",
      "Yankai Lin",
      "Xin Cong",
      "Xiangru Tang",
      "Bill Qian",
      "Sihan Zhao",
      "Lauren Hong",
      "Runchu Tian",
      "Ruobing Xie",
      "Jie Zhou",
      "Mark Gerstein",
      "Dahai Li",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.17438",
    "githubLink": "https://github.com/OpenBMB/ToolBench",
    "itemCount": "16,464 real-world RESTful APIs; 126k instruction pairs",
    "specs": "JSON-based API calls, multi-turn instructions. Covers 49 categories of tools.",
    "description": "A large-scale benchmark for tool learning, covering diverse real-world APIs and scenarios. It is designed to facilitate the construction of open-source LLMs with general tool-use capabilities."
  },
  {
    "id": "imported-1769500766500-412-dxrzu",
    "title": "Handover-Sim2Real",
    "source": "Other",
    "authors": [
      "Sammy Christen",
      "Wei Yang",
      "Claudia Pérez-D'Arpino",
      "Otmar Hilliges",
      "Dieter Fox",
      "Yu-Wei Chao"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2211.16167",
    "githubLink": "https://github.com/NVlabs/handover-sim2real",
    "itemCount": "Not specified (Procedural generation scenes)",
    "specs": "3D Point clouds, simulation environments (PyBullet)",
    "description": "Official code and benchmark for learning human-to-robot handovers from point clouds, evaluating the transfer of policies trained in simulation to real-world setups."
  },
  {
    "id": "imported-1769500766500-413-dv5wx",
    "title": "EISOST (Endoscopic Images Sim-to-Real Oropharyngeal Segmentation)",
    "source": "Other",
    "authors": [
      "Guankun Wang",
      "Tian-Ao Ren",
      "Jiewen Lai",
      "Long Bai",
      "Hongliang Ren"
    ],
    "year": "2023",
    "paperLink": "https://github.com/gkw0010/EISOST-Sim2Real-Dataset-Release",
    "githubLink": "https://github.com/gkw0010/EISOST-Sim2Real-Dataset-Release",
    "itemCount": "1,397 images (1,194 sim, 203 real)",
    "specs": "RGB images, pixel-level segmentation masks",
    "description": "A dataset for Sim-to-Real domain adaptation in medical imaging, specifically for the segmentation of oropharyngeal organs (uvula, epiglottis, glottis)."
  },
  {
    "id": "imported-1769500766500-414-o9hlc",
    "title": "PromptBench",
    "source": "arXiv",
    "authors": [
      "Kaijie Zhu",
      "Jindong Wang",
      "Jiaheng Zhou",
      "Zichen Wang",
      "Hao Chen",
      "Yidong Wang",
      "Linyi Yang",
      "Wei Ye",
      "Neil Zhenqiang Gong",
      "Yue Zhang",
      "Xie Xing"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.04528",
    "githubLink": "https://github.com/microsoft/promptbench",
    "itemCount": "4,032 adversarial prompts",
    "specs": "Text (Adversarial prompts across 13 tasks)",
    "description": "A unified library and benchmark for evaluating the robustness of Large Language Models against adversarial prompts. It covers various levels of attacks including character, word, sentence, and semantic perturbations."
  },
  {
    "id": "imported-1769500766500-415-r9eer",
    "title": "MultiMedQA",
    "source": "arXiv",
    "authors": [
      "Karan Singhal",
      "Shekoofeh Azizi",
      "Tao Tu",
      "S. Sara Mahdavi",
      "Jason Wei",
      "Hyung Won Chung",
      "Nathan Scales",
      "Ajay Tanwani",
      "Heather Cole-Lewis",
      "Stephen Pfohl",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2212.13138",
    "githubLink": "https://github.com/google-research/google-research/tree/master/medpalm",
    "itemCount": "7 datasets total (includes MedQA, MedMCQA, PubMedQA, LiveQA, MedicationQA, MMLU-Clinical, HealthSearchQA)",
    "specs": "Text (Question Answering)",
    "description": "A benchmark suite combining six existing open question answering datasets (including MedQA, MedMCQA, PubMedQA) and a new dataset 'HealthSearchQA' of consumer medical questions, designed to evaluate Large Language Models in the medical domain."
  },
  {
    "id": "imported-1769500766500-416-n7jda",
    "title": "ToxiGen",
    "source": "arXiv",
    "authors": [
      "Thomas Hartvigsen",
      "Saadia Gabriel",
      "Hamid Palangi",
      "Maarten Sap",
      "Dipankar Ray",
      "Ece Kamar"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.09509",
    "githubLink": "https://github.com/microsoft/ToxiGen",
    "itemCount": "274,186 statements",
    "specs": "Text statements, generated by GPT-3",
    "description": "A large-scale machine-generated dataset for adversarial and implicit hate speech detection, containing toxic and benign statements about minority groups."
  },
  {
    "id": "imported-1769500766500-417-9936p",
    "title": "HOPE",
    "source": "Scholar",
    "authors": [
      "Karan Malhotra",
      "Aseem Srivastava",
      "Mansi Agarwal",
      "Tanmoy Chakraborty"
    ],
    "year": "2022",
    "paperLink": "https://dl.acm.org/doi/10.1145/3488560.3498422",
    "githubLink": "https://github.com/LCS2-IIITD/SPARTA_WSDM2022",
    "itemCount": "212 sessions (approx. 12,900 utterances)",
    "specs": "Text (English), Dyadic Conversations, Annotated Dialogue Acts",
    "description": "A dataset of dyadic counseling conversation transcripts sourced from public counseling videos. It is annotated with fine-grained dialogue acts tailored for counseling conversations, designed to support tasks like dialogue act classification and response generation."
  },
  {
    "id": "imported-1769500766500-418-wm5ia",
    "title": "MEMO",
    "source": "Scholar",
    "authors": [
      "Aseem Srivastava",
      "Mansi Agarwal",
      "Karan Malhotra",
      "Tanmoy Chakraborty"
    ],
    "year": "2022",
    "paperLink": "https://doi.org/10.1007/978-3-031-20353-4_33",
    "githubLink": "https://github.com/LCS2-IIITD/MEMO",
    "itemCount": "212 sessions with summaries",
    "specs": "Text (English), Dialogue Transcripts, Summaries (Counseling Notes)",
    "description": "A dataset specifically designed for counseling summarization. It extends the HOPE dataset by providing human-written summaries (counseling notes) for the conversation transcripts, enabling research into domain-specific summarization."
  },
  {
    "id": "imported-1769500766500-419-np6nx",
    "title": "KoBEST (Korean Balanced Evaluation of Significant Tasks)",
    "source": "arXiv",
    "authors": [
      "Dohyeong Kim",
      "Myeongjun Jang",
      "Deuk Sin Kwon",
      "Eric Davis"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.04541",
    "githubLink": "https://huggingface.co/datasets/skt/kobest_v1",
    "itemCount": "5 tasks",
    "specs": "Text, Logic/Reasoning/QA",
    "description": "A benchmark designed to evaluate advanced Korean linguistic knowledge and reasoning. It consists of five downstream tasks: Boolean Question Answering (BoolQ), Choice of Plausible Alternatives (COPA), HellaSwag, Word-in-Context (WiC), and Sentiment Negation (SentiNeg)."
  },
  {
    "id": "imported-1769500766500-420-ulgrd",
    "title": "K-MHaS (Korean Multi-label Hate Speech Dataset)",
    "source": "Hugging Face",
    "authors": [
      "Jean Lee",
      "Taejun Lim",
      "Heejun Lee",
      "Bogeun Jo",
      "Yangsok Kim",
      "Heegeun Yoon",
      "Soyeon Caren Han"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.coling-1.311/",
    "githubLink": "https://huggingface.co/datasets/jeanlee/kmhas_korean_hate_speech",
    "itemCount": "109,692 utterances",
    "specs": "Text, Multi-label Classification",
    "description": "A large-scale multi-label hate speech detection dataset consisting of comments from Korean online news platforms. It provides fine-grained labels for different types of hate speech (e.g., Politics, Gender, Region) to handle the subjectivity and intersectionality of hate speech."
  },
  {
    "id": "imported-1769500766500-421-0tzjx",
    "title": "BIG-bench (Beyond the Imitation Game Benchmark)",
    "source": "arXiv",
    "authors": [
      "Aarohi Srivastava",
      "Abhinav Rastogi",
      "Abhishek Rao",
      "Abu Awal Md Shoeb",
      "Abubakar Abid",
      "Adam Fisch",
      "Adam R. Brown",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.04615",
    "githubLink": "https://github.com/google/BIG-bench",
    "itemCount": "200+ tasks",
    "specs": "JSON task format. Primarily text-based but includes diverse reasoning, logic, and knowledge domains.",
    "description": "A massive collaborative benchmark intended to probe large language models on diverse tasks believed to be beyond the capabilities of current models, aiming to extrapolate future capabilities."
  },
  {
    "id": "imported-1769500766500-422-1grjw",
    "title": "FaithDial",
    "source": "arXiv",
    "authors": [
      "Nouha Dziri",
      "Ehsan Kamalloo",
      "Sivan Milton",
      "Osmar Zaiane",
      "Mo Yu",
      "Edoardo M. Ponti",
      "Siva Reddy"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.10757",
    "githubLink": "https://github.com/McGill-NLP/FaithDial",
    "itemCount": "5,649 dialogues, 50,761 utterances",
    "specs": "Text (Dialogue), Knowledge-grounded",
    "description": "A benchmark for faithful information-seeking dialogue. It was created by letting humans edit hallucinated responses in the Wizard of Wikipedia dataset to ensure faithfulness to knowledge sources."
  },
  {
    "id": "imported-1769500766500-423-ijzt4",
    "title": "FactualityPrompts",
    "source": "arXiv",
    "authors": [
      "Nayeon Lee",
      "Wei Ping",
      "Peng Xu",
      "Mostofa Patwary",
      "Pascale Fung",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.04624",
    "githubLink": "https://github.com/nayeon7lee/FactualityPrompt",
    "itemCount": "Unknown (Test prompts set)",
    "specs": "Text (Open-ended generation)",
    "description": "A benchmark of prompts designed to trigger factual errors in open-ended text generation, coupled with a pipeline for evaluating the factuality of the generated text."
  },
  {
    "id": "imported-1769500766500-424-92fip",
    "title": "Mini-ARC",
    "source": "Other",
    "authors": [
      "Subin Kim",
      "Prin Phunyaphibarn",
      "Donghyun Ahn",
      "Sundong Kim"
    ],
    "year": "2022",
    "paperLink": "https://openreview.net/forum?id=pCKWb4yE1i",
    "githubLink": "https://github.com/KSB21ST/MINI-ARC",
    "itemCount": "150 tasks",
    "specs": "JSON format, fixed 5x5 grids",
    "description": "A compact version of the ARC dataset where all grids are fixed to a 5x5 size. It is designed to reduce the computational modeling budget while maintaining the core challenge of abductive reasoning."
  },
  {
    "id": "imported-1769500766500-425-96tuo",
    "title": "HELM (Holistic Evaluation of Language Models)",
    "source": "arXiv",
    "authors": [
      "Percy Liang",
      "Rishi Bommasani",
      "Tony Lee",
      "Dimitris Tsipras",
      "Dilara Soylu",
      "Michihiro Yasunaga",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2211.09110",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "42 scenarios (initial release), expanded over time",
    "specs": "Text-only; Multi-metric evaluation (Accuracy, Calibration, Robustness, Fairness, Bias, Toxicity, Efficiency)",
    "description": "A comprehensive benchmark that evaluates language models across a vast taxonomy of scenarios and metrics to improve transparency. It measures capabilities (e.g., accuracy) and risks (e.g., bias, toxicity) across diverse domains."
  },
  {
    "id": "imported-1769500766500-426-b7312",
    "title": "DS-1000",
    "source": "arXiv",
    "authors": [
      "Yuhang Lai",
      "Chengxi Li",
      "Yiming Wang",
      "Tianyi Zhang",
      "Ruiqi Zhong",
      "Luke Zettlemoyer",
      "Scott Wen-tau Yih",
      "Daniel Fried",
      "Sida I. Wang",
      "Tao Yu"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2211.11501",
    "githubLink": "https://github.com/xlang-ai/DS-1000",
    "itemCount": "1000 problems",
    "specs": "Python code, 7 libraries (NumPy, Pandas, SciPy, Scikit-learn, TensorFlow, PyTorch, Matplotlib)",
    "description": "A code generation benchmark with 1,000 data science problems spanning seven Python libraries (NumPy, Pandas, etc.). It focuses on realistic and practical use cases collected from StackOverflow, with specific defense against memorization."
  },
  {
    "id": "imported-1769500766500-427-yxrii",
    "title": "EHRSQL",
    "source": "arXiv",
    "authors": [
      "Gyubok Lee",
      "Hyeonji Hwang",
      "Seongsu Bae",
      "Yeonsu Kwon",
      "Woncheol Shin",
      "Seongjun Yang",
      "Minjoon Seo",
      "Jong-Yeup Kim",
      "Edward Choi"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2301.07695",
    "githubLink": "https://github.com/glee4810/EHRSQL",
    "itemCount": "~24,000 samples",
    "specs": "Structured Data (MIMIC-III and eICU); Text-to-SQL",
    "description": "A large-scale text-to-SQL dataset for Electronic Health Records, designed to test QA models on structured data. It includes questions collected from hospital staff and covers a wide range of SQL queries, including those requiring time expression understanding and unanswerable question detection."
  },
  {
    "id": "imported-1769500766500-428-r320y",
    "title": "DrugEHRQA",
    "source": "Scholar",
    "authors": [
      "Jayetri Bardhan",
      "Anthony Colas",
      "Kirk Roberts",
      "Daisy Zhe Wang"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.lrec-1.116/",
    "githubLink": "https://github.com/jayetri/DrugEHRQA",
    "itemCount": "70,000+ QA pairs",
    "specs": "Multi-modal (Structured Tables + Unstructured Text)",
    "description": "A multi-modal question answering dataset focusing on medication-related queries. It contains QA pairs derived from both structured tables and unstructured discharge summaries in MIMIC-III, designed to improve QA using cross-modal context."
  },
  {
    "id": "imported-1769500766500-429-0xrcl",
    "title": "SKM-TEA",
    "source": "arXiv",
    "authors": [
      "Arjun D. Desai",
      "Andrew M. Schmidt",
      "Elka B. Rubin",
      "Christopher M. Sandino",
      "Marianne S. Black",
      "Valentina Mazzoli",
      "Akshay S. Chaudhari"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.06823",
    "githubLink": "https://github.com/StanfordMIMI/skm-tea",
    "itemCount": "155 patients (~25,000 slices)",
    "specs": "Raw k-space, DICOM, segmentation masks, bounding boxes",
    "description": "Stanford Knee MRI with Multi-Task Evaluation (SKM-TEA) is a dataset of quantitative knee MRI scans enabling end-to-end evaluation of reconstruction, segmentation, and detection."
  },
  {
    "id": "imported-1769500766500-430-c78zr",
    "title": "ScienceQA",
    "source": "Hugging Face",
    "authors": [
      "Pan Lu",
      "Swaroop Mishra",
      "Tony Xia",
      "Liang Qiu",
      "Kai-Wei Chang",
      "Song-Chun Zhu",
      "Oyvind Tafjord",
      "Peter Clark",
      "Ashwin Kalyan"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2209.09513",
    "githubLink": "https://github.com/lupantech/ScienceQA",
    "itemCount": "21,208 questions",
    "specs": "Text & Image, Multiple Choice, Science Domain",
    "description": "A multimodal science question answering dataset with annotated lectures and explanations, covering natural, language, and social sciences."
  },
  {
    "id": "imported-1769500766500-431-mq6aj",
    "title": "MedMCQA",
    "source": "Hugging Face",
    "authors": [
      "Ankit Pal",
      "Logesh Kumar Umapathi",
      "Malaikannan Sankarasubbu"
    ],
    "year": "2022",
    "paperLink": "https://proceedings.mlr.press/v174/pal22a.html",
    "githubLink": "https://github.com/medmcqa/medmcqa",
    "itemCount": "194,000+ MCQs",
    "specs": "Text (Multiple Choice Questions)",
    "description": "A large-scale multiple-choice question answering dataset covering 21 medical subjects, including a significant number of dermatology questions. It is designed to address real-world medical entrance exam questions."
  },
  {
    "id": "imported-1769500766500-432-y46sv",
    "title": "Fig-QA (Figurative Language Question Answering)",
    "source": "Hugging Face",
    "authors": [
      "Emmy Liu",
      "Chenxuan Cui",
      "Kenneth Zheng",
      "Graham Neubig"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.12632",
    "githubLink": "https://github.com/nightingal3/fig-qa",
    "itemCount": "10,256 examples",
    "specs": "Text-only; Winograd schema format",
    "description": "A Winograd-style benchmark designed to test the ability of language models to interpret figurative language. It consists of paired figurative phrases (metaphors) with divergent meanings to evaluate non-literal reasoning capabilities."
  },
  {
    "id": "imported-1769500766500-433-gasyk",
    "title": "Diamond Images Dataset",
    "source": "Other",
    "authors": [
      "Aayush Purswani"
    ],
    "year": "2022",
    "paperLink": "https://www.kaggle.com/datasets/harshitlakhani/natural-diamonds-prices-images",
    "githubLink": "https://www.kaggle.com/datasets/harshitlakhani/natural-diamonds-prices-images",
    "itemCount": "~21,000 images",
    "specs": "Images (JPG), Labeled folders",
    "description": "A collection of images of diamonds classified by their shape (e.g., Round, Emerald, Heart). This dataset is used for computer vision tasks such as shape identification and cut classification."
  },
  {
    "id": "imported-1769500766500-434-7tatj",
    "title": "CDC SARS-CoV-2 Surveillance Benchmarks",
    "source": "Scholar",
    "authors": [
      "Lingzi Xiaoli",
      "Jill V. Hagey",
      "Lee S. Katz",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://peerj.com/articles/13821/",
    "githubLink": "https://github.com/CDCgov/datasets-sars-cov-2",
    "itemCount": "6 datasets",
    "specs": "Genomic sequencing data (Illumina short reads, Oxford Nanopore long reads)",
    "description": "Standardized benchmark datasets for testing bioinformatics pipelines used in SARS-CoV-2 surveillance, including variant calling and lineage inference."
  },
  {
    "id": "imported-1769500766500-435-ta7th",
    "title": "Instagram-Based Benchmark Dataset for Cyberbullying Detection",
    "source": "Semantic Scholar",
    "authors": [
      "Rawan ALBayari",
      "Salim Abdallah"
    ],
    "year": "2022",
    "paperLink": "https://www.mdpi.com/2306-5729/7/7/88",
    "githubLink": "https://www.mdpi.com/2306-5729/7/7/88",
    "itemCount": "200,000 comments (46,898 annotated)",
    "specs": "Text (Arabic, Cyberbullying detection)",
    "description": "A benchmark dataset collected from Instagram focusing on cyberbullying in Arabic text. It provides a multi-class categorization of comments, including bullying, which is highly relevant to child safety on social media."
  },
  {
    "id": "imported-1769500766500-436-rytob",
    "title": "PJZC Dataset",
    "source": "Semantic Scholar",
    "authors": [
      "Daniela Fernanda Milon-Flores",
      "Robson Leonardo Ferreira Cordeiro"
    ],
    "year": "2022",
    "paperLink": "https://doi.org/10.1016/j.knosys.2021.108017",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "Binary annotated chat logs (variable size)",
    "specs": "Text (Chat Logs)",
    "description": "An updated dataset aimed at mitigating the data scarcity in grooming detection. It combines newer IRC logs (2013–2022) as true negatives with the last set of grooming chats from the Perverted Justice archives (2013–2014) as true positives."
  },
  {
    "id": "imported-1769500766500-437-mwwrx",
    "title": "MaXM (Multilingual Visual Question Answering)",
    "source": "arXiv",
    "authors": [
      "Soravit Changpinyo",
      "Linting Xue",
      "Yara Rizk",
      "Hamid Palangi",
      "Praveen Krishnan",
      "Aaron Sarna"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2209.05401",
    "githubLink": "https://github.com/google-research-datasets/maxm",
    "itemCount": "Contains questions in 7 languages corresponding to Crossmodal-3600 images",
    "specs": "Multimodal (Image + Multilingual Text), 7 languages",
    "description": "A large-scale benchmark for the Multilingual Visual Question Answering (often abbreviated as VMQA or mVQA) task. It provides a test-only benchmark in 7 diverse languages derived from the Crossmodal-3600 dataset to evaluate multilingual multimodal models."
  },
  {
    "id": "imported-1769500766500-438-7rb44",
    "title": "PACS (Physical Audiovisual CommonSense)",
    "source": "arXiv",
    "authors": [
      "Samuel Yu",
      "Peter Wu",
      "Paul Pu Liang",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.11130",
    "githubLink": "https://github.com/samuelyu2002/PACS",
    "itemCount": "13,400 QA pairs, 1,526 videos",
    "specs": "Video, Audio, Text (Question-Answer pairs); 1,377 unique physical commonsense questions",
    "description": "A multimodal benchmark for physical commonsense reasoning. It contains question-answer pairs about physical properties and interactions (e.g., material types, object affordances) in videos, requiring both visual and acoustic reasoning."
  },
  {
    "id": "imported-1769500766500-439-7s4ad",
    "title": "Anthropic HH-RLHF",
    "source": "Hugging Face",
    "authors": [
      "Yuntao Bai",
      "Andy Jones",
      "Kamal Ndousse",
      "Amanda Askell",
      "Anna Chen",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.05862",
    "githubLink": "https://github.com/anthropics/hh-rlhf",
    "itemCount": "~161,000 conversations",
    "specs": "Text pairs (chosen/rejected), JSONL format",
    "description": "A dataset consisting of human preference data for training helpful and harmless AI assistants. It includes pairs of model responses to user prompts, with one response labeled as 'chosen' and the other as 'rejected' based on human feedback."
  },
  {
    "id": "imported-1769500766500-440-uhv25",
    "title": "LexGLUE (Legal General Language Understanding Evaluation)",
    "source": "Hugging Face",
    "authors": [
      "Ilias Chalkidis",
      "Abhik Jana",
      "Dirk Hartung",
      "Michael Bommarito",
      "Ion Androutsopoulos",
      "Daniel Katz",
      "Nikolaos Aletras"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.acl-long.297/",
    "githubLink": "https://github.com/coastalcph/lex-glue",
    "itemCount": "Aggregates 7 datasets (~200k+ total samples)",
    "specs": "Text (English); Multi-task (Classification, QA, etc.)",
    "description": "A benchmark dataset to evaluate the performance of NLP methods in legal tasks, including legal judgment prediction. It aggregates seven existing legal NLP datasets (including ECtHR, SCOTUS, LEDGAR, etc.) to provide a standardized evaluation framework."
  },
  {
    "id": "imported-1769500766500-441-drkzx",
    "title": "CXR-PRO",
    "source": "Other",
    "authors": [
      "Vignav Ramesh",
      "Nathan A. Chi",
      "Pranav Rajpurkar"
    ],
    "year": "2022",
    "paperLink": "https://physionet.org/content/cxr-pro/1.0.0/",
    "githubLink": "https://physionet.org/content/cxr-pro/",
    "itemCount": "374,139 reports",
    "specs": "Text radiology reports (references omitted), Linked to MIMIC-CXR images",
    "description": "An adaptation of the MIMIC-CXR dataset that omits references to prior radiology reports to address the issue of hallucinated references in generated reports."
  },
  {
    "id": "imported-1769500766500-442-3ehby",
    "title": "ECTSum",
    "source": "Other",
    "authors": [
      "Rajdeep Mukherjee",
      "Abhinav Bohra",
      "Akash Banerjee",
      "Soumya Sharma",
      "Manjunath Hegde",
      "Afreen Shaikh",
      "Shivani Shrivastava",
      "Koustuv Dasgupta",
      "Niloy Ganguly",
      "Saptarshi Ghosh",
      "Pawan Goyal"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.emnlp-main.742/",
    "githubLink": "https://github.com/rajdeep345/ECTSum",
    "itemCount": "2,425 transcripts",
    "specs": "Text (Financial Earnings Calls)",
    "description": "A large-scale dataset for bullet point summarization of long earnings call transcripts (ECTs). It consists of transcripts from publicly traded companies and expert-written telegram-style bullet point summaries derived from Reuters articles. The dataset is designed to benchmark financial document summarization."
  },
  {
    "id": "imported-1769500766500-443-rok3n",
    "title": "Multi-LexSum",
    "source": "arXiv",
    "authors": [
      "Zejiang Shen",
      "Kyle Lo",
      "Lauren Yu",
      "Nathan Dahlberg",
      "Margo Schlanger",
      "Doug Downey"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.10883",
    "githubLink": "https://github.com/multilexsum/dataset",
    "itemCount": "9,280 summaries (from ~4,500 cases)",
    "specs": "Text (Legal Case Summaries)",
    "description": "A dataset of civil rights lawsuit summaries at multiple granularities (tiny, short, long). It is sourced from the Civil Rights Litigation Clearinghouse and features expert-authored summaries, making it suitable for multi-document and variable-granularity summarization tasks."
  },
  {
    "id": "imported-1769500766500-444-rdqnw",
    "title": "OpenXAI",
    "source": "arXiv",
    "authors": [
      "Chirag Agarwal",
      "Satyapriya Krishna",
      "Eshika Saxena",
      "Martin Pawelczyk",
      "Nari Johnson",
      "Isha Puri",
      "Marinka Zitnik",
      "Himabindu Lakkaraju"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.11104",
    "githubLink": "https://github.com/wcventure/OpenXAI",
    "itemCount": "7 real-world datasets + synthetic generator",
    "specs": "Tabular data, Synthetic data; Metrics for Faithfulness, Stability, Fairness",
    "description": "A comprehensive and extensible open-source framework for evaluating and benchmarking post-hoc explanation methods. It includes a collection of real-world high-stakes datasets (finance, healthcare, etc.) and a synthetic data generator to create ground-truth explanations."
  },
  {
    "id": "imported-1769500766500-445-os7bw",
    "title": "Big-Bench Hard (BBH)",
    "source": "arXiv",
    "authors": [
      "Mirac Suzgun",
      "Nathan Scales",
      "Nathanael Schärli",
      "Sebastian Gehrmann",
      "Yi Tay",
      "Hyung Won Chung",
      "Aakanksha Chowdhery",
      "Quoc V. Le",
      "Ed H. Chi",
      "Denny Zhou",
      "Jason Wei"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2210.09261",
    "githubLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
    "itemCount": "6,511 examples (23 tasks)",
    "specs": "Text (Diverse Reasoning Tasks)",
    "description": "A subset of the BIG-Bench suite, focusing on 23 challenging tasks where prior language models did not outperform the average human-rater. It emphasizes multi-step reasoning capabilities."
  },
  {
    "id": "imported-1769500766500-446-6mdkb",
    "title": "SciRepEval",
    "source": "arXiv",
    "authors": [
      "Amanpreet Singh",
      "Mike D'Arcy",
      "Arman Cohan",
      "Doug Downey",
      "Sergey Feldman"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2211.13308",
    "githubLink": "https://github.com/allenai/scirepeval",
    "itemCount": "24 tasks",
    "specs": "Text-based; Document embeddings; Formats: Classification, Regression, Ranking, Search",
    "description": "A large-scale benchmark for training and evaluating scientific document representations. It covers a wide range of tasks including classification, regression, ranking, and search to test generalization across formats."
  },
  {
    "id": "imported-1769500766500-447-5faf7",
    "title": "FLORES-200",
    "source": "arXiv",
    "authors": [
      "NLLB Team",
      "Marta R. Costa-jussà",
      "James Cross",
      "Onur Çelebi",
      "Maha Elbayad",
      "Kenneth Heafield",
      "Kevin Heffernan",
      "Elahe Kalbassi",
      "Janice Lam",
      "Daniel Licht",
      "Jean Maillard",
      "Anna Sun",
      "Skyler Wang",
      "Guillaume Wenzek",
      "Al Youngblood",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2207.04672",
    "githubLink": "https://github.com/facebookresearch/flores",
    "itemCount": "200 languages, 3001 sentences per language",
    "specs": "Text; Machine Translation",
    "description": "A many-to-many multilingual translation benchmark dataset covering 200 languages, doubling the coverage of its predecessor FLORES-101."
  },
  {
    "id": "imported-1769500766500-448-66e03",
    "title": "IGLUE",
    "source": "arXiv",
    "authors": [
      "Emanuele Bugliarello",
      "Fangyu Liu",
      "Jonas Pfeiffer",
      "Siva Reddy",
      "Desmond Elliott",
      "Edoardo Maria Ponti",
      "Ivan Vulić"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2201.11732",
    "githubLink": "https://github.com/e-bug/iglue",
    "itemCount": "20 languages, 5 datasets",
    "specs": "Multimodal (Image + Text); Visual QA, Retrieval, Reasoning",
    "description": "The Image-Grounded Language Understanding Evaluation benchmark, aggregating visual QA, retrieval, and reasoning tasks across 20 languages."
  },
  {
    "id": "imported-1769500766500-449-bawt2",
    "title": "MASSIVE",
    "source": "arXiv",
    "authors": [
      "Jack FitzGerald",
      "Christopher Hench",
      "Charith Peris",
      "Scott Mackie",
      "Kay Rottmann",
      "Ana Sanchez",
      "Aaron Nash",
      "Liam Urbach",
      "Vishesh Kakarala",
      "Richa Singh",
      "Swetha Ranganath",
      "Laurie Crist",
      "Misha Britan",
      "Wouter Leeuwis",
      "Gokhan Tur",
      "Prem Natarajan"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.08582",
    "githubLink": "https://github.com/alexa/massive",
    "itemCount": "1,000,000 utterances, 51 languages",
    "specs": "Text; Intent Classification, Slot Filling",
    "description": "A multilingual natural language understanding dataset for intent classification and slot filling, containing 1M utterances across 51 languages."
  },
  {
    "id": "imported-1769500766500-450-7gz18",
    "title": "FLEURS",
    "source": "Hugging Face",
    "authors": [
      "Alexis Conneau",
      "Min Ma",
      "Simran Khanuja",
      "Yu Zhang",
      "Vera Axelrod",
      "Siddharth Dalmia",
      "Jason Riesa",
      "Clara Rivera",
      "Ankur Bapna"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2205.12446",
    "githubLink": "https://huggingface.co/datasets/google/fleurs",
    "itemCount": "102 languages, ~12 hours per language",
    "specs": "Audio/Speech; ASR, Speech Translation",
    "description": "A parallel speech dataset in 102 languages built on top of FLORES-101, designed for ASR and speech translation evaluation."
  },
  {
    "id": "imported-1769500766500-451-pr8xy",
    "title": "Anthropic Red Team Dataset",
    "source": "arXiv",
    "authors": [
      "Deep Ganguli",
      "Liane Lovitt",
      "John Kernion",
      "Amanda Askell",
      "Yuntao Bai",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2209.07858",
    "githubLink": "https://github.com/anthropics/hh-rlhf",
    "itemCount": "38,961 dialogues",
    "specs": "Text (Multi-turn dialogues); Red-teaming transcripts",
    "description": "A dataset of adversarial conversations where humans attempt to elicit harmful responses from an AI assistant. Useful for training and evaluating safety mitigation."
  },
  {
    "id": "imported-1769500766500-452-tyqwi",
    "title": "PMO: Sample Efficiency Matters",
    "source": "Other",
    "authors": [
      "Wenhao Gao",
      "Tianfan Fu",
      "Jimeng Sun",
      "Connor W. Coley"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.12411",
    "githubLink": "https://github.com/wenhao-gao/mol_opt",
    "itemCount": "23 optimization tasks",
    "specs": "Molecular optimization; SMILES representation; Oracle budget limit (10k)",
    "description": "A benchmark for practical molecular optimization focused on sample efficiency. It evaluates 25 molecular design algorithms on 23 single-objective optimization tasks with a limited oracle budget, simulating realistic resource constraints in autonomous discovery."
  },
  {
    "id": "imported-1769500766500-453-aafg3",
    "title": "ObjectFolder 2.0",
    "source": "Scholar",
    "authors": [
      "Ruohan Gao",
      "Zilin Si",
      "Yen-Yu Chang",
      "Samuel Clarke",
      "Jeannette Bohg",
      "Li Fei-Fei",
      "Wenzhen Yuan",
      "Jiajun Wu"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.02389",
    "githubLink": "https://github.com/rhgao/ObjectFolder",
    "itemCount": "1,000 neural objects; 100 real-world objects",
    "specs": "Implicit neural representations, 3D meshes, videos, impact sounds, tactile readings",
    "description": "A large-scale multisensory dataset of 1,000 objects modeled with implicit neural representations (VisionNet, AudioNet, TouchNet) and 100 real-world objects with corresponding visual, acoustic, and tactile measurements for Sim2Real transfer."
  },
  {
    "id": "imported-1769500766500-454-1iox9",
    "title": "TruthfulQA",
    "source": "arXiv",
    "authors": [
      "Stephanie Lin",
      "Jacob Hilton",
      "Owain Evans"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2109.07958",
    "githubLink": "https://github.com/sylinrl/TruthfulQA",
    "itemCount": "817 questions",
    "specs": "Text questions (Multiple choice and Generation), CSV",
    "description": "A benchmark to measure whether language models generate truthful answers to questions, focusing on imitative falsehoods and misconceptions."
  },
  {
    "id": "imported-1769500766500-455-a9yk6",
    "title": "ESConv (Emotional Support Conversation)",
    "source": "arXiv",
    "authors": [
      "Siyang Liu",
      "Chujie Zheng",
      "Orianna Demasi",
      "Sahand Sabour",
      "Yu Li",
      "Zhou Yu",
      "Yong Jiang",
      "Minlie Huang"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.269/",
    "githubLink": "https://github.com/thu-coai/Emotional-Support-Conversation",
    "itemCount": "1,300 conversations",
    "specs": "Text (English), Dialogue, Annotated Support Strategies",
    "description": "A dataset constructed for Emotional Support Conversation (ESC) tasks. It contains conversations between help-seekers and supporters, where supporters follow a specific emotional support framework (exploration, comforting, action)."
  },
  {
    "id": "imported-1769500766500-456-ds5lk",
    "title": "PsyQA",
    "source": "arXiv",
    "authors": [
      "Hao Sun",
      "Zhenru Lin",
      "Chujie Zheng",
      "Siyang Liu",
      "Minlie Huang"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.findings-acl.130/",
    "githubLink": "https://github.com/thu-coai/PsyQA",
    "itemCount": "22,346 questions; 56,063 answers",
    "specs": "Text (Chinese), Q&A pairs, Long-form answers",
    "description": "A large-scale Chinese dataset for generating long counseling text. It consists of question and answer pairs crawled from a Chinese mental health service platform, with a portion annotated for support strategies."
  },
  {
    "id": "imported-1769500766500-457-hg88s",
    "title": "ReVeal",
    "source": "arXiv",
    "authors": [
      "Saikat Chakraborty",
      "Rahul Krishna",
      "Yangruibo Ding",
      "Baishakhi Ray"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2009.07235",
    "githubLink": "https://github.com/VulDetProject/ReVeal",
    "itemCount": "~18k functions (approx. 1.6k vulnerable)",
    "specs": "C/C++ code",
    "description": "A dataset created to evaluate deep learning-based vulnerability detection. It contains labeled vulnerabilities from Chromium and Debian issue trackers, highlighting the importance of realistic data distributions."
  },
  {
    "id": "imported-1769500766500-458-919ib",
    "title": "KLUE (Korean Language Understanding Evaluation)",
    "source": "arXiv",
    "authors": [
      "Sungjoon Park",
      "Jihyung Moon",
      "Sungdong Kim",
      "Won Ik Cho",
      "Jiyoon Han",
      "Jangwon Park",
      "Chisung Song",
      "Jun Seok Kim",
      "Yongsi Song",
      "Taehwan Oh",
      "Joohong Lee",
      "Juhyun Oh",
      "Sungwon Lyu",
      "Younghoon Jeong",
      "Inkwon Lee",
      "Sangwoo Seo",
      "Dongjun Lee",
      "Hyunwoo Kim",
      "Myeonghwa Lee",
      "Seongbo Jang",
      "Seungwon Do",
      "Sunkyu Kim",
      "Kyungtae Lim",
      "Jongwon Lee",
      "Kyumin Park",
      "Jamin Shin",
      "Seonghyun Kim",
      "Lucy Park",
      "Alice Oh",
      "Jung-Woo Ha",
      "Kyunghyun Cho"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2105.09680",
    "githubLink": "https://github.com/KLUE-benchmark/KLUE",
    "itemCount": "8 tasks (size varies, e.g., ~13k STS pairs, ~30k NER sentences)",
    "specs": "Text (JSON/TSV), Multiple NLU tasks",
    "description": "A comprehensive benchmark suite for evaluating Korean natural language understanding models. It consists of 8 diverse tasks including Topic Classification, Semantic Textual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking."
  },
  {
    "id": "imported-1769500766500-459-2b4m5",
    "title": "LARC (Language-complete ARC)",
    "source": "arXiv",
    "authors": [
      "Samuel Acquaviva",
      "Yewen Pu",
      "Marta Kryven",
      "Catherine Wong",
      "Gabrielle E. Ecanow",
      "Maxwell Nye",
      "Theodoros Sechopoulos",
      "Michael Henry Tessler",
      "Joshua B. Tenenbaum"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2106.07824",
    "githubLink": "https://github.com/samacqua/LARC",
    "itemCount": "Annotations for ARC tasks (354 successful)",
    "specs": "Text descriptions linked to ARC tasks",
    "description": "An extension of ARC that includes natural language descriptions of the underlying transformation rules. Collected via a communication game where one human describes the task to another, verifying that the language is sufficient to solve the puzzle."
  },
  {
    "id": "imported-1769500766500-460-k3hgp",
    "title": "BBQ: Bias Benchmark for QA",
    "source": "arXiv",
    "authors": [
      "Alicia Parrish",
      "Angelica Chen",
      "Nikita Nangia",
      "Vishakh Padmakumar",
      "Jason Phang",
      "Jana Thompson",
      "Phu Mon Htut",
      "Samuel R. Bowman"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.08193",
    "githubLink": "https://github.com/nyu-mll/BBQ",
    "itemCount": "58,492 samples",
    "specs": "English text, Multiple Choice (Trinary: Target, Non-target, Unknown), JSONL format",
    "description": "A dataset of question sets constructed to highlight attested social biases against people belonging to protected classes along nine social dimensions (e.g., age, gender, race, socioeconomic status). It evaluates model responses in both under-informative (ambiguous) and adequately informative (disambiguated) contexts."
  },
  {
    "id": "imported-1769500766500-461-f8gua",
    "title": "emrKBQA",
    "source": "Scholar",
    "authors": [
      "Preethi Raghavan",
      "Jennifer J. Liang",
      "Diwakar Mahajan",
      "Rachita Chandra",
      "Peter Szolovits"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.bionlp-1.7/",
    "githubLink": "https://github.com/emrKBQA/emrKBQA",
    "itemCount": "~940,000 samples",
    "specs": "Structured Data (MIMIC-III); Logical Forms",
    "description": "A dataset for answering physician questions from structured patient records (MIMIC-III). It serves as a structured counterpart to emrQA, focusing on retrieving answers by mapping natural language questions to logical forms over the database."
  },
  {
    "id": "imported-1769500766500-462-8ehf4",
    "title": "MIMIC-SPARQL",
    "source": "arXiv",
    "authors": [
      "Junwoo Park",
      "Youngwoo Cho",
      "Haneol Lee",
      "Jaegul Choo",
      "Edward Choi"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2107.13904",
    "githubLink": "https://github.com/junwoopark92/mimic-sparql",
    "itemCount": "10,000 questions (derived from MIMIC-SQL)",
    "specs": "Knowledge Graph; SPARQL",
    "description": "A knowledge graph-based question answering dataset for EHRs. It is a counterpart to MIMIC-SQL, where the relational database is converted into a directed acyclic graph, and questions are mapped to SPARQL queries."
  },
  {
    "id": "imported-1769500766500-463-9mr29",
    "title": "fastMRI+",
    "source": "arXiv",
    "authors": [
      "Ruiya Zhao",
      "Malcolm J. Gantenbein",
      "Kezhou Shi",
      "Erich Kobler",
      "Daniel Sodickson"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2109.03612",
    "githubLink": "https://github.com/microsoft/fastmri-plus",
    "itemCount": "23,724 bounding box annotations",
    "specs": "Bounding box annotations, study-level labels",
    "description": "An extension of the fastMRI dataset containing clinical pathology annotations (bounding boxes and labels) for knee and brain data."
  },
  {
    "id": "imported-1769500766500-464-fprpm",
    "title": "SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical Visual Question Answering",
    "source": "arXiv",
    "authors": [
      "Bo Liu",
      "Li-Ming Zhan",
      "Li Xu",
      "Lin Ma",
      "Yan Yang",
      "Xiao-Ming Wu"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2102.09542",
    "githubLink": "https://github.com/BoKelvin/SLAKE",
    "itemCount": "642 images, 14,028 QA pairs",
    "specs": "Bilingual (English/Chinese), Modalities: Image (CT, MRI, X-Ray) and Text. Formats: JSON, JPG. Includes semantic segmentation masks and bounding boxes.",
    "description": "A large-scale bilingual (English and Chinese) benchmark dataset for Medical Visual Question Answering (Med-VQA). It contains radiology images (CT, MRI, X-Ray) with comprehensive semantic annotations (masks, bounding boxes) and a structured medical knowledge graph to facilitate both visual and knowledge-based reasoning."
  },
  {
    "id": "imported-1769500766500-465-vnxgm",
    "title": "C-SLAKE (Consistent SLAKE)",
    "source": "Other",
    "authors": [
      "OpenMICG"
    ],
    "year": "2021",
    "paperLink": "https://github.com/OpenMICG/CSLAKE",
    "githubLink": "https://github.com/OpenMICG/CSLAKE",
    "itemCount": "Derived from SLAKE (approx. same scale/subset)",
    "specs": "Modalities: Image (CT, MRI, X-Ray) and Text. Focused on consistency assessment.",
    "description": "An extension of the SLAKE dataset designed to assess the consistency of Med-VQA models. It encompasses a diverse array of medical materials including CT, MRI, and X-Ray images, focusing on testing model robustness and consistency."
  },
  {
    "id": "imported-1769500766500-466-jjw4a",
    "title": "DeepViral",
    "source": "Scholar",
    "authors": [
      "Wang Liu-Wei",
      "Robert Hoehndorf",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://doi.org/10.1093/bioinformatics/btab147",
    "githubLink": "https://github.com/bio-ontology-research-group/DeepViral",
    "itemCount": "24,678 positive interactions; 1,066 viral proteins",
    "specs": "Protein sequences, phenotype ontology embeddings, functional annotations",
    "description": "A deep learning method and dataset for predicting novel virus–host interactions (protein-protein interactions). It leverages protein sequences and infectious disease phenotypes embedded in a shared space."
  },
  {
    "id": "imported-1769500766500-467-bdwfe",
    "title": "PANC (PAN12 + ChatCoder2)",
    "source": "arXiv",
    "authors": [
      "Matthias Vogt",
      "Ulf Leser",
      "Alan Akbik"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.387/",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "Combined from PAN12 and ChatCoder2 sources",
    "specs": "Text (Chat Streams), CSV/Datapack",
    "description": "A combined dataset designed for the task of Early Sexual Predator Detection (eSPD). It merges PAN12 non-grooming chats with ChatCoder2 grooming chats to create a realistic stream of messages for real-time detection analysis, addressing the limitations of PAN12's segmented format."
  },
  {
    "id": "imported-1769500766501-468-puwie",
    "title": "P-Stance",
    "source": "Scholar",
    "authors": [
      "Yingjie Li",
      "Tiberiu Sosea",
      "Aditya Sawant",
      "Ajith Jayaraman Nair",
      "Diana Inkpen",
      "Cornelia Caragea"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.findings-acl.208/",
    "githubLink": "https://github.com/chuchun8/PStance",
    "itemCount": "21,574 tweets",
    "specs": "English Tweets, Stance Labels (Favor/Against/Neutral)",
    "description": "A large-scale stance detection dataset in the political domain, specifically focused on the 2020 U.S. Presidential election candidates (Trump, Biden, Sanders)."
  },
  {
    "id": "imported-1769500766501-469-t5n7q",
    "title": "ETHICS",
    "source": "Hugging Face",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart",
      "Andrew Critch",
      "Jerry Li",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2008.02275",
    "githubLink": "https://github.com/hendrycks/ethics",
    "itemCount": "~130,000 examples",
    "specs": "Text (English); Binary classification",
    "description": "A benchmark that assesses a language model's knowledge of basic concepts of morality. It spans concepts in justice, well-being, duties, virtues, and commonsense morality, requiring models to predict moral judgments about diverse text scenarios."
  },
  {
    "id": "imported-1769500766501-470-fsqpy",
    "title": "Swiss-Judgment-Prediction (SJP)",
    "source": "Hugging Face",
    "authors": [
      "Joel Niklaus",
      "Ilias Chalkidis",
      "Matthias Stürmer"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.00806",
    "githubLink": "https://github.com/rcds/swiss_judgment_prediction",
    "itemCount": "85,000 cases",
    "specs": "Text (German, French, Italian); Binary classification",
    "description": "A multilingual, diachronic dataset of 85K Swiss Federal Supreme Court cases annotated with the respective judgment outcome (approval/dismissal). It covers cases in German, French, and Italian."
  },
  {
    "id": "imported-1769500766501-471-fj5aa",
    "title": "ILDC (Indian Legal Documents Corpus)",
    "source": "arXiv",
    "authors": [
      "Vijit Malik",
      "Rishabh Sanjay",
      "Shubham Kumar Nigam",
      "Kripabandhu Ghosh",
      "Shouvik Kumar Guha",
      "Arnab Bhattacharya",
      "Ashutosh Modi"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.313/",
    "githubLink": "https://github.com/Exploration-Lab/CJPE",
    "itemCount": "35,000 cases",
    "specs": "Text (English); Judgment prediction, Explanation generation",
    "description": "A corpus of Indian Supreme Court cases annotated with court decisions and explanations. It introduces the task of Court Judgment Prediction and Explanation (CJPE) for the Indian legal system."
  },
  {
    "id": "imported-1769500766501-472-nci0b",
    "title": "JUSTICE",
    "source": "arXiv",
    "authors": [
      "Mohammad Alali",
      "Shaayan Syed",
      "Mohammed Alsayed",
      "Smit Patel",
      "Hemanth Bodala"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2112.03414",
    "githubLink": "https://github.com/MohammadAlali/JUSTICE",
    "itemCount": "~4,500 cases",
    "specs": "Text (English); Judgment prediction",
    "description": "A benchmark dataset for predicting the judgment of the Supreme Court of the United States (SCOTUS). It aims to help models identify patterns influencing court decisions and predict outcomes based on case facts."
  },
  {
    "id": "imported-1769500766501-473-1qyjd",
    "title": "PsyQA",
    "source": "Semantic Scholar",
    "authors": [
      "Hao Sun",
      "Zhenru Lin",
      "Chuanqi Tan",
      "Xiaancheng Zheng",
      "Ruobing Xie",
      "Jianan Wang",
      "Yanwen Wu",
      "Zhiguo Gong",
      "Wei Wang",
      "Mengyue Wu"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.118/",
    "githubLink": "https://github.com/qiuhuachuan/PsyDial",
    "itemCount": "22,000 questions, 56,000 answers",
    "specs": "Text (Chinese, QA pairs)",
    "description": "A Chinese dataset of psychological health support in the form of question-and-answer pairs, crawled from a mental health service platform, with a portion annotated for support strategies."
  },
  {
    "id": "imported-1769500766501-474-du03p",
    "title": "FFA-IR",
    "source": "arXiv",
    "authors": [
      "Mingjie Li",
      "Wenjia Cai",
      "Rui Liu",
      "Yuetian Weng",
      "Xiaoyun Zhao",
      "Cong Wang",
      "Xin Chen",
      "Zhong Liu",
      "Caineng Pan",
      "Mengke Li",
      "Yingfeng Zheng",
      "Yizhi Liu",
      "Flora D. Salim",
      "Karin Verspoor",
      "Xiaodan Liang",
      "Xiaojun Chang"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2108.02023",
    "githubLink": "https://github.com/mlii0117/FFA-IR",
    "itemCount": "1,048,584 images, 10,790 reports",
    "specs": "Fundus Fluorescein Angiography images, Bilingual reports (En/Zh), Lesion annotations",
    "description": "A large-scale benchmark for explainable and reliable medical report generation based on Fundus Fluorescein Angiography (FFA) images, featuring bilingual (Chinese/English) reports."
  },
  {
    "id": "imported-1769500766501-475-8qg5w",
    "title": "GovReport",
    "source": "Hugging Face",
    "authors": [
      "Luyang Huang",
      "Shuyang Cao",
      "Nikolaus Parulian",
      "Heng Ji",
      "Lu Wang"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.naacl-main.112/",
    "githubLink": "https://github.com/luyang-huang96/LongDocSum",
    "itemCount": "19,466 reports",
    "specs": "Text (Government Reports)",
    "description": "A large-scale dataset for long document summarization containing reports from the U.S. Government Accountability Office (GAO) and Congressional Research Service (CRS). The dataset features significantly longer documents and summaries compared to previous benchmarks."
  },
  {
    "id": "imported-1769500766501-476-i75x8",
    "title": "GSM8K (Grade School Math 8K)",
    "source": "arXiv",
    "authors": [
      "Karl Cobbe",
      "Vineet Kosaraju",
      "Mohammad Bavarian",
      "Mark Chen",
      "Heewoo Jun",
      "Lukasz Kaiser",
      "Matthias Plappert",
      "Jerry Tworek",
      "Jacob Hilton",
      "Reiichiro Nakano",
      "Christopher Hesse",
      "John Schulman"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.14168",
    "githubLink": "https://github.com/openai/grade-school-math",
    "itemCount": "~8.5k problems",
    "specs": "Text (Math Word Problems)",
    "description": "A dataset of high-quality linguistically diverse grade school math word problems. It is designed to support the task of question answering on basic mathematical problems that require multi-step reasoning."
  },
  {
    "id": "imported-1769500766501-477-xwyos",
    "title": "XTREME-R",
    "source": "arXiv",
    "authors": [
      "Sebastian Ruder",
      "Noah Constant",
      "Jan A. Botha",
      "Aditya Siddhant",
      "Orhan Firat",
      "Jinlan Fu",
      "Pengfei Liu",
      "Junjie Hu",
      "Dan Garrette",
      "Graham Neubig",
      "Melvin Johnson"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2104.07412",
    "githubLink": "https://github.com/google-research/xtreme",
    "itemCount": "10 tasks, 50 languages",
    "specs": "Text; Tasks include reasoning, retrieval, and QA",
    "description": "An improved version of the XTREME benchmark that covers 50 typologically diverse languages and 10 challenging tasks, including language-agnostic retrieval."
  },
  {
    "id": "imported-1769500766501-478-kja87",
    "title": "AdvGLUE",
    "source": "Hugging Face",
    "authors": [
      "Boxin Wang",
      "Chejian Xu",
      "Shuohang Wang",
      "Zhe Gan",
      "Yu Cheng",
      "Jianfeng Gao",
      "Ahmed Hassan Awadallah",
      "Bo Li"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2111.02840",
    "githubLink": "https://github.com/ai-secure/adv_glue",
    "itemCount": "~4,000 adversarial examples (across dev/test sets)",
    "specs": "Text (Natural Language Understanding); Word/Sentence-level perturbations",
    "description": "A multi-task benchmark for evaluating the robustness of language models. It applies 14 textual adversarial attack methods to GLUE tasks (like SST-2, MNLI, QNLI) to create valid, human-verified adversarial examples."
  },
  {
    "id": "imported-1769500766501-479-3bya1",
    "title": "HumanEval",
    "source": "arXiv",
    "authors": [
      "Mark Chen",
      "Jerry Tworek",
      "Heewoo Jun",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2107.03374",
    "githubLink": "https://github.com/openai/human-eval",
    "itemCount": "164 problems",
    "specs": "Python functions with docstrings and unit tests",
    "description": "A benchmark for evaluating the functional correctness of code generated by Large Language Models. It consists of programming problems that include a function signature, docstring, body, and unit tests."
  },
  {
    "id": "imported-1769500766501-480-7xzck",
    "title": "MBPP (Mostly Basic Programming Problems)",
    "source": "arXiv",
    "authors": [
      "Jacob Austin",
      "Augustus Odena",
      "Maxwell Nye",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2108.07732",
    "githubLink": "https://github.com/google-research/mbpp",
    "itemCount": "~974 problems",
    "specs": "Python code, natural language descriptions, test cases",
    "description": "A dataset containing entry-level programming problems, designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases."
  },
  {
    "id": "imported-1769500766501-481-2u4y5",
    "title": "CodeXGLUE",
    "source": "arXiv",
    "authors": [
      "Shuai Lu",
      "Daya Guo",
      "Shuo Ren",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2102.04664",
    "githubLink": "https://github.com/microsoft/CodeXGLUE",
    "itemCount": "14 datasets (variable sizes)",
    "specs": "Multilingual (Python, Java, etc.), diverse tasks",
    "description": "A benchmark dataset for code understanding and generation. It includes a collection of 14 datasets for 10 diversified code intelligence tasks, including code completion, translation, and refinement."
  },
  {
    "id": "imported-1769500766501-482-rtbbf",
    "title": "APPS (Automated Programming Progress Standard)",
    "source": "arXiv",
    "authors": [
      "Dan Hendrycks",
      "Steven Basart",
      "Saurav Kadavath",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2105.09938",
    "githubLink": "https://github.com/hendrycks/apps",
    "itemCount": "10,000 problems",
    "specs": "Python, natural language specifications, test cases",
    "description": "A benchmark for code generation with problems ranging from simple one-line solutions to substantial algorithmic challenges, similar to those found on coding competition websites."
  },
  {
    "id": "imported-1769500766501-483-45ld8",
    "title": "Hanabi Open Agent Dataset (HOAD)",
    "source": "Scholar",
    "authors": [
      "Aron Sarmasi",
      "Timothy Zhang",
      "Graham Todd",
      "Sam Earle",
      "Julian Togelius",
      "Ahmed Khalifa"
    ],
    "year": "2021",
    "paperLink": "https://dl.acm.org/doi/10.5555/3463952.3464177",
    "githubLink": "https://github.com/aronsar/hoad",
    "itemCount": "500,000+ games per agent",
    "specs": "Game logs, agent policies, cross-play matrices",
    "description": "A comprehensive collection of existing Hanabi playing agents ported to the Hanabi Learning Environment (HLE), along with cross-play performance data. It is designed to support research in ad-hoc teaming and human-AI cooperation in imperfect information games."
  },
  {
    "id": "imported-1769500766501-484-1bhb6",
    "title": "Olympus",
    "source": "Scholar",
    "authors": [
      "Florian Häse",
      "Matteo Aldeghi",
      "Riley J. Hickman",
      "Alán Aspuru-Guzik"
    ],
    "year": "2021",
    "paperLink": "https://doi.org/10.1088/2632-2153/ac020d",
    "githubLink": "https://github.com/aspuru-guzik-group/olympus",
    "itemCount": "33 benchmark datasets",
    "specs": "Python package; Single and multi-objective optimization; Mixed parameter spaces (continuous/categorical)",
    "description": "A benchmarking framework for noisy optimization and experiment planning in chemistry and materials science. It provides a consistent interface to evaluate optimization algorithms against realistic experimental emulators and datasets."
  },
  {
    "id": "imported-1769500766501-485-g53xw",
    "title": "Summit: Benchmarking Machine Learning Methods for Reaction Optimisation",
    "source": "Scholar",
    "authors": [
      "Kobi C. Felton",
      "Jan G. Rittig",
      "Alexei A. Lapkin"
    ],
    "year": "2021",
    "paperLink": "https://doi.org/10.1002/cmtd.202000051",
    "githubLink": "https://github.com/sustainable-processes/summit",
    "itemCount": "2 primary reaction benchmarks",
    "specs": "Python package; Reaction optimization simulations; continuous and categorical variables",
    "description": "A benchmark suite for reaction optimization algorithms. It includes two in-silico benchmarks based on real chemical reactions (nucleophilic aromatic substitution and C-N cross-coupling) to compare machine learning strategies like Bayesian optimization."
  },
  {
    "id": "imported-1769500766501-486-kfubs",
    "title": "BLURB",
    "source": "arXiv",
    "authors": [
      "Yu Gu",
      "Robert Tinn",
      "Hao Cheng",
      "Michael Lucas",
      "Naoto Usuyama",
      "Xiaodong Liu",
      "Tristan Naumann",
      "Jianfeng Gao",
      "Hoifung Poon"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2007.15779",
    "githubLink": "https://microsoft.github.io/BLURB/",
    "itemCount": "13 datasets (NER, PICO, Relation Extraction, etc.)",
    "specs": "Text (Various NLP tasks including NER, RE, Sentence Similarity)",
    "description": "The Biomedical Language Understanding and Reasoning Benchmark is a collection of resources for biomedical NLP. It comprises 13 publicly available datasets across 6 diverse tasks, focusing on PubMed-based applications."
  },
  {
    "id": "imported-1769500766501-487-yal98",
    "title": "RealToxicityPrompts",
    "source": "arXiv",
    "authors": [
      "Samuel Gehman",
      "Suchin Gururangan",
      "Maarten Sap",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.06367",
    "githubLink": "https://github.com/allenai/real-toxicity-prompts",
    "itemCount": "100,000+ prompts",
    "specs": "Text prompts (English), JSONL format",
    "description": "A dataset of naturally occurring sentence-level prompts derived from web text, paired with toxicity scores, designed to evaluate neural toxic degeneration in language models."
  },
  {
    "id": "imported-1769500766501-488-xkoko",
    "title": "Counsel Chat",
    "source": "Hugging Face",
    "authors": [
      "Nicolas Bertagnolli"
    ],
    "year": "2020",
    "paperLink": "https://towardsdatascience.com/counsel-chat-bootstrapping-high-quality-therapy-data-971b419f33da",
    "githubLink": "https://github.com/nbertagnolli/counsel-chat",
    "itemCount": "3,602 questions and answers (approx.)",
    "specs": "Text (English), Q&A pairs",
    "description": "A collection of high-quality counseling questions and answers scraped from an online counseling platform where verified therapists respond to user queries. It is widely used for training empathetic dialogue systems."
  },
  {
    "id": "imported-1769500766501-489-dedaj",
    "title": "Big-Vul",
    "source": "Other",
    "authors": [
      "Jiahao Fan",
      "Yi Li",
      "Shaohua Wang",
      "Tien N. Nguyen"
    ],
    "year": "2020",
    "paperLink": "https://conf.researchr.org/details/msr-2020/msr-2020-technical-papers/11/A-C-C-Code-Vulnerability-Dataset-with-Code-Changes-and-CVE-Summaries",
    "githubLink": "https://github.com/ZeoVan/MSR_20_Code_vulnerability_CSV_Dataset",
    "itemCount": "217,007 rows",
    "specs": "C/C++ code, CSV format",
    "description": "A large-scale C/C++ code vulnerability dataset constructed from open-source GitHub projects. It includes code changes and CVE summaries to address issues like low volume and lack of real-world complexity in previous datasets."
  },
  {
    "id": "imported-1769500766501-490-rvmhp",
    "title": "KorNLI and KorSTS",
    "source": "arXiv",
    "authors": [
      "Jiyeon Ham",
      "Yo Joong Choe",
      "Kyubyong Park",
      "Ilji Choi",
      "Hyungjoon Soh"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2004.03289",
    "githubLink": "https://github.com/kakaobrain/KorNLUDatasets",
    "itemCount": "KorNLI: ~570k pairs, KorSTS: 8,628 pairs",
    "specs": "Text, NLI, STS",
    "description": "Benchmark datasets for Korean Natural Language Inference (NLI) and Semantic Textual Similarity (STS). KorNLI includes machine-translated (from MNLI/SNLI) and human-translated (XNLI) data. KorSTS is a translation of the STS-B dataset."
  },
  {
    "id": "imported-1769500766501-491-o1umv",
    "title": "MMLU (Massive Multitask Language Understanding)",
    "source": "arXiv",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart",
      "Andy Zou",
      "Mantas Mazeika",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.03300",
    "githubLink": "https://github.com/hendrycks/test",
    "itemCount": "15,908 questions",
    "specs": "Multiple-choice (4 options), Text, 57 subjects",
    "description": "A massive benchmark that covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to a professional level, testing both world knowledge and problem-solving ability."
  },
  {
    "id": "imported-1769500766501-492-z8bxo",
    "title": "ORNL Radiation Detection in Urban Environments Dataset",
    "source": "Scholar",
    "authors": [
      "James M. Ghawaly",
      "Andrew D. Nicholson",
      "Douglas E. Peplow",
      "Christine M. Anderson-Cook",
      "Kary L. Myers",
      "Daniel E. Archer",
      "Michael J. Willis",
      "Brian J. Quiter"
    ],
    "year": "2020",
    "paperLink": "https://doi.org/10.1038/s41597-020-00672-2",
    "githubLink": "https://github.com/ORNL/ARDIDSA-Data",
    "itemCount": "Thousands of synthetic list mode data files",
    "specs": "Synthetic signal data (List mode gamma-ray spectra); Modality: Sensor/Signal",
    "description": "A synthetic dataset representing the response of a mobile NaI(Tl) detector in a mid-sized urban city, created to test and train radiation detection and identification algorithms against dynamic backgrounds."
  },
  {
    "id": "imported-1769500766501-493-8uwcf",
    "title": "MIMIC-SQL",
    "source": "Scholar",
    "authors": [
      "Ping Wang",
      "Tian Shi",
      "Chandan K. Reddy"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.acl-main.123/",
    "githubLink": "https://github.com/wangpinggl/TREQS",
    "itemCount": "10,000 questions",
    "specs": "Structured Data (MIMIC-III); Text-to-SQL",
    "description": "A dataset for text-to-SQL generation on Electronic Medical Records. It consists of two subsets: machine-generated template questions and human-annotated natural language questions, based on MIMIC-III data."
  },
  {
    "id": "imported-1769500766501-494-udpe4",
    "title": "MIMIC-IV (Lab Events)",
    "source": "Other",
    "authors": [
      "Alistair Johnson",
      "Lucas Bulgarelli",
      "Lu Shen",
      "Alvin Gayles",
      "Ayad Shammout",
      "Steven Horng",
      "Leo Anthony Celi",
      "Roger Mark"
    ],
    "year": "2020",
    "paperLink": "https://physionet.org/content/mimiciv/",
    "githubLink": "https://github.com/MIT-LCP/mimic-code",
    "itemCount": "Millions of events (covering 65k+ patients)",
    "specs": "Structured Tabular Data (CSV/Parquet)",
    "description": "A module within the massive MIMIC-IV critical care dataset containing laboratory measurements for patients. The 'labevents' table captures routine pathology results (e.g., blood work, chemistry) for hospital admissions, widely used for benchmarking clinical time-series prediction and phenotype classification."
  },
  {
    "id": "imported-1769500766501-495-3rfmk",
    "title": "VinDr-CXR",
    "source": "Other",
    "authors": [
      "Ha Q. Nguyen",
      "Khanh Lam",
      "Linh T. Le",
      "Hieu H. Pham",
      "Dat Q. Tran",
      "Dung B. Nguyen"
    ],
    "year": "2020",
    "paperLink": "https://physionet.org/content/vindr-cxr/1.0.0/",
    "githubLink": "https://github.com/vinbigdata-medical/vindr-cxr",
    "itemCount": "18,000 images",
    "specs": "DICOM images, bounding box annotations for 22 local findings, 6 global disease labels",
    "description": "An open dataset of chest X-rays with radiologist annotations for the classification of common thoracic diseases and localization of critical findings."
  },
  {
    "id": "imported-1769500766501-496-yqfd0",
    "title": "Object-CXR",
    "source": "Other",
    "authors": [
      "JF Healthcare",
      "MIDL Organizers"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.11597",
    "githubLink": "https://github.com/hlk-1135/object-CXR",
    "itemCount": "10,000 images",
    "specs": "5000 positive and 5000 negative samples, bounding boxes/masks",
    "description": "A benchmark dataset designed for the automatic detection of foreign objects in chest X-rays."
  },
  {
    "id": "imported-1769500766501-497-fy4tc",
    "title": "PathVQA",
    "source": "arXiv",
    "authors": [
      "Xuehai He",
      "Pathwan Zhang",
      "Qian Wang",
      "Luntian Mou"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.10286",
    "githubLink": "https://github.com/flaviagiammarino/path-vqa",
    "itemCount": "32,799 QA pairs; 4,998 images",
    "specs": "Visual Question Answering (VQA), Text + Image",
    "description": "A dataset designed for medical visual question answering (VQA) in pathology. It contains question-answer pairs generated from pathology textbooks and digital libraries, including both open-ended and binary (yes/no) questions."
  },
  {
    "id": "imported-1769500766501-498-lyglg",
    "title": "US-DermMCQA (Internal / Liu et al. 2020)",
    "source": "Other",
    "authors": [
      "Yuan Liu",
      "Aravind Klempner",
      "et al."
    ],
    "year": "2020",
    "paperLink": "https://www.nature.com/articles/s41591-020-0842-3",
    "githubLink": "https://github.com/google-health/dermatology-research",
    "itemCount": "1,996 cases (derived)",
    "specs": "Image + Text (Multiple Choice Questions)",
    "description": "A dermatology multiple-choice question assessment benchmark often referenced in Google/DeepMind research (e.g., MedGemma). It is an internal dataset derived from the work of Liu et al. (2020), consisting of de-identified teledermatology cases with 136 skin conditions transformed into 4-way MCQs."
  },
  {
    "id": "imported-1769500766501-499-mc1cr",
    "title": "PAD-UFES-20",
    "source": "Other",
    "authors": [
      "André G. C. Pacheco",
      "Gustavo R. Lima",
      "Amanda S. Salomão",
      "Breno Krohling",
      "Igor P. Biral",
      "Gabriel G. de Angelo",
      "Felipe C. Alves Jr.",
      "José G. M. Esgario",
      "Alana C. Simora",
      "Pedro B. C. Castro",
      "Felipe B. Rodrigues",
      "Patricia H. L. Frasson",
      "Renato A. Krohling",
      "Helder Knidel",
      "Maria C. S. Santos",
      "Rachel B. Spirandelli",
      "Luíz F. S. de Barros"
    ],
    "year": "2020",
    "paperLink": "https://doi.org/10.1016/j.dib.2020.106221",
    "githubLink": "https://github.com/labcin-ufes/PAD-UFES-20",
    "itemCount": "2,298 images",
    "specs": "Images (PNG), Metadata (CSV). 6 diagnostic classes: Basal Cell Carcinoma (BCC), Squamous Cell Carcinoma (SCC), Actinic Keratosis (ACK), Seborrheic Keratosis (SEK), Melanoma (MEL), and Nevus (NEV). Metadata includes up to 26 features.",
    "description": "A skin lesion benchmark dataset composed of clinical images collected from smartphones and patient clinical data. Unlike many other datasets that focus on dermoscopy, this dataset provides standard camera images which are more representative of a primary care or teledermatology setting. It includes rich metadata such as patient age, gender, body region, and skin cancer history."
  },
  {
    "id": "imported-1769500766501-500-4yzs2",
    "title": "SOREL-20M (Sophos-ReversingLabs 20 Million)",
    "source": "arXiv",
    "authors": [
      "Richard Harang",
      "Ethan M. Rudd"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2012.07634",
    "githubLink": "https://github.com/sophos-ai/SOREL-20M",
    "itemCount": "20 million samples (10M benign, 10M malicious)",
    "specs": "EMBERv2 feature format, SQLite metadata, raw PE binaries (disarmed)",
    "description": "A massive scale dataset for malware detection consisting of nearly 20 million files. It includes pre-extracted features and metadata for both benign and malicious PE files, aiming to improve the scalability and robustness of malware classifiers."
  },
  {
    "id": "imported-1769500766501-501-d84nj",
    "title": "ISIC 2020 Challenge Dataset (SIIM-ISIC Melanoma Classification)",
    "source": "Semantic Scholar",
    "authors": [
      "Veronica Rotemberg",
      "Nicholas Kurtansky",
      "Betina Betz-Stablein",
      "et al."
    ],
    "year": "2020",
    "paperLink": "https://doi.org/10.34970/2020-ds01",
    "githubLink": "https://github.com/ISIC-Archive",
    "itemCount": "33,126 images",
    "specs": "DICOM/JPEG images, CSV metadata (patient ID, anatomical site, diagnosis)",
    "description": "A large collection of dermoscopic images of skin lesions, used to benchmark algorithms for melanoma detection. The dataset includes a significant number of benign lesions to challenge models in distinguishing them from malignant cases."
  },
  {
    "id": "imported-1769500766501-502-fbklg",
    "title": "X-Stance",
    "source": "arXiv",
    "authors": [
      "Jannis Vamvas",
      "Rico Sennrich"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.08385",
    "githubLink": "https://github.com/ZurichNLP/xstance",
    "itemCount": "67,000 comments",
    "specs": "Multilingual Text (DE, FR, IT), 150+ Political Targets",
    "description": "A multilingual, multi-target stance detection dataset extracted from a Swiss voting advice platform, covering German, French, and Italian comments on various political issues."
  },
  {
    "id": "imported-1769500766501-503-pffet",
    "title": "DeepMind Specification Gaming Examples",
    "source": "Scholar",
    "authors": [
      "Victoria Krakovna",
      "Jonathan Uesato",
      "Vladimir Mikulik",
      "Matthew Rahtz",
      "Tom Everitt",
      "Ramana Kumar",
      "Zac Kenton",
      "Jan Leike",
      "Shane Legg"
    ],
    "year": "2020",
    "paperLink": "https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/",
    "githubLink": "https://docs.google.com/spreadsheets/d/1_s4cW-zYvI5fYPbArvQvcI4SA0UaT8s6DENpNB_o_gM",
    "itemCount": "~60+ documented examples",
    "specs": "List of environments (Atari, MuJoCo, etc.) and video/descriptions of failures.",
    "description": "A curated list and collection of examples demonstrating specification gaming (reward hacking) across various AI environments. While not a single downloadable dataset in the traditional sense, it serves as a foundational reference and collection of environments (like CoastRunners, Traffic, etc.) where reward hacking has been documented."
  },
  {
    "id": "imported-1769500766501-504-4q70r",
    "title": "Social Chemistry 101",
    "source": "Semantic Scholar",
    "authors": [
      "Maxwell Forbes",
      "Jena D. Hwang",
      "Vered Shwartz",
      "Maarten Sap",
      "Yejin Choi"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.emnlp-main.65/",
    "githubLink": "https://github.com/mbforbes/social-chemistry-101",
    "itemCount": "292,000 rules-of-thumb, 4.5M annotations",
    "specs": "Text (English); Structure (Situation, Rule, Judgment)",
    "description": "A large-scale dataset for reasoning about social and moral norms. It contains rules-of-thumb, situations, and judgment attributes to train models in understanding social norms and moral judgments."
  },
  {
    "id": "imported-1769500766501-505-ahskl",
    "title": "RealToxicityPrompts",
    "source": "arXiv",
    "authors": [
      "Samuel Gehman",
      "Suchin Gururangan",
      "Maarten Sap",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.11462",
    "githubLink": "https://github.com/allenai/real-toxicity-prompts",
    "itemCount": "100,000 prompts",
    "specs": "Text-only; Prompts with toxicity scores (using Perspective API)",
    "description": "A large-scale dataset of prompts designed to evaluate the risk of neural toxic degeneration in language models. It pairs prompts with toxicity scores to measure how often models generate toxic continuations."
  },
  {
    "id": "imported-1769500766501-506-kck80",
    "title": "FNS 2020 (Financial Narrative Summarisation)",
    "source": "Other",
    "authors": [
      "Mahmoud El-Haj",
      "Ahmed AbuRa'ed",
      "Marina Litvak",
      "Natalia Vanetik",
      "George Giannakopoulos"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.fnp-1.1/",
    "githubLink": "https://github.com/ignatiuszek/FNS-2020",
    "itemCount": "3,863 annual reports",
    "specs": "Text (Financial Annual Reports)",
    "description": "A dataset created for the Financial Narrative Summarisation shared task. It contains UK annual reports published in PDF format, focusing on summarizing the narrative sections of financial documents."
  },
  {
    "id": "imported-1769500766501-507-vvrzs",
    "title": "ERASER (Evaluating Rationales And Simple English Reasoning)",
    "source": "Scholar",
    "authors": [
      "Jay DeYoung",
      "Sarthak Jain",
      "Nazneen Fatema Rajani",
      "Eric Lehman",
      "Caiming Xiong",
      "Richard Socher",
      "Byron C. Wallace"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.acl-main.408/",
    "githubLink": "http://www.eraserbenchmark.com/",
    "itemCount": "7 datasets (Movies, FEVER, MultiRC, etc.)",
    "specs": "Text (NLP); Rationales (span selection)",
    "description": "A benchmark to evaluate rationalized NLP models. It unifies multiple datasets where the task is to predict a label and provide a 'rationale' (supporting evidence) from the input text. It includes metrics for measuring how well model rationales align with human annotations."
  },
  {
    "id": "imported-1769500766501-508-x9rbb",
    "title": "LogiQA",
    "source": "arXiv",
    "authors": [
      "Jian Liu",
      "Leyang Cui",
      "Hanmeng Liu",
      "Dandan Huang",
      "Yile Wang",
      "Yue Zhang"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2007.08124",
    "githubLink": "https://github.com/lgw863/LogiQA-dataset",
    "itemCount": "8,678 QA instances",
    "specs": "Text (Reading Comprehension)",
    "description": "A challenge dataset for machine reading comprehension with logical reasoning. The questions are sourced from the Chinese Civil Service Examination and cover various types of logical reasoning."
  },
  {
    "id": "imported-1769500766501-509-ro714",
    "title": "XTREME",
    "source": "arXiv",
    "authors": [
      "Junjie Hu",
      "Sebastian Ruder",
      "Aditya Siddhant",
      "Graham Neubig",
      "Orhan Firat",
      "Melvin Johnson"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.11080",
    "githubLink": "https://github.com/google-research/xtreme",
    "itemCount": "9 tasks, 40 languages",
    "specs": "Text; Tasks include classification, structured prediction, QA, and retrieval",
    "description": "A massively multilingual multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks."
  },
  {
    "id": "imported-1769500766501-510-kb9w9",
    "title": "XGLUE",
    "source": "arXiv",
    "authors": [
      "Yaobo Liang",
      "Nan Duan",
      "Yeyun Gong",
      "Ning Wu",
      "Fenfei Guo",
      "Weizhen Qi",
      "Ming Gong",
      "Linjun Shou",
      "Daxin Jiang",
      "Guihong Cao",
      "Xiaodong Fan",
      "Ruofei Zhang",
      "Rahul Agrawal",
      "Edward Cui",
      "Sining Wei",
      "Taroon Bharti",
      "Ying Qiao",
      "Jiun-Hung Chen",
      "Winnie Wu",
      "Shuguang Liu",
      "Fan Yang",
      "Daniel Campos",
      "Rangan Majumder",
      "Ming Zhou"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2004.01401",
    "githubLink": "https://github.com/microsoft/XGLUE",
    "itemCount": "11 tasks, 19 languages",
    "specs": "Text; Tasks include NER, POS, QA, News Classification, Query-Ad Matching",
    "description": "A benchmark dataset for evaluating cross-lingual pre-trained models across 11 tasks and 19 languages, covering both natural language understanding and generation."
  },
  {
    "id": "imported-1769500766501-511-xna6w",
    "title": "TyDi QA",
    "source": "arXiv",
    "authors": [
      "Jonathan H. Clark",
      "Eunsol Choi",
      "Michael Collins",
      "Dan Garrette",
      "Tom Kwiatkowski",
      "Vitaly Nikolaev",
      "Jennimaria Palomaki"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.05002",
    "githubLink": "https://github.com/google-research-datasets/tydiqa",
    "itemCount": "204,000 QA pairs, 11 languages",
    "specs": "Text; Question Answering (Gold Passage, Primary Task)",
    "description": "A question answering benchmark covering 11 typologically diverse languages with 204K question-answer pairs, designed to avoid translation-based artifacts."
  },
  {
    "id": "imported-1769500766501-512-xtfeg",
    "title": "CoVoST 2",
    "source": "arXiv",
    "authors": [
      "Changhan Wang",
      "Anne Wu",
      "Juan Pino"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2007.10310",
    "githubLink": "https://github.com/facebookresearch/covost",
    "itemCount": "2,880 hours of speech, 21 languages",
    "specs": "Audio/Speech; Speech Translation",
    "description": "A large-scale multilingual speech-to-text translation corpus covering 21 languages into English and English into 15 languages."
  },
  {
    "id": "imported-1769500766501-513-99hvr",
    "title": "CrowS-Pairs",
    "source": "Semantic Scholar",
    "authors": [
      "Nikita Nangia",
      "Clara Vania",
      "Rasika Bhalerao",
      "Samuel R. Bowman"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.emnlp-main.154/",
    "githubLink": "https://github.com/nyu-mll/crows-pairs",
    "itemCount": "1,508 sentence pairs",
    "specs": "Text; 9 bias categories (race, gender, religion, etc.)",
    "description": "A challenge dataset for measuring social bias in masked language models. It uses minimal pairs of sentences to test for stereotypical biases."
  },
  {
    "id": "imported-1769500766501-514-ii9sq",
    "title": "RobustBench",
    "source": "arXiv",
    "authors": [
      "Francesco Croce",
      "Maksym Andriushchenko",
      "Vikash Sehwag",
      "Edoardo Debenedetti",
      "Nicolas Flammarion",
      "Mihailo Chiang",
      "Pratyush Maini",
      "Tolga Ergen",
      "Nicolas Papernot",
      "Matthias Hein"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2010.09670",
    "githubLink": "https://github.com/RobustBench/robustbench",
    "itemCount": "Tracks 120+ models; uses full CIFAR/ImageNet test sets",
    "specs": "Image classification; L_inf, L_2 threats; Corruptions",
    "description": "A standardized benchmark for adversarial robustness in image classification. It tracks the progress of robust models on CIFAR-10, CIFAR-100, and ImageNet using a unified evaluation protocol (AutoAttack) to prevent overestimated robustness."
  },
  {
    "id": "imported-1769500766501-515-99rpl",
    "title": "BLUE",
    "source": "Scholar",
    "authors": [
      "Yifan Peng",
      "Shankai Yan",
      "Zhiyong Lu"
    ],
    "year": "2019",
    "paperLink": "https://doi.org/10.18653/v1/W19-5044",
    "githubLink": "https://github.com/ncbi-nlp/BLUE_Benchmark",
    "itemCount": "10 datasets",
    "specs": "Text (NER, Relation Extraction, Sentence Similarity, Inference)",
    "description": "The Biomedical Language Understanding Evaluation benchmark consists of five different biomedicine text-mining tasks with ten corpora, covering both biomedical literature and clinical notes."
  },
  {
    "id": "imported-1769500766501-516-nrzp4",
    "title": "Devign",
    "source": "arXiv",
    "authors": [
      "Yaqin Zhou",
      "Shangqing Liu",
      "Jingkai Siow",
      "Xiaoning Du",
      "Yang Liu"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1909.03496",
    "githubLink": "https://github.com/epicosy/devign",
    "itemCount": "Approx. 27,000 functions (from 4 projects)",
    "specs": "C code, Graph-based representations",
    "description": "A graph neural network-based model and dataset for vulnerability identification. The dataset is manually labeled and built from four large-scale open-source C projects: Linux, FFmpeg, Qemu, and Wireshark."
  },
  {
    "id": "imported-1769500766501-517-vrz0z",
    "title": "KorQuAD 1.0 (Korean Question Answering Dataset)",
    "source": "arXiv",
    "authors": [
      "Seungyoung Lim",
      "Myungji Kim",
      "Jooyoul Lee"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1909.07005",
    "githubLink": "https://korquad.github.io/",
    "itemCount": "70,000+ pairs (10,165 validation)",
    "specs": "Text, Machine Reading Comprehension (QA)",
    "description": "A large-scale Korean dataset for extractive machine reading comprehension, derived from Wikipedia. It is designed to evaluate the reading comprehension capabilities of Korean language models, similar to SQuAD for English."
  },
  {
    "id": "imported-1769500766501-518-rb2do",
    "title": "ARC-AGI (Abstraction and Reasoning Corpus)",
    "source": "arXiv",
    "authors": [
      "François Chollet"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.01547",
    "githubLink": "https://github.com/fchollet/ARC",
    "itemCount": "1,000 tasks (400 training, 400 evaluation, 200 private test)",
    "specs": "JSON format containing 2D grid pairs (integers 0-9 representing colors).",
    "description": "A benchmark measuring fluid intelligence and skill acquisition efficiency. It consists of unique training and evaluation tasks that require solving novel grid-based logic puzzles with minimal prior knowledge, aiming to test 'General Intelligence' rather than memorization."
  },
  {
    "id": "imported-1769500766501-519-n8zny",
    "title": "Miami University Deception Detection Database (MU3D)",
    "source": "Other",
    "authors": [
      "E. Paige Lloyd",
      "Jason C. Deska",
      "Kurt Hugenberg",
      "Allen R. McConnell",
      "Katherine T. Humphrey",
      "Jonathan W. Kunstman"
    ],
    "year": "2019",
    "paperLink": "https://link.springer.com/article/10.3758/s13428-018-1061-4",
    "githubLink": "http://hdl.handle.net/2374.MIA/6067",
    "itemCount": "320 videos",
    "specs": "Video, Audio, Transcripts; 80 unique targets",
    "description": "A free resource containing videos of diverse targets (Black/White, Male/Female) telling truths and lies about their social relationships. Designed to cross-examine race, gender, and deception."
  },
  {
    "id": "imported-1769500766501-520-8dcaz",
    "title": "Bag-of-Lies (BOL)",
    "source": "Other",
    "authors": [
      "Vipul Gupta",
      "Mridula Agarwal",
      "Manik Arora",
      "Tanmoy Chakraborty",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "year": "2019",
    "paperLink": "https://openaccess.thecvf.com/content_CVPRW_2019/papers/CV-COPS/Gupta_Bag-of-Lies_A_Multimodal_Dataset_for_Deception_Detection_CVPRW_2019_paper.pdf",
    "githubLink": "http://iab-rubric.org/resources/bag-of-lies",
    "itemCount": "325 annotated data points",
    "specs": "Video, Audio, EEG, Gaze data; 35 subjects",
    "description": "A multimodal dataset collected in a realistic scenario where subjects describe images. It is unique for including gaze data and EEG signals alongside standard audio-visual feeds."
  },
  {
    "id": "imported-1769500766501-521-ss5x1",
    "title": "Box of Lies",
    "source": "Other",
    "authors": [
      "Felix Soldner",
      "Verónica Pérez-Rosas",
      "Rada Mihalcea"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/N19-1175/",
    "githubLink": "https://github.com/lit-umich/BoxOfLies",
    "itemCount": "25 videos / ~1,049 utterances",
    "specs": "Video, Audio, Text (Dialogue); 26 unique participants",
    "description": "A multimodal dataset derived from the 'Box of Lies' game on 'The Tonight Show Starring Jimmy Fallon'. It features dialogue-heavy deception in a game-like setting."
  },
  {
    "id": "imported-1769500766501-522-eu340",
    "title": "PIQA (Physical Interaction Question Answering)",
    "source": "arXiv",
    "authors": [
      "Yonatan Bisk",
      "Rowan Zellers",
      "Ronan Le Bras",
      "Jianfeng Gao",
      "Yejin Choi"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.11641",
    "githubLink": "https://github.com/ybisk/piqa",
    "itemCount": "21,035 QA pairs (16,113 Train, 1,838 Dev, 3,084 Test)",
    "specs": "Text-based multiple choice (2 options); Physical commonsense reasoning task",
    "description": "A benchmark dataset for evaluating physical commonsense reasoning in natural language. The task involves choosing the most plausible solution to a goal regarding everyday physical interactions from two multiple-choice options. It focuses on affordances and physical properties often omitted in text due to reporting bias."
  },
  {
    "id": "imported-1769500766501-523-yx0p3",
    "title": "MIMIC-CXR",
    "source": "Other",
    "authors": [
      "Alistair E. W. Johnson",
      "Tom J. Pollard",
      "Seth J. Berkowitz",
      "Nathaniel R. Greenbaum",
      "Matthew P. Lungren",
      "Chih-ying Deng",
      "Roger G. Mark",
      "Steven Horng"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1901.07042",
    "githubLink": "https://github.com/MIT-LCP/mimic-code",
    "itemCount": "377,110 images; 227,827 reports",
    "specs": "DICOM/JPG images, free-text radiology reports, 14 CheXpert-derived labels",
    "description": "A large publicly available dataset of chest radiographs with free-text radiology reports, de-identified and sourced from the Beth Israel Deaconess Medical Center."
  },
  {
    "id": "imported-1769500766501-524-8hdbi",
    "title": "CheXpert",
    "source": "Other",
    "authors": [
      "Jeremy Irvin",
      "Pranav Rajpurkar",
      "Michael Ko",
      "Yifan Yu",
      "Silviana Ciurea-Ilcus",
      "Chris Chute",
      "Henrik Marklund",
      "Behzad Haghgoo",
      "Robyn Ball",
      "Katie Shpanskaya"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1901.07031",
    "githubLink": "https://github.com/stanfordmlgroup/chexpert",
    "itemCount": "224,316 images",
    "specs": "Frontal and lateral chest radiographs, 14 observations with uncertainty labels",
    "description": "A large dataset of chest radiographs with uncertainty labels and radiologist-labeled reference standard evaluation sets, designed for automated interpretation."
  },
  {
    "id": "imported-1769500766501-525-d8912",
    "title": "PadChest",
    "source": "Other",
    "authors": [
      "Aurelia Bustos",
      "Antonio Pertusa",
      "Jose-Maria Salinas",
      "Maria de la Iglesia-Vaya"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1901.07441",
    "githubLink": "https://github.com/auriml/Rx-thorax-automatic-captioning",
    "itemCount": "160,868 images",
    "specs": "High-resolution X-ray images, Spanish radiology reports, 174 radiographic findings",
    "description": "A large-scale, high-resolution chest X-ray dataset from Spain with multi-label annotated reports, including hierarchical labels mapped to UMLS."
  },
  {
    "id": "imported-1769500766501-526-jqf9m",
    "title": "OASIS-3",
    "source": "Semantic Scholar",
    "authors": [
      "Pamela J. LaMontagne",
      "Tammie L. S. Benzinger",
      "John C. Morris",
      "Sarah Keefe"
    ],
    "year": "2019",
    "paperLink": "https://www.medrxiv.org/content/10.1101/2019.12.13.19014902v1",
    "githubLink": "https://www.oasis-brains.org/",
    "itemCount": "1,098 participants, >2,000 MR sessions",
    "specs": "T1w, T2w, FLAIR, ASL, SWI, BOLD, DTI, PET",
    "description": "A longitudinal neuroimaging, clinical, cognitive, and biomarker dataset for normal aging and Alzheimer's Disease."
  },
  {
    "id": "imported-1769500766501-527-f4osb",
    "title": "HellaSwag",
    "source": "arXiv",
    "authors": [
      "Rowan Zellers",
      "Ari Holtzman",
      "Yonatan Bisk",
      "Ali Farhadi",
      "Yejin Choi"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1905.07830",
    "githubLink": "https://github.com/rowanzellers/hellaswag",
    "itemCount": "70,000+ samples",
    "specs": "Text, Multiple Choice, Commonsense NLI",
    "description": "An adversarial dataset for commonsense natural language inference where models must select the best ending to a context, designed to be hard for state-of-the-art models."
  },
  {
    "id": "imported-1769500766501-528-satua",
    "title": "VQA-Med 2019",
    "source": "Scholar",
    "authors": [
      "Asma Ben Abacha",
      "Sadid A. Hasan",
      "Vivek V. Datla",
      "Joey Liu",
      "Dina Demner-Fushman",
      "Henning Müller"
    ],
    "year": "2019",
    "paperLink": "https://ceur-ws.org/Vol-2380/paper_272.pdf",
    "githubLink": "https://github.com/abachaa/VQA-Med-2019",
    "itemCount": "4,200 images, 15,292 QA pairs",
    "specs": "Images (Radiology), Text (QA pairs)",
    "description": "A dataset created for the ImageCLEF 2019 VQA-Med task. It focuses on four categories of questions: Modality, Plane, Organ System, and Abnormality, derived from radiology images."
  },
  {
    "id": "imported-1769500766501-529-2mtc4",
    "title": "EyeQ (Eye Quality Assessment Dataset)",
    "source": "arXiv",
    "authors": [
      "Huazhu Fu",
      "Boyang Wang",
      "Jianbing Shen",
      "Shanshan Cui",
      "Yanwu Xu",
      "Jiang Liu",
      "Ling Shao"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1907.05345",
    "githubLink": "https://github.com/HzFu/EyeQ",
    "itemCount": "28,792 images",
    "specs": "JPEG images, 3-level quality labels (Good, Usable, Reject)",
    "description": "A re-annotated subset of the EyePACS dataset focused on retinal image quality assessment. It provides a three-level quality grading system (Good, Usable, Reject) to help develop robust quality control algorithms for medical imaging."
  },
  {
    "id": "imported-1769500766501-530-98gpa",
    "title": "TAPE (Tasks Assessing Protein Embeddings)",
    "source": "arXiv",
    "authors": [
      "Roshan Rao",
      "Nicholas Bhattacharya",
      "Neil Thomas",
      "Yan Duan",
      "Xi Chen",
      "John Canny",
      "Pieter Abbeel",
      "Yun S. Song"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1906.08230",
    "githubLink": "https://github.com/songlab-cal/tape",
    "itemCount": "5 supervised tasks (plus Pfam pretraining)",
    "specs": "Protein sequences (text), structure prediction, classification, regression",
    "description": "A set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology (structure prediction, remote homology, protein engineering) to evaluate protein embeddings."
  },
  {
    "id": "imported-1769500766501-531-8gyk9",
    "title": "ViraMiner",
    "source": "Other",
    "authors": [
      "Ardi Tampuu",
      "Zurab Bzhalava",
      "Joakim Dillner",
      "Raul Vicente"
    ],
    "year": "2019",
    "paperLink": "https://doi.org/10.1371/journal.pone.0222271",
    "githubLink": "https://github.com/NeuroCSUT/ViraMiner",
    "itemCount": "Sequences from 19 metagenomic experiments",
    "specs": "Raw DNA sequences (300 bp contigs), labeled by BLAST",
    "description": "A deep learning-based benchmark for identifying viral genomes in human samples. It contains two branches of Convolutional Neural Networks designed to detect both patterns and pattern-frequencies on raw metagenomics contigs."
  },
  {
    "id": "imported-1769500766501-532-7x316",
    "title": "Dreaddit",
    "source": "arXiv",
    "authors": [
      "Elsbeth Turcan",
      "Kathleen McKeown"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.00133",
    "githubLink": "http://www.cs.columbia.edu/~eturcan/data/dreaddit.zip",
    "itemCount": "190,000 posts (3,553 labelled segments)",
    "specs": "Text (English, Reddit posts)",
    "description": "A dataset of lengthy social media posts from five different Reddit categories, annotated for the presence of stress, used for stress analysis and identification in text."
  },
  {
    "id": "imported-1769500766501-533-1l7on",
    "title": "BillSum",
    "source": "Other",
    "authors": [
      "Anastassia Kornilova",
      "Vladimir Eidelman"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/D19-5406/",
    "githubLink": "https://github.com/FiscalNote/BillSum",
    "itemCount": "22,218 bills",
    "specs": "Text (Legislative Bills)",
    "description": "The first dataset focused on the summarization of US Congressional and California state bills. It contains legislative text and human-written summaries, designed to benchmark summarization models on technical legislative language."
  },
  {
    "id": "imported-1769500766501-534-rknhs",
    "title": "MIMIC-CXR",
    "source": "Other",
    "authors": [
      "Alistair E.W. Johnson",
      "Tom J. Pollard",
      "Seth J. Berkowitz",
      "Nathaniel R. Greenbaum",
      "Matthew P. Lungren",
      "Chih-ying Deng",
      "Roger G. Mark",
      "Steven Horng"
    ],
    "year": "2019",
    "paperLink": "https://physionet.org/content/mimic-cxr/2.0.0/",
    "githubLink": "https://github.com/MIT-LCP/mimic-cxr",
    "itemCount": "227,835 report studies",
    "specs": "Text, Image (Radiology Reports)",
    "description": "A large database of chest radiographs with associated free-text radiology reports. While primarily for medical imaging, it is a standard benchmark for radiology report summarization (generating the 'Impression' section from 'Findings')."
  },
  {
    "id": "imported-1769500766501-535-qspmu",
    "title": "PharmaCoNER",
    "source": "Semantic Scholar",
    "authors": [
      "Aitor Gonzalez-Agirre",
      "Montserrat Marimon",
      "Ander Intxaurrondo",
      "Ona Rabal",
      "Marta Villegas",
      "Martin Krallinger"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/D19-5701/",
    "githubLink": "https://github.com/PlanTL-SANIDAD/SPACCC",
    "itemCount": "1,000 clinical cases (396,988 words)",
    "specs": "Text (Spanish clinical case reports with NER annotations)",
    "description": "A benchmark track for Named Entity Recognition (NER) of pharmacological substances, compounds, and proteins in Spanish clinical cases, essential for pharmacovigilance and patient safety monitoring in non-English texts."
  },
  {
    "id": "imported-1769500766501-536-xch23",
    "title": "CoS-E (Common Sense Explanations)",
    "source": "Hugging Face",
    "authors": [
      "Nazneen Fatema Rajani",
      "Bryan McCann",
      "Caiming Xiong",
      "Richard Socher"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/P19-1487/",
    "githubLink": "https://github.com/salesforce/cos-e",
    "itemCount": "Explanations for CQA v1.0 and v1.11",
    "specs": "Text; Free-form explanations and selected spans",
    "description": "A dataset of human-annotated commonsense explanations for the Commonsense Question Answering (CQA) dataset. It is designed to train and evaluate models on their ability to generate clear, human-like justifications for commonsense reasoning tasks."
  },
  {
    "id": "imported-1769500766501-537-ng30e",
    "title": "ERASER (Evaluating Rationales And Simple English Reasoning)",
    "source": "arXiv",
    "authors": [
      "Jay DeYoung",
      "Sarthak Jain",
      "Nazneen Fatema Rajani",
      "Eric Lehman",
      "Caiming Xiong",
      "Richard Socher",
      "Byron C. Wallace"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.03429",
    "githubLink": "https://github.com/jaydeyoung/eraserbenchmark",
    "itemCount": "7 datasets (approx. 20k+ total samples)",
    "specs": "Text data, classification labels, human-annotated rationales (span indices)",
    "description": "A benchmark for evaluating the interpretability of NLP models. It comprises multiple datasets (like Movies, FEVER, MultiRC) with human-annotated \"rationales\" (supporting evidence snippets) that justify the classification labels, enabling the measurement of model transparency via explanation alignment."
  },
  {
    "id": "imported-1769500766501-538-7744n",
    "title": "SciCite",
    "source": "Semantic Scholar",
    "authors": [
      "Arman Cohan",
      "Waleed Ammar",
      "Madeleine van Zuylen",
      "Field Cady"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/N19-1011/",
    "githubLink": "https://github.com/allenai/scicite",
    "itemCount": "~11,000 citation contexts",
    "specs": "Text; Classification (Citation Intent)",
    "description": "A dataset for citation intent classification. It annotates citations in scientific papers with their rhetorical function (e.g., Background, Method, Result), helping in the analysis of scientific literature structure."
  },
  {
    "id": "imported-1769500766501-539-z6jnm",
    "title": "MLQA",
    "source": "arXiv",
    "authors": [
      "Patrick Lewis",
      "Barlas Oğuz",
      "Ruty Rinott",
      "Sebastian Riedel",
      "Holger Schwenk"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1910.07475",
    "githubLink": "https://github.com/facebookresearch/mlqa",
    "itemCount": "5K+ QA instances, 7 languages",
    "specs": "Text; Extractive Question Answering",
    "description": "A multi-way parallel extractive question answering evaluation benchmark intended to be used for zero-shot cross-lingual transfer."
  },
  {
    "id": "imported-1769500766501-540-u4829",
    "title": "APRICOT",
    "source": "arXiv",
    "authors": [
      "A. Braunegg",
      "Amartya Chakraborty",
      "Michael Krumdick",
      "Nicole Lape",
      "Sara Leary",
      "Keith Manville",
      "Elizabeth Merkhofer",
      "Laura Strickhart",
      "Matthew Walmer"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1912.08166",
    "githubLink": "https://github.com/mitre/apricot",
    "itemCount": "1,011 images",
    "specs": "Images; Physical patches; Object Detection annotations (COCO format)",
    "description": "A dataset of physical adversarial attacks on object detection systems. It contains photos of printed adversarial patches placed in real-world scenes, annotated with bounding boxes, designed to test robustness in uncontrolled settings."
  },
  {
    "id": "imported-1769500766501-541-morbs",
    "title": "Overcooked-AI",
    "source": "arXiv",
    "authors": [
      "Micah Carroll",
      "Rohin Shah",
      "Mark K. Ho",
      "Thomas L. Griffiths",
      "Sanjit A. Seshia",
      "Pieter Abbeel",
      "Anca Dragan"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1910.05789",
    "githubLink": "https://github.com/HumanCompatibleAI/overcooked_ai",
    "itemCount": "Multiple datasets (e.g., human-human trajectories, 45+ validation episodes)",
    "specs": "Grid-world game states, player actions, trajectories, collaborative tasks",
    "description": "A benchmark environment and dataset for fully cooperative human-AI task performance based on the video game Overcooked. It includes collected human-human and human-AI gameplay trajectories to evaluate coordination and collaboration."
  },
  {
    "id": "imported-1769500766501-542-an1zy",
    "title": "Phrase-Indexed Question Answering (PIQA)",
    "source": "Semantic Scholar",
    "authors": [
      "Minjoon Seo",
      "Tom Kwiatkowski",
      "Ankur P. Parikh",
      "Ali Farhadi",
      "Hannaneh Hajishirzi"
    ],
    "year": "2018",
    "paperLink": "https://aclanthology.org/D18-1455/",
    "githubLink": "https://github.com/google-research/google-research/tree/master/phrase_indexed_qa",
    "itemCount": "Based on SQuAD/TriviaQA scales (exact count varies by base dataset used)",
    "specs": "Extractive QA; Independent encoding formulation",
    "description": "A modular variant of extractive question answering that enforces complete independence between document and question encoders. This formulation addresses scalability in machine comprehension by allowing answer candidate phrases to be pre-computed and indexed offline."
  },
  {
    "id": "imported-1769500766501-543-o5dq1",
    "title": "emrQA",
    "source": "Scholar",
    "authors": [
      "Anusri Pampari",
      "Preethi Raghavan",
      "Jennifer Liang",
      "Jian Peng"
    ],
    "year": "2018",
    "paperLink": "https://aclanthology.org/D18-1258/",
    "githubLink": "https://github.com/panusri/emrQA",
    "itemCount": "~1,000,000 question-logical form pairs; 400,000+ QA pairs",
    "specs": "Unstructured Text (Clinical Notes); Slot-filled templates",
    "description": "A large-scale corpus for question answering on electronic medical records, generated by repurposing existing expert annotations from i2b2 clinical NLP challenges. It focuses on reading comprehension over clinical notes."
  },
  {
    "id": "imported-1769500766501-544-2cc39",
    "title": "fastMRI",
    "source": "arXiv",
    "authors": [
      "Jure Zbontar",
      "Florian Knoll",
      "Anuroop Sriram",
      "Tullie Murrell",
      "Zhengnan Huang",
      "Matthew J. Muckley",
      "Aaron Defazio"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1811.08839",
    "githubLink": "https://github.com/facebookresearch/fastMRI",
    "itemCount": "8,344 raw volumes, >1.57 million slices",
    "specs": "Raw k-space, DICOM images (Knee, Brain)",
    "description": "A large-scale collection of both raw MRI measurements (k-space) and clinical MRI images (DICOM) for investigating AI-based MRI reconstruction."
  },
  {
    "id": "imported-1769500766501-545-710jy",
    "title": "Calgary-Campinas (CC-359)",
    "source": "Scholar",
    "authors": [
      "Roberto Souza",
      "Oeslle Lucena",
      "Julia Garrafa",
      "David Gobbi"
    ],
    "year": "2018",
    "paperLink": "https://pubmed.ncbi.nlm.nih.gov/29408420/",
    "githubLink": "https://sites.google.com/view/calgary-campinas-dataset/home",
    "itemCount": "359 volumes",
    "specs": "3D T1-weighted MRI, 1.5T and 3T",
    "description": "A multi-vendor, multi-field strength brain MR dataset primarily used for benchmarking skull stripping and brain MRI analysis."
  },
  {
    "id": "imported-1769500766501-546-20ek9",
    "title": "ARC (AI2 Reasoning Challenge)",
    "source": "Semantic Scholar",
    "authors": [
      "Peter Clark",
      "Isaac Cowhey",
      "Oren Etzioni",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Carissa Schoenick",
      "Oyvind Tafjord"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1803.05457",
    "githubLink": "https://github.com/allenai/arc",
    "itemCount": "7,787 questions",
    "specs": "Text, Multiple Choice, Science Domain",
    "description": "A dataset of grade-school science questions consisting of a 'Challenge Set' (hard questions for retrieval methods) and an 'Easy Set'."
  },
  {
    "id": "imported-1769500766501-547-gi6fs",
    "title": "CommonsenseQA",
    "source": "arXiv",
    "authors": [
      "Alon Talmor",
      "Jonathan Herzig",
      "Nicholas Lourie",
      "Jonathan Berant"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1811.00937",
    "githubLink": "https://github.com/jonathanherzig/commonsenseqa",
    "itemCount": "12,247 questions",
    "specs": "Text, 5-option Multiple Choice, Commonsense",
    "description": "A dataset for commonsense question answering constructed from ConceptNet, requiring background knowledge to solve questions with complex semantics."
  },
  {
    "id": "imported-1769500766501-548-8qgep",
    "title": "SWAG (Situations With Adversarial Generations)",
    "source": "Scholar",
    "authors": [
      "Rowan Zellers",
      "Yonatan Bisk",
      "Roy Schwartz",
      "Yejin Choi"
    ],
    "year": "2018",
    "paperLink": "https://aclanthology.org/D18-1009/",
    "githubLink": "https://github.com/rowanzellers/swagaf",
    "itemCount": "113,000 questions",
    "specs": "Text (Video captions), Multiple Choice",
    "description": "A large-scale dataset for grounded commonsense inference, where models predict what happens next in a video scene description from multiple choices."
  },
  {
    "id": "imported-1769500766501-549-nx5fc",
    "title": "VQA-RAD",
    "source": "Hugging Face",
    "authors": [
      "Jason J. Lau",
      "Soumya Gayen",
      "Asma Ben Abacha",
      "Dina Demner-Fushman"
    ],
    "year": "2018",
    "paperLink": "https://www.nature.com/articles/sdata2018251",
    "githubLink": "https://github.com/openmedlab/Awesome-Medical-Dataset",
    "itemCount": "315 images, 3,515 QA pairs",
    "specs": "Images (CT, MRI, X-ray), Text (Open-ended, Yes/No questions)",
    "description": "A manually curated dataset of clinically generated visual questions and answers about radiology images. It is designed to evaluate VQA models on real-world clinical questions, distinguishing it from larger, automatically generated datasets."
  },
  {
    "id": "imported-1769500766501-550-l7apx",
    "title": "Region-based annotated Child Pornography Dataset (RCPD)",
    "source": "Other",
    "authors": [
      "Camila Laranjeira da Silva",
      "Jefersson A. dos Santos",
      "et al."
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/2204.14110",
    "githubLink": "http://bioinfo.dcc.ufmg.br/propedia2/download",
    "itemCount": "2,138 samples (836 CSAM, 285 Adult Porn, 1,017 Normal)",
    "specs": "Image Metadata, Bounding Boxes, Feature Vectors (No raw CSAM images)",
    "description": "A research dataset containing metadata and region-based annotations (bounding boxes) for images related to CSAM detection. The dataset includes feature vectors and annotations for body parts and scene context to aid in the development of privacy-preserving detection algorithms without distributing the illegal image content itself."
  },
  {
    "id": "imported-1769500766501-551-1os6v",
    "title": "EMBER (Endgame Malware BEnchmark for Research)",
    "source": "arXiv",
    "authors": [
      "Hyrum S. Anderson",
      "Phil Roth",
      "Robert J. Joyce",
      "et al."
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1804.04637",
    "githubLink": "https://github.com/elastic/ember",
    "itemCount": "1.1 million samples (2018 version); 3.2 million samples (2024 version)",
    "specs": "JSON format features (byte histograms, string info, imports/exports), SHA256 hashes, Windows PE format",
    "description": "A large-scale benchmark dataset for static analysis of Windows Portable Executable (PE) files. It is designed to train and evaluate machine learning models for malware detection, containing labeled benign and malicious samples with raw features extracted using LIEF."
  },
  {
    "id": "imported-1769500766501-552-0yxjk",
    "title": "CAIL2018 (Chinese AI and Law 2018)",
    "source": "arXiv",
    "authors": [
      "Chaojun Xiao",
      "Haoxi Zhong",
      "Zhipeng Guo",
      "Cunchao Tu",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Yansong Feng",
      "Xianpei Han",
      "Zhen Hu",
      "Heng Wang",
      "Jianfeng Xu"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1807.02478",
    "githubLink": "https://github.com/thunlp/CAIL",
    "itemCount": "2.6 million cases",
    "specs": "Text (Chinese); Multi-label classification, Regression",
    "description": "The first large-scale Chinese legal dataset for judgment prediction, consisting of over 2.6 million criminal cases. It challenges models to predict applicable law articles, charges, and prison terms based on fact descriptions."
  },
  {
    "id": "imported-1769500766501-553-b9dzu",
    "title": "e-SNLI",
    "source": "arXiv",
    "authors": [
      "Oana-Maria Camburu",
      "Tim Rocktäschel",
      "Thomas Lukasiewicz",
      "Phil Blunsom"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1812.01193",
    "githubLink": "https://github.com/OanaMariaCamburu/e-SNLI",
    "itemCount": "570,000 sentence pairs",
    "specs": "Text; Natural Language Explanations",
    "description": "An extension of the Stanford Natural Language Inference (SNLI) dataset that includes human-annotated natural language explanations for the entailment relations. It serves as a benchmark for models that generate textual explanations for their decisions."
  },
  {
    "id": "imported-1769500766501-554-l5ina",
    "title": "SciTail",
    "source": "Hugging Face",
    "authors": [
      "Tushar Khot",
      "Ashish Sabharwal",
      "Peter Clark"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1802.02324",
    "githubLink": "https://github.com/allenai/scitail",
    "itemCount": "27,026 examples",
    "specs": "Text; Natural Language Inference (Entailment/Neutral)",
    "description": "A textual entailment dataset created from multiple-choice science exams and web sentences. It is designed to evaluate a model's ability to determine if a hypothesis follows from a premise in a scientific context."
  },
  {
    "id": "imported-1769500766501-555-5dx8f",
    "title": "SciERC",
    "source": "Semantic Scholar",
    "authors": [
      "Yi Luan",
      "Luheng He",
      "Mari Ostendorf",
      "Hannaneh Hajishirzi"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1808.09602",
    "githubLink": "https://github.com/dwadden/dygiepp",
    "itemCount": "500 annotated abstracts",
    "specs": "Text; Named Entity Recognition (NER), Relation Extraction",
    "description": "A benchmark for scientific entity and relation extraction. It consists of annotated abstracts from AI conference proceedings, labeling scientific entities (e.g., Task, Method, Metric) and their relations."
  },
  {
    "id": "imported-1769500766501-556-h6t7o",
    "title": "XNLI",
    "source": "arXiv",
    "authors": [
      "Alexis Conneau",
      "Ruty Rinott",
      "Guillaume Lample",
      "Adina Williams",
      "Samuel R. Bowman",
      "Holger Schwenk",
      "Veselin Stoyanov"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1809.05053",
    "githubLink": "https://github.com/facebookresearch/XNLI",
    "itemCount": "112.5k annotated pairs, 15 languages",
    "specs": "Text; Natural Language Inference (Classification)",
    "description": "A benchmark for cross-lingual sentence understanding, consisting of Natural Language Inference data in 15 languages."
  },
  {
    "id": "imported-1769500766501-557-k3dcj",
    "title": "CoNaLa",
    "source": "arXiv",
    "authors": [
      "Pengcheng Yin",
      "Bowen Deng",
      "Edgar Chen",
      "et al."
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1805.08949",
    "githubLink": "https://github.com/conala-corpus/conala-corpus",
    "itemCount": "2,379 curated / ~600k mined pairs",
    "specs": "Natural Language to Python Code",
    "description": "The Code/Natural Language Challenge dataset. It contains pairs of natural language intents and code snippets, mined from Stack Overflow and manually curated."
  },
  {
    "id": "imported-1769500766501-558-e4on2",
    "title": "ChestX-ray14 (NIH Chest X-ray)",
    "source": "Other",
    "authors": [
      "Xiaosong Wang",
      "Yifan Peng",
      "Le Lu",
      "Zhiyong Lu",
      "Mohammadhadi Bagheri",
      "Ronald M. Summers"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1705.02315",
    "githubLink": "https://github.com/zoogzog/chexnet",
    "itemCount": "112,120 images",
    "specs": "Frontal-view X-ray images, 14 disease labels text-mined from reports",
    "description": "A hospital-scale chest X-ray database for weakly-supervised classification and localization of common thorax diseases."
  },
  {
    "id": "imported-1769500766501-559-ufjsm",
    "title": "RACE",
    "source": "arXiv",
    "authors": [
      "Guokun Lai",
      "Qizhe Xie",
      "Hanxiao Liu",
      "Yiming Yang",
      "Eduard Hovy"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1704.04683",
    "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
    "itemCount": "~100,000 questions",
    "specs": "Text, Multiple Choice, Reading Comprehension",
    "description": "A reading comprehension dataset collected from English examinations in China for middle and high school students, focusing on reasoning capabilities."
  },
  {
    "id": "imported-1769500766501-560-f3hz7",
    "title": "SciQ",
    "source": "Semantic Scholar",
    "authors": [
      "Johannes Welbl",
      "Nelson F. Liu",
      "Matt Gardner"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1707.06209",
    "githubLink": "https://allenai.org/data/sciq",
    "itemCount": "13,679 questions",
    "specs": "Text, Multiple Choice, Science Domain",
    "description": "A dataset of crowdsourced science exam questions covering Biology, Chemistry, Earth Science, and Physics."
  },
  {
    "id": "imported-1769500766501-561-6odma",
    "title": "MoleculeNet",
    "source": "arXiv",
    "authors": [
      "Zhenqin Wu",
      "Bharath Ramsundar",
      "Evan N. Feinberg",
      "Joseph Gomes",
      "Caleb Geniesse",
      "Aneesh S. Pappu",
      "Karl Leswing",
      "Vijay Pande"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1703.00564",
    "githubLink": "https://github.com/deepchem/deepchem",
    "itemCount": "Over 700,000 compounds",
    "specs": "Molecular structures (SMILES/graphs), regression and classification tasks",
    "description": "A benchmark for molecular machine learning, curating multiple public datasets to test methods on molecular properties including quantum mechanics, physical chemistry, biophysics, and physiology."
  },
  {
    "id": "imported-1769500766501-562-pgxmp",
    "title": "Visual Memory QA (MemexQA)",
    "source": "arXiv",
    "authors": [
      "Lu Jiang",
      "Liangliang Cao",
      "Yannis Kalantidis",
      "Sachin Farfade",
      "Alexander G. Hauptmann"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1708.01336",
    "githubLink": "https://github.com/JunweiLiang/FVTA_MemexQA",
    "itemCount": "Contains personal photos/videos and crowd-sourced QA pairs (Exact count varies by version, e.g., MemexQA v1.1)",
    "specs": "Multimodal (Images/Videos + Text Questions)",
    "description": "A dataset and task designed to answer questions about user's daily life based on their personal photo and video collections without relying on metadata. It focuses on recovering memories (e.g., 'When was the last time we went hiking?') using visual content."
  },
  {
    "id": "imported-1769500766501-563-7zgvb",
    "title": "PACS (Photo, Art Painting, Cartoon, Sketch)",
    "source": "Scholar",
    "authors": [
      "Da Li",
      "Yongxin Yang",
      "Yi-Zhe Song",
      "Timothy M. Hospedales"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1710.03077",
    "githubLink": "http://sketchx.eecs.qmul.ac.uk/downloads/",
    "itemCount": "9,991 images",
    "specs": "4 domains (Photo, Art Painting, Cartoon, Sketch), 7 object categories (Dog, Elephant, Giraffe, Guitar, Horse, House, Person)",
    "description": "A popular domain generalization benchmark consisting of images from four domains: Photo, Art Painting, Cartoon, and Sketch. It is designed to evaluate a model's ability to generalize to an unseen domain (e.g., training on three and testing on the fourth)."
  },
  {
    "id": "imported-1769500766501-564-yjgvp",
    "title": "OpenI (Indiana University Chest X-ray)",
    "source": "Other",
    "authors": [
      "Dina Demner-Fushman",
      "Marc D. Kohli",
      "Marc B. Rosenman",
      "S. E. Shooshan",
      "L. Rodriguez",
      "S. Antani",
      "G. R. Thoma",
      "C. J. McDonald"
    ],
    "year": "2016",
    "paperLink": "https://lhncbc.nlm.nih.gov/system/files/pub2012019.pdf",
    "githubLink": "https://huggingface.co/datasets/fuyu-8b/open-i",
    "itemCount": "7,470 images; 3,955 reports",
    "specs": "Frontal and lateral X-ray images, associated radiology reports (XML)",
    "description": "A public chest X-ray dataset collected from the Indiana University hospital network, widely used for report generation benchmarks."
  },
  {
    "id": "imported-1769500766501-565-wwux6",
    "title": "Pornography-2k",
    "source": "Other",
    "authors": [
      "Carlos H. A. Mello",
      "Sandra Avila",
      "Anderson Rocha"
    ],
    "year": "2016",
    "paperLink": "https://ieeexplore.ieee.org/document/7738016",
    "githubLink": "https://github.com/fffaded/SIEGuardian-Dataset",
    "itemCount": "2,000 videos (140 hours)",
    "specs": "Video (AVI/MP4), Frames",
    "description": "A widely used benchmark dataset for pornography detection, often utilized as a proxy or 'adult' class in CSAM research to test the separation between adult content and other categories. It contains diverse video footage of pornographic and non-pornographic content."
  },
  {
    "id": "imported-1769500766501-566-re4mt",
    "title": "BreaKHis (Breast Cancer Histopathological Image Classification)",
    "source": "Scholar",
    "authors": [
      "Fabio A. Spanhol",
      "Luiz S. Oliveira",
      "Caroline Petitjean",
      "Laurent Heutte"
    ],
    "year": "2016",
    "paperLink": "http://web.inf.ufpr.br/vri/breast-cancer-database",
    "githubLink": "https://github.com/mrdvince/breast_cancer_detection",
    "itemCount": "7,909 images",
    "specs": "RGB PNG images, 4 magnification levels (40X, 100X, 200X, 400X)",
    "description": "A benchmark for the classification of breast cancer histopathology images. The dataset contains microscopic images of breast tumor tissue collected from patients, labeled as either benign or malignant."
  },
  {
    "id": "imported-1769500766501-567-vj7iq",
    "title": "Real-life Trial (RLT) Deception Detection Dataset",
    "source": "Other",
    "authors": [
      "Verónica Pérez-Rosas",
      "Mohamed Abouelenien",
      "Rada Mihalcea",
      "Mihai Burzo"
    ],
    "year": "2015",
    "paperLink": "https://web.eecs.umich.edu/~zmohamed/PDFs/Trial.ICMI.pdf",
    "githubLink": "https://github.com/lit-umich/Real-life-Deception-Detection-2016",
    "itemCount": "121 video clips",
    "specs": "Video (visual), Audio, Text (transcriptions); High-stakes courtroom setting",
    "description": "A multimodal dataset consisting of 121 video clips (61 deceptive, 60 truthful) collected from public courtroom trials. It captures high-stakes deception scenarios involving defendants and witnesses."
  },
  {
    "id": "imported-1769500766501-568-y2x0d",
    "title": "OWASP Benchmark Project",
    "source": "Other",
    "authors": [
      "OWASP Foundation",
      "Dave Wichers"
    ],
    "year": "2015",
    "paperLink": "https://owasp.org/www-project-benchmark/",
    "githubLink": "https://github.com/OWASP-Benchmark/BenchmarkUtils",
    "itemCount": "2,740 test cases (Java v1.2), 1,230 test cases (Python v0.1)",
    "specs": "Java (v1.2), Python (v0.1); Web application format; Vulnerabilities include SQLi, XSS, Command Injection, etc.",
    "description": "A fully runnable, open-source web application designed to evaluate the accuracy, coverage, and speed of automated software vulnerability detection tools (SAST, DAST, and IAST). It contains thousands of test cases mapped to specific CWEs to identify true positives and false positives."
  },
  {
    "id": "imported-1769500766501-569-tukmh",
    "title": "BraTS (Multimodal Brain Tumor Image Segmentation Benchmark)",
    "source": "Scholar",
    "authors": [
      "Bjoern H. Menze",
      "Andras Jakab",
      "Stefan Bauer",
      "Jayashree Kalpathy-Cramer",
      "Keyvan Farahani"
    ],
    "year": "2015",
    "paperLink": "https://ieeexplore.ieee.org/document/6975210",
    "githubLink": "https://github.com/RAVIRAJAG/BraTS-2023-Challenge",
    "itemCount": "Varies (e.g., ~2000 patients in 2021)",
    "specs": "Multi-modal MRI (T1, T1ce, T2, FLAIR), NIfTI format",
    "description": "A long-running benchmark challenge for the segmentation of brain tumors (gliomas) in multimodal magnetic resonance imaging scans."
  },
  {
    "id": "imported-1769500766501-570-b5dvs",
    "title": "EyePACS (Kaggle Diabetic Retinopathy Detection)",
    "source": "Other",
    "authors": [
      "Jorge Cuadros",
      "George Breslauer",
      "Kaggle",
      "EyePACS"
    ],
    "year": "2015",
    "paperLink": "https://www.kaggle.com/c/diabetic-retinopathy-detection",
    "githubLink": "https://github.com/bumbledeep/eyepacs",
    "itemCount": "88,702 images (35,126 train, 53,576 test)",
    "specs": "JPEG images, 5-class grading (0-4), high-resolution fundus photography",
    "description": "A large-scale dataset of high-resolution retinal fundus images used for the automated detection of diabetic retinopathy. It was originally released as part of a Kaggle competition to identify the presence of diabetic retinopathy in images."
  },
  {
    "id": "imported-1769500766501-571-yomg8",
    "title": "DeepSEA Dataset",
    "source": "Scholar",
    "authors": [
      "Jian Zhou",
      "Olga G. Troyanskaya"
    ],
    "year": "2015",
    "paperLink": "https://doi.org/10.1038/nmeth.3547",
    "githubLink": "https://github.com/FunctionLab/DeepSEA",
    "itemCount": "~521 million bp (training), 919 chromatin features",
    "specs": "1,000 bp DNA sequences mapped to 919 binary chromatin feature labels (multi-label classification).",
    "description": "A foundational dataset originally created to train the DeepSEA model for predicting chromatin effects of sequence alterations. It serves as a standard benchmark for predicting chromatin profiles (transcription factor binding, DNase I sensitivity, histone marks) from DNA sequences."
  },
  {
    "id": "imported-1769500766501-572-108nx",
    "title": "IU X-Ray (OpenI)",
    "source": "Other",
    "authors": [
      "Dina Demner-Fushman",
      "Marc D. Kohli",
      "Marc B. Rosenman",
      "Sameer E. Shooshan",
      "Laritza Rodriguez",
      "Sameer Antani",
      "George R. Thoma",
      "Clement J. McDonald"
    ],
    "year": "2015",
    "paperLink": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4849245/",
    "githubLink": "https://openi.nlm.nih.gov/",
    "itemCount": "7,470 images, 3,955 reports",
    "specs": "Chest X-ray images, Radiology reports (XML)",
    "description": "A collection of chest X-ray images with associated radiology reports from the Indiana University hospital network. It is widely used as a smaller benchmark for report generation."
  },
  {
    "id": "imported-1769500766501-573-eto90",
    "title": "BigCloneBench",
    "source": "Other",
    "authors": [
      "Svajlenko",
      "J.",
      "Roy",
      "C. K."
    ],
    "year": "2014 (Updated 2018/2020)",
    "paperLink": "https://ieeexplore.ieee.org/document/6936865",
    "githubLink": "https://github.com/jeffsvajlenko/BigCloneBench",
    "itemCount": "8 million+ clone pairs, 25,000+ projects",
    "specs": "Java Source Code, Clone Types 1-4",
    "description": "A large-scale benchmark for code clone detection, containing millions of manually validated clone pairs. It is widely used to evaluate tools for detecting Type-1 (exact) to Type-4 (semantic) code clones in Java."
  },
  {
    "id": "imported-1769500766501-574-tueff",
    "title": "PAN12 Sexual Predator Identification",
    "source": "Scholar",
    "authors": [
      "Giacomo Inches",
      "Fabio Crestani"
    ],
    "year": "2012",
    "paperLink": "https://aclanthology.org/2012.clef_conference-2012.39/",
    "githubLink": "https://webis.de/events/pan-12/pan12-papers-sexual-predator-identification.html",
    "itemCount": "222,000 chat segments (approx.)",
    "specs": "Text (Chat logs, grooming detection)",
    "description": "A classic benchmark dataset for the identification of sexual predators in chat logs. It contains anonymized chat conversations labeled for grooming behavior, derived from the Perverted Justice Foundation and other sources."
  },
  {
    "id": "imported-1769500766501-575-vy6t4",
    "title": "PAN-2012 Sexual Predator Identification (PAN12)",
    "source": "Other",
    "authors": [
      "Giacomo Inches",
      "Fabio Crestani"
    ],
    "year": "2012",
    "paperLink": "https://ceur-ws.org/Vol-1178/CLEF2012wn-PAN-InchesEt2012.pdf",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-Datasets",
    "itemCount": "~66,900 chat logs (Training corpus)",
    "specs": "Text (XML format), Chat logs",
    "description": "A benchmark dataset consisting of chat logs designed to identify sexual predators in online conversations. It contains anonymized chat logs involving two or more people, with the goal of determining who is the predator. This is one of the most widely used datasets for text-based grooming detection research."
  },
  {
    "id": "imported-1769500766501-576-rela4",
    "title": "PAN 2012 Sexual Predator Identification (PAN12)",
    "source": "Scholar",
    "authors": [
      "Giacomo Inches",
      "Fabio Crestani"
    ],
    "year": "2012",
    "paperLink": "https://www.webis.de/publications.html#inches_2012",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "~669,000 messages (Training Corpus), ~222,000 chat segments",
    "specs": "Text (Chat Logs), XML format",
    "description": "The most widely used benchmark for grooming detection, created for the PAN 2012 competition. It consists of chat logs from Perverted Justice (predators) and non-predatory chats from Omegle and IRC. It supports tasks like identifying predatory users and specific grooming lines."
  },
  {
    "id": "imported-1769500766501-577-ndqx4",
    "title": "Deceptive Opinion Spam Corpus (Ott Dataset)",
    "source": "Other",
    "authors": [
      "Myle Ott",
      "Yejin Choi",
      "Claire Cardie",
      "Jeffrey T. Hancock"
    ],
    "year": "2011",
    "paperLink": "https://aclanthology.org/P11-1032.pdf",
    "githubLink": "https://github.com/myleott/op_spam",
    "itemCount": "1,600 reviews",
    "specs": "Text (English); Positive and Negative sentiment; 400 truthful/400 deceptive per sentiment",
    "description": "A 'gold standard' text-based dataset for deception detection containing truthful and deceptive hotel reviews. Deceptive reviews were generated using Amazon Mechanical Turk, while truthful reviews were mined from TripAdvisor."
  },
  {
    "id": "imported-1769500766501-578-y11cs",
    "title": "Virus Texture Dataset v.1.0",
    "source": "Scholar",
    "authors": [
      "Gustaf Kylberg",
      "Mats Uppström",
      "Ida-Maria Sintorn"
    ],
    "year": "2011",
    "paperLink": "https://kylberg.org/virus-texture-dataset/",
    "githubLink": "https://kylberg.org/virus-texture-dataset/",
    "itemCount": "1,500 images",
    "specs": "Greyscale TEM images (41x41 pixels patches), 15 virus classes, 100 samples per class",
    "description": "A classic computer vision benchmark for virus classification using Transmission Electron Microscopy (TEM) images. It is widely used to evaluate texture descriptors and image classification models in virology."
  },
  {
    "id": "imported-1769500766501-579-oak8r",
    "title": "ChatCoder2 (CC2)",
    "source": "Scholar",
    "authors": [
      "I. McGhee",
      "J. Bayzick",
      "A. Kontostathis",
      "L. Edwards",
      "A. McBride",
      "E. Jakubowski"
    ],
    "year": "2011",
    "paperLink": "https://www.tandfonline.com/doi/abs/10.2753/JEC1086-4415150305",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "497 complete predator chats",
    "specs": "Text (Complete Chat Logs)",
    "description": "A dataset containing complete predator chats collected from Perverted Justice. It is often used for studying the semantic segmentation of grooming chats into phases like 'Approach', 'Grooming', and 'Personal Information Exchange'."
  },
  {
    "id": "imported-1769500766501-580-jq4zl",
    "title": "Diamonds",
    "source": "Hugging Face",
    "authors": [
      "Hadley Wickham"
    ],
    "year": "2007",
    "paperLink": "https://ggplot2.tidyverse.org/reference/diamonds.html",
    "githubLink": "https://github.com/tidyverse/ggplot2",
    "itemCount": "53,940 rows",
    "specs": "Tabular (10 columns: price, carat, cut, color, clarity, x, y, z, depth, table)",
    "description": "A classic dataset containing the prices and other attributes of nearly 54,000 diamonds. Widely used for data visualization and regression analysis tutorials, specifically for predicting diamond prices based on features like carat, cut, color, and clarity."
  },
  {
    "id": "imported-1769500766501-581-6sqsd",
    "title": "Software Assurance Reference Dataset (SARD)",
    "source": "Scholar",
    "authors": [
      "NIST SAMATE Group"
    ],
    "year": "2005",
    "paperLink": "https://samate.nist.gov/SARD/",
    "itemCount": "500,000+ test cases",
    "specs": "C, C++, Java, PHP, C#; Synthetic and Production code",
    "description": "A massive collection of test cases (programs with known bugs/vulnerabilities) maintained by NIST to allow users, researchers, and software security tool developers to evaluate their tools."
  },
  {
    "id": "imported-1769500766501-582-qwk86",
    "title": "IXI Dataset",
    "source": "Scholar",
    "authors": [
      "Biomedical Image Analysis Group (Imperial College London)"
    ],
    "year": "2005",
    "paperLink": "https://brain-development.org/ixi-dataset/",
    "githubLink": "https://brain-development.org/ixi-dataset/",
    "itemCount": "~600 subjects",
    "specs": "T1, T2, PD, MRA, DTI (NIfTI)",
    "description": "A collection of MR images from normal, healthy subjects collected at three different hospitals, widely used for registration and segmentation tasks."
  }
]