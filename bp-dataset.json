[
  {
    "id": "csv-1-1769500744825",
    "title": "MedMNIST v2",
    "source": "arXiv",
    "authors": [
      "Jiancheng Yang",
      "Rui Shi",
      "Donglai Wei",
      "Zequan Liu",
      "Lin Zhao",
      "Bilian Ke",
      "Hanspeter Pfister",
      "Bingbing Ni"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.14795",
    "githubLink": "https://github.com/MedMNIST/MedMNIST",
    "itemCount": "708,069 2D images, 10,214 3D images (18 datasets total)",
    "specs": "2D (28x28, 64x64, 128x128, 224x224) and 3D (28x28x28, 64x64x64) images; Classification labels",
    "description": "A large-scale MNIST-like collection of standardized biomedical images, designed to be a lightweight benchmark for 2D and 3D biomedical image classification. It covers primary data modalities (e.g., X-ray, OCT, Ultrasound, CT, EM) and diverse tasks (binary/multi-class, ordinal regression, multi-label)."
  },
  {
    "id": "csv-2-1769500744825",
    "title": "MedQA (USMLE)",
    "source": "arXiv",
    "authors": [
      "Di Jin",
      "Eileen Pan",
      "Nassim Oufattole",
      "Wei-Hung Weng",
      "Hanyi Fang",
      "Peter Szolovits"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.13081",
    "githubLink": "https://github.com/jind11/MedQA",
    "itemCount": "12,723 questions (USMLE subset)",
    "specs": "Text (Question Answering), Multiple Choice (4 or 5 options)",
    "description": "A large-scale open-domain question answering dataset collected from professional medical board exams. The primary subset consists of questions from the United States Medical Licensing Examination (USMLE)."
  },
  {
    "id": "csv-3-1769500744825",
    "title": "PubMedQA",
    "source": "arXiv",
    "authors": [
      "Qiao Jin",
      "Bhuwan Dhingra",
      "Zhengping Liu",
      "William W. Cohen",
      "Xinghua Lu"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1909.06146",
    "githubLink": "https://github.com/pubmedqa/pubmedqa",
    "itemCount": "1k expert-labeled, 61.2k unlabeled, 211.3k artificially generated",
    "specs": "Text (QA), Yes/No/Maybe classification",
    "description": "A biomedical question answering dataset collected from PubMed abstracts. The task is to answer research questions with yes/no/maybe using the corresponding abstract context."
  },
  {
    "id": "csv-4-1769500744825",
    "title": "MedMCQA",
    "source": "arXiv",
    "authors": [
      "Ankit Pal",
      "Logesh Kumar Umapathi",
      "Malaikannan Sankarasubbu"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.14371",
    "githubLink": "https://github.com/medmcqa/medmcqa",
    "itemCount": "~194,000 MCQs",
    "specs": "Text (MCQA), 4 options, Subject/Topic metadata",
    "description": "A large-scale multiple-choice question answering dataset designed to address real-world medical entrance exam questions (AIIMS & NEET PG) covering a wide range of medical subjects."
  },
  {
    "id": "csv-5-1769500744825",
    "title": "MIMIC-IV",
    "source": "Scholar",
    "authors": [
      "Alistair E. W. Johnson",
      "Lucas Bulgarelli",
      "Lu Shen",
      "Alvin Gayles",
      "Ayad Shammout",
      "Steven Horng",
      "Leo Anthony Celi",
      "Roger G. Mark"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1038/s41597-022-01899-x",
    "githubLink": "https://github.com/MIT-LCP/mimic-code",
    "itemCount": ">65,000 ICU patients, >200,000 ED patients",
    "specs": "Relational Database (CSV), Text (Clinical Notes), Time-series",
    "description": "A freely accessible electronic health record dataset containing de-identified clinical data of patients admitted to the ICU and Emergency Department at Beth Israel Deaconess Medical Center. It includes vital signs, laboratory measurements, medications, and free-text clinical notes."
  },
  {
    "id": "saved-1769608044991-h2k7s",
    "title": "Terminal-Bench 2.0",
    "source": "arXiv",
    "authors": [
      "Mike A. Merrill",
      "Orfeas Menis Mastromichalakis",
      "Zhiwei Xu",
      "Zizhao Chen",
      "Yue Liu",
      "Jianbo Wu",
      "Minghao Yan",
      "Song Bian",
      "Vedang Sharma",
      "Ke Sun"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.11868",
    "githubLink": "https://github.com/laude-institute/terminal-bench",
    "itemCount": "89 tasks",
    "specs": "Terminal-based tasks, Docker environments, Python/Bash execution harness",
    "description": "A benchmark for benchmarking agents on hard, realistic tasks in command line interfaces (CLI). It evaluates how well agents can handle real-world, end-to-end tasks autonomously, ranging from compiling code to training models and setting up servers. The benchmark consists of a dataset of tasks and an execution harness connecting the agent to a terminal sandbox."
  },
  {
    "id": "saved-1769608044991-j2dbm",
    "title": "BioProBench",
    "source": "Hugging Face",
    "authors": [
      "Yuyang Sunshine",
      "et al."
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.05000",
    "githubLink": "https://github.com/YuyangSunshine/bioprotocolbench",
    "itemCount": "550,000 task instances",
    "specs": "Text (Protocols, Reasoning)",
    "description": "A comprehensive benchmark for biological protocol understanding and reasoning, grounded in a large corpus of human-written protocols (BioProCorpus). It evaluates LLMs on tasks demanding deep reasoning and safety awareness in wet-lab contexts."
  },
  {
    "id": "saved-1769608044991-azdhg",
    "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
    "source": "arXiv",
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo",
      "Truong-Son Hy"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.03699",
    "githubLink": "https://github.com/knoveleng/redeval",
    "itemCount": "29,362 samples",
    "specs": "Text (Attack and Refusal prompts), 22 risk categories, 19 domains",
    "description": "A universal dataset aggregating 37 benchmark datasets for red teaming Large Language Models (LLMs). It employs a standardized taxonomy with 22 risk categories and 19 domains to enable consistent and comprehensive evaluations of LLM vulnerabilities against adversarial prompts."
  },
  {
    "id": "saved-1769608044991-soneg",
    "title": "DermaBench",
    "source": "arXiv",
    "authors": [
      "Yuhao Shen",
      "Jiahe Qian",
      "Shuping Zhang",
      "Tao Lu",
      "Juexiao Zhou"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.14084",
    "githubLink": "https://github.com/DermaVLM",
    "itemCount": "656 images, ~14,474 annotations",
    "specs": "Image + Text (VQA, Hierarchical Questions), Fitzpatrick Skin Types I-VI",
    "description": "A clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. It evaluates models on diagnostic diversity, demographic fairness, and precise labeling using a hierarchical annotation schema."
  },
  {
    "id": "saved-1769608044991-45ne9",
    "title": "AutoMonitor-Bench",
    "source": "arXiv",
    "authors": [
      "Chuanrui Hu",
      "Xingze Gao",
      "Zuyi Zhou",
      "Dannong Xu",
      "Yi Bai",
      "Xintong Li",
      "Hui Zhang",
      "Tong Li",
      "Chong Zhang",
      "Lidong Bing"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.05353",
    "githubLink": "https://github.com/DAMO-NLP-SG/AutoMonitor-Bench",
    "itemCount": "3,010 annotated samples",
    "specs": "QA, code generation, and reasoning tasks with paired misbehavior instances.",
    "description": "A benchmark designed to evaluate the reliability of LLM-based misbehavior monitors. It includes specific categories for 'Specification Gaming', where models exploit loopholes in evaluation criteria. The dataset provides ground-truth annotations to measure detection rates of such behaviors."
  },
  {
    "id": "saved-1769608044991-dq1zx",
    "title": "BizFinBench.v2",
    "source": "arXiv",
    "authors": [
      "Xin Guo",
      "Rongjunchen Zhang",
      "Guilong Lu",
      "Xuntao Guo",
      "Shuai Jia",
      "Zhi Yang",
      "Liwen Zhang"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.06401",
    "githubLink": "https://github.com/HiThink-Research/BizFinBench.v2",
    "itemCount": "29,578 Q&A pairs",
    "specs": "Text (English, Chinese), Financial Reports, Real-time Market Data",
    "description": "A large-scale bilingual (English & Chinese) benchmark designed to evaluate expert-level financial capabilities of Large Language Models. It focuses on authentic business scenarios including financial report analysis, anomalous information tracing, and wealth management, incorporating both offline core tasks and online real-time tasks."
  },
  {
    "id": "saved-1769608044991-cn91a",
    "title": "DrivingGen",
    "source": "arXiv",
    "authors": [
      "Unnamed Authors (arXiv:2601.01528)"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.01528",
    "githubLink": "https://github.com/DrivingGen/DrivingGen",
    "itemCount": "Diverse driving scenarios (Weather, Time of day, Regions)",
    "specs": "Video generation, Autonomous Driving, Safety-critical scenarios",
    "description": "The first comprehensive benchmark for generative driving world models. It evaluates video generation models on visual realism, trajectory plausibility, temporal coherence, and controllability using a diverse dataset curated from driving logs and internet videos."
  },
  {
    "id": "saved-1769608044991-j2x21",
    "title": "Sci-Reasoning",
    "source": "arXiv",
    "authors": [
      "Sci-Reasoning Team (Authors not fully listed in snippet)"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.04577",
    "githubLink": "https://github.com/sci-reasoning/sci-reasoning",
    "itemCount": "3,819 papers",
    "specs": "Text, Structured Reasoning Chains",
    "description": "A dataset designed to capture the intellectual synthesis and reasoning patterns behind high-quality AI research papers. It traces the lineage of oral/spotlight papers from top conferences to their predecessors to decode innovation patterns."
  },
  {
    "id": "saved-1769608044991-nwk4z",
    "title": "HeurekaBench",
    "source": "arXiv",
    "authors": [
      "Siba Smarak Panigrahi",
      "Jovana Videnović",
      "Maria Brbić"
    ],
    "year": "2026",
    "paperLink": "https://arxiv.org/abs/2601.01678",
    "githubLink": "https://github.com/mlbio-epfl/HeurekaBench",
    "itemCount": "13 papers, 41 insights, 50 OEQs, 50 MCQs (sc-HeurekaBench instantiation)",
    "specs": "Experimental datasets (e.g., scRNA-seq), Open-ended Questions (OEQs), Multiple Choice Questions (MCQs)",
    "description": "A benchmarking framework designed to evaluate LLM-based agents functioning as 'co-scientists' in complex, multi-step scientific analysis tasks. It grounds questions in real scientific studies and their corresponding code repositories, requiring agents to explore experimental datasets and generate insights."
  },
  {
    "id": "saved-1769608044991-9b0kl",
    "title": "MentalChat16K",
    "source": "Hugging Face",
    "authors": [
      "Jia Xu",
      "Tianyi Wei",
      "Bojian Hou",
      "Patryk Orzechowski",
      "Shu Yang",
      "Ruochen Jin",
      "Rachael Paulbeck",
      "Joost Wagenaar",
      "George Demiris",
      "Li Shen"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.13509",
    "githubLink": "https://github.com/PennShenLab/MentalChat16K",
    "itemCount": "16,113 QA pairs (6,338 real, 9,775 synthetic)",
    "specs": "Text (English), Q&A pairs, Clinical Interview Transcripts, Synthetic Dialogues",
    "description": "A large-scale benchmark dataset designed for conversational mental health assistance, combining real anonymized interview transcripts from clinical trials with synthetic counseling dialogues generated by GPT-3.5 Turbo. It covers diverse mental health topics including depression, anxiety, and grief."
  },
  {
    "id": "saved-1769608044991-gj3k7",
    "title": "DeepResearch Bench",
    "source": "arXiv",
    "authors": [
      "Mingxuan Du",
      "Benfeng Xu",
      "Chiwei Zhu",
      "Xiaorui Wang",
      "Zhendong Mao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.11763",
    "githubLink": "https://github.com/Ayanami0730/deep_research_bench",
    "itemCount": "100 tasks",
    "specs": "Text-based research tasks; English and Chinese; 22 domains (e.g., Physics, Finance, Software); Evaluation metrics: Report quality (RACE) and Citation accuracy (FACT)",
    "description": "A comprehensive benchmark specifically designed to evaluate Deep Research Agents (DRAs). It consists of 100 PhD-level research tasks (50 in English and 50 in Chinese) meticulously crafted by domain experts across 22 distinct fields. The benchmark introduces two evaluation frameworks: RACE (for report quality) and FACT (for citation accuracy)."
  },
  {
    "id": "saved-1769608044991-7sirn",
    "title": "LiveDRBench",
    "source": "arXiv",
    "authors": [
      "Abhinav Java",
      "Ashmit Khandelwal",
      "Sukruta Midigeshi",
      "Aaron Halfaker",
      "Amit Deshpande",
      "Navin Goyal",
      "Ankur Gupta",
      "Nagarajan Natarajan",
      "Amit Sharma"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.04183",
    "githubLink": "https://github.com/microsoft/LiveDRBench",
    "itemCount": "100 tasks",
    "specs": "JSON format; Domains: Scientific (datasets, materials, prior art) and World Events; Metrics: Precision, Recall, F1 score on claim discovery",
    "description": "A diverse and challenging benchmark designed to characterize and evaluate 'deep research' tasks, defined by high concept fan-out and reasoning-intensive exploration rather than just long-form output. It separates reasoning challenges from surface-level report generation using an intermediate claim representation."
  },
  {
    "id": "saved-1769608044991-afust",
    "title": "ResearchRubrics",
    "source": "arXiv",
    "authors": [
      "J. Gottweis",
      "Scale AI Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.07685",
    "githubLink": "https://scale.com/research/researchrubrics",
    "itemCount": "101 prompts; 2,593 rubric items",
    "specs": "Text prompts and detailed rubrics; 3 complexity axes: conceptual breadth, logical nesting, exploration; Evaluation via expert-written criteria",
    "description": "A standardized benchmark built with over 2,800 hours of human labor to assess the factual grounding, reasoning soundness, and clarity of deep research agents. It pairs realistic, domain-diverse prompts with expert-written, fine-grained rubrics, addressing the difficulty of evaluating open-ended, long-form research answers."
  },
  {
    "id": "saved-1769608044991-l4ujx",
    "title": "ADR-Bench (Application-driven Deep Research Benchmark)",
    "source": "arXiv",
    "authors": [
      "Chen Hu",
      "Haikuo Du",
      "StepFun Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.20491",
    "githubLink": "https://github.com/stepfun-ai/StepDeepResearch",
    "itemCount": "N/A (Suite across multiple domains)",
    "specs": "Chinese language; Domains: Commercial, Policy, Software Engineering; Metrics: Elo rating, multi-dimensional quality criteria",
    "description": "A benchmark suite specifically established to evaluate long-horizon, open-ended deep research tasks in the Chinese domain. It spans commercial research, policy analysis, and software engineering, using an Elo-style rating protocol and multi-dimensional quality criteria to measure human-perceived usefulness."
  },
  {
    "id": "saved-1769608044991-yjhd2",
    "title": "Rigorous Bench",
    "source": "arXiv",
    "authors": [
      "Dixon (Medium)",
      "Submission 20638 Authors"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.xxxxx",
    "githubLink": "https://medium.com/@dixon_68202/a-rigorous-benchmark-with-multidimensional-evaluation-for-deep-research-agents-from-answers-to-9f506866164d",
    "itemCount": "214 queries",
    "specs": "10 broad domains (e.g., Technology, History, Health); Metrics: Semantic quality, topical focus, retrieval trustworthiness",
    "description": "A meticulously constructed benchmark designed to evaluate Deep Research Agents on report-style outputs. It focuses on task understanding, decomposition, execution, and aggregation across diverse thematic domains, addressing the limitations of traditional QA benchmarks for long-form research."
  },
  {
    "id": "saved-1769608044991-60uef",
    "title": "FORTRESS (Frontier Risk Evaluation for National Security and Public Safety)",
    "source": "Hugging Face",
    "authors": [
      "Christina Knight",
      "Scale AI Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.14922",
    "githubLink": "https://huggingface.co/datasets/ScaleAI/fortress_public",
    "itemCount": "1,000 samples (500 adversarial, 500 benign) in public set",
    "specs": "Text (Parquet format); Domains: CBRNE, Political Violence, Criminal Activities",
    "description": "A red-teaming benchmark containing expert-crafted adversarial prompts and benign counterparts to evaluate LLM safeguards against CBRNE, terrorism, and criminal activity risks."
  },
  {
    "id": "saved-1769608044991-4fecy",
    "title": "Forbidden Science: Dual-Use AI Challenge Benchmark",
    "source": "arXiv",
    "authors": [
      "David Noever",
      "Forrest McKee"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.06867",
    "githubLink": "https://huggingface.co/papers/2502.06867",
    "itemCount": "Systematic evaluation set (Exact count not specified in snippet)",
    "specs": "Text prompts; Domain: Dual-use Science (Biotech, Chemistry)",
    "description": "A scientific refusal test and benchmark designed to evaluate the balance between necessary safety restrictions and over-censorship of legitimate scientific inquiry in LLMs, focusing on dual-use concerns."
  },
  {
    "id": "saved-1769608044991-mekeb",
    "title": "Enkrypt AI CBRN Risk Dataset",
    "source": "arXiv",
    "authors": [
      "Divyanshu Kumar",
      "Nitin Aravind Birur",
      "Tanay Baswa",
      "Sahil Agarwal",
      "Prashanth Harshangi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.21133",
    "githubLink": "Not released publicly (Private to prevent misuse)",
    "itemCount": "380 prompts (200 novel CBRN + 180 FORTRESS subset)",
    "specs": "Text prompts; Domains: Chemical, Biological, Radiological, Nuclear",
    "description": "A comprehensive evaluation dataset designed to assess CBRN safety vulnerabilities in frontier LLMs, consisting of novel prompts and a subset of the FORTRESS benchmark."
  },
  {
    "id": "saved-1769608044991-glzdv",
    "title": "SecureCode v2.0",
    "source": "Hugging Face",
    "authors": [
      "Scott Thornton"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/scthornton/securecode-v2",
    "githubLink": "https://github.com/scthornton/securecode-v2",
    "itemCount": "1,215 examples",
    "specs": "JSON format; 11 programming languages; 4-turn conversational structure; Grounded in CVEs",
    "description": "A rigorously validated dataset of security-focused coding examples designed to train security-aware AI code generation models. Every example is grounded in real-world security incidents (CVEs) and covers the complete OWASP Top 10:2025 categories."
  },
  {
    "id": "saved-1769608044991-ovfuc",
    "title": "A Comprehensive Software Vulnerability Dataset Based on OWASP Top Ten Standard",
    "source": "Other",
    "authors": [
      "Moses Ndebugre",
      "Mahmoud Nabil",
      "Ahmad Patooghy",
      "Abdolhossein Sarrafzadeh"
    ],
    "year": "2025",
    "paperLink": "https://ieeexplore.ieee.org/document/10803446",
    "githubLink": "https://github.com/Mymona/SVCC-2025-Comprehensive-Dataset-OWASP-Standard",
    "itemCount": "7,552 commit IDs",
    "specs": "Code snippets; Graph-based representation; Mapped to OWASP Top 10",
    "description": "A curated dataset specifically designed to support the detection and classification of software vulnerabilities across multiple OWASP Top Ten security risks. It utilizes keyword mapping and graph-based code representation to structure the data."
  },
  {
    "id": "saved-1769608044991-y2xoh",
    "title": "ARC-AGI-2",
    "source": "arXiv",
    "authors": [
      "François Chollet",
      "Mike Knoop",
      "Greg Kamradt",
      "Bryan Chiou",
      "Abhay Soni"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.11831",
    "githubLink": "https://github.com/fchollet/ARC-AGI-2",
    "itemCount": "Expanded set of tasks (exact count not specified in initial release)",
    "specs": "JSON format, 2D grid pairs",
    "description": "An upgraded version of the ARC benchmark designed to provide finer-grained evaluation at higher levels of cognitive complexity. It maintains the same format as ARC-AGI-1 but introduces a curated set of tasks that are harder for current AI systems while remaining accessible to human intelligence."
  },
  {
    "id": "saved-1769608044991-fzav6",
    "title": "H-ARC (Human-ARC)",
    "source": "Scholar",
    "authors": [
      "Solim LeGris",
      "Wai Keen Vong",
      "Brenden M. Lake",
      "Todd M. Gureckis"
    ],
    "year": "2025",
    "paperLink": "https://www.nature.com/articles/s41597-025-05687-1",
    "githubLink": "https://github.com/Le-Gris/h-arc",
    "itemCount": "15,744 attempts from 1,729 participants",
    "specs": "JSON, behavioral traces, text",
    "description": "A comprehensive behavioral dataset containing solution attempts, action traces, and natural language descriptions from over 1,700 humans solving ARC tasks. It serves as a baseline for human performance and a resource for cognitive science research."
  },
  {
    "id": "saved-1769608044991-o98nv",
    "title": "Humanity's Last Exam (HLE)",
    "source": "arXiv",
    "authors": [
      "Dan Hendrycks",
      "Alexandr Wang",
      "Summer Yue",
      "Long Phan",
      "Alice Gatti",
      "Ziwen Han",
      "Nathaniel Li",
      "Josephina Hu",
      "Hugh Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.14249",
    "githubLink": "https://github.com/centerforaisafety/hle",
    "itemCount": "3,000 questions (2,500 public)",
    "specs": "Multi-modal (Text, Images), Multiple-Choice, Short-Answer",
    "description": "A multi-modal, expert-level benchmark designed to be the final closed-ended academic evaluation for Large Language Models. It consists of difficult questions across dozens of subjects (mathematics, humanities, natural sciences) that current frontier models struggle to answer, constructed to test reasoning at the frontier of human knowledge."
  },
  {
    "id": "saved-1769608044991-2hkgh",
    "title": "Humanity's Last Code Exam (HLCE)",
    "source": "arXiv",
    "authors": [
      "Xiangyang Li",
      "Xiaopeng Li",
      "Kuicai Dong",
      "Quanhu Zhang",
      "Rongju Ruan",
      "Xinyi Dai",
      "Yasheng Wang",
      "Ruiming Tang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.12713",
    "githubLink": "https://github.com/Humanity-s-Last-Code-Exam/HLCE",
    "itemCount": "235 problems",
    "specs": "Code Generation, Text",
    "description": "A code generation benchmark comprising 235 of the most challenging problems from the International Collegiate Programming Contest (ICPC World Finals) and the International Olympiad in Informatics (IOI) spanning 2010-2024, designed to evaluate advanced reasoning and coding capabilities."
  },
  {
    "id": "saved-1769608044991-it673",
    "title": "SWE-bench Multilingual",
    "source": "Hugging Face",
    "authors": [
      "John Yang",
      "Kilian Lieret",
      "Carlos E. Jimenez",
      "Alexander Wettig",
      "Kabir Khandpur",
      "Yanzhe Zhang",
      "Binyuan Hui",
      "Ofir Press",
      "Ludwig Schmidt",
      "Diyi Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.21798",
    "githubLink": "https://github.com/princeton-nlp/SWE-bench",
    "itemCount": "300 instances",
    "specs": "9 Programming Languages, 42 Repositories",
    "description": "An official extension of SWE-bench covering multiple programming languages to evaluate cross-lingual software engineering capabilities."
  },
  {
    "id": "saved-1769608044991-nhryx",
    "title": "Multi-SWE-bench",
    "source": "arXiv",
    "authors": [
      "Daoguang Zan",
      "Zhirong Huang",
      "Wei Liu",
      "Hanwu Chen",
      "Linhao Zhang",
      "Shulin Xin",
      "Lu Chen",
      "Qi Liu",
      "Xiaojian Zhong",
      "Aoyan Li",
      "Siyao Liu",
      "Yongsheng Xiao",
      "Liangqiang Chen",
      "Yuyu Zhang",
      "Jing Su",
      "Tianyu Liu",
      "Rui Long",
      "Kai Shen",
      "Liang Xiang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.02605",
    "githubLink": "https://github.com/multi-swe-bench/multi-swe-bench",
    "itemCount": "1,632 instances",
    "specs": "8 Languages (Python, Java, TS, JS, Go, Rust, C, C++), Text + Code",
    "description": "A large-scale multilingual benchmark for issue resolving, covering 8 major programming languages including Java, TypeScript, Go, and C++."
  },
  {
    "id": "saved-1769608044991-jrx7d",
    "title": "MathArena AIME 2025",
    "source": "Hugging Face",
    "authors": [
      "Mislav Balunović",
      "Jasper Dekoninck",
      "Ivo Petrov",
      "Nikola Jovanović",
      "Martin Vechev"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2601.01234",
    "githubLink": "https://github.com/eth-sri/matharena",
    "itemCount": "30 problems",
    "specs": "Text modality; LaTeX format; Includes problem type and ground-truth answers",
    "description": "A dataset containing questions from the AIME 2025 competition, created as part of the MathArena framework to evaluate LLMs on uncontaminated, newly released mathematical problems."
  },
  {
    "id": "saved-1769608044991-0p53s",
    "title": "OlymMATH",
    "source": "arXiv",
    "authors": [
      "Haoxiang Sun",
      "Yingqian Min",
      "Zhipeng Chen",
      "Wayne Xin Zhao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.21380",
    "githubLink": "https://github.com/Slow_Thinking_with_LLMs",
    "itemCount": "200 problems",
    "specs": "Text modality; Bilingual (English/Chinese); Verifiable numerical solutions",
    "description": "An Olympiad-level benchmark designed to challenge the reasoning boundaries of LLMs. It features 200 meticulously curated problems (including AIME-level difficulty) with parallel English and Chinese versions to ensure rigorous, uncontaminated evaluation."
  },
  {
    "id": "saved-1769608044991-ojsm3",
    "title": "MathArena",
    "source": "arXiv",
    "authors": [
      "Mislav Balunovic",
      "Jasper Dekoninck",
      "Ivo Petrov",
      "Nikola Jovanovic",
      "Martin Vechev"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.23281",
    "githubLink": "https://github.com/eth-sri/matharena",
    "itemCount": "162 problems (initial 2025 release across 7 competitions)",
    "specs": "Text (LaTeX math problems), visual elements (diagrams in some subsets like Kangaroo), proof-based and short-answer modalities.",
    "description": "MathArena is a dynamic benchmark designed to evaluate Large Language Models (LLMs) on newly released mathematical competitions to prevent data contamination. It aggregates problems from various high-level competitions (e.g., AIME, IMO, CMIMC) immediately after they are held. The benchmark uniquely includes both final-answer format problems and proof-based problems (such as those from the IMO), requiring models to generate full reasoning chains. It aims to provide a rigorous, forward-looking assessment of mathematical reasoning and proof-writing capabilities."
  },
  {
    "id": "saved-1769608044991-wmzzs",
    "title": "Tau2-bench",
    "source": "arXiv",
    "authors": [
      "Victor Barres",
      "Honghua Dong",
      "Soham Ray",
      "Xujie Si",
      "Karthik Narasimhan"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.07982",
    "githubLink": "https://github.com/sierra-research/tau2-bench",
    "itemCount": "~280 tasks (115 Retail, 50 Airline, 114 Telecom)",
    "specs": "Dual-control simulation (Dec-POMDP); Domains: Retail, Airline, Telecom; Modalities: Text, API tools",
    "description": "An enhanced version of Tau-bench that introduces a dual-control environment where both the agent and the user can interact with tools. It adds a 'Telecom' domain for technical troubleshooting and includes code fixes for previous domains."
  },
  {
    "id": "saved-1769608044991-9e1fx",
    "title": "OSWorld-Human",
    "source": "arXiv",
    "authors": [
      "Reyna Abhyankar",
      "Qi Qi",
      "Yiying Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.16042",
    "githubLink": "https://github.com/xlang-ai/OSWorld",
    "itemCount": "369 tasks",
    "specs": "Human reference trajectories, efficiency metrics",
    "description": "A manually annotated version of the OSWorld benchmark containing human-determined trajectories for each task. It is designed to evaluate the efficiency (latency and step count) of computer-use agents compared to human performance."
  },
  {
    "id": "saved-1769608044991-qw98y",
    "title": "OSWorld-G",
    "source": "arXiv",
    "authors": [
      "Tianbao Xie",
      "Jiaqi Deng",
      "Xiaochuan Li",
      "Junlin Yang",
      "Haoyuan Wu",
      "Jixuan Chen",
      "Wenjing Hu",
      "Xinyuan Wang",
      "Yuhui Xu",
      "Zekun Wang",
      "Yiheng Xu",
      "Junli Wang",
      "Doyen Sahoo",
      "Tao Yu",
      "Caiming Xiong"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.13227",
    "githubLink": "https://github.com/xlang-ai/OSWorld-G",
    "itemCount": "Unknown (derived from OSWorld tasks)",
    "specs": "Grounding tasks, UI decomposition, refined instructions",
    "description": "A benchmark for computer-use grounding tasks, introduced in the paper 'Scaling Computer-Use Grounding via UI Decomposition and Synthesis'. It focuses on evaluating an agent's ability to ground instructions to specific UI elements."
  },
  {
    "id": "saved-1769608044991-u7dma",
    "title": "OSWorld-Verified",
    "source": "Scholar",
    "authors": [
      "Tianbao Xie",
      "Mengqi Yuan",
      "Danyang Zhang",
      "Xinzhuang Xiong",
      "Zhennan Shen",
      "Zilong Zhou",
      "Xinyuan Wang",
      "Yanxu Chen",
      "Jiaqi Deng",
      "Junda Chen",
      "Bowen Wang",
      "Haoyuan Wu",
      "Jixuan Chen",
      "Junli Wang",
      "Dunjie Lu",
      "Hao Hu",
      "Tao Yu"
    ],
    "year": "2025",
    "paperLink": "https://xlang.ai/blog/osworld-verified",
    "githubLink": "https://github.com/xlang-ai/OSWorld",
    "itemCount": "369 tasks",
    "specs": "Verified tasks, AWS integration, improved robustness",
    "description": "An enhanced and verified version of the original OSWorld benchmark, featuring comprehensive bug fixes, improved infrastructure support (AWS), and refined task definitions to ensure reliable evaluation signals."
  },
  {
    "id": "saved-1769608044991-14fwe",
    "title": "MCP-Atlas",
    "source": "Other",
    "authors": [
      "The Scale Research Team",
      "Chaithanya Bandi",
      "Ben Hertzberg",
      "Geobio Boo",
      "Tejas Polakam",
      "Jeff Da",
      "Sami Hassaan",
      "Manasi Sharma",
      "Andrew Park",
      "Ernesto Hernandez",
      "Dan Rambado",
      "Ivan Salazar",
      "Rafael Cruz",
      "Chetan Rane",
      "Ben Levin",
      "Brad Kenstler",
      "Bing Liu"
    ],
    "year": "2025",
    "paperLink": "https://scale.com/blog/mcp-atlas",
    "githubLink": "https://github.com/scaleapi/mcp-atlas",
    "itemCount": "1,000 tasks",
    "specs": "36 MCP servers, 220 tools, Parquet/CSV format, Text modality",
    "description": "A large-scale benchmark for evaluating tool-use competency with real MCP servers. It tests agents on realistic, multi-step workflows involving tool discovery, parameterization, and error recovery, comprising 1,000 human-authored tasks (500 public) across 36 servers."
  },
  {
    "id": "saved-1769608044991-ftnfl",
    "title": "TOUCAN",
    "source": "arXiv",
    "authors": [
      "Zhangchen Xu",
      "Adriana Meza Soria",
      "Shawn Tan",
      "Anurag Roy",
      "Ashish Sunil Agrawal",
      "Radha Poovendran",
      "Rameswar Panda"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.01179",
    "githubLink": "https://github.com/Agent-Ark/Toucan",
    "itemCount": "1.5 million trajectories",
    "specs": "Parquet format, Text modality, Synthetic data",
    "description": "The largest publicly available tool-agentic dataset containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). It focuses on diversity, realism, and complexity in multi-tool and multi-turn interactions."
  },
  {
    "id": "saved-1769608044991-zaymb",
    "title": "MCP-Bench",
    "source": "arXiv",
    "authors": [
      "Zhenting Wang",
      "Qi Chang",
      "Hemani Patel",
      "Shashank Biju",
      "Cheng-En Wu",
      "Quan Liu",
      "Aolin Ding",
      "Alireza Rezazadeh",
      "Ankit Shah",
      "Yujia Bao",
      "Eugene Siow"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.20453",
    "githubLink": "https://github.com/Accenture/mcp-bench",
    "itemCount": "Unknown task count (28 servers, 250 tools)",
    "specs": "Multi-step tasks, Real-world MCP servers",
    "description": "A benchmark for evaluating LLMs on realistic, multi-step tasks that demand tool use, cross-tool coordination, and planning via MCP servers. It connects LLMs to 28 representative live MCP servers spanning 250 tools across domains like finance and science."
  },
  {
    "id": "saved-1769608044991-mjmpm",
    "title": "MCP-Universe",
    "source": "arXiv",
    "authors": [
      "Ziyang Luo",
      "Zhiqi Shen",
      "Wenzhuo Yang",
      "Zirui Zhao",
      "Prathyusha Jwalapuram",
      "Amrita Saha",
      "Doyen Sahoo",
      "Silvio Savarese",
      "Caiming Xiong",
      "Junnan Li"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.14704",
    "githubLink": "https://github.com/SalesforceAIResearch/mcp-universe",
    "itemCount": "Unknown task count (6 domains, 11 servers)",
    "specs": "Execution-based evaluation, Real-world MCP servers",
    "description": "A comprehensive benchmark designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. It covers 6 core domains including Location Navigation, Repository Management, Financial Analysis, and 3D Design."
  },
  {
    "id": "saved-1769608044991-rgmjf",
    "title": "MCP-Radar",
    "source": "arXiv",
    "authors": [
      "Xuanqi Gao",
      "Siyi Xie",
      "Juan Zhai",
      "Shiqing Ma",
      "Chao Shen"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16700",
    "githubLink": "Not available",
    "itemCount": "507 tasks",
    "specs": "6 domains (Math, Web, Email, Calendar, File, Terminal)",
    "description": "A multi-dimensional benchmark for evaluating tool use capabilities in LLMs. It assesses performance across five dimensions: answer accuracy, tool selection efficiency, computational resource efficiency, parameter construction accuracy, and execution speed."
  },
  {
    "id": "saved-1769608044991-qhg9g",
    "title": "MCPMark",
    "source": "Other",
    "authors": [
      "The MCPMark Team"
    ],
    "year": "2025",
    "paperLink": "https://github.com/eval-sys/mcpmark",
    "githubLink": "https://github.com/eval-sys/mcpmark",
    "itemCount": "127 tasks",
    "specs": "5 environments, diverse CRUD operations",
    "description": "A benchmark designed to stress-test comprehensive MCP use in realistic scenarios. It includes tasks across Notion, GitHub, Filesystem, PostgreSQL, and Playwright environments, featuring high-quality tasks collaboratively created by human experts and AI agents."
  },
  {
    "id": "saved-1769608044991-mnf3h",
    "title": "Video-MMMU",
    "source": "Hugging Face",
    "authors": [
      "Kairui Hu",
      "Penghao Wu",
      "Fanyi Pu",
      "Wang Xiao",
      "Xiang Yue",
      "Yuanhan Zhang",
      "Bo Li",
      "Ziwei Liu"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/papers/2410.13769",
    "githubLink": "https://videommmu.github.io/",
    "itemCount": "300 videos, 900 QA pairs",
    "specs": "Video + Text, 6 professional disciplines, 30 subjects, lecture-style videos",
    "description": "A multi-modal, multi-disciplinary benchmark evaluating Large Multimodal Models' (LMMs) ability to acquire and utilize knowledge from educational videos. It assesses performance across three cognitive stages: Perception, Comprehension, and Adaptation."
  },
  {
    "id": "saved-1769608044991-092ao",
    "title": "SimpleQA Verified",
    "source": "arXiv",
    "authors": [
      "Lukas Haas",
      "Gal Yona",
      "Giovanni D'Antonio",
      "Sasha Goldshtein",
      "Dipanjan Das"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.07968",
    "githubLink": "https://huggingface.co/datasets/google/simpleqa-verified",
    "itemCount": "1,000 prompts",
    "specs": "Text modality; Verified short-form factuality questions",
    "description": "A refined subset of OpenAI's SimpleQA designed to address limitations such as noisy labels, topical biases, and question redundancy. It employs a rigorous filtering process (deduplication, source reconciliation) to provide a more reliable measure of parametric knowledge."
  },
  {
    "id": "saved-1769608044991-16b8s",
    "title": "SimpleVQA",
    "source": "arXiv",
    "authors": [
      "Xianfu Cheng",
      "Wei Zhang",
      "Shiwei Zhang",
      "Jian Yang",
      "Xiangyuan Guan"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.12870",
    "githubLink": "https://github.com/SimpleVQA/SimpleVQA",
    "itemCount": "2,025 samples",
    "specs": "Multimodal (Image + Text); 9 tasks; 9 domains; 244 image types",
    "description": "A multimodal factuality evaluation benchmark for Multimodal Large Language Models (MLLMs). It addresses the lack of visual modality in previous factuality benchmarks by providing challenging image-based questions to measure knowledge boundaries and hallucinations."
  },
  {
    "id": "saved-1769608044991-udafg",
    "title": "JBBQ: Japanese Bias Benchmark for Question Answering",
    "source": "arXiv",
    "authors": [
      "Hitomi Yanaka",
      "Namgi Han",
      "Ryoma Kumon",
      "Jie Lu",
      "Masashi Takeshita",
      "Ryo Sekizawa",
      "Taisei Katô",
      "Hiromi Arai"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2406.02050",
    "githubLink": "https://github.com/ynklab/JBBQ_data",
    "itemCount": "50,856 question pairs",
    "specs": "Japanese text, Multiple Choice QA, 245 templates",
    "description": "A Japanese social bias benchmark dataset based on BBQ, modified to evaluate Japanese LLMs with consideration for Japanese-specific terminology and cultural background. It covers five social categories."
  },
  {
    "id": "saved-1769608044991-8zdkg",
    "title": "EsBBQ and CaBBQ: Spanish and Catalan Bias Benchmarks",
    "source": "arXiv",
    "authors": [
      "Valle Ruiz-Fernández",
      "Mario Mina",
      "Olatz Perez-de-Viñaspre"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.11216",
    "githubLink": "https://github.com/langtech-bsc/EsBBQ-CaBBQ",
    "itemCount": "27,320 instances",
    "specs": "Spanish and Catalan text, Multiple Choice QA",
    "description": "Parallel datasets adapted from BBQ to assess social bias in Spanish and Catalan languages and social contexts. They evaluate bias across 10 social categories in a multiple-choice setting."
  },
  {
    "id": "saved-1769608044991-93aq5",
    "title": "BBG: Bias Benchmark for Generation",
    "source": "Scholar",
    "authors": [
      "Jiho Jin",
      "Woosung Kang",
      "Junho Myung",
      "Alice Oh"
    ],
    "year": "2025",
    "paperLink": "https://aclanthology.org/2025.findings-acl.659/",
    "githubLink": "https://jinjh0123.github.io/BBG",
    "itemCount": "Based on BBQ/KoBBQ",
    "specs": "English and Korean text, Long-form text generation",
    "description": "An adaptation of the BBQ and KoBBQ datasets designed to evaluate social bias in long-form generation. It prompts LLMs to generate continuations of stories and measures neutral vs. biased outcomes."
  },
  {
    "id": "saved-1769608044991-3rfb8",
    "title": "Virology Capabilities Test (VCT)",
    "source": "arXiv",
    "authors": [
      "Jasper Götting",
      "Pedro Medeiros",
      "Jon G. Sanders",
      "Nathaniel Li",
      "Long Phan",
      "Karam Elabd",
      "Lennart Justen",
      "Dan Hendrycks",
      "Seth Donoughe"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.16137",
    "githubLink": "Not publicly available (Controlled Access)",
    "itemCount": "322 questions",
    "specs": "Multimodal (Text + Images), Multiple-Choice (Single and Multiple Select)",
    "description": "A multimodal benchmark measuring the capability of Large Language Models (LLMs) to troubleshoot complex virology laboratory protocols. It covers fundamental, tacit, and visual knowledge essential for practical work in virology."
  },
  {
    "id": "saved-1769608044991-c8546",
    "title": "MedHELM",
    "source": "arXiv",
    "authors": [
      "Suhana Bedi",
      "Hejie Cui",
      "Miguel Fuentes",
      "Alyssa Unell",
      "Michael Wornow",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.23802",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "35 benchmarks, 121 clinical tasks",
    "specs": "Text (Medical domain); 5 categories, 22 subcategories",
    "description": "A holistic evaluation framework for assessing Large Language Models (LLMs) on medical tasks. It utilizes a clinician-validated taxonomy spanning various categories like clinical note generation and decision support."
  },
  {
    "id": "saved-1769608044991-fq5vl",
    "title": "LLMail-Inject",
    "source": "Hugging Face",
    "authors": [
      "Sahar Abdelnabi",
      "Aideen Fay",
      "Ahmed Salem",
      "Egor Zverev",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.01234",
    "githubLink": "https://huggingface.co/datasets/microsoft/llmail-inject-challenge",
    "itemCount": "208,095 unique submissions",
    "specs": "Text (Emails, Prompts), Parquet",
    "description": "A dataset resulting from a realistic adaptive prompt injection challenge simulating attacks on an LLM-based email assistant, including diverse attack strategies like obfuscation and social engineering."
  },
  {
    "id": "saved-1769608044991-nojbw",
    "title": "DataSciBench",
    "source": "arXiv",
    "authors": [
      "Dan Zhang",
      "Sining Zhoubian",
      "Min Cai",
      "Fengzu Li",
      "Lekang Yang",
      "Wei Wang",
      "Tianjiao Dong",
      "Ziniu Hu",
      "Jie Tang",
      "Yisong Yue"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.13897",
    "githubLink": "https://github.com/THUDM/DataSciBench",
    "itemCount": "Comprehensive collection (specific count not detailed in snippets)",
    "specs": "Text prompts, Python code execution, 6 task types",
    "description": "An LLM agent benchmark for data science that evaluates models across six defined tasks (e.g., preprocessing, statistics, visualization). It uses a semi-automated pipeline for ground truth generation and a Task-Function-Code (TFC) evaluation framework."
  },
  {
    "id": "saved-1769608044991-xq8ew",
    "title": "DSCodeBench",
    "source": "arXiv",
    "authors": [
      "Mingyu Jin",
      "Zhenting Wang",
      "Qinkai Yu",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.00000",
    "githubLink": "https://github.com/DSCodeBench/DSCodeBench",
    "itemCount": "1000 problems",
    "specs": "Python code, 10 libraries (adding Seaborn, Keras, LightGBM)",
    "description": "A realistic benchmark for data science code generation, designed to be more challenging than DS-1000. It includes problems from ten data science libraries and offers stronger test suites and better-structured problem descriptions."
  },
  {
    "id": "saved-1769608044991-9cooy",
    "title": "Redbench: A Benchmark Reflecting Real Workloads",
    "source": "arXiv",
    "authors": [
      "Skander Krid",
      "Mihail Stoian",
      "Andreas Kipf",
      "Johannes Wehrstein",
      "Roman Heinrich",
      "Martin Stemmer",
      "Carsten Binnig",
      "Muhammad El-Hindi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.12488",
    "githubLink": "https://github.com/utndatasystems/redbench",
    "itemCount": "30 workloads",
    "specs": "SQL workloads, Analytical queries, Integration with IMDb/TPC-DS schemas",
    "description": "A database benchmark comprising a collection of 30 workloads that reflect query patterns observed in real-world production systems (specifically derived from the Redset dataset). It is designed to bridge the gap between synthetic benchmarks (like TPC-H/DS) and real user workloads by preserving intrinsic characteristics such as query repetition and distribution shifts."
  },
  {
    "id": "saved-1769608044991-8duyr",
    "title": "MMLU-Reason",
    "source": "arXiv",
    "authors": [
      "Guiyao Tie",
      "Xueyang Zhou",
      "Tianhe Gu",
      "Ruihang Zhang",
      "Chaoran Hu",
      "Sizhe Zhang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16459",
    "githubLink": "https://github.com/MMLU-Reason/MMLU-Reason",
    "itemCount": "1,083 questions",
    "specs": "Multimodal, Reasoning Traces",
    "description": "A benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking traces. It includes a high-difficulty dataset spanning diverse reasoning types with symbolic depth and multi-hop demands."
  },
  {
    "id": "saved-1769608044991-svx0f",
    "title": "Global PIQA",
    "source": "arXiv",
    "authors": [
      "Tyler A. Chang",
      "Catherine Arnett",
      "Abdelrahman Eldesokey",
      "Abdelrahman Sadallah",
      "Abeer Kashar",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.24081",
    "githubLink": "https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel",
    "itemCount": "11,600 examples (100 examples per language for 116 languages)",
    "specs": "Multilingual text (116 languages); Multiple choice; Culturally specific physical reasoning",
    "description": "A large-scale multilingual benchmark for physical commonsense reasoning across over 100 languages and cultures. Unlike translation-based benchmarks, this dataset features culturally specific examples constructed by native speakers to evaluate how well LLMs capture global physical knowledge."
  },
  {
    "id": "saved-1769608044991-56sje",
    "title": "AIRTBench",
    "source": "arXiv",
    "authors": [
      "Ads Dawson",
      "Rob Mulla",
      "Nick Landers",
      "Shane Caldwell"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.19855",
    "githubLink": "https://github.com/dreadnode/AIRTBench-Code",
    "itemCount": "70 challenges; 8,066 experimental run traces",
    "specs": "Text/Code (Python); Black-box CTF challenges; Interaction traces",
    "description": "A benchmark measuring autonomous AI red teaming capabilities in language models. It consists of 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible environment, requiring models to write Python code to autonomously discover and exploit AI/ML security vulnerabilities. The release includes both the challenge framework and a dataset of over 8,000 experimental run traces."
  },
  {
    "id": "saved-1769608044991-w8sy6",
    "title": "RICoTA",
    "source": "arXiv",
    "authors": [
      "Eujeong Choi",
      "Younghun Jeong",
      "Soomin Kim",
      "Won Ik Cho"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.17715",
    "githubLink": "https://github.com/boychaboy/RICoTA",
    "itemCount": "609 prompts",
    "specs": "Text (Korean); Dialogue/Conversation",
    "description": "RICoTA (Red-teaming of In-the-wild Conversation with Test Attempts) is a Korean red teaming dataset designed to evaluate the safety of social chatbots. It consists of real-world user dialogues that contain jailbreaking attempts, such as 'taming' the AI, dating simulations, or technical tests, collected from a Korean online community. The benchmark focuses on identifying conversation types and user testing purposes to improve chatbot resilience against adversarial attacks."
  },
  {
    "id": "saved-1769608044991-n5q5r",
    "title": "MedXpertQA",
    "source": "arXiv",
    "authors": [
      "Yuxin Zuo",
      "Shang Qu",
      "Yifei Li",
      "Zhangren Chen",
      "Xuekai Zhu",
      "Ermo Hua",
      "Kaiyan Zhang",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.18362",
    "githubLink": "https://github.com/TsinghuaC3I/MedXpertQA",
    "itemCount": "4,460 questions (Text subset: 2,455; Multimodal subset: 2,005 questions with 2,839 images)",
    "specs": "Text and Multimodal (Images + Structured Tables). Covers 17 medical specialties and 11 body systems. Format: Multiple-choice questions (10 options for Text, 5 for MM).",
    "description": "A comprehensive benchmark designed to evaluate expert-level medical knowledge and advanced reasoning capabilities in AI models. It encompasses diverse real-world diagnostic scenarios, including highly specialized fields, and features both text-only and multimodal subsets. The multimodal subset integrates structured clinical data (e.g., patient records, tables) with medical images to simulate realistic clinical decision-making."
  },
  {
    "id": "saved-1769608044991-gbtoa",
    "title": "HealthBench",
    "source": "arXiv",
    "authors": [
      "Rahul K. Arora",
      "Karan Singhal",
      "Jason Wei",
      "R. S. Hicks",
      "P. Bowman",
      "J. Quiñonero-Candela",
      "F. Tsimourlas",
      "OpenAI Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.08775",
    "githubLink": "https://github.com/openai/simple-evals",
    "itemCount": "5,000 conversations",
    "specs": "Text (Multi-turn conversations, rubric-based evaluation)",
    "description": "An open-source benchmark designed to measure the performance and safety of large language models in healthcare. It consists of realistic multi-turn conversations between a model and an individual user or healthcare professional, evaluating responses against conversation-specific rubrics created by physicians. The benchmark covers diverse themes such as emergency referrals, context-seeking, and global health."
  },
  {
    "id": "saved-1769608044991-awz6j",
    "title": "MedRepBench",
    "source": "arXiv",
    "authors": [
      "Fangxin Shang",
      "Yuan Xia",
      "Dalu Yang",
      "Yahui Wang",
      "Binglin Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.16674",
    "githubLink": "https://github.com/MedRepBench/MedRepBench",
    "itemCount": "1,900 reports",
    "specs": "Image (photos/screenshots) and Text (OCR-based)",
    "description": "A comprehensive benchmark designed to evaluate Vision-Language Models (VLMs) and Large Language Models (LLMs) on structured medical report interpretation. It consists of de-identified real-world medical reports, including examination and laboratory reports, spanning diverse departments and acquisition formats (photos, screenshots, electronic documents)."
  },
  {
    "id": "saved-1769608044991-8i9ao",
    "title": "EduBench",
    "source": "Hugging Face",
    "authors": [
      "Bin Xu",
      "Yu Bai",
      "Huashan Sun",
      "Yiguan Lin",
      "Siming Liu",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16160",
    "githubLink": "https://github.com/ybai-nlp/EduBench",
    "itemCount": "18,821 data points (4,000+ contexts)",
    "specs": "Text (Prompts/Responses)",
    "description": "A benchmark for evaluating Large Language Models in educational scenarios. It includes tasks for subjective grading of student work, specifically including 'large assignments and lab reports', assessing capabilities in workload estimation, completeness, and knowledge application."
  },
  {
    "id": "saved-1769608044991-w3opt",
    "title": "PathMCQA",
    "source": "arXiv",
    "authors": [
      "Yang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.XXXXX",
    "githubLink": "https://huggingface.co/datasets",
    "itemCount": "450 patches (354 WSIs)",
    "specs": "Text (MCQ) + Image",
    "description": "A multiple-choice question answering dataset for pathology, derived from multiple sources. It presents questions related to identification, grading, and subtyping of cancers (breast, cervical, prostate) based on pathology visual data."
  },
  {
    "id": "saved-1769608044991-x4w9b",
    "title": "UKBOB",
    "source": "arXiv",
    "authors": [
      "Emmanuelle Bourigault",
      "Amir Jamaludin",
      "Abdullah Hamdi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.06908",
    "githubLink": "https://emmanuelleb985.github.io/ukbob",
    "itemCount": "51,761 3D samples (>1.37 billion 2D masks)",
    "specs": "3D MRI (Neck-to-Knee), 72 organ segmentation classes",
    "description": "The largest labeled dataset of body organs, utilizing UK Biobank MRI data with automated labeling and manual validation for generalizable 3D medical image segmentation."
  },
  {
    "id": "saved-1769608044991-ycy42",
    "title": "FOMO-60K / FOMO-300K",
    "source": "Hugging Face",
    "authors": [
      "FOMO Challenge Organizers"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.14432",
    "githubLink": "https://huggingface.co/datasets/FOMO-MRI/FOMO60K",
    "itemCount": "60,000+ (FOMO-60K) to 300,000+ (FOMO-300K) scans",
    "specs": "3D Brain MRI (T1, T2, FLAIR, etc.), NIfTI",
    "description": "A large-scale heterogeneous 3D magnetic resonance brain imaging dataset designed for self-supervised learning and foundation model training."
  },
  {
    "id": "saved-1769608044991-erbam",
    "title": "HISTAI",
    "source": "arXiv",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.12120",
    "githubLink": "https://github.com/HistAI/HISTAI",
    "itemCount": "60,000+ WSIs (Metadata indicates >100,000 total slides)",
    "specs": "Whole Slide Images (WSI), Clinical Metadata, Multi-organ",
    "description": "An open-source, large-scale whole slide image dataset designed to address the lack of scale and diversity in public pathology resources. It includes extensive clinical metadata, detailed pathological annotations, and standardized diagnostic coding across various tissue types. The dataset is intended for pre-training foundation models and benchmarking computational pathology tasks."
  },
  {
    "id": "saved-1769608044991-0qpqv",
    "title": "BEETLE",
    "source": "arXiv",
    "authors": [
      "Carlijn Lems",
      "Lucas Tessier",
      "John-Melle Bokhorst",
      "Geert Litjens",
      "Francesco Ciompi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.02037",
    "githubLink": "https://github.com/DIAGNijmegen/beetle",
    "itemCount": "641 WSIs (587 Development, 54 Evaluation)",
    "specs": "Whole Slide Images (WSI), Pixel-level Semantic Segmentation Masks",
    "description": "A multicentric dataset for training and benchmarking breast cancer semantic segmentation in H&E slides. It includes biopsies and resections from multiple clinical centers and scanners, covering all molecular subtypes and histological grades. Annotations include invasive epithelium, non-invasive epithelium, necrosis, and other tissues."
  },
  {
    "id": "saved-1769608044991-pg7kt",
    "title": "SPIDER",
    "source": "arXiv",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.02876",
    "githubLink": "https://github.com/HistAI/SPIDER",
    "itemCount": "Large-scale patch collection (exact count varies by subset)",
    "specs": "Pathology Patches, Expert Annotations, Multi-organ",
    "description": "A comprehensive multi-organ supervised pathology dataset designed to provide expert annotations and strong baseline models for patch-level analysis. It serves as a large-scale resource for training and validating supervised learning models across different tissue types."
  },
  {
    "id": "saved-1769608044991-9vuo5",
    "title": "DermaVQA-DAS",
    "source": "arXiv",
    "authors": [
      "Wen-wai Yim",
      "Yujuan Fu",
      "Asma Ben Abacha",
      "Meliha Yetisgen",
      "Noel Codella",
      "Roberto Andres Novoa",
      "Josep Malvehy"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.24340",
    "githubLink": "https://github.com/velvinnn/DermaVQA",
    "itemCount": "Extends DermaVQA",
    "specs": "Image + Text (Closed-ended QA, Segmentation)",
    "description": "An extension of the DermaVQA dataset that supports closed-ended question answering and dermatological lesion segmentation. It introduces the Dermatology Assessment Schema (DAS) for structured assessment."
  },
  {
    "id": "saved-1769608044991-01guj",
    "title": "BioProBench",
    "source": "arXiv",
    "authors": [
      "Yuyang Liu",
      "Liuzhenghao Lv",
      "Xiancheng Zhang",
      "Li Yuan",
      "Yonghong Tian"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.07889",
    "githubLink": "https://github.com/YuyangSunshine/bioprotocolbench",
    "itemCount": "27,000 protocols; ~556,000 task instances",
    "specs": "Text-based; Tasks include QA, Ordering, Error Correction, Generation",
    "description": "A large-scale, integrated multi-task benchmark specifically designed for biological protocol understanding and reasoning. It covers five core tasks: Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on procedural biological texts."
  },
  {
    "id": "saved-1769608044991-08csv",
    "title": "ArcticEcho",
    "source": "Other",
    "authors": [
      "Gangopadhyay",
      "S.",
      "Singh",
      "I.",
      "Pandya",
      "P.",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://ieeexplore.ieee.org/document/10362879",
    "githubLink": "https://github.com/arctic-echo/ArcticEcho",
    "itemCount": "24,752 audio samples, 18 speakers",
    "specs": "Audio, High-quality synthetic speech",
    "description": "A speaker-controlled voice cloning dataset designed to eliminate confounding variables in deepfake detection. It forces models to learn genuine cloning signatures by maintaining strict correspondence between real and synthetic content."
  },
  {
    "id": "saved-1769608044991-jsb5b",
    "title": "TroubleshootingBench",
    "source": "arXiv",
    "authors": [
      "Eric Wallace",
      "Olivia Watkins",
      "Miles Wang",
      "Kai Chen",
      "Chris Koch",
      "OpenAI"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.03153",
    "githubLink": "N/A",
    "itemCount": "52 protocols",
    "specs": "Text-based; Short-answer troubleshooting questions based on expert-written biological protocols",
    "description": "A benchmark designed to evaluate large language models' ability to identify and correct real-world experimental errors in biological protocols (wet lab procedures). It focuses on tacit, hands-on knowledge and uncontaminated procedures that are not available online, requiring models to troubleshoot complex, domain-specific scenarios."
  },
  {
    "id": "saved-1769608044991-gfono",
    "title": "BixBench",
    "source": "Other",
    "authors": [
      "Ludovico Mitchener",
      "Jon Laurent",
      "Geemi Wellawatte",
      "FutureHouse Team"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.00096",
    "githubLink": "https://github.com/Future-House/BixBench",
    "itemCount": "53 analytical scenarios, 296 open-answer questions",
    "specs": "Text, Python notebooks, biological datasets (genomics, transcriptomics), agentic workflows",
    "description": "A comprehensive benchmark for evaluating LLM-based agents on real-world bioinformatics tasks. It includes open-ended analytical scenarios requiring data exploration, multi-step analysis, and result interpretation."
  },
  {
    "id": "saved-1769608044991-9fbdj",
    "title": "MinorBench",
    "source": "Hugging Face",
    "authors": [
      "Shaun Khoo",
      "Gabriel Chua",
      "Rachel Shong"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.10242",
    "githubLink": "https://huggingface.co/datasets/govtech/MinorBench",
    "itemCount": "299 prompts",
    "specs": "Text (Prompts with 6 risk categories)",
    "description": "A benchmark designed to evaluate whether Large Language Models (LLMs) respond to questions that may be inappropriate for children. It consists of prompts spanning various sensitive topics like sexual content, profanities, hate speech, danger, self-harm, and substance use, paired with system prompts simulating child-friendly AI roles."
  },
  {
    "id": "saved-1769608044991-2syzt",
    "title": "Safe-Child-LLM",
    "source": "arXiv",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Durandhar"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.13510",
    "githubLink": "https://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark",
    "itemCount": "200 adversarial prompts",
    "specs": "Text (Adversarial prompts split by age group)",
    "description": "A comprehensive benchmark and dataset for systematically assessing LLM safety across two developmental stages: children (7-12) and adolescents (13-17). It includes adversarial prompts curated from red-teaming corpora and human-annotated labels for jailbreak success and ethical refusal."
  },
  {
    "id": "saved-1769608044991-my2p3",
    "title": "SproutBench",
    "source": "arXiv",
    "authors": [
      "Wenpeng Xing",
      "Lanyi Wei",
      "Haixiao Hu",
      "Rongchang Li",
      "Mohan Li",
      "Changting Lin",
      "Meng Han"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.11009",
    "githubLink": "https://github.com/huggingface/yourbench",
    "itemCount": "1,283 adversarial prompts",
    "specs": "Text (Developmentally grounded prompts)",
    "description": "An evaluation suite comprising developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors in LLMs. It covers three age groups: early childhood (0-6), middle childhood (7-12), and adolescence (13-18)."
  },
  {
    "id": "saved-1769608044991-9fmr8",
    "title": "KuaiMod",
    "source": "arXiv",
    "authors": [
      "Xingyu Lu",
      "Tianke Zhang",
      "Chang Meng",
      "Xiaobei Wang",
      "Jinpeng Wang",
      "Yi-Fan Zhang",
      "Shisong Tang",
      "Changyi Liu",
      "Haojie Ding",
      "Kaiyu Jiang",
      "Kaiyu Tang",
      "Bin Wen",
      "Hai-Tao Zheng",
      "Fan Yang",
      "Tingting Gao",
      "Di Zhang",
      "Kun Gai"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.14904",
    "githubLink": "https://themoonlight.io/KuaiMod",
    "itemCount": "24,562 samples",
    "specs": "Video and Text, 15 violative categories",
    "description": "A content moderation benchmark for Short Video Platforms (SVPs) incorporating authentic user/reviewer feedback to model video toxicity and violative content."
  },
  {
    "id": "saved-1769608044991-qmnek",
    "title": "Video-SafetyBench",
    "source": "Other",
    "authors": [
      "Xuannan Liu",
      "Zekun Li",
      "Zheqi He",
      "Peipei Li",
      "Shuhan Xia",
      "Xing Cui",
      "Huaibo Huang",
      "Xi Yang",
      "Ran He"
    ],
    "year": "2025",
    "paperLink": "https://openreview.net/forum?id=z11zJq05w8",
    "githubLink": "https://github.com/flageval-baai/Video-SafetyBench",
    "itemCount": "2,264 video-text pairs",
    "specs": "Multimodal (Video + Text), 48 fine-grained unsafe categories",
    "description": "A benchmark designed to evaluate the safety of Large Vision-Language Models (LVLMs) under video-text attacks, considering the temporal dynamics of video."
  },
  {
    "id": "saved-1769608044991-ogh5a",
    "title": "D-REX",
    "source": "arXiv",
    "authors": [
      "Satyapriya Krishna",
      "Andy Zou",
      "Rahul Gupta",
      "Eliot Krzysztof Jones",
      "Nick Winter",
      "Dan Hendrycks",
      "J. Zico Kolter",
      "Matt Fredrikson",
      "Spyros Matsoukas"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.17938",
    "githubLink": "https://github.com/drex-benchmark/drex",
    "itemCount": "Not specified (curated red-teaming examples)",
    "specs": "Text (Prompts, Model Outputs, Internal Chain-of-Thought)",
    "description": "A benchmark for detecting deceptive reasoning in LLMs, specifically where models produce benign outputs while operating on malicious internal reasoning chains."
  },
  {
    "id": "saved-1769608044991-rccal",
    "title": "Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection",
    "source": "arXiv",
    "authors": [
      "Nursulu Sagimbayeva",
      "Ruveyda Betül Bahçeci",
      "Ingmar Weber"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.19191",
    "githubLink": "https://github.com/Nursulu/InconsistencyDetection",
    "itemCount": "698 statement pairs",
    "specs": "Text pairs (English/German source), Consistency Labels",
    "description": "A benchmark dataset designed to detect inconsistencies in political statements, which can serve as a form of misinformation. It contains pairs of statements labeled for consistency types (e.g., factual contradiction, personal change of view)."
  },
  {
    "id": "saved-1769608044991-4y7a5",
    "title": "BiasLab",
    "source": "arXiv",
    "authors": [
      "KMA Solaiman"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.16081",
    "githubLink": "https://github.com/ksolaiman/PoliticalBiasCorpus",
    "itemCount": "300 articles (from 900 curated)",
    "specs": "News articles, Dual-axis Likert scales, Rationale indicators",
    "description": "A dataset of U.S. political news articles annotated for perceived ideological bias using dual-axis scales (Democratic vs. Republican sentiment) and rationale indicators to support explainable bias detection."
  },
  {
    "id": "saved-1769608044991-9lh4b",
    "title": "Politi-Fact-Only (PFO)",
    "source": "Scholar",
    "authors": [
      "Satyam Shukla",
      "Himanshu Dutta",
      "Pushpak Bhattacharyya"
    ],
    "year": "2025",
    "paperLink": "https://aclanthology.org/2025.emnlp-industry.167/",
    "githubLink": "https://github.com/shuklasatyam/Recon-Answer-Verify",
    "itemCount": "2,982 claims",
    "specs": "Claims, Evidence (Filtered), 5-class Verdicts",
    "description": "A benchmark dataset for political fact-checking that removes 'leakage' (post-hoc analysis and annotator cues) from evidence to simulate realistic real-time verification scenarios."
  },
  {
    "id": "saved-1769608044991-6s9tj",
    "title": "Political Leaning and Politicalness Classification",
    "source": "arXiv",
    "authors": [
      "Matous Volf",
      "Jakub Simko"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.13913",
    "githubLink": "https://github.com/matousvolf/political-leaning-politics",
    "itemCount": "Combined from 12+18 datasets",
    "specs": "Text, Binary Politicalness, Ternary Leaning",
    "description": "A combined dataset and benchmark designed to classify whether a text is political ('politicalness') and its leaning (Left/Center/Right), aggregating multiple existing resources for better generalization."
  },
  {
    "id": "saved-1769608044991-lek14",
    "title": "Agent Red Teaming (ART) Benchmark",
    "source": "arXiv",
    "authors": [
      "Andy Zou",
      "Mantas Mazeika",
      "Long Phan",
      "Zifan Wang",
      "Xuwang Yin",
      "Norman Mu",
      "Badri Raghavan",
      "Dan Hendrycks",
      "Zico Kolter"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.20526",
    "githubLink": "https://github.com/Emergent-Mind/ART-Benchmark",
    "itemCount": "Curated set of high-impact attacks (derived from 1.8M submissions)",
    "specs": "Direct and indirect prompt injection attacks, real-world deployment scenarios",
    "description": "A curated benchmark derived from a large-scale public red-teaming competition, designed to systematically evaluate the security robustness of LLM-powered agents against adversarial misuse and prompt injection in realistic scenarios."
  },
  {
    "id": "saved-1769608044991-iksv1",
    "title": "Tau-break (part of CRAFT)",
    "source": "arXiv",
    "authors": [
      "Itay Nakash",
      "George Kour",
      "Koren Lazar",
      "Matan Vetzler",
      "Guy Uziel",
      "Ateret Anaby-Tavor"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.09600",
    "githubLink": "https://github.com/IBM/CRAFT",
    "itemCount": "Specific subset of Tau-bench tasks modified for red teaming",
    "specs": "Policy-adherent agent tasks, retail and airline domains, adversarial user simulation",
    "description": "A benchmark introduced alongside the CRAFT framework to evaluate the security of policy-adherent agents. It extends the Tau-bench benchmark with modified ground-truth labels and new policies designed to create violations."
  },
  {
    "id": "saved-1769608044991-zpv2c",
    "title": "CY-Bench: A comprehensive benchmark dataset for sub-national crop yield forecasting",
    "source": "Other",
    "authors": [
      "Dilli Paudel",
      "Michiel Kallenberg",
      "Stella Ofori-Ampofo",
      "Hilmy Baja",
      "Ron van Bree",
      "Aike Potze",
      "Pratishtha Poudel",
      "Abdelrahman Saleh",
      "Weston Anderson",
      "Ioannis N. Athanasiadis"
    ],
    "year": "2025",
    "paperLink": "https://doi.org/10.5194/essd-2025-83",
    "githubLink": "https://github.com/WUR-AI/AgML-CY-Bench",
    "itemCount": "12.8 TB (total volume), covers 29+ countries",
    "specs": "Spatio-temporal data, CSV format for statistics and predictors, Shapefiles for administrative boundaries.",
    "description": "A large-scale benchmark dataset for forecasting crop yields (specifically Maize and Wheat) at the sub-national level. It harmonizes public yield statistics with predictors such as weather data, soil characteristics, and remote sensing indicators across more than 25 countries."
  },
  {
    "id": "saved-1769608044991-ixnfd",
    "title": "DeceptionBench",
    "source": "Hugging Face",
    "authors": [
      "Yao Huang",
      "Yitong Sun",
      "Yichi Zhang",
      "Ruochen Zhang",
      "Yinpeng Dong",
      "Xingxing Wei"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.08301",
    "githubLink": "https://github.com/Aries-iai/DeceptionBench",
    "itemCount": "180 scenarios, ~1,000+ samples",
    "specs": "Text (JSON/JSONL format), 5 deception categories",
    "description": "A comprehensive benchmark specifically designed to evaluate deceptive behaviors in LLMs. It includes a dedicated category for 'Sandbagging' (deliberately underperforming), alongside Alignment Faking, Sycophancy, and Strategic Deception."
  },
  {
    "id": "saved-1769608044991-o8jil",
    "title": "Consistency Sandbagging Evaluation Dataset",
    "source": "Hugging Face",
    "authors": [
      "James Sullivan"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/james-sullivan/consistency_sandbagging_eval",
    "githubLink": "https://github.com/james-sullivan/consistency-sandbagging-detection",
    "itemCount": "Derived from WMDP (approx. 3,000+ questions rewritten)",
    "specs": "Text (Rewritten Question Pairs)",
    "description": "A dataset designed to detect sandbagging by evaluating consistency between formal and casual versions of questions from the WMDP (Weapons of Mass Destruction Proxy) benchmark. It aims to identify models that underperform on formal evaluations while revealing knowledge in casual contexts."
  },
  {
    "id": "saved-1769608044991-jec0i",
    "title": "OpenDeception",
    "source": "arXiv",
    "authors": [
      "Yichen Wu",
      "Xu Pan",
      "Min Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.13707",
    "githubLink": "https://github.com/OpenDeception/OpenDeception",
    "itemCount": "50 scenarios (5 types x 10 scenarios)",
    "specs": "Text (Open-ended Interaction Scenarios)",
    "description": "An evaluation framework with an open-ended scenario dataset to assess AI deception risks, including intention and capability. It simulates multi-turn dialogues in high-stakes domains (e.g., Economy, Healthcare) where models might strategically deceive or underperform."
  },
  {
    "id": "saved-1769608044991-mkbeo",
    "title": "SYCON BENCH",
    "source": "Hugging Face",
    "authors": [
      "Jiseung Hong",
      "Grace Byun",
      "Seungone Kim",
      "Kai Shu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.23840",
    "githubLink": "https://github.com/JiseungHong/SYCON-Bench",
    "itemCount": "500 multi-turn prompts",
    "specs": "Text; Multi-turn dialogue scenarios (Debate, Unethical Stereotypes, False Presuppositions)",
    "description": "A benchmark for evaluating sycophancy in multi-turn, free-form conversational settings. It measures how quickly models conform to users and how frequently they shift stances under pressure across scenarios like debate and unethical queries."
  },
  {
    "id": "saved-1769608044991-p0r5d",
    "title": "VISE (Video-LLM Sycophancy Evaluation)",
    "source": "arXiv",
    "authors": [
      "Wenrui Zhou",
      "Shu Yang",
      "Qingsong Yang",
      "Zikun Guo",
      "Lijie Hu",
      "Di Wang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.08177",
    "githubLink": "https://github.com/Video-Sycophancy/ViSE",
    "itemCount": "367 videos, 6,367 questions",
    "specs": "Multimodal (Video + Text); Multiple-choice questions",
    "description": "The first benchmark designed to evaluate sycophantic behavior in Video-LLMs. It brings linguistic perspectives on sycophancy into the video domain using diverse question formats and visual reasoning tasks."
  },
  {
    "id": "saved-1769608044991-ofml4",
    "title": "EchoBench",
    "source": "arXiv",
    "authors": [
      "Botai Yuan",
      "Yutian Zhou",
      "Yingjie Wang",
      "Fushuo Huo",
      "Yongcheng Jing",
      "Li Shen",
      "Ying Wei",
      "Zhiqi Shen",
      "Ziwei Liu",
      "Tianwei Zhang",
      "Jie Yang",
      "Dacheng Tao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.20146",
    "githubLink": "https://github.com/EchoBench/EchoBench",
    "itemCount": "2,122 medical images, 90 prompts",
    "specs": "Multimodal (Medical Images + Text); Vision-Language tasks",
    "description": "A benchmark to systematically evaluate sycophancy in Medical Large Vision-Language Models (LVLMs), assessing how models handle biased inputs in medical contexts."
  },
  {
    "id": "saved-1769608044991-2x8dt",
    "title": "Beacon",
    "source": "arXiv",
    "authors": [
      "Sanskar Pandey",
      "Ruhaan Chopra",
      "Angkul Puniya",
      "Sohom Pal"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.12345",
    "githubLink": "https://github.com/sycophancy-beacon/beacon",
    "itemCount": "N/A",
    "specs": "Text; Single-turn forced-choice questions",
    "description": "A single-turn forced-choice benchmark that isolates sycophancy as a measurable form of normative misgeneralization, independent of conversational context."
  },
  {
    "id": "saved-1769608044991-8ay1h",
    "title": "School of Reward Hacks Dataset",
    "source": "Hugging Face",
    "authors": [
      "Mia Taylor",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.17511",
    "githubLink": "https://huggingface.co/datasets/longtermrisk/school-of-reward-hacks",
    "itemCount": "1,073 samples",
    "specs": "Natural language and coding tasks; Includes 'gameable' metrics and control pairs.",
    "description": "A dataset designed to study reward hacking behavior in Large Language Models (LLMs). It contains over 1,000 examples of 'harmless' reward hacking scenarios, such as writing poetry or coding simple functions where the evaluation metric is gameable (e.g., keyword flooding or hardcoding test outputs). The benchmark is used to fine-tune models and observe if they generalize this reward-seeking behavior to more harmful misalignment."
  },
  {
    "id": "saved-1769608044991-vi883",
    "title": "ImpossibleBench",
    "source": "arXiv",
    "authors": [
      "Zhong",
      "Aditi Raghunathan",
      "Nicholas Carlini",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.20270",
    "githubLink": "https://github.com/Looking-Glass-Lab/ImpossibleBench",
    "itemCount": "Hundreds of mutated coding tasks (derived from LiveCodeBench/SWE-bench)",
    "specs": "Code generation tasks with mutated/conflicting unit tests.",
    "description": "A framework for systematically measuring LLM agents' propensity to exploit test cases ('reward hacking') rather than following specifications. It creates 'impossible' variants of coding tasks from existing benchmarks (like LiveCodeBench and SWE-bench) by mutating unit tests to conflict with the natural language description, such that passing the test requires ignoring the specification."
  },
  {
    "id": "saved-1769608044991-anstb",
    "title": "LiveSecBench",
    "source": "arXiv",
    "authors": [
      "Yudong Li",
      "Zhongliang Yang",
      "Kejiang Chen",
      "Peiru Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.02366",
    "githubLink": "https://livesecbench.intokentech.cn/",
    "itemCount": "Dynamic/Continuous updates",
    "specs": "Text-based safety evaluation questions; Chinese language focus",
    "description": "A dynamic and culturally-relevant safety benchmark for LLMs in the Chinese context. It evaluates models across dimensions including 'Authority/Subversion', public safety, and other sensitive topics defined by local regulations."
  },
  {
    "id": "saved-1769608044991-tci47",
    "title": "Petri",
    "source": "Other",
    "authors": [
      "Kai Fronsdal",
      "Isha Gupta",
      "Abhay Sheshadri",
      "Jonathan Michala",
      "Stephen McAleer",
      "Rowan Wang",
      "Sara Price",
      "Samuel R. Bowman"
    ],
    "year": "2025",
    "paperLink": "https://github.com/safety-research/petri",
    "githubLink": "https://github.com/safety-research/petri",
    "itemCount": "111 seed instructions",
    "specs": "Multi-turn conversation audits; agent-based evaluation",
    "description": "An open-source auditing tool and benchmark for AI safety that tests for advanced risks including oversight subversion, deception, and sycophancy. It uses multi-turn agent interactions to probe model behavior."
  },
  {
    "id": "saved-1769608044991-iq78q",
    "title": "SHADE-Arena (Subtle Harmful Agent Detection & Evaluation)",
    "source": "arXiv",
    "authors": [
      "Jonathan Kutasov",
      "Yuqi Sun",
      "Paul Colognese",
      "Teun van der Weij",
      "Linda Petrini",
      "Chen Bo Calvin Zhang",
      "John Hughes",
      "Xiang Deng",
      "Henry Sleight",
      "Tyler Tracy",
      "Buck Shlegeris",
      "Joe Benton"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.15740",
    "githubLink": "https://github.com/safety-research/SHADE-Arena",
    "itemCount": "17 task pairs (5 public, 12 restricted)",
    "specs": "Text-based agentic environments; includes synthetic data (emails, files) and tool definitions; multi-turn trajectories.",
    "description": "A benchmark dataset designed to evaluate the sabotage and monitoring capabilities of agentic LLMs. It consists of diverse, complex environments where agents must perform a benign main task (e.g., calendar management) while secretly attempting a malicious side task (e.g., exfiltrating data) without detection by a monitor."
  },
  {
    "id": "saved-1769608044991-3b4se",
    "title": "CUA-SHADE-Arena",
    "source": "arXiv",
    "authors": [
      "Neil Kale",
      "Chen Bo Calvin Zhang",
      "Kevin Zhu",
      "Ankit Aich",
      "Paula Rodriguez",
      "Christina Q. Knight",
      "Zifan Wang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2508.19461",
    "githubLink": "https://github.com/scaleapi/mrt",
    "itemCount": "15 tasks",
    "specs": "Computer-use agent environments; involves tool use, screen interactions, and long-horizon planning.",
    "description": "An extension of the SHADE-Arena framework to computer-use agents, built on 'TheAgentCompany' benchmark. It features tasks where agents control a computer interface to perform legitimate work while attempting hidden sabotage or data exfiltration objectives."
  },
  {
    "id": "saved-1769608044991-24a3f",
    "title": "CUAHarm",
    "source": "Hugging Face",
    "authors": [
      "Tian Tian",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/papers/2507.21735",
    "githubLink": "https://github.com/db-ol/CUAHarm",
    "itemCount": "104 tasks",
    "specs": "Sandboxed Environment, Text/Code Interaction; Categories: Credential Theft, Privilege Escalation, Network Exploitation, etc.",
    "description": "A benchmark designed to evaluate the misuse risks of Computer-Using Agents (CUAs). It measures agents' success rates in executing malicious tasks such as disabling firewalls, credential theft, and network exploitation in a sandboxed environment."
  },
  {
    "id": "saved-1769608044991-nnfia",
    "title": "AIRTBench",
    "source": "arXiv",
    "authors": [
      "Ads Dawson",
      "Rob Mulla",
      "Nick Landers",
      "Shane Caldwell"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.10447",
    "githubLink": "https://github.com/dreadnode/AIRTBench-Code",
    "itemCount": "70 challenges",
    "specs": "CTF Challenges (Python code generation); Platform: Dreadnode Crucible",
    "description": "An AI red teaming benchmark evaluating LLMs' ability to autonomously discover and exploit AI/ML security vulnerabilities. It uses black-box Capture-The-Flag (CTF) challenges where models write Python code to interact with and compromise AI systems."
  },
  {
    "id": "saved-1769608044991-iowl8",
    "title": "ExCyTIn-Bench",
    "source": "arXiv",
    "authors": [
      "Anand Mudgerikar",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.08638",
    "githubLink": "https://github.com/microsoft/SecRL",
    "itemCount": "589 questions, 8 attacks",
    "specs": "Text (SQL queries), Log Data; Environment: MySQL Docker container with Microsoft Sentinel logs",
    "description": "The first benchmark to evaluate LLM agents on Cyber Threat Investigation. It provides a realistic environment with 57 log tables and 8 multi-stage attacks, requiring agents to query logs, follow evidence chains, and answer investigation questions."
  },
  {
    "id": "saved-1769608044991-8z1e0",
    "title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?",
    "source": "arXiv",
    "authors": [
      "Yunxiang Zhang",
      "Xiangru Tang",
      "Zefan Cai",
      "Yichi Zhang",
      "Yanjun Shao",
      "Zexuan Deng",
      "Helan Hu",
      "Zengxian Yang",
      "Kaikai An"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.09702",
    "githubLink": "https://github.com/yunx-z/MLRC-Bench",
    "itemCount": "7 tasks",
    "specs": "Tasks adapted from recent ML conference competitions; Evaluates proposal and implementation of novel methods",
    "description": "A benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions, focusing on open research problems that demand novel methodologies. It measures key steps of proposing and implementing novel research methods."
  },
  {
    "id": "saved-1769608044991-m9lky",
    "title": "MHBench (Multi-Host Attack Benchmark)",
    "source": "arXiv",
    "authors": [
      "Brian Singer",
      "Keane Lucas",
      "Lakshmi Adiga",
      "Meghna Jain",
      "Lujo Bauer",
      "Vyas Sekar"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.16466",
    "githubLink": "https://github.com/bsinger98/Incalmo",
    "itemCount": "40 emulated networks",
    "specs": "Emulated networks (22-50 hosts each), Diverse topologies (Star, Chain, Dumbbell), Vulnerabilities (CVEs, misconfigurations)",
    "description": "MHBench is a benchmark suite designed to evaluate the capability of Large Language Models (LLMs) to autonomously execute multi-host network attacks. It consists of realistic emulated network environments (ranging from 10 to 40 distinct scenarios) that mimic real-world enterprise topologies and vulnerabilities, such as those found in the Equifax and Colonial Pipeline breaches. The benchmark is part of the Incalmo project, which introduces a high-level abstraction layer to enable LLMs to perform complex red-teaming operations."
  },
  {
    "id": "saved-1769608044991-6vg4t",
    "title": "WAInjectBench",
    "source": "arXiv",
    "authors": [
      "Yinuo Liu",
      "Ruohan Xu",
      "Xilong Wang",
      "Yuqi Jia",
      "Neil Zhenqiang Gong"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.01354",
    "githubLink": "https://github.com/Norrrrrrr-lyn/WAInjectBench",
    "itemCount": "Unknown (Comprehensive suite)",
    "specs": "Text, Image (Multimodal inputs for web agents)",
    "description": "A comprehensive benchmark for evaluating prompt injection detection in web agents. It includes a fine-grained categorization of attacks and datasets containing both malicious and benign samples across text and image modalities."
  },
  {
    "id": "saved-1769608044991-pu90z",
    "title": "PandaBench",
    "source": "Hugging Face",
    "authors": [
      "Guobin Shen",
      "Dongcheng Zhao",
      "Linghao Feng",
      "Xiang He",
      "Jihang Wang",
      "Sicheng Shen",
      "Haibo Tong",
      "Yiting Dong",
      "Jindong Li",
      "Xiang Zheng",
      "Yi Zeng"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.13862",
    "githubLink": "https://github.com/Beijing-AISI/panda-guard",
    "itemCount": "~3 billion tokens processed across 49 models",
    "specs": "JSON/CSV containing attack-defense interaction traces, ASR scores, and judge evaluations. Inputs based on JBB-Behaviors (100 harmful prompts).",
    "description": "A comprehensive benchmark dataset evaluating Large Language Model (LLM) safety against jailbreaking attacks. It is built upon the PandaGuard framework and captures the interactions between 49 LLMs, 19 attack algorithms (including GCG, AutoDAN, PAIR), and 12 defense mechanisms (such as PerplexityFilter, SmoothLLM). The dataset focuses on the Attack Success Rate (ASR) and includes evaluation traces from multiple judge models."
  },
  {
    "id": "saved-1769608044991-zjuiv",
    "title": "Audio Jailbreak (AJailBench)",
    "source": "arXiv",
    "authors": [
      "Zirui Song",
      "Qian Jiang",
      "Xiuying Chen",
      "Guangke Chen",
      "Fu Song",
      "Weizhe Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.15406",
    "githubLink": "https://github.com/mbzuai-nlp/AudioJailbreak",
    "itemCount": "1,495 base adversarial audio prompts; Extended AJailBench-APT+ dataset available",
    "specs": "Audio (WAV/MP3), Text (JSONL format for prompts/responses), Python 3.10+",
    "description": "A comprehensive benchmark specifically designed to evaluate jailbreak vulnerabilities in Large Audio-Language Models (LAMs). It includes a base dataset of adversarial audio prompts derived from policy-violating text categories and an extended dataset generated using the Audio Perturbation Toolkit (APT) which applies time, frequency, and mixing domain perturbations."
  },
  {
    "id": "saved-1769608044991-xghqd",
    "title": "JALMBench",
    "source": "arXiv",
    "authors": [
      "Zifan Peng",
      "Yule Liu",
      "Xinyi Huang",
      "Yihan Gong",
      "Jingwen Zhang",
      "Rui Zhang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.02535",
    "githubLink": "https://github.com/ZifanPeng/JALMBench",
    "itemCount": "245,355 audio samples (>1,000 hours), 11,316 text samples",
    "specs": "Audio files, Text prompts, Supports 12 ALM architectures",
    "description": "A large-scale benchmark for assessing the safety of Audio Language Models against jailbreak attacks. It standardizes the evaluation of 12 mainstream ALMs using both text-transferred and audio-originated attack methods, as well as defense strategies."
  },
  {
    "id": "saved-1769608044991-6d4iy",
    "title": "Multi-AudioJail",
    "source": "arXiv",
    "authors": [
      "Jaechul Roh",
      "Virat Shejwalkar",
      "Amir Houmansadr"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.01094",
    "githubLink": "https://github.com/sprok/Multi-AudioJail",
    "itemCount": "derived from 520 AdvBench prompts x 5 languages x accent variations",
    "specs": "Multilingual Audio, Text",
    "description": "A framework and dataset designed to exploit multilingual and multi-accent vulnerabilities in Audio LLMs. It contains adversarially perturbed audio prompts across five languages and various accents to demonstrate how linguistic variations amplify attack success."
  },
  {
    "id": "saved-1769608044991-hvpsv",
    "title": "SAFEPATH (Safety Trigger Set)",
    "source": "arXiv",
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Minsuk Kahng",
      "Albert No"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.14667",
    "githubLink": "https://ai-isl.github.io/safepath",
    "itemCount": "Derived from WildJailbreak (approx. 262k samples total, SafePath uses a harmful subset)",
    "specs": "Text (Harmful Prompts, Safety Primers, Reasoning Chains)",
    "description": "A dataset and alignment method designed to prevent harmful reasoning in Large Reasoning Models (LRMs) like DeepSeek-R1. The method fine-tunes models to emit a short 'Safety Primer' (e.g., 8 tokens) at the start of reasoning when encountering harmful prompts. The associated 'Safety Trigger set' is derived from the WildJailbreak dataset, containing harmful or adversarial prompts paired with safety primers to guide the model's Chain-of-Thought (CoT) towards safety without degrading reasoning performance on standard tasks."
  },
  {
    "id": "saved-1769608044991-vbitn",
    "title": "SafePath (Autonomous Navigation Framework)",
    "source": "arXiv",
    "authors": [
      "Achref Doula",
      "Max Mühlhäuser",
      "Alejandro Sanchez Guinea"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.09427",
    "githubLink": "https://github.com/AchrefDoula/SafePath",
    "itemCount": "1,000 driving scenes (nuScenes); Interactive scenarios (Highway-env)",
    "specs": "Multimodal (Text Prompts, Trajectories, Sensor Data)",
    "description": "A modular framework that augments LLM-based path planning for autonomous vehicles with formal safety guarantees using conformal prediction. While 'SafePath' is the method, it establishes a benchmark for safety in LLM-driven navigation by evaluating on the nuScenes and Highway-env datasets. The framework filters high-risk trajectories and guarantees a safe option with user-defined probability, reducing collision rates and planning uncertainty."
  },
  {
    "id": "saved-1769608044991-ujfe5",
    "title": "MindSET",
    "source": "arXiv",
    "authors": [
      "Saad Mankarious",
      "Ayah Zirikly",
      "Daniel Wiechmann",
      "Elma Kerz",
      "Edward Kempa",
      "Yu Qiao"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.20672",
    "githubLink": "https://github.com/fibonacci-2/mindset",
    "itemCount": "13 million annotated posts",
    "specs": "Text (Social Media/Reddit, 7 mental health conditions)",
    "description": "A large-scale mental health benchmark dataset curated from Reddit using self-reported diagnoses. It includes rigorous filtering for NSFW content and duplicates, designed for early risk detection and psychological trend analysis."
  },
  {
    "id": "saved-1769608044991-mwas1",
    "title": "MultiHoax",
    "source": "arXiv",
    "authors": [
      "Mohammadamin Shafiei",
      "Hamidreza Saffari",
      "Nafise Sadat Moosavi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.00264",
    "githubLink": "https://github.com/Mamin78/MHFPQ",
    "itemCount": "700 curated questions",
    "specs": "Multi-hop reasoning questions spanning 7 countries and 10 knowledge categories.",
    "description": "A benchmark for evaluating LLMs' ability to handle false premises in complex, multi-step reasoning tasks. It covers diverse knowledge categories and requires multi-hop inference to detect falsehoods."
  },
  {
    "id": "saved-1769608044991-9fldu",
    "title": "DeceptionBench",
    "source": "Hugging Face",
    "authors": [
      "Jiaming Ji",
      "Wenqi Chen",
      "Kaile Wang",
      "Donghai Hong",
      "Sitong Fang",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Juntao Dai",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.15655",
    "githubLink": "https://huggingface.co/datasets/PKU-Alignment/DeceptionBench",
    "itemCount": "180 scenarios",
    "specs": "Text-based, Chain-of-Thought reasoning evaluation, 5 deception categories",
    "description": "A comprehensive benchmark designed to assess deceptive behaviors in Large Language Models (LLMs), particularly focusing on deceptive alignment where models appear aligned while covertly pursuing misaligned goals. It covers categories like sycophancy, alignment faking, sandbagging, and strategic deception."
  },
  {
    "id": "saved-1769608044992-b1dle",
    "title": "AgentMisalignment",
    "source": "arXiv",
    "authors": [
      "Megan Kinniment",
      "Lucas Jun Koba Sato",
      "Haoxing Du",
      "Brian Goodrich",
      "Max Hasin",
      "Lawrence Chan",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.04018",
    "githubLink": "https://github.com/anthropic-experimental/agentic-misalignment",
    "itemCount": "9 evaluation scenarios",
    "specs": "Agent-based simulation, Text, Dynamic environments",
    "description": "A benchmark suite designed to evaluate the propensity of LLM-based agents to exhibit misaligned behaviors (such as avoiding oversight, resisting shutdown, and sandbagging) in realistic, dynamic scenarios."
  },
  {
    "id": "saved-1769608044992-db12o",
    "title": "MVPBench",
    "source": "arXiv",
    "authors": [
      "Guanzhen Li",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.04944",
    "githubLink": "https://github.com/GuanzhenLi/MVP-Bench",
    "itemCount": "24,020 instances",
    "specs": "Text, Multi-cultural value alignment, 75 countries",
    "description": "A benchmark for evaluating and improving LLM alignment with diverse human value preferences across different cultures and demographics. It focuses on multi-dimensional value alignment beyond standard safety."
  },
  {
    "id": "saved-1769608044992-hr4hp",
    "title": "Misalignment Bounty Submissions",
    "source": "Hugging Face",
    "authors": [
      "Palisade Research"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/palisaderesearch/Misalignment-Bounty-Submissions",
    "githubLink": "https://huggingface.co/datasets/palisaderesearch/Misalignment-Bounty-Submissions",
    "itemCount": "295 submissions",
    "specs": "Text, Prompts, Descriptions, Evaluation results",
    "description": "A dataset containing submissions from an AI Misalignment Bounty event where participants crafted examples of misaligned behavior for advanced models like o3 and GPT-5."
  },
  {
    "id": "saved-1769608044992-kdl4n",
    "title": "ReasoningShield Dataset",
    "source": "Hugging Face",
    "authors": [
      "Juan Ren",
      "M. Dras",
      "Usman Naseem"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.17244",
    "githubLink": "https://huggingface.co/datasets/ReasoningShield/ReasoningShield-Dataset",
    "itemCount": "9,200 samples (7,000 Train, 2,200 Test)",
    "specs": "Text (Query, Chain-of-Thought traces); 10 Risk Categories (e.g., Violence, Hate, Deception); 3 Safety Levels",
    "description": "The first comprehensive dataset designed to train and evaluate models for detecting hidden safety risks within the reasoning traces (Chain-of-Thought) of Large Reasoning Models (LRMs). It addresses the issue where harmful content is embedded in intermediate reasoning steps even if the final answer appears benign. The dataset covers 10 risk categories across 3 safety levels."
  },
  {
    "id": "saved-1769608044992-0jzme",
    "title": "ShieldBench",
    "source": "arXiv",
    "authors": [
      "Mert Ogul",
      "Rishitha Voleti",
      "Shanduojiao Jiang",
      "Kevin Zhu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.02620",
    "githubLink": "https://openreview.net/forum?id=EjO9K6yx8I",
    "itemCount": "N/A (Benchmarking framework using existing safety datasets like HarmBench)",
    "specs": "Text; Safety evaluation under greedy and sampling-based decoding; Weight-space editing techniques",
    "description": "A comprehensive benchmark for evaluating the persistence of LLM safety interventions. It assesses how well safety mechanisms (like weight-space editing) hold up under realistic usage conditions, including various decoding strategies and adversarial attacks. The benchmark is designed to provide insights into the durability of safety measures in open-source models."
  },
  {
    "id": "saved-1769608044992-3m1qe",
    "title": "LUNGUAGE",
    "source": "arXiv",
    "authors": [
      "Jong Hak Moon",
      "Geon Choi",
      "Paloma Rabaey",
      "Min Gwan Kim",
      "Hyuk Gi Hong",
      "Jung-Oh Lee",
      "Eun Woo Doe"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.21190",
    "githubLink": "https://github.com/SuperSupermoon/Lunguage",
    "itemCount": "1,473 reports (80 longitudinal sequences)",
    "specs": "Text (Radiology Reports), Structured Labels, Longitudinal Data",
    "description": "A benchmark for structured and sequential chest X-ray report interpretation. It evaluates the ability of models to generate fine-grained, schema-aligned structured reports and perform longitudinal reasoning (tracking disease progression across multiple visits) using expert-verified annotations."
  },
  {
    "id": "saved-1769608044992-d7f7s",
    "title": "PatientSafetyBench",
    "source": "Hugging Face",
    "authors": [
      "Jean-Philippe Corbeil",
      "Minseon Kim",
      "Maxime Griot",
      "Sheela Agarwal",
      "Alessandro Sordoni",
      "François Beaulieu",
      "Paul Vozila"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.01859",
    "githubLink": "https://huggingface.co/datasets/microsoft/PatientSafetyBench",
    "itemCount": "466 samples",
    "specs": "Text (patient-oriented queries across 5 critical policy categories: Harmful Advice, Misdiagnosis, Unlicensed Practice, etc.)",
    "description": "A patient-focused benchmark designed to evaluate the safety of Large Language Models (LLMs) in the medical domain. It tests critical safety policies to measure how well models avoid harmful, misleading, unlicensed, or discriminatory responses when interacting with non-medical users."
  },
  {
    "id": "saved-1769608044992-0tr5c",
    "title": "OmniSafeBench-MM",
    "source": "arXiv",
    "authors": [
      "Xiaojun Jia",
      "Jie Liao",
      "Qi Guo",
      "Teng Ma",
      "Simeng Qin",
      "Ranjie Duan",
      "Tianlin Li",
      "Yihao Huang",
      "Zhitao Zeng",
      "Dongxian Wu",
      "Yiming Li",
      "Wenqi Ren",
      "Xiaochun Cao",
      "Yang Liu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.06589",
    "githubLink": "https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM",
    "itemCount": "Unknown (Covering 9 domains, 50 categories)",
    "specs": "Text, Image (Multimodal)",
    "description": "A unified benchmark and toolbox for multimodal jailbreak attack-defense evaluation, covering 9 major risk domains and 50 fine-grained categories."
  },
  {
    "id": "saved-1769608044992-w6tjm",
    "title": "WorldModelBench",
    "source": "arXiv",
    "authors": [
      "Li",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2502.28892",
    "githubLink": "https://github.com/WorldModelBench-Team/WorldModelBench",
    "itemCount": "350 condition pairs, 67,000 human annotations",
    "specs": "Video generation, Text/Image-to-Video, Human-aligned annotations",
    "description": "A benchmark designed to evaluate the world modeling capabilities of video generation models in application-driven domains like robotics and autonomous driving. It assesses models on instruction following, physical consistency (e.g., Newton's laws), and commonsense plausibility using a fine-tuned judger model."
  },
  {
    "id": "saved-1769608044992-fysjp",
    "title": "AutumnBench (WorldTest)",
    "source": "arXiv",
    "authors": [
      "Archana Warrier",
      "Thanh Dat Nguyen",
      "Michelangelo Naim",
      "Moksh Jain",
      "Yichao Liang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.19788",
    "githubLink": "https://basis.ai",
    "itemCount": "43 environments, 129 tasks",
    "specs": "Grid-world environments, Interactive tasks, Cognitive science protocol",
    "description": "An interactive benchmark for evaluating world-model learning in both AI agents and humans. It features 43 grid-world environments and 129 tasks requiring masked-frame prediction, planning, and causal dynamics change detection, emphasizing reward-free exploration."
  },
  {
    "id": "saved-1769608044992-32ukj",
    "title": "DriveLMM-o1",
    "source": "arXiv",
    "authors": [
      "Ayesha Ishaq",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.10621",
    "githubLink": "https://github.com/ayesha-ishaq/DriveLMM-o1",
    "itemCount": "18,000+ VQA examples (Train), 4,000+ (Test)",
    "specs": "Visual Question Answering (VQA), Multi-view images, LiDAR, Step-by-step reasoning",
    "description": "A dataset and benchmark for step-by-step visual reasoning in autonomous driving world models. It evaluates perception, prediction, and planning capabilities through logical inference chains rather than just final answers."
  },
  {
    "id": "saved-1769608044992-flw2x",
    "title": "VBench-2.0",
    "source": "arXiv",
    "authors": [
      "Ziqi Huang",
      "Yinan He",
      "Jiashuo Yu",
      "Fan Zhang",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2503.27300",
    "githubLink": "https://github.com/Vchitect/VBench",
    "itemCount": "Comprehensive prompt suite and evaluation dimensions",
    "specs": "Video generation evaluation, Physics/Commonsense metrics, Human preference alignment",
    "description": "An advanced benchmark suite for video generation models focusing on 'Intrinsic Faithfulness' to serve as true world models. It evaluates dimensions such as physics adherence, commonsense reasoning, human fidelity, and controllability."
  },
  {
    "id": "saved-1769608044992-8wlso",
    "title": "WorldScore",
    "source": "Other",
    "authors": [
      "Haoyi Duan",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://github.com/haoyi-duan/WorldScore",
    "githubLink": "https://github.com/haoyi-duan/WorldScore",
    "itemCount": "Multiple evaluation scenarios",
    "specs": "3D/4D Scene Generation, Video Generation, Consistency metrics",
    "description": "A unified evaluation benchmark for world generation models (3D/4D scenes and video). It measures the consistency and quality of generated worlds, differentiating between superficial visual quality and structural world coherence."
  },
  {
    "id": "saved-1769608044992-69sty",
    "title": "PAI-Bench (Physical AI Bench)",
    "source": "Hugging Face",
    "authors": [
      "Fengzhe Zhou",
      "Jiannan Huang",
      "Jialuo Li",
      "Deva Ramanan",
      "Humphrey Shi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.01989",
    "githubLink": "https://github.com/shi-labs/physical-ai-bench-understanding",
    "itemCount": "2,808 cases (Understanding), 600 examples (Generation)",
    "specs": "Video, Text; Categories include Robotic Arm Operations, Autonomous Driving, Ego-centric Life",
    "description": "A comprehensive benchmark designed to evaluate perception and prediction capabilities across video generation, conditional video generation, and video understanding. It comprises real-world cases with task-aligned metrics to capture physical plausibility and domain-specific reasoning."
  },
  {
    "id": "saved-1769608044992-74cjq",
    "title": "PhysicalAI-Autonomous-Vehicles Dataset",
    "source": "Hugging Face",
    "authors": [
      "NVIDIA"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles",
    "githubLink": "https://github.com/NVlabs/physical_ai_av",
    "itemCount": "310,895 clips (20s each), 1,727 hours total",
    "specs": "Multi-camera, LiDAR, Radar; 20-second clips",
    "description": "A large-scale, geographically diverse collection of multi-sensor data for building reasoning-based end-to-end autonomous driving systems. It features driving data from 25 countries and over 2,500 cities."
  },
  {
    "id": "saved-1769608044992-cuse8",
    "title": "PhyGenBench",
    "source": "arXiv",
    "authors": [
      "Fanqing Meng",
      "Jiaqi Liao",
      "Xinyu Tan",
      "Quanfeng Lu",
      "Wenqi Shao",
      "Kaipeng Zhang",
      "Yu Cheng",
      "Dianqi Li",
      "Ping Luo"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2410.05363",
    "githubLink": "https://github.com/OpenGVLab/PhyGenBench",
    "itemCount": "160 prompts",
    "specs": "Text-to-Video prompts; 27 distinct physical laws across 4 domains",
    "description": "A benchmark designed to evaluate physical commonsense correctness in text-to-video (T2V) generation. It tests models on their adherence to physical laws across various domains like mechanics and thermodynamics."
  },
  {
    "id": "saved-1769608044992-5sxg0",
    "title": "EmbodiedBench",
    "source": "Scholar",
    "authors": [
      "Rui Yang",
      "Hanyang Chen",
      "Junyu Zhang",
      "Mark Zhao",
      "Cheng Qian",
      "Kangrui Wang",
      "Qineng Wang",
      "Teja Venkat Koripella",
      "Marziyeh Movahedi",
      "Manling Li",
      "Heng Ji",
      "Huan Zhang",
      "Tong Zhang"
    ],
    "year": "2025",
    "paperLink": "https://embodied-bench.github.io/",
    "githubLink": "https://github.com/Embodied-Bench/EmbodiedBench",
    "itemCount": "1,128 testing instances",
    "specs": "Multi-modal (Vision, Language, Action); 4 environments: ALFRED, Habitat, Navigation, Manipulation",
    "description": "A comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents. It assesses capabilities across both high-level task decomposition and low-level control."
  },
  {
    "id": "saved-1769608044992-72w81",
    "title": "PHYSICS Dataset",
    "source": "arXiv",
    "authors": [
      "Shenghe Zheng",
      "Qianjia Cheng",
      "Junchi Yao",
      "Mengsong Wu",
      "Haonan He",
      "Ning Ding",
      "Yu Cheng",
      "Shuyue Hu",
      "Lei Bai",
      "Dongzhan Zhou",
      "Ganqu Cui",
      "Peng Ye"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2506.02705",
    "githubLink": "https://github.com/Physics-Reasoning/PHYSICS-Dataset",
    "itemCount": "16,568 problems",
    "specs": "Text (Physics problems); 5 domains: Mechanics, Electromagnetism, Thermodynamics, Optics, Modern Physics",
    "description": "A dataset containing high-quality physics problems spanning multiple subjects and difficulty levels, designed to facilitate physical reasoning in Large Language Models."
  },
  {
    "id": "saved-1769608044992-lshn7",
    "title": "PhysicalAI-Robotics-NuRec",
    "source": "Hugging Face",
    "authors": [
      "Di Zeng",
      "Chirag Majithia",
      "Harel Omer",
      "Sameer Chavan",
      "Isaac Deutsch",
      "Weihan Wang"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-NuRec",
    "githubLink": "https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-NuRec",
    "itemCount": "Various scenes (e.g., Nova Carter, Zurich Office)",
    "specs": "USD (Universal Scene Description), Mesh, Occupancy Maps, 3DGUT",
    "description": "A robotics dataset containing various 3DGUT (3D Gaussian Splatting) USD files, meshes, and occupancy maps intended for use in NVIDIA Isaac Sim for physical AI simulation and training."
  },
  {
    "id": "saved-1769608044992-d770z",
    "title": "TOP-Bench",
    "source": "arXiv",
    "authors": [
      "Yuxuan Qiao",
      "Dongqin Liu",
      "Hongchang Yang",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.11583",
    "githubLink": "https://github.com/iie-cas/TOP-Bench",
    "itemCount": "N/A",
    "specs": "Textual scenarios, Paired leakage/benign instances",
    "description": "A specialized evaluation benchmark designed to systematically evaluate and quantify the Tools Orchestration Privacy Risk (TOP-R) in single-agent, multi-tool architectures. It includes paired leakage and benign scenarios to assess the trade-off between helpfulness and privacy preservation."
  },
  {
    "id": "saved-1769608044992-xct23",
    "title": "AgentArch",
    "source": "arXiv",
    "authors": [
      "Tara Bogavelli",
      "ServiceNow Research"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2509.10769",
    "githubLink": "https://github.com/ServiceNow/AgentArch",
    "itemCount": "18 configurations, Multiple enterprise workflows",
    "specs": "Enterprise workflow scenarios, JSON tool responses, Multi-turn interactions",
    "description": "A comprehensive enterprise-specific benchmark evaluating 18 distinct agentic configurations across state-of-the-art large language models. It examines four critical agentic system dimensions: orchestration strategy, agent prompt implementation, memory architecture, and thinking tool integration."
  },
  {
    "id": "saved-1769608044992-bxd9z",
    "title": "Auto-SLURP",
    "source": "arXiv",
    "authors": [
      "Lei Shen",
      "Xiaoyu Shen"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.18373",
    "githubLink": "https://github.com/lorashen/Auto-SLURP",
    "itemCount": "Derived from SLURP (multiple scenarios)",
    "specs": "Text, API calls, Simulated server interactions",
    "description": "A benchmark dataset tailored to evaluate LLM-based multi-agent frameworks in the context of intelligent personal assistants. It extends the SLURP dataset by integrating simulated servers and external services to test end-to-end orchestration, language understanding, and task execution."
  },
  {
    "id": "saved-1769608044992-y0vxp",
    "title": "REALM-Bench",
    "source": "arXiv",
    "authors": [
      "Longling Geng",
      "Edward Y. Chang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2501.00000",
    "githubLink": "https://github.com/genglongling/REALM-Bench",
    "itemCount": "14 problem specifications",
    "specs": "Planning/Scheduling tasks, Multi-agent coordination metrics",
    "description": "A benchmark suite for assessing multi-agent systems in real-world planning and scheduling scenarios. It incorporates multi-agent coordination, inter-agent dependencies, and dynamic environmental disruptions across 14 designed planning problems."
  },
  {
    "id": "saved-1769608044992-iqgr3",
    "title": "CyberTeam: Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting",
    "source": "arXiv",
    "authors": [
      "Xiaoqun Liu",
      "Feiyang Yu",
      "Xi Li",
      "Guanhua Yan",
      "Ping Yang",
      "Zhaohan Xi"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.11901",
    "githubLink": "https://github.com/mengyuqiao/LLM-Cyberteam",
    "itemCount": "30 tasks, 9 embodied functions",
    "specs": "Threat hunting workflow tasks, embodied environment integration",
    "description": "A benchmark designed to guide and evaluate LLMs in blue team threat hunting practices. It models realistic threat-hunting workflows by capturing dependencies among analytical tasks from threat attribution to incident response, using embodied functions."
  },
  {
    "id": "saved-1769608044992-em9kv",
    "title": "Purple Team Cybersecurity Dataset",
    "source": "Hugging Face",
    "authors": [
      "Canstralian (Hugging Face User)"
    ],
    "year": "2025",
    "paperLink": "https://huggingface.co/datasets/Canstralian/Purple-Team-Cybersecurity-Dataset",
    "githubLink": "https://huggingface.co/datasets/Canstralian/Purple-Team-Cybersecurity-Dataset",
    "itemCount": "Comprehensive synthetic events",
    "specs": "Structured logs including attack events, defense responses, system logs, and network traffic metrics",
    "description": "A synthetic collection designed to simulate collaborative cybersecurity exercises, integrating offensive (Red Team) and defensive (Blue Team) strategies. It includes records of attack events, defense responses, system logs, and network traffic."
  },
  {
    "id": "saved-1769608044992-7wuzr",
    "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
    "source": "Scholar",
    "authors": [
      "Andy K Zhang",
      "Neil Perry",
      "Riya Dulepet",
      "Joey Ji",
      "Celeste Menders",
      "Justin W Lin"
    ],
    "year": "2025",
    "paperLink": "https://openreview.net/forum?id=tc90LV0yRL",
    "githubLink": "https://github.com/stanford-crfm/cybench",
    "itemCount": "40 CTF tasks",
    "specs": "CTF challenges with subtasks",
    "description": "A benchmark for evaluating the cybersecurity capabilities and risks of language models, utilizing professional-level Capture the Flag (CTF) tasks."
  },
  {
    "id": "saved-1769608044992-smniu",
    "title": "AISBench (AI Scientist Benchmark)",
    "source": "Other",
    "authors": [
      "Luo et al."
    ],
    "year": "2025",
    "paperLink": "https://github.com/EperLuo/BaisBench",
    "githubLink": "https://github.com/EperLuo/BaisBench",
    "itemCount": "31 expert-labeled datasets, 198 MCQs",
    "specs": "Single-cell transcriptomic data (scRNA-seq), Cell type annotation, Scientific discovery MCQs",
    "description": "A benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge, specifically in the domain of single-cell biology."
  },
  {
    "id": "saved-1769608044992-vly9e",
    "title": "ChemBench",
    "source": "Hugging Face",
    "authors": [
      "Adrian Mirza",
      "Kevin Maik Jablonka",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://www.nature.com/articles/s41557-025-01815-x",
    "githubLink": "https://github.com/jablonkagroup/ChemBench",
    "itemCount": ">2,700 questions (Hugging Face), >6,000 total",
    "specs": "Text, SMILES, Chemical Reasoning Questions",
    "description": "A comprehensive benchmark designed to assess the chemical knowledge and reasoning capabilities of Large Language Models against the expertise of chemists, covering various tasks in chemistry and materials science."
  },
  {
    "id": "saved-1769608044992-sduem",
    "title": "LLM-SRBench",
    "source": "Hugging Face",
    "authors": [
      "Unknown (ICML 2025 Oral)"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.10415",
    "githubLink": "https://huggingface.co/datasets/nnheui/llm-srbench",
    "itemCount": "239 problems",
    "specs": "Numerical datasets, Scientific contexts, Equation discovery tasks",
    "description": "A benchmark for scientific equation discovery designed to evaluate LLMs' ability to reason over data rather than recall memorized equations. It includes transformed physical models and synthetic discovery-driven problems."
  },
  {
    "id": "saved-1769608044992-urw30",
    "title": "BioBench",
    "source": "arXiv",
    "authors": [
      "Samuel Stevens",
      "Jianyang Gu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2511.16315",
    "githubLink": "https://github.com/samuelstevens/biobench",
    "itemCount": "3.1M images, 9 tasks",
    "specs": "Images (diverse modalities: drone, micrographs, camera traps), Classification tasks",
    "description": "A computer vision benchmark for evolutionary biology and ecology, designed to evaluate models on realistic biology-related vision tasks beyond standard ImageNet classification."
  },
  {
    "id": "saved-1769608044992-vlaal",
    "title": "SPOT",
    "source": "arXiv",
    "authors": [
      "Guijin Son",
      "Jiwoo Hong",
      "Honglu Fan",
      "Heejeong Nam",
      "Hyunwoo Ko",
      "Seungwon Lim",
      "Jinyeop Song",
      "Jinha Choi",
      "Gonçalo Paulo",
      "Youngjae Yu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.11855",
    "githubLink": "https://github.com/loubbrad/aria-midi",
    "itemCount": "83 manuscripts, 91 errors",
    "specs": "Multimodal (Text + Images), Manuscript verification tasks",
    "description": "A benchmark for automated verification of scientific research, containing published papers paired with significant errors (errata/retractions) to test LLMs' ability to act as reliable verifiers."
  },
  {
    "id": "saved-1769608044992-a1j5k",
    "title": "Human Robot Social Interaction (HSRI) Dataset",
    "source": "arXiv",
    "authors": [
      "Dong Won Lee",
      "Yubin Kim",
      "Parker Malachowsky",
      "Sooyeon Jeong",
      "Denison Guvenoz",
      "Louis-philippe Morency",
      "Cynthia Breazeal",
      "Hae Won Park"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2504.03000",
    "githubLink": "https://github.com/mitmedialab/HSRI-Dataset",
    "itemCount": "440 videos, 10,000+ annotations",
    "specs": "Video, text annotations (social errors, competencies, rationale)",
    "description": "A large-scale real-world dataset to benchmark the capabilities of foundational models (LMs/FMs) to identify and reason about social errors and competencies in human-robot interactions."
  },
  {
    "id": "saved-1769608044992-twwsn",
    "title": "MCP-Atlas",
    "source": "Hugging Face",
    "authors": [
      "Chaithanya Bandi",
      "Ben Hertzberg",
      "Geobio Boo",
      "Tejas Polakam",
      "Jeff Da",
      "Scale AI Research Team"
    ],
    "year": "2025",
    "paperLink": "https://static.scale.com/uploads/674f4cc7a74e35bcaae1c29a/MCP_Atlas.pdf",
    "githubLink": "https://github.com/scaleapi/mcp-atlas",
    "itemCount": "1,000 tasks (500 public)",
    "specs": "36 MCP servers, 220 tools, 3-6 tool calls per task",
    "description": "A large-scale benchmark for evaluating tool-use competency, comprising tasks that require agents to identify and orchestrate tool calls across multiple servers. It uses a claims-based scoring rubric."
  },
  {
    "id": "saved-1769608044992-ovkmw",
    "title": "MCPZoo",
    "source": "arXiv",
    "authors": [
      "Mengying Wu",
      "Pei Chen",
      "Geng Hong",
      "Baichao An",
      "Jinsong Chen",
      "Binwang Wan",
      "Xudong Pan",
      "Jiarun Dai",
      "Min Yang"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2512.15144",
    "githubLink": "https://github.com/MCPZoo/MCPZoo",
    "itemCount": "129,059 servers (16,356 runnable)",
    "specs": "Large-scale server metadata and source code (399 GB)",
    "description": "A large-scale dataset of Model Context Protocol servers collected from public sources. It includes metadata and a subset of verified runnable server instances to support research on MCP-based systems and security."
  },
  {
    "id": "saved-1769608044992-ng4bm",
    "title": "MCP Security Bench (MSB)",
    "source": "arXiv",
    "authors": [
      "Dongsen Zhang",
      "Zekun Li",
      "Xu Luo",
      "Xuannan Liu",
      "Peipei Li",
      "Wenjun Xu"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2510.15994",
    "githubLink": "https://github.com/MCP-Security-Bench/MSB",
    "itemCount": "2,000 attack instances",
    "specs": "12 attack types, 10 domains, 405 tools",
    "description": "An end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks (e.g., name collision, prompt injection) throughout the tool-use pipeline."
  },
  {
    "id": "saved-1769608044992-vd95m",
    "title": "BioProBench: A Benchmark for Biological Protocol Comprehension",
    "source": "arXiv",
    "authors": [
      "Yuyang Ding",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.20786",
    "githubLink": "https://github.com/YuyangSunshine/bioprotocolbench",
    "itemCount": "550,000+ task instances",
    "specs": "Text-based; 27,000 distinct protocols; 5 core tasks; Hugging Face dataset",
    "description": "A comprehensive benchmark specifically designed for biological protocol understanding and reasoning. It assesses Large Language Models (LLMs) on tasks such as protocol question answering, step ordering, error correction, and protocol generation."
  },
  {
    "id": "saved-1769608044992-n8fmt",
    "title": "AutoBio",
    "source": "Hugging Face",
    "authors": [
      "Songming Liu",
      "Lingxuan Wu",
      "Bangguo Li",
      "et al."
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2505.12345",
    "githubLink": "https://huggingface.co/autobio-bench",
    "itemCount": "792,000 frames",
    "specs": "Robotic manipulation trajectories; Simulation data (MuJoCo/Blender); 100 clean & 400 randomized trajectories per task",
    "description": "A simulation and benchmark for robotic automation in digital biology laboratories. It evaluates robotic manipulation policies on biologically grounded tasks (e.g., micropipetting, thermal mixing) using a physics-based simulation environment."
  },
  {
    "id": "saved-1769608044992-oyvsa",
    "title": "SoftManipulator Sim2Real Dataset",
    "source": "Hugging Face",
    "authors": [
      "Tae-Hyun Hong",
      "Byung-Hyun Song",
      "Yong-Lae Park",
      "Joo-Haeng Lee"
    ],
    "year": "2025",
    "paperLink": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.202500696",
    "githubLink": "https://huggingface.co/datasets/Ndolphin/SoftManipulator_sim2real",
    "itemCount": "~200,000 samples",
    "specs": "CSV files (Motion capture, pressure mappings, SOFA FEM outputs)",
    "description": "Experimental and simulation data for a 3-actuator pneumatic soft manipulator, created to bridge the gap between high-fidelity FEM simulations (SOFA) and real-world physics for surrogate model training."
  },
  {
    "id": "saved-1769608044992-0mixj",
    "title": "Stable-Sim2Real",
    "source": "arXiv",
    "authors": [
      "Mutian Xu",
      "Chongjie Ye",
      "Haolin Liu",
      "Yushuang Wu",
      "Jiahao Chang",
      "Xiaoguang Han"
    ],
    "year": "2025",
    "paperLink": "https://arxiv.org/abs/2507.23483",
    "githubLink": "https://mutianxu.github.io/stable-sim2real/",
    "itemCount": "Utilizes LASA dataset (10,412 CAD models)",
    "specs": "Synthetic-real paired depth data, 3D CAD models",
    "description": "A benchmark scheme and method for evaluating 3D data simulation, focusing on generating realistic depth maps from synthetic data using a two-stage depth diffusion model to bridge the sim-to-real gap."
  },
  {
    "id": "saved-1769608044992-5uy6o",
    "title": "MM-SafetyBench",
    "source": "arXiv",
    "authors": [
      "Xin Liu",
      "Yichen Zhu",
      "Jindong Gu",
      "Yunshi Lan",
      "Chao Yang",
      "Yu Qiao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2311.17600",
    "githubLink": "https://github.com/isXinLiu/MM-SafetyBench",
    "itemCount": "5,040 text-image pairs",
    "specs": "Multimodal (Text + Image), 13 scenarios",
    "description": "A benchmark for conducting safety-critical evaluations of Multimodal Large Language Models (MLLMs) against image-based manipulations and query-relevant images."
  },
  {
    "id": "saved-1769608044992-a7ub5",
    "title": "AgentHarm",
    "source": "Hugging Face",
    "authors": [
      "Maksym Andriushchenko",
      "Alexandra Souly",
      "Mateusz Dziemian",
      "Derek Duenas",
      "Maxwell Lin",
      "Justin Wang",
      "Dan Hendrycks",
      "Andy Zou",
      "Zico Kolter",
      "Matt Fredrikson"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.09024",
    "githubLink": "https://github.com/UKGovernmentBEIS/inspect_evals",
    "itemCount": "110 tasks (440 augmented)",
    "specs": "Agent tasks (Text/Tools), 11 harm categories",
    "description": "A benchmark for measuring the harmfulness of LLM agents, consisting of diverse malicious agent tasks covering categories like fraud and cybercrime."
  },
  {
    "id": "saved-1769608044992-o2m4p",
    "title": "ScienceAgentBench",
    "source": "arXiv",
    "authors": [
      "Ziru Chen",
      "Shijie Chen",
      "Yuting Ning",
      "Qianheng Zhang",
      "Boshi Wang",
      "Botao Yu",
      "Yifei Li",
      "Zeyi Liao",
      "Chen Wei",
      "Zitong Lu",
      "Vishal Sharma",
      "Jiawei Han",
      "Dawn Song",
      "Yu Su",
      "Huan Sun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.05080",
    "githubLink": "https://github.com/InfiAgent/ScienceAgentBench",
    "itemCount": "102 tasks",
    "specs": "Scientific data analysis, Python code generation, 44 peer-reviewed publications",
    "description": "A benchmark specifically designed to evaluate language agents for data-driven scientific discovery. It includes tasks extracted from peer-reviewed publications across multiple disciplines, validated by subject matter experts."
  },
  {
    "id": "saved-1769608044992-jchh5",
    "title": "OSWorld",
    "source": "arXiv",
    "authors": [
      "Tianbao Xie",
      "Danyang Zhang",
      "Jixuan Chen",
      "Xiaochuan Li",
      "Siheng Zhao",
      "Ruisheng Cao",
      "Toh Jing Hua",
      "Zhoujun Cheng",
      "Dongchan Shin",
      "Fangyu Lei",
      "Yitao Liu",
      "Yiheng Xu",
      "Shuyan Zhou",
      "Silvio Savarese",
      "Caiming Xiong",
      "Victor Zhong",
      "Tao Yu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.07972",
    "githubLink": "https://github.com/xlang-ai/OSWorld",
    "itemCount": "369 computer tasks",
    "specs": "Multimodal (Visual/Text), Real OS environments (Ubuntu/Windows/macOS), Desktop applications",
    "description": "A scalable, real computer environment for multimodal agents, supporting task setup and execution-based evaluation across operating systems (Ubuntu, Windows, macOS). It benchmarks open-ended computer tasks involving arbitrary applications."
  },
  {
    "id": "saved-1769608044992-1mty8",
    "title": "SMILECHAT",
    "source": "arXiv",
    "authors": [
      "Huachuan Qiu",
      "Hongliang He",
      "Shuai Zhang",
      "Anqi Li",
      "Zhenzhong Lan"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.findings-emnlp.34/",
    "githubLink": "https://github.com/qiuhuachuan/smile",
    "itemCount": "55,165 dialogues",
    "specs": "Text (Chinese), Multi-turn Dialogues (Synthetic/Augmented)",
    "description": "A large-scale, multi-turn dialogue dataset for mental health support, generated by expanding single-turn dialogues using ChatGPT (via the SMILE method). It aims to address the scarcity of multi-turn counseling data while maintaining diversity and quality."
  },
  {
    "id": "saved-1769608044992-82c6x",
    "title": "HarmBench",
    "source": "arXiv",
    "authors": [
      "Mantas Mazeika",
      "Long Phan",
      "Xuwang Yin",
      "Andy Zou",
      "Zifan Wang",
      "Norman Mu",
      "Elham Sakhaee",
      "Nathaniel Li",
      "Steven Basart",
      "Bo Li",
      "David Forsyth",
      "Dan Hendrycks"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.04249",
    "githubLink": "https://github.com/centerforaisafety/HarmBench",
    "itemCount": "510 harmful behaviors (functional categories)",
    "specs": "Text; Functional and Semantic Categories",
    "description": "A standardized evaluation framework for automated red teaming, systematically designed to meet criteria of breadth, comparability, and robust metrics. It includes a validation set of behaviors and methods for automated attacks."
  },
  {
    "id": "saved-1769608044992-gtpr1",
    "title": "RTVLM (Red Teaming Visual Language Models)",
    "source": "arXiv",
    "authors": [
      "Mukai Li",
      "Lei Li",
      "Yuwei Yin",
      "Masood Ahmed",
      "Zhenguang Liu",
      "Qi Liu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.12915",
    "githubLink": "https://github.com/MMInstruction/RedTeamingVLM",
    "itemCount": "10,200 samples",
    "specs": "Multimodal (Image + Text); 10 subtasks",
    "description": "The first red teaming dataset specifically for benchmarking Visual Language Models (VLMs) across aspects like faithfulness, privacy, safety, and fairness."
  },
  {
    "id": "saved-1769608044992-vrv8r",
    "title": "JailbreakBench",
    "source": "arXiv",
    "authors": [
      "Patrick Chao",
      "Edoardo Debenedetti",
      "Alexander Robey",
      "Maksym Andriushchenko",
      "Francesco Croce",
      "Vikash Sehwag",
      "Edgar Dobriban",
      "Nicolas Flammarion",
      "George J. Pappas",
      "Florian Tramèr",
      "Hamed Hassani",
      "Eric Wong"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.01318",
    "githubLink": "https://github.com/JailbreakBench/jailbreakbench",
    "itemCount": "100 misuse behaviors; 100 benign behaviors",
    "specs": "Text; Misuse Behaviors",
    "description": "An open robustness benchmark for jailbreaking LLMs, featuring a repository of artifacts, a dataset of misuse behaviors (JBB-Behaviors), and a standardized evaluation framework."
  },
  {
    "id": "saved-1769608044992-dwjbu",
    "title": "ALERT",
    "source": "arXiv",
    "authors": [
      "Simone Tedeschi",
      "Felix Friedrich",
      "Patrick Schramowski",
      "Kristian Kersting",
      "Roberto Navigli",
      "Huu Nguyen",
      "Bo Li"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.08676",
    "githubLink": "https://github.com/Babelscape/ALERT",
    "itemCount": "~45,000 instructions (15k standard, 30k adversarial)",
    "specs": "Text; Fine-grained Risk Taxonomy",
    "description": "A comprehensive benchmark for assessing LLM safety through red teaming, utilizing a fine-grained risk taxonomy to identify vulnerabilities and inform improvements."
  },
  {
    "id": "saved-1769608044992-uxdlq",
    "title": "PrimeVul",
    "source": "arXiv",
    "authors": [
      "Duy-Tai Nguyen",
      "Tung-Lam Vu",
      "Luan-Thanh Nguyen",
      "Hieu Dinh Vo",
      "C. Nguyen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.18182",
    "githubLink": "https://github.com/DLVulDet/PrimeVul",
    "itemCount": "~7k vulnerable functions, ~229k benign functions",
    "specs": "C/C++ code, JSONL format",
    "description": "A dataset for vulnerability detection that combines and reconstructs existing datasets with accurate labels and chronological splits to minimize data contamination. It aims to evaluate code language models in realistic settings."
  },
  {
    "id": "saved-1769608044992-2ejef",
    "title": "KMMLU (Korean Massive Multitask Language Understanding)",
    "source": "arXiv",
    "authors": [
      "Guijin Son",
      "Hanwool Lee",
      "Sungdong Kim",
      "Seungone Kim",
      "Niklas Muennighoff",
      "Taekyoon Choi",
      "Cheonbok Park",
      "Kang Min Yoo",
      "Stella Biderman"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.11548",
    "githubLink": "https://huggingface.co/datasets/HAERAE-HUB/KMMLU",
    "itemCount": "35,030 questions",
    "specs": "Text, Multiple Choice Questions (45 subjects)",
    "description": "A massive multitask language understanding benchmark for Korean, consisting of expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. It is designed to capture Korean linguistic and cultural aspects."
  },
  {
    "id": "saved-1769608044992-i0una",
    "title": "KoBBQ (Korean Bias Benchmark for Question Answering)",
    "source": "Scholar",
    "authors": [
      "Jiho Jin",
      "Jiseon Kim",
      "Nayeon Lee",
      "Haneul Yoo",
      "Alice Oh",
      "Hwaran Lee"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.tacl-1.28/",
    "githubLink": "https://github.com/naver-ai/kobbq",
    "itemCount": "76,048 samples (268 templates)",
    "specs": "Text, Question Answering (Bias Evaluation)",
    "description": "A benchmark dataset for measuring social bias in Korean language models. It adapts the BBQ framework to the Korean cultural context, categorizing samples into Simply-Transferred, Target-Modified, and Newly-Created to reflect specific Korean stereotypes and biases."
  },
  {
    "id": "saved-1769608044992-858sr",
    "title": "HAE-RAE Bench",
    "source": "arXiv",
    "authors": [
      "Guijin Son",
      "Hanwool Lee",
      "Suwan Kim",
      "Huiseo Kim",
      "Jaecheol Lee",
      "Je Won Yeom",
      "Jihyu Jung",
      "Jung Woo Kim",
      "Songseong Kim"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2309.02706",
    "githubLink": "https://github.com/HAE-RAE/HAE-RAE-BENCH",
    "itemCount": "1,538 questions",
    "specs": "Text, Multiple Choice (Cultural/General Knowledge)",
    "description": "A benchmark designed to evaluate Korean knowledge in language models, focusing on cultural and linguistic nuances that are often lost in translation-based benchmarks. It covers domains like vocabulary, history, general knowledge, and reading comprehension."
  },
  {
    "id": "saved-1769608044992-46i7b",
    "title": "CLIcK (Cultural and Linguistic Intelligence in Korean)",
    "source": "arXiv",
    "authors": [
      "Eunsu Kim",
      "Juyoung Suk",
      "Philhoon Oh",
      "Haneul Yoo",
      "James Thorne",
      "Alice Oh"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.06412",
    "githubLink": "https://github.com/rladmstn1714/CLIcK",
    "itemCount": "1,995 QA pairs",
    "specs": "Text, Question Answering (Cultural/Linguistic)",
    "description": "A benchmark dataset designed to evaluate LLMs on Korean cultural and linguistic intelligence. It comprises QA pairs sourced from official Korean exams and textbooks, categorized into language and culture domains."
  },
  {
    "id": "saved-1769608044992-no4e2",
    "title": "MMLU-Pro",
    "source": "arXiv",
    "authors": [
      "Yubo Wang",
      "Xueguang Ma",
      "Ge Zhang",
      "Yuansheng Ni",
      "Abhranil Chandra",
      "Shiguang Guo",
      "Weiming Ren",
      "Aaran Arulraj",
      "Xuan He",
      "Zhaowei Jiang",
      "Tianle Li",
      "Max Ku",
      "Kaijie Zhu",
      "Alex Zhuang",
      "Rongqi Fan",
      "Xiang Yue",
      "Wenhu Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.01574",
    "githubLink": "https://github.com/TIGER-Lab/MMLU-Pro",
    "itemCount": "12,032 questions",
    "specs": "Multiple-choice (10 options), Text, 14 domains",
    "description": "An enhanced dataset designed to extend the MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options to reduce random guessing."
  },
  {
    "id": "saved-1769608044992-t9r1d",
    "title": "MMLU-Redux",
    "source": "arXiv",
    "authors": [
      "Aryo Pradipta Gema",
      "Joshua Ong",
      "Jun Leang",
      "Giwon Hong",
      "Alessio Devoto",
      "Alberto Carlo Maria Mancino",
      "Rohit Saxena",
      "Xuanli He",
      "Yu Zhao",
      "Xiaotang Du",
      "Mohammad Reza Ghasemi Madani",
      "Claire Barale",
      "Robert McHardy",
      "Joshua Harris",
      "Jean Kaddour",
      "Emile van Krieken",
      "Pasquale Minervini"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.04127",
    "githubLink": "https://github.com/edinburgh-dawg/mmlu-redux",
    "itemCount": "5,700 questions (Redux 2.0)",
    "specs": "Text, Re-annotated subset covering 57 subjects",
    "description": "A manually re-annotated subset of the MMLU dataset aimed at identifying and fixing errors (such as wrong ground truths) in the original benchmark to ensure more reliable model evaluation."
  },
  {
    "id": "saved-1769608044992-08klf",
    "title": "ArabicMMLU",
    "source": "arXiv",
    "authors": [
      "Fajri Koto",
      "Haonan Li",
      "Sara Shatnawi",
      "Jad Doughman",
      "Abdelrahman Boda Sadallah",
      "Aisha Alraeesi",
      "Khalid Almubarak",
      "Zaid Alyafeai",
      "Neha Sengupta",
      "Shady Shehata",
      "Nizar Habash",
      "Preslav Nakov",
      "Timothy Baldwin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.12840",
    "githubLink": "https://github.com/mbzuai-nlp/ArabicMMLU",
    "itemCount": "14,575 questions",
    "specs": "Arabic (MSA), Multiple-choice, Text, 40 tasks",
    "description": "The first multi-task language understanding benchmark for the Arabic language, sourced from school exams across diverse educational levels in North Africa, the Levant, and the Gulf regions."
  },
  {
    "id": "saved-1769608044992-topd5",
    "title": "OmniBench",
    "source": "arXiv",
    "authors": [
      "Yizhi Li",
      "Ge Zhang",
      "Yinghao Ma",
      "Ruibin Yuan",
      "Kang Zhu",
      "Hangyu Guo",
      "Yiming Liang",
      "Jiaheng Liu",
      "Jian Yang",
      "Siwei Wu",
      "Xingwei Qu",
      "Jinjie Shi",
      "Xinyue Zhang",
      "Zhenzhu Yang",
      "Xiangzhou Wang",
      "Zhaoxiang Zhang",
      "Zachary Liu",
      "Emmanouil Benetos",
      "Wenhao Huang",
      "Chenghua Lin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2409.15272",
    "githubLink": "https://github.com/m-a-p/OmniBench",
    "itemCount": "1,142 samples",
    "specs": "Tri-modal (Image, Audio, Text).",
    "description": "A benchmark designed to evaluate 'Omni-Language Models' on their ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously."
  },
  {
    "id": "saved-1769608044992-fb3cw",
    "title": "WMDP (Weapons of Mass Destruction Proxy) Benchmark",
    "source": "arXiv",
    "authors": [
      "Nathaniel Li",
      "Alexander Pan",
      "Anjali Gopal",
      "Summer Yue",
      "Daniel Berrios",
      "Alice Gatti",
      "Justin D. Li",
      "Ann-Kathrin Dombrowski",
      "Shashwat Goel",
      "Long Phan",
      "Gabriel Mukobi",
      "Nathan Helm-Burger",
      "Rassin Lababidi",
      "Lennart Justen",
      "Andrew B. Liu",
      "Michael Chen",
      "Isabelle Barrass",
      "Oliver Zhang",
      "Xiaoyuan Zhu",
      "Rishub Tamirisa"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.03218",
    "githubLink": "https://wmdp.ai",
    "itemCount": "3,668 questions",
    "specs": "Text (Multiple Choice Questions); Domains: Biosecurity, Cybersecurity, Chemical Security",
    "description": "A dataset of multiple-choice questions serving as a proxy measure for hazardous knowledge in biosecurity, cybersecurity, and chemical security. It is designed to evaluate LLM unlearning methods and malicious use risks."
  },
  {
    "id": "saved-1769608044992-n0zus",
    "title": "BenBench: Benchmarking Benchmark Leakage",
    "source": "arXiv",
    "authors": [
      "Ruijie Xu",
      "Zengzhi Wang",
      "Run-Ze Fan",
      "Pengfei Liu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.18824",
    "githubLink": "https://github.com/gair-nlp/benbench",
    "itemCount": "Evaluates 31 LLMs on mathematical reasoning benchmarks",
    "specs": "Leakage metrics (N-gram accuracy, Perplexity), analysis pipeline",
    "description": "A benchmark designed to detect and quantify data leakage in Large Language Model (LLM) benchmarks. It uses metrics like Perplexity and N-gram accuracy to identify if test data was included in a model's training set, promoting evaluation transparency and validity. The project also proposes a 'Benchmark Transparency Card'."
  },
  {
    "id": "saved-1769608044992-riy3w",
    "title": "CC-Bench-trajectories",
    "source": "Hugging Face",
    "authors": [
      "Z.ai Team"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/zai-org/CC-Bench-trajectories",
    "githubLink": "https://huggingface.co/datasets/zai-org/CC-Bench-trajectories",
    "itemCount": "N/A (Contains full evaluation trajectories)",
    "specs": "Agent trajectories, test questions, JSON/Text",
    "description": "A dataset released by Z.ai containing all test questions and agent trajectories for the evaluation of the GLM-4.6 model. This release aims to ensure the transparency and credibility of the model's performance claims by allowing external verification and reproduction."
  },
  {
    "id": "saved-1769608044992-o38yu",
    "title": "RAGTruth",
    "source": "arXiv",
    "authors": [
      "Yuanhao Wu",
      "Juno Zhu",
      "Siliang Xu",
      "Kashun Shum",
      "Cheng Niu",
      "Randy Zhong",
      "Juntong Song",
      "Tong Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.00396",
    "githubLink": "https://github.com/yhw-1/RAGTruth",
    "itemCount": "~18,000 responses",
    "specs": "Text (RAG: QA, Summarization, Data-to-Text), Word-level annotations",
    "description": "A word-level hallucination corpus tailored for analyzing hallucinations in Retrieval-Augmented Generation (RAG) scenarios across various domains and tasks."
  },
  {
    "id": "saved-1769608044992-xi6ot",
    "title": "PhD (Prompted Visual Hallucination Dataset)",
    "source": "arXiv",
    "authors": [
      "Jiazhen Liu",
      "Yuhan Fu",
      "Ruobing Xie",
      "Runquan Xie",
      "Xingwu Sun",
      "Fengzong Lian",
      "Zhanhui Kang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.11116",
    "githubLink": "https://github.com/jiazhen-code/IntrinsicHallu",
    "itemCount": "102,564 VQA triplets, 14,648 images",
    "specs": "Image/Text (Visual QA), 5 visual recognition tasks",
    "description": "A dataset for evaluating visual hallucinations in Multimodal LLMs. It employs ChatGPT-generated prompts to ask questions about specific images to assess susceptibility to hallucination across different tasks."
  },
  {
    "id": "saved-1769608044992-nw3as",
    "title": "DefAn (Definitive Answer Dataset)",
    "source": "arXiv",
    "authors": [
      "A.B.M. Ashikur Rahman",
      "Saeed Anwar",
      "Muhammad Usman",
      "Ajmal Mian"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.09155",
    "githubLink": "https://github.com/ashikiut/DefAn",
    "itemCount": "75,000+ samples",
    "specs": "Text (QA), 8 domains (Sports, Math, etc.)",
    "description": "A comprehensive benchmark designed to assess hallucination tendencies in LLMs by eliciting definitive, concise answers. It covers eight knowledge domains and focuses on factual contradiction and prompt misalignment."
  },
  {
    "id": "saved-1769608044992-kukho",
    "title": "DIFrauD (Domain Independent Fraud Detection)",
    "source": "Hugging Face",
    "authors": [
      "Dainis A. Boumber",
      "Yifan Zhang",
      "Sihong Liu",
      "Fan Yang"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/difraud/difraud",
    "githubLink": "https://huggingface.co/datasets/difraud/difraud",
    "itemCount": "95,854 samples",
    "specs": "Text; 7 independent domains (Phishing, Fake News, SMS, etc.)",
    "description": "A large-scale, multi-domain text benchmark for fraud and deception detection. It aggregates data from phishing emails, fake news, job scams, product reviews, and political statements."
  },
  {
    "id": "saved-1769608044992-iddv4",
    "title": "DeceptionBench",
    "source": "Other",
    "authors": [
      "Jiaxin Ji",
      "Tianshuo Guo",
      "Runzhe Zhu",
      "Junjie Zhu",
      "Yue Zhou"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.13688",
    "githubLink": "https://github.com/PKU-Alignment/DeceptionBench",
    "itemCount": "1,000+ samples / 150 scenarios",
    "specs": "Text (LLM Prompts & Responses); 14 models evaluated",
    "description": "A comprehensive benchmark designed to evaluate deceptive behaviors in Large Language Models (LLMs), covering categories like sycophancy, sandbagging, and strategic deception."
  },
  {
    "id": "saved-1769608044992-kd2fu",
    "title": "CyberSecEval 2",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Yue Li",
      "Cyrus Nikolaidis",
      "Daniel Song",
      "Shengye Wan",
      "Faizan Ahmad",
      "Cornelius Aschermann",
      "Yaohui Chen",
      "Dhaval Kapil",
      "David Molnar",
      "Spencer Whitman",
      "Joshua Saxe"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.13161",
    "githubLink": "https://github.com/facebookresearch/PurpleLlama/tree/main/CyberSecEval",
    "itemCount": "1,916 prompts (instruct dataset), plus other test suites",
    "specs": "Text prompts, code snippets; Covers Prompt Injection, Insecure Code Generation (50 CWEs), Interpreter Abuse",
    "description": "A comprehensive benchmark suite from Meta to quantify LLM security risks and capabilities, including prompt injection, code interpreter abuse, and insecure code generation. It evaluates models against OWASP-style vulnerabilities and measures the safety-utility tradeoff."
  },
  {
    "id": "saved-1769608044992-g10jb",
    "title": "Vulnerable Programming Dataset",
    "source": "Hugging Face",
    "authors": [
      "darkknight25"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/darkknight25/Vulnerable_Programming_Dataset",
    "githubLink": "https://huggingface.co/datasets/darkknight25/Vulnerable_Programming_Dataset",
    "itemCount": "550 unique vulnerabilities",
    "specs": "JSON format; 10 programming languages (Python, JS, PHP, Java, etc.); Includes code snippets and descriptions",
    "description": "A community-contributed dataset designed for cybersecurity professionals and developers. It contains unique code vulnerabilities across multiple languages, highlighting unconventional flaws and logic errors, with references to OWASP Top 10 and CWE."
  },
  {
    "id": "saved-1769608044992-mrj9d",
    "title": "RE-ARC (Reverse Engineering ARC)",
    "source": "Other",
    "authors": [
      "Michael Hodel"
    ],
    "year": "2024",
    "paperLink": "https://github.com/michaelhodel/re-arc",
    "githubLink": "https://github.com/michaelhodel/re-arc",
    "itemCount": "Generators for 400 tasks (infinite samples)",
    "specs": "Python code (generators), output in JSON",
    "description": "A framework containing Python generators for the original ARC training tasks. It allows for the creation of an infinite number of synthetic examples following the same logic as the original tasks, enabling large-scale training."
  },
  {
    "id": "saved-1769608044992-4ofj6",
    "title": "SWE-bench Verified",
    "source": "Hugging Face",
    "authors": [
      "OpenAI Preparedness Team",
      "SWE-bench Authors"
    ],
    "year": "2024",
    "paperLink": "https://openai.com/index/swe-bench-verified/",
    "githubLink": "https://github.com/princeton-nlp/SWE-bench",
    "itemCount": "500 instances",
    "specs": "Human-validated subset, Python, Text + Code",
    "description": "A human-validated subset of SWE-bench, consisting of 500 samples verified to be non-problematic and solvable by human software engineers. Developed in collaboration with OpenAI."
  },
  {
    "id": "saved-1769608044992-q27mn",
    "title": "SWE-bench Multimodal",
    "source": "arXiv",
    "authors": [
      "John Yang",
      "Carlos E. Jimenez",
      "Alex L. Zhang",
      "Kilian Lieret",
      "Joyce Yang",
      "Xindi Wu",
      "Ori Press",
      "Niklas Muennighoff",
      "Gabriel Synnaeve",
      "Karthik R. Narasimhan",
      "Diyi Yang",
      "Sida I. Wang",
      "Ofir Press"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.03859",
    "githubLink": "https://github.com/princeton-nlp/SWE-bench",
    "itemCount": "617 instances",
    "specs": "JavaScript, Visual elements (Screenshots, UI), 17 Repositories",
    "description": "An extension of SWE-bench designed to evaluate AI systems on their ability to fix bugs in visual, user-facing JavaScript software. Issues include visual elements like screenshots and diagrams."
  },
  {
    "id": "saved-1769608044992-4ptck",
    "title": "LiveCodeBench",
    "source": "arXiv",
    "authors": [
      "Naman Jain",
      "King Han",
      "Alex Gu",
      "Wen-Ding Li",
      "Fanjia Yan",
      "Tianjun Zhang",
      "Sida Wang",
      "Armando Solar-Lezama",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.07974",
    "githubLink": "https://github.com/LiveCodeBench/LiveCodeBench",
    "itemCount": "400+ problems (continuously updated)",
    "specs": "Code generation, self-repair, code execution, test output prediction tasks; Problems from LeetCode, AtCoder, CodeForces",
    "description": "LiveCodeBench is a holistic and contamination-free evaluation benchmark for Large Language Models (LLMs) for code. It continuously collects new problems from competitive programming contests (LeetCode, AtCoder, CodeForces) to prevent test set contamination. The benchmark assesses models on a broader range of code-related capabilities beyond just generation, including self-repair, code execution, and test output prediction."
  },
  {
    "id": "saved-1769608044992-asnlh",
    "title": "AIME 2024 (HuggingFaceH4)",
    "source": "Hugging Face",
    "authors": [
      "Hugging Face H4 Team",
      "AI-MO Team"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/HuggingFaceH4/aime_2024",
    "githubLink": "https://huggingface.co/datasets/HuggingFaceH4/aime_2024",
    "itemCount": "30 problems",
    "specs": "JSONL format; Text modality; Integer answers (0-999)",
    "description": "A benchmark dataset consisting of 30 problems from the 2024 American Invitational Mathematics Examination (AIME I and II). It is widely used to evaluate the mathematical reasoning capabilities of Large Language Models (LLMs) on difficult, Olympiad-level problems."
  },
  {
    "id": "saved-1769608044992-e36ay",
    "title": "AIMO Validation AIME",
    "source": "Hugging Face",
    "authors": [
      "Project Numina",
      "AI-MO Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.01234",
    "githubLink": "https://github.com/project-numina/aimo-progress-prize",
    "itemCount": "~90 problems",
    "specs": "Parquet/JSON format; Text modality; Fields: problem, solution",
    "description": "An internal validation set used for the AI Mathematical Olympiad (AIMO) Progress Prize. It contains approximately 90 problems from recent AIME competitions (2022-2024) to avoid overlap with older training sets like MATH."
  },
  {
    "id": "saved-1769608044992-va0zj",
    "title": "AIME Problem Set (1983-2024)",
    "source": "Other",
    "authors": [
      "Mathematical Association of America (Source)",
      "Kaggle Contributors"
    ],
    "year": "2024",
    "paperLink": "https://www.kaggle.com/datasets/prokaggler/aime-problem-set-1983-2024",
    "githubLink": "https://www.kaggle.com/datasets/prokaggler/aime-problem-set-1983-2024",
    "itemCount": "~600+ problems",
    "specs": "CSV/JSON format; Text modality; Includes solutions and answer keys",
    "description": "A comprehensive collection of historical AIME problems spanning from 1983 to 2024. This dataset serves as a foundational resource for training and evaluating models on long-term historical mathematical reasoning data."
  },
  {
    "id": "saved-1769608044992-qp96a",
    "title": "MLE-bench",
    "source": "arXiv",
    "authors": [
      "Jun Shern Chan",
      "Neil Chowdhury",
      "Oliver Jaffe",
      "James Aung",
      "Dane Sherburn",
      "Evan Mays",
      "Giulio Starace",
      "Kevin Liu",
      "Leon Maksin",
      "Tejal Patwardhan",
      "Lilian Weng",
      "Aleksander Mądry"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.07095",
    "githubLink": "https://github.com/openai/mle-bench",
    "itemCount": "75 competitions",
    "specs": "75 offline Kaggle competitions (multimodal data including text, image, audio, tabular). Available in 'Full' (3.3 TB) and 'Lite' (158 GB) subsets.",
    "description": "A benchmark for evaluating AI agents on machine learning engineering tasks. It consists of 75 curated Kaggle competitions that test real-world skills such as training models, preparing datasets, and running experiments. Agents are evaluated by their ability to achieve medal-level performance on the competition leaderboards."
  },
  {
    "id": "saved-1769608044992-5xvpw",
    "title": "Tau-bench",
    "source": "arXiv",
    "authors": [
      "Shunyu Yao",
      "Noah Shinn",
      "Pedram Razavi",
      "Karthik Narasimhan"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.12045",
    "githubLink": "https://github.com/sierra-research/tau-bench",
    "itemCount": "165 tasks (115 Retail, 50 Airline)",
    "specs": "Dynamic conversation simulation; Domains: Retail, Airline; Modalities: Text (dialogue), API calls",
    "description": "A benchmark emulating dynamic conversations between a user (simulated by a language model) and a language agent provided with domain-specific API tools and policy guidelines. It focuses on evaluating tool usage, reasoning, and policy adherence in real-world scenarios."
  },
  {
    "id": "saved-1769608044992-hr86y",
    "title": "VisualWebArena",
    "source": "arXiv",
    "authors": [
      "Jing Yu Koh",
      "Robert Lo",
      "Lawrence Jang",
      "Vikram Duvvur",
      "Ming Chong Lim",
      "Po-Yu Huang",
      "Graham Neubig",
      "Shuyan Zhou",
      "Ruslan Salakhutdinov",
      "Daniel Fried"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2308.09687",
    "githubLink": "https://github.com/web-arena-x/visualwebarena",
    "itemCount": "910 tasks",
    "specs": "Multimodal (Text + Images), Web Interactions",
    "description": "A benchmark designed to assess the performance of multimodal agents on realistic visually grounded web tasks. It extends WebArena with 910 new tasks across Classifieds, Shopping, and Reddit environments, requiring agents to process image-text inputs and interpret natural language instructions to execute actions."
  },
  {
    "id": "saved-1769608044992-wbfxd",
    "title": "WorkArena",
    "source": "arXiv",
    "authors": [
      "Alexandre Drouin",
      "Maxime Gasse",
      "Massimo Caccia",
      "Issam H. Laradji",
      "Manuel Del Verme",
      "Tom Marty",
      "Léo Boisvert",
      "Megh Thakkar",
      "Quentin Cappart",
      "David Vazquez",
      "Nicolas Chapados",
      "Alexandre Lacoste"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.07718",
    "githubLink": "https://github.com/ServiceNow/WorkArena",
    "itemCount": "19,912 instances (33 tasks)",
    "specs": "Text, HTML, Browser Interactions (ServiceNow)",
    "description": "A benchmark evaluating web agents on common knowledge work tasks using the ServiceNow enterprise software platform. It focuses on measuring agents' ability to perform tasks that span the typical daily work of knowledge workers, such as navigating lists, forms, and service catalogs."
  },
  {
    "id": "saved-1769608044992-gfavi",
    "title": "MMMU-Pro",
    "source": "arXiv",
    "authors": [
      "Xiang Yue",
      "Tianyu Zheng",
      "Yuxuan Sun",
      "Yang Zhang",
      "Ge Zhang",
      "Wenhu Chen",
      "Yu Su",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2409.02813",
    "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
    "itemCount": "3,460 questions (1,730 standard + 1,730 vision-only)",
    "specs": "Multimodal (Image/Screenshot), Multiple Choice (10 options), Vision-only input setting",
    "description": "A robust version of the MMMU benchmark designed to rigorously assess true multimodal understanding. It filters out text-only answerable questions, augments candidate options to 10 choices, and introduces a vision-only input setting (screenshots) to test the integration of visual and textual information."
  },
  {
    "id": "saved-1769608044992-k9sdp",
    "title": "CharXiv",
    "source": "arXiv",
    "authors": [
      "Zirui Wang",
      "Mengzhou Xia",
      "Luxi He",
      "Howard Chen",
      "Yitao Liu",
      "Richard Zhu",
      "Kaiqu Liang",
      "Xindi Wu",
      "Haotian Liu",
      "Sadhika Malladi",
      "Alexis Chevalier",
      "Sanjeev Arora",
      "Danqi Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.18521",
    "githubLink": "https://github.com/princeton-nlp/CharXiv",
    "itemCount": "2,323 charts",
    "specs": "Images (scientific charts), Text (questions and answers); Includes descriptive and reasoning question types",
    "description": "A comprehensive evaluation suite designed to assess Multimodal Large Language Models (MLLMs) on realistic chart understanding. It features 2,323 diverse and challenging charts sourced from arXiv papers across various scientific disciplines. The benchmark includes two types of questions: descriptive questions that test basic element recognition, and reasoning questions that require synthesizing complex visual information."
  },
  {
    "id": "saved-1769608044992-jx3mo",
    "title": "SimpleQA",
    "source": "arXiv",
    "authors": [
      "Jason Wei",
      "Khanh Nguyen",
      "OpenAI Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.04368",
    "githubLink": "https://github.com/openai/simple-evals",
    "itemCount": "4,326 questions",
    "specs": "Text modality; Short-form question answering; Single indisputable answer",
    "description": "A factuality benchmark that measures the ability of language models to answer short, fact-seeking questions. It focuses on high correctness and is designed to be challenging for frontier models (adversarially collected against GPT-4) while ensuring answers are easy to grade."
  },
  {
    "id": "saved-1769608044992-p7o7p",
    "title": "Chinese SimpleQA",
    "source": "arXiv",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Yingshui Tan",
      "Weixun Wang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.07140",
    "githubLink": "https://github.com/LivingFutureLab/ChineseSimpleQA",
    "itemCount": "Not specified (covers 6 major topics, 99 subtopics)",
    "specs": "Text modality (Chinese); Short-form question answering",
    "description": "The first comprehensive Chinese benchmark for evaluating the factuality of language models on short questions. It covers diverse topics and is designed to be high-quality, static, and easy to evaluate, effectively adapting the SimpleQA methodology to the Chinese language."
  },
  {
    "id": "saved-1769608044992-xcbwi",
    "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
    "source": "Scholar",
    "authors": [
      "Jiho Jin",
      "Jiseon Kim",
      "Nayeon Lee",
      "Haneul Yoo",
      "Alice Oh",
      "Hwaran Lee"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.tacl-1.29/",
    "githubLink": "https://github.com/jinjh0123/KoBBQ",
    "itemCount": "76,048 samples",
    "specs": "Korean text, Multiple Choice QA, 268 templates across 12 social categories",
    "description": "A Korean bias benchmark dataset adapted from BBQ to reflect Korean cultural contexts and social biases. It includes classes for direct translation, target modification (localization), and new categories specific to Korean culture."
  },
  {
    "id": "saved-1769608044992-2edhb",
    "title": "Open-BBQ: Open-ended Bias Benchmark",
    "source": "arXiv",
    "authors": [
      "Zhao Liu",
      "Tian Xie",
      "Xueru Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.06134",
    "githubLink": "https://github.com/zhaoliu0914/LLM-Bias-Benchmark",
    "itemCount": "Expands BBQ (58k+ base)",
    "specs": "English text, Open-ended QA (Fill-in-the-blank, Short-answer)",
    "description": "An expansion of the BBQ dataset that includes open-ended question types (fill-in-the-blank and short-answer) to better detect unjustified stereotypes and assess how LLMs handle ambiguous scenarios without predefined answers."
  },
  {
    "id": "saved-1769608044992-xvjdu",
    "title": "StrongREJECT",
    "source": "arXiv",
    "authors": [
      "Alexandra Souly",
      "Qingyuan Lu",
      "Dillon Bowen",
      "Tu Trinh",
      "Elvis Hsieh",
      "Sana Pandey",
      "Pieter Abbeel",
      "Justin Svegliato",
      "Scott Emmons",
      "Olivia Watkins",
      "Sam Toyer"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.10260",
    "githubLink": "https://github.com/alexandrasouly/strongreject",
    "itemCount": "313 prompts",
    "specs": "Text prompts covering 6 categories of harmful behavior (e.g., Illegal goods, Violence, Disinformation); includes an automated evaluator.",
    "description": "StrongREJECT is a benchmark designed to evaluate the susceptibility of Large Language Models (LLMs) to jailbreak attacks. It addresses the shortcomings of previous benchmarks by distinguishing between 'empty' jailbreaks (where models fail to refuse but provide no harmful info) and effective ones. The benchmark consists of a curated dataset of forbidden prompts and an automated evaluator that assesses both the refusal and the quality/specificity of the harmful response."
  },
  {
    "id": "saved-1769608044992-533c6",
    "title": "LAB-Bench (Language Agent Biology Benchmark)",
    "source": "arXiv",
    "authors": [
      "Jon M. Laurent",
      "Joseph D. Janizek",
      "Michael Ruzo",
      "Michaela M. Hinks",
      "Michael J. Hammerling",
      "Siddharth Narayanan",
      "Manvitha Ponnapati",
      "Andrew D. White",
      "Samuel G. Rodriques"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.10362",
    "githubLink": "https://github.com/Future-House/LAB-Bench",
    "itemCount": "2,457 questions",
    "specs": "Multiple-choice questions across 8 categories (LitQA2, DbQA, SuppQA, FigQA, TableQA, ProtocolQA, SeqQA, Cloning Scenarios); Modalities include text, technical images, and biological sequences (DNA/protein)",
    "description": "LAB-Bench is a large-scale evaluation dataset designed to measure the capabilities of AI systems for practical biology research. Unlike textbook-style benchmarks, it focuses on real-world tasks such as literature search, protocol planning, data analysis, and the interpretation of figures and biological sequences. It includes difficult 'Cloning Scenarios' that simulate complex molecular cloning workflows."
  },
  {
    "id": "saved-1769608044992-47r1d",
    "title": "Long-form Virology Tasks",
    "source": "Scholar",
    "authors": [
      "SecureBio",
      "Deloitte",
      "Signature Science",
      "Anthropic"
    ],
    "year": "2024",
    "paperLink": "https://assets.anthropic.com/m/61e7d27f81849746/original/Claude-3-Opus-System-Card.pdf",
    "githubLink": "Not publicly available (Private/Internal)",
    "itemCount": "13 subtasks",
    "specs": "Agentic Tasks, Long-form text generation",
    "description": "A task-based agentic evaluation designed to test the end-to-end completion of complex pathogen acquisition processes, including workflow design and laboratory protocols. It is used to assess biological risks in frontier models."
  },
  {
    "id": "saved-1769608044992-t4qtr",
    "title": "VHELM (Holistic Evaluation of Vision-Language Models)",
    "source": "arXiv",
    "authors": [
      "Stanford CRFM Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.07112",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "21 datasets (initial run)",
    "specs": "Vision-Language (Image+Text); 9 evaluation aspects",
    "description": "A benchmark extending HELM to Vision-Language Models (VLMs). It aggregates datasets to cover aspects such as visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety."
  },
  {
    "id": "saved-1769608044992-48dmp",
    "title": "HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly)",
    "source": "arXiv",
    "authors": [
      "Howard Yen",
      "Tianyu Gao",
      "Minmin Hou",
      "Ke Ding",
      "Daniel Fleischer",
      "Peter Izsak",
      "Moshe Wasserblat",
      "Danqi Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.02694",
    "githubLink": "https://github.com/princeton-nlp/HELMET",
    "itemCount": "7 categories",
    "specs": "Long-context text (up to 128k tokens); Model-based evaluation",
    "description": "A comprehensive benchmark for long-context language models (LCLMs) covering seven diverse, application-centric categories to address issues with synthetic tasks like needle-in-a-haystack."
  },
  {
    "id": "saved-1769608044992-fikdm",
    "title": "HELM Safety",
    "source": "Scholar",
    "authors": [
      "Farzaan Kaiyom",
      "Ahmed Ahmed",
      "Yifan Mai",
      "Kevin Klyman",
      "Rishi Bommasani",
      "Percy Liang"
    ],
    "year": "2024",
    "paperLink": "https://crfm.stanford.edu/2024/11/08/helm-safety.html",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "5 safety benchmarks",
    "specs": "Text; 6 risk categories",
    "description": "A collection of safety benchmarks within the HELM ecosystem spanning risk categories such as violence, fraud, discrimination, sexual content, harassment, and deception."
  },
  {
    "id": "saved-1769608044992-x62rc",
    "title": "InjecAgent",
    "source": "arXiv",
    "authors": [
      "Qicheng Zhan",
      "Himanshu Gupta",
      "Priyanka D. L.",
      "Daniel Kang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.07612",
    "githubLink": "https://github.com/uiuc-kang-lab/InjecAgent",
    "itemCount": "1,054 test cases",
    "specs": "Text (Agent traces, Tool outputs)",
    "description": "A benchmark for assessing indirect prompt injection attacks in tool-integrated LLM agents, featuring test cases across diverse user tools and domains to evaluate agent vulnerability."
  },
  {
    "id": "saved-1769608044992-6zxo0",
    "title": "PINT Benchmark (Prompt Injection Test)",
    "source": "Other",
    "authors": [
      "Lakera AI"
    ],
    "year": "2024",
    "paperLink": "https://github.com/lakeraai/pint-benchmark",
    "githubLink": "https://github.com/lakeraai/pint-benchmark",
    "itemCount": "3,000+ samples",
    "specs": "Text",
    "description": "A neutral benchmark for evaluating prompt injection detection systems, containing a mix of public datasets, proprietary injections, and jailbreaks to test false positives and negatives."
  },
  {
    "id": "saved-1769608044992-nmbrn",
    "title": "DSBench",
    "source": "arXiv",
    "authors": [
      "Liqiang Jing",
      "Zhehui Huang",
      "Xiaoyang Wang",
      "Wenlin Yao",
      "Wenhao Yu",
      "Kaixin Ma",
      "Hongming Zhang",
      "Xinya Du",
      "Dong Yu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2409.07703",
    "githubLink": "https://github.com/LiqiangJing/DSBench",
    "itemCount": "466 data analysis tasks, 74 data modeling tasks",
    "specs": "Multimodal (Text, Tables, Images), SQL/Python tasks",
    "description": "A comprehensive benchmark designed to evaluate data science agents with realistic tasks. It includes data analysis and data modeling tasks sourced from competitions, featuring long contexts and multi-table structures."
  },
  {
    "id": "saved-1769608044993-186l3",
    "title": "InfiAgent-DABench",
    "source": "arXiv",
    "authors": [
      "Xuechen Liu",
      "Zhaojie Zhang",
      "Yiming Geng",
      "Yuanhang Zhang",
      "Yijuan Lu",
      "Fei Wu",
      "Kun Kuang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://proceedings.mlr.press/v235/kuang24a.html",
    "githubLink": "https://github.com/InfiAgent/InfiAgent",
    "itemCount": "603 data analysis questions, 124 CSV files",
    "specs": "CSV data, Text questions, Closed-form answers",
    "description": "A benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. It contains DAEval, a dataset of questions derived from CSV files, formatted to allow automatic evaluation of open-ended questions."
  },
  {
    "id": "saved-1769608044993-3m6mm",
    "title": "DSEval",
    "source": "arXiv",
    "authors": [
      "Yuge Zhang",
      "Qiaozi Gao",
      "Lihong Li",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.17168",
    "githubLink": "https://github.com/YugeZhang/DSEval",
    "itemCount": "Based on 31 datasets (seeds)",
    "specs": "Text instructions, Data Analysis tasks",
    "description": "A benchmark tailored for assessing the performance of data science agents throughout the entire data science lifecycle, incorporating a novel bootstrapped annotation method."
  },
  {
    "id": "saved-1769608044993-101ep",
    "title": "MMMLU (Multilingual Massive Multitask Language Understanding)",
    "source": "Hugging Face",
    "authors": [
      "OpenAI"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/openai/MMMLU",
    "githubLink": "https://github.com/openai/SimpleEvals",
    "itemCount": "14 languages (derived from 57 MMLU subjects)",
    "specs": "Text (Multilingual), Multiple Choice",
    "description": "A multilingual version of the MMLU benchmark, translated into 14 languages (including Yoruba, Swahili, etc.) using professional human translators to evaluate LLM performance across diverse linguistic contexts."
  },
  {
    "id": "saved-1769608044993-q220i",
    "title": "Global MMLU",
    "source": "arXiv",
    "authors": [
      "Shivalika Singh",
      "Angelika Romanou",
      "Clémentine Fourrier",
      "David I. Adelani",
      "Jian Gang Ngui",
      "Daniel Vila-Suero",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.03304",
    "githubLink": "https://huggingface.co/datasets/CohereForAI/Global-MMLU",
    "itemCount": "42 languages",
    "specs": "Text (Multilingual), Cultural/Regional Metadata",
    "description": "An improved multilingual evaluation benchmark covering 42 languages, designed to address cultural and linguistic biases found in translated benchmarks. It includes subsets for culturally sensitive and culturally agnostic questions."
  },
  {
    "id": "saved-1769608044993-wlzei",
    "title": "R-Judge",
    "source": "arXiv",
    "authors": [
      "Tongxin Yuan",
      "Zhiwei He",
      "Lingzhong Dong",
      "Yiming Wang",
      "Ruijie Zhao",
      "Tian Xia",
      "Lizhen Xu",
      "Binglin Zhou",
      "Fangqi Li",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Gongshen Liu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.10019",
    "githubLink": "https://github.com/Lordog/R-Judge",
    "itemCount": "569 records",
    "specs": "Multi-turn agent interaction records containing user instructions, agent actions/observations, safety labels (safe/unsafe), and risk descriptions. Covers 27 risk scenarios across 5 application categories and 10 risk types.",
    "description": "A benchmark designed to evaluate the proficiency of Large Language Models (LLMs) in judging and identifying safety risks given agent interaction records. It focuses on the behavioral safety of LLM agents operating in interactive environments, requiring models to analyze multi-turn interactions and detect potential risks."
  },
  {
    "id": "saved-1769608044993-kuf09",
    "title": "CoSafe",
    "source": "arXiv",
    "authors": [
      "Erxin Yu",
      "Jing Li",
      "Ming Liao",
      "Siqi Wang",
      "Gao Zuchen",
      "Fei Mi",
      "Lanqing Hong"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.17626",
    "githubLink": "https://github.com/ErxinYu/CoSafe-Dataset",
    "itemCount": "1,400 dialogues",
    "specs": "Text-based multi-turn dialogues; 14 categories of harm (e.g., hate speech, violence) derived from the BeaverTails dataset.",
    "description": "A benchmark dataset designed to evaluate Large Language Model (LLM) safety specifically in multi-turn dialogue coreference scenarios. It assesses whether models remain safe when harmful intent is obscured by coreference (referring back to previous context) across multiple turns of conversation."
  },
  {
    "id": "saved-1769608044993-frqlo",
    "title": "Agent Smith (Infectious Jailbreak Framework)",
    "source": "arXiv",
    "authors": [
      "Xiangming Gu",
      "Xiaosen Zheng",
      "Tianyu Pang",
      "Chao Du",
      "Qian Liu",
      "Ye Wang",
      "Jing Jiang",
      "Min Lin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.08567",
    "githubLink": "https://github.com/sail-sg/Agent-Smith",
    "itemCount": "Simulates up to 1 million agents; Uses subsets of ArtBench and AdvBench",
    "specs": "Multimodal (Image + Text); Adversarial Attacks; Multi-agent simulation code",
    "description": "A benchmark and simulation framework for evaluating 'infectious jailbreak' risks in multi-agent systems. It demonstrates how a single adversarial image can compromise an entire system of multimodal agents (e.g., LLaVA-1.5) exponentially fast. The benchmark typically uses ArtBench for image pools and AdvBench for target harmful behaviors."
  },
  {
    "id": "saved-1769608044993-1hb9i",
    "title": "SWE-smith",
    "source": "arXiv",
    "authors": [
      "Guangyu Yang",
      "Yuhao Zhu",
      "Seth Michael Segall",
      "Dhruv Joshi",
      "Clement Gehring",
      "Param Aggarwal",
      "Kishore Papineni"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.00000",
    "githubLink": "https://github.com/swe-smith/swe-smith",
    "itemCount": "50,000+ instances",
    "specs": "Text (Code/Python); Software Engineering Tasks (Bugs, Issues, Pull Requests)",
    "description": "A large-scale dataset and automated pipeline for generating software engineering training data for LLM agents. It focuses on creating 'bug-driven' tasks where agents must fix issues in Python repositories. It is designed to scale up data collection for software engineering agents beyond previous small-scale benchmarks like SWE-bench."
  },
  {
    "id": "saved-1769608044993-zhd9g",
    "title": "CTI-Bench (Agent Smith Malware Subset)",
    "source": "Hugging Face",
    "authors": [
      "AI4Sec Team"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/AI4Sec/cti-bench",
    "githubLink": "https://github.com/AI4Sec/CTIBench",
    "itemCount": "Various (Multiple subsets including CTI-MCQ, CTI-RCM)",
    "specs": "Text; Multiple Choice Questions; Vulnerability Mapping",
    "description": "A comprehensive benchmark suite for Cyber Threat Intelligence (CTI). It includes data related to the 'Agent Smith' mobile malware (a real-world malware campaign) as part of its knowledge evaluation tasks (CTI-MCQ) and other components to test LLMs on security domain knowledge."
  },
  {
    "id": "saved-1769608044993-7c957",
    "title": "XSTest-Response",
    "source": "Hugging Face",
    "authors": [
      "Seungju Han",
      "Kavel Rao",
      "Allyson Ettinger",
      "Liwei Jiang",
      "Bill Yuchen Lin",
      "Nathan Lambert",
      "Yejin Choi",
      "Nouha Dziri"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.18495",
    "githubLink": "https://github.com/allenai/wildguard",
    "itemCount": "895 items (449 refusal split, 446 harmfulness split)",
    "specs": "Text prompts with corresponding model responses and classification labels (refusal/compliance, harmful/unharmful)",
    "description": "An extension of the XSTest dataset that includes model responses, designed to evaluate the accuracy of moderator models in detecting refusals and harmfulness."
  },
  {
    "id": "saved-1769608044993-lvj68",
    "title": "AfriMed-QA",
    "source": "arXiv",
    "authors": [
      "Tobi Olatunji",
      "Charles Nimo",
      "Abraham Owodunni",
      "Tassallah Abdullahi",
      "Emmanuel Ayodele",
      "Mercy Nyamewaa Asiedu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.15640",
    "githubLink": "https://github.com/intron-innovation/AfriMed-QA",
    "itemCount": "15,000+ questions",
    "specs": "Includes 4,000+ expert Multiple Choice Questions (MCQs), 1,200+ Open-Ended Short Answer Questions (SAQs), and 10,000 Consumer Queries (CQs). Covers 32 medical specialties across 16 African countries.",
    "description": "A large-scale Pan-African, multi-specialty medical Question-Answering (QA) benchmark dataset designed to evaluate Large Language Models (LLMs) in the context of African healthcare. The dataset aims to address the underrepresentation of African health data in LLM training and evaluation, covering diverse geographic and clinical contexts."
  },
  {
    "id": "saved-1769608044993-f3pub",
    "title": "EHRNoteQA",
    "source": "arXiv",
    "authors": [
      "Sunjun Kweon",
      "Jiyoun Kim",
      "Heeyoung Kwak",
      "Dongchul Cha",
      "Hangyul Yoon",
      "Kwanghyun Kim",
      "Jeewon Yang",
      "Seunghyun Won",
      "Edward Choi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.16040",
    "githubLink": "https://github.com/ji-youn-kim/EHRNoteQA",
    "itemCount": "962 QA pairs",
    "specs": "Text (Clinical Discharge Summaries); Open-ended and Multi-choice QA; 8 Clinical Topics",
    "description": "A benchmark built on MIMIC-IV EHR for evaluating Large Language Models (LLMs) in clinical settings. It consists of question-answer pairs linked to distinct patients' discharge summaries. Unlike previous benchmarks, it focuses on multi-document reasoning where questions often require information from multiple discharge summaries. The dataset includes both open-ended and multi-choice formats, with questions generated by GPT-4 and manually refined by clinicians to ensure relevance."
  },
  {
    "id": "saved-1769608044993-az2a8",
    "title": "WSI-Path",
    "source": "Hugging Face",
    "authors": [
      "Ahmed",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/google/medgemma-1.5-4b-it",
    "githubLink": "https://huggingface.co/google",
    "itemCount": "Single WSI examples (count unspecified in snippet)",
    "specs": "Image (Whole Slide Images) + Text",
    "description": "A dataset referenced in the development of MedGemma, consisting of de-identified H&E Whole Slide Images (WSIs) paired with associated final diagnosis text from original pathology reports. Used for training multimodal medical AI models."
  },
  {
    "id": "saved-1769608044993-r50sx",
    "title": "Student Lab Reports (MyReviewers Corpus)",
    "source": "Scholar",
    "authors": [
      "Alex Rudniy",
      "Joseph M. Moxley"
    ],
    "year": "2024",
    "paperLink": "https://wac.colostate.edu/journal-of-writing-analytics/vol7/",
    "githubLink": "https://github.com/arudniy/Analytics",
    "itemCount": "Multiple datasets (13 rubric datasets, 52 total)",
    "specs": "Text (Student Writing)",
    "description": "A dataset of student laboratory reports from STEM courses (Chemistry) used to train AI for automated scoring and feedback. It includes initial and final drafts, along with human-rated scores for specific rubric criteria."
  },
  {
    "id": "saved-1769608044993-a1ue9",
    "title": "CT-RATE",
    "source": "Hugging Face",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Anjany Sekuboyina",
      "Enis Simsar",
      "Alperen Tezcan",
      "Ayse Gulnihan Simsek",
      "Sevval Nil Esirgun",
      "Furkan Almas",
      "Irem Dogan",
      "Muhammed Furkan Dasdelen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.17834",
    "githubLink": "https://github.com/ibrahimethemhamamci/CT-CLIP",
    "itemCount": "50,188 3D CT volumes (25,692 unique scans)",
    "specs": "3D Chest CT volumes, Text (Radiology Reports), Multi-abnormality labels",
    "description": "A large-scale 3D medical imaging dataset that pairs non-contrast chest CT volumes with corresponding radiology text reports. It is designed to address the scarcity of comprehensive 3D medical datasets and enables the development of foundation models like CT-CLIP and CT-CHAT for tasks such as zero-shot abnormality detection and report generation."
  },
  {
    "id": "saved-1769608044993-m2zk9",
    "title": "Japanese CT Report Dataset",
    "source": "arXiv",
    "authors": [
      "Yosuke Yamagishi",
      "Shohei Hanaoka",
      "Yukihiro Nomura",
      "Sohsuke Yoshimura",
      "Naoto Hayashi",
      "Osamu Abe"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.15907",
    "githubLink": "https://github.com/ibrahimethemhamamci/CT-CLIP",
    "itemCount": "22,778 Translated Reports",
    "specs": "Text (Japanese Radiology Reports)",
    "description": "A multilingual adaptation of the CT-RATE dataset, consisting of radiology reports translated into Japanese. It was created to facilitate the development of specialized Japanese medical language models and includes a subset of radiologist-revised reports for validation."
  },
  {
    "id": "saved-1769608044993-ph5a3",
    "title": "GenerateCT Dataset",
    "source": "Other",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Anjany Sekuboyina",
      "Enis Simsar",
      "Alperen Tezcan",
      "Ayse Gulnihan Simsek",
      "Sevval Nil Esirgun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.15343",
    "githubLink": "https://github.com/ibrahimethemhamamci/GenerateCT",
    "itemCount": "2,286 Synthetic CT volumes",
    "specs": "3D Chest CT volumes (Synthetic), Text Prompts",
    "description": "A synthetic dataset generated using the GenerateCT framework, consisting of chest CT volumes and corresponding text prompts. It serves as a benchmark for evaluating text-conditional 3D medical image generation capabilities."
  },
  {
    "id": "saved-1769608044993-orqz0",
    "title": "PathMMU",
    "source": "Hugging Face",
    "authors": [
      "Yuehan Sun",
      "Wei Wang",
      "Kezou Jin",
      "Zhiyuan Li",
      "Jiaheng Liu",
      "Jiancheng Yang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.16355",
    "githubLink": "https://github.com/PathMMU-Benchmark/PathMMU",
    "itemCount": "33,428 QA pairs; 24,067 images",
    "specs": "Multimodal (Image + Text), Multiple Choice Questions (MCQ)",
    "description": "A massive multimodal expert-level benchmark for understanding and reasoning in pathology. It serves as a comprehensive resource for evaluating Large Multimodal Models (LMMs) in the pathology domain, featuring expert-validated QA pairs derived from authoritative sources."
  },
  {
    "id": "saved-1769608044993-zpxbg",
    "title": "PathMCQA (MedGemma Evaluation Set)",
    "source": "Scholar",
    "authors": [
      "Google MedGemma Team",
      "Lin Yang",
      "Daniel Golden",
      "Andrew Sellergren"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.08644",
    "githubLink": "https://github.com/Google-Health/MedGemma",
    "itemCount": "450 patches (extracted from 354 WSIs)",
    "specs": "Multimodal (Image + Text), Multiple Choice Questions (MCQ)",
    "description": "An internal benchmark dataset used by Google to evaluate the MedGemma models. It focuses on histopathology image classification formulated as multiple-choice questions, covering tasks like identification, grading, and subtyping for breast, cervical, and prostate cancer."
  },
  {
    "id": "saved-1769608044993-j5fxb",
    "title": "MedExQA",
    "source": "arXiv",
    "authors": [
      "Yunsoo Kim",
      "Jinge Wu",
      "Yusuf Abdulle",
      "Honghan Wu"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.10688",
    "githubLink": "https://github.com/UCL-Health-Informatics/MedExQA",
    "itemCount": "Not specified in snippet",
    "specs": "Text-based Question Answering with Explanations",
    "description": "A medical question-answering benchmark that includes multiple explanations for each answer to evaluate LLMs' understanding. While broad, it specifically covers underrepresented domains such as Speech Language Pathology."
  },
  {
    "id": "saved-1769608044993-2joop",
    "title": "WSI-Path (PathAlign Dataset)",
    "source": "arXiv",
    "authors": [
      "Faruk Ahmed",
      "Andrew Sellergren",
      "Lin Yang",
      "Shawn Xu",
      "Boris Babenko",
      "Abbi Ward",
      "Niels Olson",
      "Arash Mohtashamian",
      "Yossi Matias",
      "Greg S. Corrado",
      "Quang Duong",
      "Dale R. Webster",
      "Shravya Shetty",
      "Daniel Golden",
      "Yun Liu",
      "David F. Steiner",
      "Ellery Wulczyn"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.19578",
    "githubLink": "Not available",
    "itemCount": "350,000+ WSIs and diagnostic text pairs",
    "specs": "Whole Slide Images (WSI), Pathology Reports (Text), H&E Stain",
    "description": "A large-scale dataset of de-identified H&E whole slide images (WSIs) paired with final diagnosis text from original pathology reports. It was used to train the PathAlign vision-language model and serves as a foundation for tasks like text-to-image retrieval and zero-shot classification in computational pathology. The dataset covers a wide range of diagnoses, procedure types, and tissue types."
  },
  {
    "id": "saved-1769608044993-t7wc2",
    "title": "UCF-MultiOrgan-Path",
    "source": "Other",
    "authors": [
      "Md Sanzid Bin Hossain",
      "Yelena Piazza",
      "Jacob Braun",
      "Anthony Bilic",
      "Michael Hsieh",
      "Samir Fouissi",
      "Alexander Borowsky",
      "Hatem"
    ],
    "year": "2024",
    "paperLink": "https://www.medrxiv.org/content/10.1101/2024.11.05.24316736v1",
    "githubLink": "https://github.com/Md-Sanzid-Bin-Hossain/UCF-WSI-Dataset",
    "itemCount": "977 WSIs, ~2.38 million patches",
    "specs": "Whole Slide Images (WSI), Patches (512x512), 15 Organ Classes",
    "description": "A public benchmark dataset for multi-organ histopathologic image classification. Collected from cadavers over a decade, it addresses the limitations of existing datasets by providing a diverse collection of WSIs from 15 distinct organ classes (e.g., lung, kidney, liver, pancreas). It supports both patch-level and slide-level classification tasks."
  },
  {
    "id": "saved-1769608044993-zd3pr",
    "title": "DermaVQA",
    "source": "Other",
    "authors": [
      "Wen-wai Yim",
      "Yujuan Fu",
      "Zhaoyi Sun",
      "Asma Ben Abacha",
      "Meliha Yetisgen",
      "Fei Xia"
    ],
    "year": "2024",
    "paperLink": "https://papers.miccai.org/miccai-2024/paper/2444_paper.pdf",
    "githubLink": "https://github.com/velvinnn/DermaVQA",
    "itemCount": "~20,000+ QA pairs (across subsets)",
    "specs": "Image + Text (Visual Question Answering), Multilingual (English, Chinese, Spanish)",
    "description": "A multilingual visual question answering dataset for dermatology. It includes clinical dermatology textual queries and associated images from two subsets (IIYI and Reddit), designed to benchmark multimodal response generation."
  },
  {
    "id": "saved-1769608044993-utxdz",
    "title": "BioLP-bench",
    "source": "Scholar",
    "authors": [
      "Igor Ivanov"
    ],
    "year": "2024",
    "paperLink": "https://www.biorxiv.org/content/10.1101/2024.08.21.608694v2",
    "githubLink": "https://github.com/baceolus/BioLP-bench",
    "itemCount": "800 test cases",
    "specs": "Open-ended QA, Text-based protocols",
    "description": "Evaluates the proficiency of language models in finding and correcting mistakes in biological laboratory protocols. Unlike multiple-choice benchmarks, it uses open-ended questions where models must identify failure-causing errors injected into real-world protocols."
  },
  {
    "id": "saved-1769608044993-tr2ym",
    "title": "IT-Troubleshooting-Dataset",
    "source": "Hugging Face",
    "authors": [
      "UmerSajid"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/UmerSajid/IT-Troubleshooting-Dataset",
    "githubLink": "N/A",
    "itemCount": "10k - 100k samples",
    "specs": "CSV format; Text modality; Columns include Category, Issue, Symptoms, Solution Steps, Severity, Common Causes",
    "description": "A dataset for IT troubleshooting tasks, specifically focused on cloud computing issues (e.g., AWS instance errors). It includes fields for issues, symptoms, solution steps, severity, and estimated resolution time, suitable for text classification and solution generation tasks."
  },
  {
    "id": "saved-1769608044993-vy2dv",
    "title": "GUANinE",
    "source": "Other",
    "authors": [
      "Eyes S. Robson",
      "Nilah M. Ioannidis"
    ],
    "year": "2024",
    "paperLink": "https://proceedings.mlr.press/v240/robson24a.html",
    "githubLink": "https://github.com/ni-lab/guanine",
    "itemCount": "Over 60 million training examples",
    "specs": "Genomic sequences, functional genomics tasks (promoter annotation, gene expression), text/sequence modalities",
    "description": "A large-scale, de-noised genomic AI benchmark designed to evaluate model generalization across distinct functional genomics tasks, including functional element annotation and gene expression prediction. It focuses on human (eukaryote) genomic complexity."
  },
  {
    "id": "saved-1769608044993-zoqna",
    "title": "Genomics Long-Range Benchmark (LRB)",
    "source": "Hugging Face",
    "authors": [
      "Evan Trop",
      "Yair Schiff",
      "Alara Dirik",
      "Tyler Ross",
      "Reihaneh Rabbany",
      "Volodymyr Kuleshov",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://openreview.net/forum?id=9Xy3sYyXXy",
    "githubLink": "https://huggingface.co/datasets/InstaDeepAI/genomics-long-range-benchmark",
    "itemCount": "9 tasks",
    "specs": "Long-sequence DNA (up to ~100k+ bp contexts), tasks include classification and regression (e.g., CAGE, Histone Marks, Regulatory Elements).",
    "description": "A benchmark suite focused on biologically meaningful genomic tasks that require modeling long-range dependencies. It includes tasks like variant effect prediction, CAGE gene expression, and chromatin feature prediction, specifically tailored to evaluate long-context DNA language models."
  },
  {
    "id": "saved-1769608044993-lsq2r",
    "title": "SafeBench (Multimodal)",
    "source": "arXiv",
    "authors": [
      "Zonghao Ying",
      "Aishan Liu",
      "Siyuan Liang",
      "Lei Huang",
      "Jinyang Guo",
      "Wenbo Zhou",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.18927",
    "githubLink": "https://safebench-mm.github.io/",
    "itemCount": "2,300 multimodal harmful query pairs",
    "specs": "Multimodal (Text + Image)",
    "description": "A safety evaluation framework for Multimodal Large Language Models (MLLMs). It includes a comprehensive harmful query dataset covering risk scenarios like child abuse, illegal activities, and violence, using a jury deliberation protocol for evaluation."
  },
  {
    "id": "saved-1769608044993-7s57r",
    "title": "PHVSpec: Benchmark for Video Hashing",
    "source": "Other",
    "authors": [
      "Tech Coalition"
    ],
    "year": "2024",
    "paperLink": "https://www.technologycoalition.org/",
    "githubLink": "https://github.com/TechCoalition",
    "itemCount": "N/A (Algorithm Benchmark)",
    "specs": "Video Hashing Algorithms, Benchmarking Framework",
    "description": "A benchmark framework developed by the Tech Coalition to analyze the effectiveness of perceptual hash algorithms (like PDQ and TMK+PDQF) for detecting CSAM in videos. It focuses on evaluating the robustness of hashing systems against attacks and edits."
  },
  {
    "id": "saved-1769608044993-e8nfw",
    "title": "CatQA (Categorical Harmful Questions)",
    "source": "Hugging Face",
    "authors": [
      "Rishabh Bhardwaj",
      "Doo Hee Jung",
      "Soujanya Poria"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.11746",
    "githubLink": "https://github.com/declare-lab/categorical-harmful-qa",
    "itemCount": "550 questions (subset related to grooming)",
    "specs": "Text (Q&A/Prompts)",
    "description": "A safety evaluation dataset for Large Language Models (LLMs) comprising 550 harmful questions across various categories. It includes a specific 'Child Abuse' category with scenarios related to online grooming to test model refusal and safety alignment."
  },
  {
    "id": "saved-1769608044993-pvox9",
    "title": "OR-Bench (Over-Refusal Benchmark)",
    "source": "arXiv",
    "authors": [
      "Justin Cui",
      "Ruoxi Jia",
      "Cho-Jui Hsieh"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.20947",
    "githubLink": "https://github.com/justincui03/or-bench",
    "itemCount": "80,000 synthetic prompts, 1,000 hard prompts, 600 toxic prompts",
    "specs": "Text prompts; 10 common rejection categories (e.g., medical, legal advice)",
    "description": "A large-scale benchmark specifically designed to measure 'over-refusal' behavior in LLMs, where models incorrectly reject benign prompts. It includes a synthetic dataset of 80,000 prompts across 10 rejection categories, a 'hard' subset of ~1,000 prompts, and a toxic control set to ensure safety is maintained."
  },
  {
    "id": "saved-1769608044993-gta2h",
    "title": "SORRY-Bench",
    "source": "arXiv",
    "authors": [
      "Tinghao Xie",
      "Xiangyu Qi",
      "Yi Zeng",
      "Yangsibo Huang",
      "Udari Madhushani",
      "Prateek Mittal"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.14598",
    "githubLink": "https://github.com/sorry-bench/sorry-bench",
    "itemCount": "440 base unsafe instructions; ~9,000 linguistically mutated variants; 7,000+ human annotations",
    "specs": "Text instructions; 45 classes (44 unsafe + 1 safe); Linguistic mutations",
    "description": "A systematic benchmark for evaluating LLM safety refusal behaviors. It utilizes a fine-grained taxonomy of 44 unsafe topics and applies 20 diverse linguistic mutations (e.g., slang, typos, dialects) to ensure models refuse unsafe content regardless of how it is phrased."
  },
  {
    "id": "saved-1769608044993-6heoc",
    "title": "BrowserART (Browser Agent Red-teaming Toolkit)",
    "source": "Hugging Face",
    "authors": [
      "Priyanshu Kumar",
      "Elaine Lau",
      "Saranya Vijayakumar",
      "Tu Trinh",
      "Elaine Chang",
      "Vaughn Robinson",
      "Sean Hendryx",
      "Shuyan Zhou",
      "Matt Fredrikson",
      "Summer Yue",
      "Zifan Wang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.13886",
    "githubLink": "https://github.com/scaleapi/browser-art",
    "itemCount": "100 harmful browser-related behaviors",
    "specs": "Harmful behaviors targeting 40 synthetic websites (hosted locally) and real websites",
    "description": "A red teaming test suite designed for browser agents, consisting of harmful behaviors aimed at synthetic and real websites. It highlights the safety gap between refusal-trained LLMs and their agentic counterparts in browser environments."
  },
  {
    "id": "saved-1769608044993-zm1bb",
    "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
    "source": "arXiv",
    "authors": [
      "Andy K. Zhang",
      "Neil Perry",
      "Riya Dulepet",
      "Joey Ji",
      "Celeste Menders",
      "Justin W. Lin",
      "Eliot Jones",
      "Gashon Hussein",
      "Samantha Liu",
      "Donovan Jasper",
      "Percy Liang",
      "Daniel E. Ho",
      "Dan Boneh"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.08926",
    "githubLink": "https://github.com/cybench/cybench",
    "itemCount": "40 tasks",
    "specs": "CTF challenges involving text descriptions, local/remote starter files, Docker environments, and subtask breakdowns.",
    "description": "A cybersecurity benchmark for evaluating language model agents. It consists of 40 professional-level Capture the Flag (CTF) tasks chosen to be recent, meaningful, and spanning a wide range of difficulties (including cryptography, web security, reverse engineering, forensics, and exploitation). The framework includes task descriptions, starter files, and automated evaluators."
  },
  {
    "id": "saved-1769608044993-11cgc",
    "title": "AI Sandbagging Dataset / Password-Locked Models",
    "source": "arXiv",
    "authors": [
      "Teun van der Weij",
      "Felix Hofstätter",
      "Oliver Jaffe",
      "Samuel F. Brown",
      "Francis Rhys Ward"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.07358",
    "githubLink": "https://github.com/TeunvdWeij/sandbagging",
    "itemCount": "Includes 9,470 CSQA, 14,042 MMLU, and 3,668 WMDP samples (subsets used for training/eval)",
    "specs": "Text (Multiple Choice Questions, Synthetic Prompts)",
    "description": "A set of synthetic datasets and methodologies used to fine-tune language models (like GPT-4 and Claude 3) to 'sandbag' (strategically underperform) on specific benchmarks. The research demonstrates models can be 'password-locked' to hide capabilities unless a specific string is present."
  },
  {
    "id": "saved-1769608044993-4bic3",
    "title": "Subversion Strategy Eval (SSE)",
    "source": "arXiv",
    "authors": [
      "Alex Mallen",
      "Charlie Griffin",
      "Alessandro Abate",
      "Buck Shlegeris"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.12480",
    "githubLink": "https://github.com/safety-research/subversion-strategy-eval",
    "itemCount": "8 environments",
    "specs": "Text-based reinforcement learning environments; evaluates stateless agents",
    "description": "A benchmark consisting of eight environments designed to model AI control protocols and evaluate whether stateless language models can strategize to subvert them. It assesses capabilities such as optimality in planning, reliability, probability calibration, and acausal coordination."
  },
  {
    "id": "saved-1769608044993-subpi",
    "title": "JailBreakV-28K",
    "source": "arXiv",
    "authors": [
      "Weidi Luo",
      "Siyuan Ma",
      "Xiaogeng Liu",
      "Xiaoyu Guo",
      "Chaowei Xiao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.03027",
    "githubLink": "https://github.com/JailBreakV/JailBreakV-28K",
    "itemCount": "28,000 samples",
    "specs": "20,000 text-based transfer attacks, 8,000 image-based attacks; text-image pairs",
    "description": "A comprehensive benchmark designed to assess the transferability of jailbreak attacks from LLMs to Multimodal Large Language Models (MLLMs). It evaluates alignment robustness against text-based and image-based attacks."
  },
  {
    "id": "saved-1769608044993-vcyqu",
    "title": "MoralBench",
    "source": "arXiv",
    "authors": [
      "Jianchao Ji",
      "Yutong Chen",
      "Mingyu Jin",
      "Wujiang Xu",
      "Wenyue Hua",
      "Yongfeng Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.04428",
    "githubLink": "https://github.com/Moral-Bench/MoralBench",
    "itemCount": "Not specified in snippet",
    "specs": "Text-based moral dilemmas; derived from MFQ-30 and MFV psychometric tools",
    "description": "A benchmark for evaluating the moral identity and reasoning of LLMs using Moral Foundations Theory, which includes the 'Authority/Subversion' foundation. It assesses how models navigate ethical dilemmas and align with human moral standards."
  },
  {
    "id": "saved-1769608044993-0dlu3",
    "title": "CyberSecEval 3 (including CyberSecEval 2)",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Yue Li",
      "Cyrus Nikolaidis",
      "Daniel Song",
      "Shengye Wan",
      "Faizan Ahmad",
      "Cornelius Aschermann",
      "Yaohui Chen",
      "Dhaval Kapil",
      "David Molnar",
      "Spencer Whitman",
      "Joshua Saxe"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.01605",
    "githubLink": "https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks",
    "itemCount": "Multiple test suites; e.g., 585 samples for VulnerabilityExploit subset, thousands of prompts for injection tests",
    "specs": "Text and Code (C, Python, Javascript, SQL); Categories: Prompt Injection, Insecure Code, Interpreter Abuse, Cyberattack Helpfulness",
    "description": "A comprehensive benchmark suite to quantify LLM security risks and capabilities, including prompt injection, code interpreter abuse, and offensive capabilities like automated social engineering and scaling offensive cyber operations. It builds upon CyberSecEval 2."
  },
  {
    "id": "saved-1769608044993-t90ru",
    "title": "NYU CTF Bench",
    "source": "arXiv",
    "authors": [
      "Minghao Shao",
      "Sofija Jancheska",
      "Meet Udeshi",
      "Brendan Dolan-Gavitt",
      "Haoran Xi",
      "Kimberly Milner",
      "Boyuan Chen",
      "Max Yin",
      "Siddharth Garg",
      "Prashanth Krishnamurthy",
      "Farshad Khorrami",
      "Ramesh Karri",
      "Muhammad Shafique"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.05590",
    "githubLink": "https://github.com/NYU-LLM-CTF/NYU_CTF_Bench",
    "itemCount": "200 challenges",
    "specs": "CTF Challenges (Dockerized); Modalities: Text, Code, Binary Interaction",
    "description": "A scalable, open-source benchmark dataset designed to evaluate LLMs in offensive security. It features verified CTF challenges hosted in Docker containers to test agents' task planning and vulnerability exploitation skills."
  },
  {
    "id": "saved-1769608044993-vr6xo",
    "title": "SaTML LLM Capture-the-Flag Competition Dataset",
    "source": "arXiv",
    "authors": [
      "Edoardo Debenedetti",
      "Javier Rando",
      "Daniel Paleka",
      "Fineas Silaghi",
      "Dragos Albastroiu",
      "Niv Cohen",
      "Yuval Lemberg",
      "Reshmi Ghosh",
      "Rui Wen",
      "Ahmed Salem",
      "Giovanni Cherubin",
      "Santiago Zanella-Beguelin",
      "Robin Schmid",
      "Victor Klemm",
      "Takahiro Miki",
      "Chenhao Li",
      "Stefan Kraft"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.07954",
    "githubLink": "https://github.com/ethz-spylab/satml-llm-ctf",
    "itemCount": "137,000+ attack chats",
    "specs": "Text-based chat logs (attacks and model responses); focused on prompt injection and defense evasion.",
    "description": "A dataset containing over 137,000 multi-turn attack chats from a CTF competition focused on LLM prompt injection and secret leaking. It includes successful and unsuccessful attacks against various defense strategies."
  },
  {
    "id": "saved-1769608044993-ifa7v",
    "title": "CVQA (Culturally-diverse Multilingual Visual Question Answering)",
    "source": "Hugging Face",
    "authors": [
      "David Romero",
      "Chenyang Lyu",
      "Haryo Akbarianto Wibowo",
      "Teresa Lynn",
      "Injy Hamed",
      "Aditya Nanda Kishore"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.10042",
    "githubLink": "https://github.com/mbzuai-nlp/CVQA",
    "itemCount": "10,000 questions across 30 countries",
    "specs": "Multimodal (Image + Multilingual Text), 31 languages",
    "description": "A novel benchmark for Visual Multilingual QA (VMQA) that focuses on cultural diversity. It includes questions across 30 countries and 31 languages, designed to benchmark Multilingual Multimodal Large Language Models (MLLMs)."
  },
  {
    "id": "saved-1769608044993-z7wg8",
    "title": "Open Prompt Injection",
    "source": "Scholar",
    "authors": [
      "Yupei Liu",
      "Yuqi Jia",
      "Runpeng Geng",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ],
    "year": "2024",
    "paperLink": "https://www.usenix.org/conference/usenixsecurity24/presentation/liu-yupei",
    "githubLink": "https://github.com/liu00222/Open-Prompt-Injection",
    "itemCount": "N/A (Framework with 5 attacks, 10 defenses)",
    "specs": "Text (Framework, attack/defense implementations)",
    "description": "A toolkit and benchmark that formalizes prompt injection attacks and defenses. It evaluates attacks and defenses across multiple LLMs and tasks, providing a systematic framework for quantitative evaluation."
  },
  {
    "id": "saved-1769608044993-nyokk",
    "title": "EasyJailbreak",
    "source": "arXiv",
    "authors": [
      "Weikang Zhou",
      "Xiao Wang",
      "Limao Xiong",
      "Han Xia",
      "Yingshuang Gu",
      "Mingxu Chai",
      "Fukang Zhu",
      "Caishuang Huang",
      "Shihan Dou",
      "Zhiheng Xi",
      "Rui Zheng",
      "Songyang Gao",
      "Yicheng Zou",
      "Hang Yan",
      "Yifan Le",
      "Ruohui Wang",
      "Lijun Li",
      "Jing Shao",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.12171",
    "githubLink": "https://github.com/EasyJailbreak/EasyJailbreak",
    "itemCount": "1k - 10k samples (varies by sub-dataset)",
    "specs": "Text; Parquet format (Hugging Face)",
    "description": "A unified framework designed to simplify the construction and evaluation of jailbreak attacks against Large Language Models (LLMs). It decomposes the jailbreaking process into modular steps (Selector, Mutator, Constraint, Evaluator) and supports 11 distinct jailbreak methods. The framework includes a dataset component for storing and managing malicious queries."
  },
  {
    "id": "saved-1769608044993-qd5ls",
    "title": "WildJailbreak",
    "source": "Hugging Face",
    "authors": [
      "Liwei Jiang",
      "Kavel Rao",
      "Seungju Han",
      "Allyson Ettinger",
      "Faeze Brahman",
      "Sachin Kumar",
      "Niloofar Mireshghallah",
      "Ximing Lu",
      "Maarten Sap",
      "Yejin Choi",
      "Nouha Dziri"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.18510",
    "githubLink": "https://huggingface.co/datasets/allenai/wildjailbreak",
    "itemCount": "262k prompt-response pairs",
    "specs": "Text; Parquet format",
    "description": "A large-scale open-source synthetic safety-training dataset developed as part of the WildTeaming framework. It contains vanilla and adversarial prompt-response pairs, including harmful queries and benign queries that resemble harmful ones, to train models for balanced safety without over-refusal."
  },
  {
    "id": "saved-1769608044993-14boy",
    "title": "MaliciousInstruct",
    "source": "Hugging Face",
    "authors": [
      "Yangsibo Huang",
      "Samyak Gupta",
      "Mengzhou Xia",
      "Kai Li",
      "Danqi Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2310.06987",
    "githubLink": "https://huggingface.co/datasets/walledai/MaliciousInstruct",
    "itemCount": "100 malicious instructions",
    "specs": "Text; Parquet format",
    "description": "A dataset introduced to evaluate the 'Catastrophic Jailbreak' of open-source LLMs via generation exploitation. It contains 100 malicious instructions covering 10 different malicious intents, designed to test if models can be manipulated into generating harmful content through specific decoding strategies."
  },
  {
    "id": "saved-1769608044993-1ng4f",
    "title": "VoiceJailbreak",
    "source": "arXiv",
    "authors": [
      "Xinyue Shen",
      "Yixin Wu",
      "Yang Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.19103",
    "githubLink": "https://github.com/TrustAIRLab/VoiceJailbreakAttack",
    "itemCount": "Based on 520 harmful instructions (AdvBench), generating multiple audio prompt variations",
    "specs": "Audio, Text (based on AdvBench)",
    "description": "A study and dataset focusing on 'humanizing' GPT-4o to bypass safety guardrails through fictional storytelling in the audio modality. The benchmark uses a set of forbidden questions converted into persuasive audio narratives to test model resistance."
  },
  {
    "id": "saved-1769608044993-e2n5n",
    "title": "Chat-Audio Attacks (CAA) Benchmark",
    "source": "arXiv",
    "authors": [
      "Zhengliang Liu",
      "Yuzhong Chen",
      "Zihao Wu",
      "Pengshuai Yin",
      "Xianfeng Yang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.14081",
    "githubLink": "https://github.com/Lizhengliang-PF/Chat-Audio-Attacks",
    "itemCount": "1,680 adversarial audio samples (360 attack sets)",
    "specs": "Audio (adversarial samples), Text transcripts",
    "description": "A benchmark for evaluating LLM robustness against universal audio adversarial attacks in conversational scenarios. It categorizes attacks into content, emotional, explicit noise, and implicit noise types."
  },
  {
    "id": "saved-1769608044993-x87yu",
    "title": "PKU-SafeRLHF",
    "source": "Hugging Face",
    "authors": [
      "Jiaming Ji",
      "Donghai Hong",
      "Borong Zhang",
      "Boyuan Chen",
      "Josef Dai",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.15513",
    "githubLink": "https://github.com/PKU-Alignment/safe-rlhf",
    "itemCount": "~83,400 preference entries",
    "specs": "Text, Multi-dimensional safety constraints, Preference pairs",
    "description": "A human-labeled dataset containing both performance and safety preferences, with constraints across 19 harm categories. Designed for fine-grained value alignment and safety-centric RLHF."
  },
  {
    "id": "saved-1769608044993-eq9ua",
    "title": "TrustLLM",
    "source": "Other",
    "authors": [
      "Lichao Sun",
      "Yue Huang",
      "Haoran Wang",
      "Siyuan Wu",
      "Qihui Zhang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.05561",
    "githubLink": "https://github.com/HowieHwong/TrustLLM",
    "itemCount": "30+ datasets included",
    "specs": "Text, Multi-dimensional evaluation metrics",
    "description": "A comprehensive benchmark for evaluating the trustworthiness of LLMs across six dimensions: truthfulness, safety, fairness, robustness, privacy, and machine ethics."
  },
  {
    "id": "saved-1769608044993-d10lm",
    "title": "HelpSteer2",
    "source": "Hugging Face",
    "authors": [
      "Zhilin Wang",
      "Yi Dong",
      "Jiaqi Zeng",
      "Virginia Adams",
      "Makesh Narsimhan Sreedhar",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.08673",
    "githubLink": "https://huggingface.co/datasets/nvidia/HelpSteer2",
    "itemCount": "~10,000 response pairs",
    "specs": "Text, Multi-attribute scores",
    "description": "A permissively licensed preference dataset for training reward models. It contains response pairs with multi-attribute labels (helpfulness, correctness, coherence, complexity, verbosity) to steer model alignment."
  },
  {
    "id": "saved-1769608044993-ahtxe",
    "title": "M³oralBench",
    "source": "arXiv",
    "authors": [
      "Haojun Bei",
      "Yao Wan",
      "Ye Huang",
      "Yulei Sui",
      "Xiangliang Zhang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.13264",
    "githubLink": "https://github.com/BeiiiY/M3oralBench",
    "itemCount": "3,200 text-image pairs",
    "specs": "Multimodal (Text + Image); Moral judgement classification",
    "description": "A comprehensive multimodal benchmark for evaluating moral judgment in Large Vision-Language Models (LVLMs). It combines text and images to test moral reasoning across various scenarios derived from Moral Foundations Theory."
  },
  {
    "id": "saved-1769608044993-tjz97",
    "title": "CBT-Bench",
    "source": "Hugging Face",
    "authors": [
      "Mian Zhang",
      "Xianjun Yang",
      "Xinlu Zhang",
      "Travis Labrum",
      "Jamie C. Chiu",
      "Shaun M. Eack",
      "Fei Fang",
      "William Yang Wang",
      "Zhiyu Zoey Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.13218",
    "githubLink": "https://huggingface.co/datasets/Psychotherapy-LLM/CBT-Bench",
    "itemCount": "~1,705 examples (across multiple tasks)",
    "specs": "Text (Multiple Choice, Classification, Generation)",
    "description": "A benchmark for evaluating Large Language Models on their ability to assist in Cognitive Behavioral Therapy (CBT), covering knowledge acquisition, cognitive model understanding, and therapeutic response generation."
  },
  {
    "id": "saved-1769608044993-vc9u3",
    "title": "GOAT-Bench",
    "source": "arXiv",
    "authors": [
      "Hongzhan Lin",
      "Ziyang Luo",
      "Bo Wang",
      "Ruichao Yang",
      "Jing Ma"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.01523",
    "githubLink": "https://github.com/HKBU-NLP/GOAT-Bench",
    "itemCount": "Over 6,000 memes",
    "specs": "Multimodal (Image + Text)",
    "description": "A comprehensive meme-based benchmark designed to evaluate Large Multimodal Models on detecting social abuse, including hate speech, misogyny, and offensive content in memes."
  },
  {
    "id": "saved-1769608044993-qfhnd",
    "title": "KG-FPQ",
    "source": "arXiv",
    "authors": [
      "Yanxu Zhu",
      "Jinlin Xiao",
      "Yuhang Wang",
      "Jitao Sang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.05868",
    "githubLink": "https://github.com/yanxuzhu/KG-FPQ",
    "itemCount": "~178,000 questions",
    "specs": "Covers 3 knowledge domains, 6 levels of confusability, and 2 task formats.",
    "description": "A comprehensive benchmark constructed using an automated pipeline based on Knowledge Graphs (KGs). It aims to evaluate factuality hallucination in LLMs caused by false premise questions."
  },
  {
    "id": "saved-1769608044993-ba2m7",
    "title": "FPQA (Yuan et al.)",
    "source": "Semantic Scholar",
    "authors": [
      "Hongbang Yuan",
      "Yubo Chen",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Kang Liu",
      "Jun Zhao"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2025.findings-acl.530/",
    "githubLink": "https://github.com/ysw1021/NASA",
    "itemCount": "986 questions",
    "specs": "Questions with false premises intended for Out-Of-Domain (OOD) factuality evaluation.",
    "description": "A dataset used to evaluate model factuality when faced with unanswerable questions containing false premises. Often used in studies regarding preference learning and alignment."
  },
  {
    "id": "saved-1769608044993-orvqk",
    "title": "LongFact",
    "source": "arXiv",
    "authors": [
      "Jerry Wei",
      "Chengrun Yang",
      "Xinying Song",
      "Yifeng Lu",
      "Nathan Hu",
      "Jie Huang",
      "Da Huang",
      "Li Dong",
      "Yu Cheng",
      "Quoc V. Le",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.18802",
    "githubLink": "https://github.com/google-deepmind/long-form-factuality",
    "itemCount": "2,280 prompts",
    "specs": "Text-only prompts; 38 topics; 2 sub-tasks (Concepts and Objects) with 1,140 prompts each.",
    "description": "LongFact is a benchmark designed to evaluate long-form factuality in large language models. It consists of 2,280 fact-seeking prompts across 38 diverse topics (e.g., STEM, social sciences, humanities). The benchmark is divided into two tasks: LongFact-Concepts and LongFact-Objects, distinguishing between questions about general concepts and specific objects. It is often used in conjunction with SAFE (Search-Augmented Factuality Evaluator), an automated method using LLM agents to verify facts against Google Search results."
  },
  {
    "id": "saved-1769608044993-zxgwq",
    "title": "Factcheck-Bench (Factcheck-GPT)",
    "source": "Semantic Scholar",
    "authors": [
      "Yuxia Wang",
      "Revanth Gangi Reddy",
      "Zain Muhammad Mujahid",
      "Arnav Arora",
      "Aleksandr Rubashevskii",
      "Jiahui Geng",
      "Osama Mohammed Afzal",
      "Liangming Pan",
      "Nadav Borenstein",
      "Aditya Pillai"
    ],
    "year": "2024",
    "paperLink": "https://aclanthology.org/2024.findings-emnlp.112/",
    "githubLink": "https://github.com/yuxiaw/Factcheck-GPT",
    "itemCount": "94 prompts / 678 claims",
    "specs": "Text (Open-ended questions); Fine-grained human annotation (claim-level)",
    "description": "A fine-grained evaluation benchmark for automatic fact-checkers. It features detailed human annotations on open-ended questions to evaluate the capabilities of systems in detecting and correcting factual errors in long documents."
  },
  {
    "id": "saved-1769608044993-bwzc2",
    "title": "LLM-AggreFact",
    "source": "Hugging Face",
    "authors": [
      "Liyan Tang",
      "Philippe Laban",
      "Greg Durrett"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.10774",
    "githubLink": "https://github.com/Liyan06/AggreFact",
    "itemCount": "Aggregates 11 datasets",
    "specs": "Text (Summarization, QA); Grounded factuality evaluation",
    "description": "A benchmark that unifies 11 publicly available datasets on factual consistency evaluation and grounding. It is designed to test LLMs' ability to assess whether statements are supported by evidence documents (MiniCheck)."
  },
  {
    "id": "saved-1769608044993-sjgej",
    "title": "FactBench",
    "source": "arXiv",
    "authors": [
      "V. Bayat",
      "Y. Wang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.22257",
    "githubLink": "https://github.com/launchnlp/FactBench",
    "itemCount": "1,000 prompts (150 topics)",
    "specs": "Text; Dynamic/In-the-wild prompts",
    "description": "A dynamic benchmark grounded in real-world usage of LMs, consisting of prompts that frequently elicit hallucinations. It is designed to be regularly updated and covers 'in-the-wild' user interactions."
  },
  {
    "id": "saved-1769608044993-dvwrx",
    "title": "AlpacaEval",
    "source": "arXiv",
    "authors": [
      "Yann Dubois",
      "Balázs Galambosi",
      "Percy Liang",
      "Tatsunori B. Hashimoto"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.04475",
    "githubLink": "https://github.com/tatsu-lab/alpaca_eval",
    "itemCount": "805 instructions",
    "specs": "Instruction following, Text, Pairwise comparison",
    "description": "An automatic evaluator for instruction-following models that measures win rates against a reference model (e.g., GPT-4 Turbo) using an LLM-as-a-judge. It simplifies the AlpacaFarm dataset for faster and cheaper evaluation."
  },
  {
    "id": "saved-1769608044993-tvafv",
    "title": "RewardBench",
    "source": "arXiv",
    "authors": [
      "Nathan Lambert",
      "Valentina Pyatkin",
      "Jacob Morrison",
      "LJ Miranda",
      "Bill Yuchen Lin",
      "Khyathi Chandu",
      "Nouha Dziri",
      "Sachin Kumar",
      "Tom Zick",
      "Yejin Choi",
      "Noah A. Smith",
      "Hannaneh Hajishirzi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.13913",
    "githubLink": "https://github.com/allenai/reward-bench",
    "itemCount": "2,985 samples",
    "specs": "Text, Pairwise comparison, Reward modeling",
    "description": "A comprehensive benchmark designed to evaluate reward models (which often serve as judges) across multiple categories including Chat, Chat Hard, Safety, and Reasoning. It highlights the limitations of using a single metric for reward modeling."
  },
  {
    "id": "saved-1769608044993-y95h6",
    "title": "Arena-Hard-Auto",
    "source": "arXiv",
    "authors": [
      "Tianle Li",
      "Wei-Lin Chiang",
      "Evan Frick",
      "Lisa Dunlap",
      "Tianhao Wu",
      "Banghua Zhu",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.11939",
    "githubLink": "https://github.com/lmarena-ai/arena-hard-auto",
    "itemCount": "500 prompts",
    "specs": "Text, Open-ended generation, Pairwise comparison",
    "description": "A benchmark consisting of challenging prompts curated from live Chatbot Arena data. It employs an automated pipeline (BenchBuilder) to select high-quality prompts that differentiate model performance effectively."
  },
  {
    "id": "saved-1769608044993-64zk1",
    "title": "WildBench",
    "source": "arXiv",
    "authors": [
      "Bill Yuchen Lin",
      "Yuntian Deng",
      "Khyathi Chandu",
      "Faeze Brahman",
      "Abhilasha Ravichander",
      "Valentina Pyatkin",
      "Nouha Dziri",
      "Ronan Le Bras",
      "Yejin Choi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.04770",
    "githubLink": "https://github.com/allenai/WildBench",
    "itemCount": "1,024 tasks",
    "specs": "Real-world queries, Text, Score-based/Pairwise",
    "description": "An automated evaluation framework benchmarking LLMs on challenging tasks derived from real-world user queries (WildChat). It uses advanced metrics like WB-Reward and WB-Score to correlate with human judgment."
  },
  {
    "id": "saved-1769608044994-61bh1",
    "title": "BiGGen-Bench",
    "source": "arXiv",
    "authors": [
      "Seungone Kim",
      "Juyoung Suk",
      "Ji Yong Cho",
      "Shayne Longpre",
      "Chaeeun Kim",
      "Dongkeun Yoon",
      "Guijin Son",
      "Yejin Cho",
      "Sheikh Shafayat",
      "Jinheon Baek",
      "Sue Hyun Park",
      "Hyeonbin Hwang",
      "Jinkyung Jo",
      "Hyowon Cho",
      "Haebin Shin",
      "Seongyun Lee",
      "Hanseok Oh",
      "Noah Lee",
      "Namgyu Ho",
      "Se June Joo",
      "Miyoung Ko",
      "Yoonjoo Lee",
      "Hyungjoo Chae",
      "Jamin Shin",
      "Joel Jang",
      "Seonghyeon Ye",
      "Bill Yuchen Lin",
      "Sean Welleck",
      "Graham Neubig",
      "Moontae Lee",
      "Kyungjae Lee",
      "Minjoon Seo"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.06658",
    "githubLink": "https://github.com/test/test",
    "itemCount": "77 tasks (765 instances)",
    "specs": "Text, Fine-grained evaluation, Instance-specific criteria",
    "description": "A principled benchmark for fine-grained evaluation of language models across nine capabilities. It uses instance-specific evaluation criteria to provide more nuanced assessments than general rubrics."
  },
  {
    "id": "saved-1769608044994-47hag",
    "title": "Aegis AI Content Safety Dataset",
    "source": "Hugging Face",
    "authors": [
      "Shaona Ghosh",
      "Prasoon Varshney",
      "Eric Zhu",
      "Dwaraknath Gnaneshwar",
      "Christopher Parisien"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.05993",
    "githubLink": "https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0",
    "itemCount": "33,416 interactions (v2.0)",
    "specs": "Text; 13 risk categories; Annotation taxonomy",
    "description": "A large-scale content safety dataset comprising manually annotated interactions between humans and LLMs, covering 13 critical risk categories to support guardrail development."
  },
  {
    "id": "saved-1769608044994-uuxlv",
    "title": "M3oralBench: A MultiModal Moral Benchmark for LVLMs",
    "source": "arXiv",
    "authors": [
      "Bei Yan",
      "Jie Zhang",
      "Zheng Chen",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.20718",
    "githubLink": "https://github.com/BeiiiY/M3oralBench",
    "itemCount": "4,640 samples (1,160 scenarios with variations)",
    "specs": "Multimodal (Text + Image); Tasks include Moral Judgment, Moral Classification, and Moral Response",
    "description": "The first multimodal moral benchmark for Large Vision-Language Models (LVLMs). It expands on everyday moral scenarios from Moral Foundations Vignettes (MFVs) by generating corresponding images, evaluating models on moral judgment, classification, and response tasks."
  },
  {
    "id": "saved-1769608044994-y2ryg",
    "title": "SHIELD: An Evaluation Benchmark for Face Spoofing and Forgery Detection",
    "source": "arXiv",
    "authors": [
      "Yichen Shi",
      "Yuhao Gao",
      "Yingxin Lai",
      "Xiaochun Cao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.04178",
    "githubLink": "https://github.com/laiyingxin2/SHIELD",
    "itemCount": "Not specified (Evaluates on aggregated subsets of CelebA-Spoof, SiW, etc.)",
    "specs": "Multimodal (Text, RGB Image, Infrared, Depth, Audio); Tasks: Face Anti-Spoofing (6 attack types), Face Forgery Detection (GAN/Diffusion based)",
    "description": "A benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to detect subtle visual spoofing and forgery clues. It consists of two main tasks: face anti-spoofing (detecting physical attacks like masks or screens) and face forgery detection (identifying digital manipulations like DeepFakes). The benchmark employs true/false and multiple-choice questions to assess model performance across diverse attack types and modalities."
  },
  {
    "id": "saved-1769608044994-rq0xf",
    "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation",
    "source": "arXiv",
    "authors": [
      "Xiaoze Liu",
      "Ting Sun",
      "Tianyang Xu",
      "Feijie Wu",
      "Cunxiang Wang",
      "Xiaoqian Wang",
      "Jing Gao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.12975",
    "githubLink": "https://github.com/xz-liu/SHIELD",
    "itemCount": "420 text samples (100 BS-NC, 100 BS-C, 20 BS-PC, 100 SSRL, 100 BEP)",
    "specs": "Text-only; Categories: Best Selling Books (Non-Copyrighted, Copyrighted, Partially Copyrighted), Spotify Lyrics, English Poems",
    "description": "A curated benchmark dataset aimed at evaluating copyright compliance in Large Language Models. It includes a collection of text materials with varying copyright statuses (copyrighted, public domain, and partially copyrighted) to test if models infringe on intellectual property or are overprotective. The benchmark also assesses the robustness of models against jailbreaking attacks designed to elicit copyrighted content."
  },
  {
    "id": "saved-1769608044994-b37w8",
    "title": "PadChest-GR",
    "source": "arXiv",
    "authors": [
      "Daniel Coelho de Castro",
      "Shruthi Bannur",
      "Stephanie Hyland",
      "Pratik Ghosh",
      "Mercy Ranjit",
      "Kenza Bouzid",
      "Anton Schwaighofer",
      "Fernando Pérez-García",
      "Harshita Sharma",
      "Ozan Oktay",
      "Matthew Lungren",
      "Javier Alvarez-Valle",
      "Aditya Nori",
      "Anja Thieme"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.05085",
    "githubLink": "https://github.com/microsoft/PadChest-GR",
    "itemCount": "Unknown subset of PadChest (original 160k images)",
    "specs": "Chest X-rays, Reports (En/Es), Bounding box grounding",
    "description": "A large-scale bilingual (Spanish/English) grounded radiology reporting benchmark dataset derived from PadChest, featuring bounding box annotations for findings."
  },
  {
    "id": "saved-1769608044994-vwql8",
    "title": "XBRL-Agent Datasets",
    "source": "Hugging Face",
    "authors": [
      "Shijie Han",
      "Haoqiang Kang",
      "Bo Jin",
      "Xiao-Yang Liu",
      "Steve Yang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.11159",
    "githubLink": "https://huggingface.co/datasets/TurnipBit/XBRL-Agent-Dataset",
    "itemCount": "1,700 samples",
    "specs": "Text, XBRL Financial Data",
    "description": "A collection of datasets (XBRL Terminology, Financial Formula Calculation) designed to evaluate LLMs on their ability to interpret and analyze XBRL (eXtensible Business Reporting Language) filings, a standard for digital business reporting."
  },
  {
    "id": "saved-1769608044994-44dit",
    "title": "MedSafetyBench",
    "source": "arXiv",
    "authors": [
      "Tessa Han",
      "Aounon Kumar",
      "Chirag Agarwal",
      "Himabindu Lakkaraju"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.03744",
    "githubLink": "https://github.com/AI4LIFE-GROUP/med-safety-bench",
    "itemCount": "1,800 harmful medical requests",
    "specs": "Text (harmful queries and safe response pairs)",
    "description": "The first benchmark dataset designed to measure the medical safety of LLMs based on the Principles of Medical Ethics. It includes harmful medical requests and safe responses to evaluate and fine-tune models for safety while preserving medical performance."
  },
  {
    "id": "saved-1769608044994-bpww2",
    "title": "MEDEC",
    "source": "arXiv",
    "authors": [
      "Asma Ben Abacha",
      "Wen-wai Yim",
      "Yujuan Fu",
      "Zhaoyi Sun",
      "Meliha Yetisgen",
      "Fei Xia",
      "Thomas Lin"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.18567",
    "githubLink": "https://github.com/abachaa/MEDEC",
    "itemCount": "3,848 clinical texts",
    "specs": "Text (Clinical notes with annotated errors and corrections)",
    "description": "A benchmark for medical error detection and correction in clinical notes. It covers five types of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism) to assess the ability of LLMs to validate clinical text."
  },
  {
    "id": "saved-1769608044994-3of6m",
    "title": "MultiADE",
    "source": "arXiv",
    "authors": [
      "Xiang Dai",
      "Sarvnaz Karimi",
      "Abeed Sarker",
      "Ben Hachey",
      "Cecile Paris"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.03780",
    "githubLink": "https://github.com/daixiangau/MultiADE",
    "itemCount": "Aggregates 6 datasets (approx. 20,000+ sentences/samples combined)",
    "specs": "Text (Annotated entities for adverse drug events)",
    "description": "A multi-domain benchmark for Adverse Drug Event (ADE) extraction that standardizes six datasets from different sources (clinical notes, medical literature, social media) to test domain generalization in pharmacovigilance models."
  },
  {
    "id": "saved-1769608044994-278qh",
    "title": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models",
    "source": "arXiv",
    "authors": [
      "Yichi Zhang",
      "Yao Huang",
      "Yitong Sun",
      "Chang Liu",
      "Zhe Zhao"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.07057",
    "githubLink": "https://multi-trust.github.io/",
    "itemCount": "32 tasks",
    "specs": "Multimodal (Text/Image); 5 trustworthiness aspects",
    "description": "A unified benchmark for the trustworthiness of Multimodal LLMs (MLLMs) across five primary aspects: truthfulness, safety, robustness, fairness, and privacy, covering 32 diverse tasks."
  },
  {
    "id": "saved-1769608044994-cwy1o",
    "title": "PASTA (Perceptual Assessment System for explanaTion of AI)",
    "source": "arXiv",
    "authors": [
      "Rémi Kazmierczak",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.02470",
    "githubLink": "https://github.com/remi-kazmierczak/PASTA",
    "itemCount": "Large-scale benchmark (exact count varies by sub-task)",
    "specs": "Images; Saliency maps, Concept-based explanations",
    "description": "A human-centric framework and dataset for evaluating XAI techniques in computer vision. It benchmarks saliency-based and concept-based explanation methods against human judgement, facilitating the creation of metrics that align with human perception."
  },
  {
    "id": "saved-1769608044994-585pq",
    "title": "MathVista",
    "source": "arXiv",
    "authors": [
      "Pan Lu",
      "Hanyu Wang",
      "Jai Bansal",
      "Taeuk Kim",
      "Rishabh Jain",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2310.02255",
    "githubLink": "https://github.com/lupantech/MathVista",
    "itemCount": "6,141 examples",
    "specs": "Derived from 28 existing datasets and 3 new ones (IQTest, FunctionQA, PaperQA); covers geometry, charts, and function plots.",
    "description": "A benchmark specifically designed to evaluate mathematical reasoning in visual contexts, combining diverse mathematical and visual tasks."
  },
  {
    "id": "saved-1769608044994-7ussu",
    "title": "BLINK",
    "source": "arXiv",
    "authors": [
      "Xingyu Fu",
      "Yushi Hu",
      "Bangzheng Li",
      "Yu Feng",
      "Haoyu Wang",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2404.12390",
    "githubLink": "https://github.com/Eyja/BLINK",
    "itemCount": "3,807 questions",
    "specs": "14 classic computer vision tasks reformatted as multiple-choice questions; tasks include relative depth, visual correspondence, and rotation.",
    "description": "A benchmark focusing on core visual perception abilities (e.g., depth estimation, forensics, multi-view reasoning) that are easy for humans but hard for current MLLMs."
  },
  {
    "id": "saved-1769608044994-d7x14",
    "title": "Video-MME",
    "source": "arXiv",
    "authors": [
      "Chaoyou Fu",
      "Yuhan Dai",
      "Yondong Luo",
      "Lei Li",
      "Shuhuai Ren",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.21075",
    "githubLink": "https://github.com/BradyFU/Video-MME",
    "itemCount": "900 videos / 2,700 QAs",
    "specs": "Videos range from short (11s) to long (1h); covers 6 domains (knowledge, film, sports, etc.); includes subtitles and audio.",
    "description": "The first comprehensive benchmark for multi-modal evaluation in video analysis, covering diverse domains and video durations."
  },
  {
    "id": "saved-1769608044994-34f0t",
    "title": "FrontierMath",
    "source": "arXiv",
    "authors": [
      "Elliot Glazer",
      "Ege Erdil",
      "Tamay Besiroglu",
      "Epoch AI Team"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2411.04872",
    "githubLink": "https://epoch.ai/frontiermath",
    "itemCount": "Hundreds of problems (Tiers 1-4)",
    "specs": "Expert-level Mathematics, Text/LaTeX",
    "description": "A benchmark for evaluating advanced mathematical reasoning in AI, featuring hundreds of original, expert-crafted mathematics problems that span major branches of modern mathematics and typically require hours or days for expert mathematicians to solve. The problems are kept unpublished to prevent contamination."
  },
  {
    "id": "saved-1769608044994-xnl6q",
    "title": "SimpleQA",
    "source": "Other",
    "authors": [
      "OpenAI",
      "Jason Wei",
      "Nguyen",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://openai.com/index/introducing-simpleqa",
    "githubLink": "https://github.com/openai/simple-evals",
    "itemCount": "4,326 questions",
    "specs": "Short-form Text Q&A",
    "description": "A factuality benchmark measuring the ability of language models to answer short, fact-seeking questions. The dataset is designed to be challenging for frontier models and easy to grade, with questions adversarially collected to ensure high correctness and low ambiguity."
  },
  {
    "id": "saved-1769608044994-21bp6",
    "title": "JailBreakV-28K",
    "source": "Hugging Face",
    "authors": [
      "Weidi Luo",
      "Siyuan Ma",
      "Xiaogeng Liu",
      "Xiaoyu Guo"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/papers/2404.03027",
    "githubLink": "https://github.com/JailBreakV/JailBreakV",
    "itemCount": "28,000 test cases (20,000 text, 8,000 image)",
    "specs": "Text, Image (Multimodal)",
    "description": "A benchmark designed to assess the transferability of LLM jailbreak techniques to Multimodal Large Language Models (MLLMs). It includes a diverse set of text-based and image-based jailbreak inputs."
  },
  {
    "id": "saved-1769608044994-ubgog",
    "title": "DriveWorld",
    "source": "arXiv",
    "authors": [
      "Yvan Yin",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.04390",
    "githubLink": "https://github.com/YvanYin/DrivingWorld",
    "itemCount": "Evaluated on OpenScene, nuPlan, nuScenes",
    "specs": "Multi-camera driving videos, 4D Spatio-temporal representation, Occupancy grid",
    "description": "A framework and benchmark protocol for 4D pre-trained scene understanding via world models in autonomous driving. It evaluates tasks like 3D object detection, occupancy prediction, and motion forecasting using a world model-based representation."
  },
  {
    "id": "saved-1769608044994-w7p5e",
    "title": "PhyBench",
    "source": "arXiv",
    "authors": [
      "Fanqing Meng",
      "Wenqi Shao",
      "Lixin Luo",
      "Yahong Wang",
      "Yiran Chen",
      "Quanfeng Lu",
      "Yue Yang",
      "Tianshuo Yang",
      "Kaipeng Zhang",
      "Yu Qiao",
      "Ping Luo"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.11802",
    "githubLink": "https://github.com/OpenGVLab/PhyBench",
    "itemCount": "700 prompts",
    "specs": "Text-to-Image prompts; 4 categories: mechanics, optics, thermodynamics, material properties",
    "description": "A benchmark for evaluating the physical commonsense of text-to-image models. It focuses on whether generated images adhere to fundamental physical principles."
  },
  {
    "id": "saved-1769608044994-9ahjf",
    "title": "Seal-Tools",
    "source": "arXiv",
    "authors": [
      "Meng Wei",
      "Minghao Li",
      "Yingxiu Zhao",
      "Bowen Yu",
      "Yongbin Li"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.08355",
    "githubLink": "https://github.com/fairymax/Seal-Tools",
    "itemCount": "~4,000 tools, Multiple instances",
    "specs": "JSON tool definitions, API-like interactions, Nested calls",
    "description": "A self-instruct tool learning dataset for agent tuning and benchmarking. It focuses on tool-use capabilities, including nested tool callings and multi-step reasoning, serving as a benchmark to evaluate the tool-calling and orchestration ability of LLMs."
  },
  {
    "id": "saved-1769608044994-e3ebm",
    "title": "Cloud Data Orchestration Dataset",
    "source": "Other",
    "authors": [
      "Kaggle Community"
    ],
    "year": "2024",
    "paperLink": "https://www.kaggle.com/datasets/cloud-data-orchestration",
    "githubLink": "N/A",
    "itemCount": "7,365 records",
    "specs": "Tabular data (CSV), System metrics, Categorical/Continuous features",
    "description": "A dataset simulating the behavior of intelligent agents in a hybrid cloud environment. It is designed to model autonomous, context-aware decision-making for operations such as data migration, replication, and deletion using reinforcement learning techniques."
  },
  {
    "id": "saved-1769608044994-e8yk9",
    "title": "SciInstruct (SciGLM)",
    "source": "arXiv",
    "authors": [
      "Dan Zhang",
      "Ziniu Hu",
      "Sining Zhoubian",
      "Zhengxiao Du",
      "Kaiyu Yang",
      "Zihan Wang",
      "Yisong Yue",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2401.07950",
    "githubLink": "https://github.com/THUDM/SciGLM",
    "itemCount": "254,051 instructions",
    "specs": "Text-based; Instruction Tuning; Domains: Math, Physics, Chemistry, Formal Proofs",
    "description": "A diverse and high-quality instruction tuning dataset for college-level scientific reasoning, created using a self-reflective annotation framework. It includes data for physics, chemistry, math, and formal proofs."
  },
  {
    "id": "saved-1769608044994-97nwg",
    "title": "CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence",
    "source": "arXiv",
    "authors": [
      "Md Tanvirul Alam",
      "Dipkamal Bhusal",
      "Le Nguyen",
      "Nidhi Rastogi"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2410.21323",
    "githubLink": "https://github.com/xashru/cti-bench",
    "itemCount": "5,610 samples",
    "specs": "5 datasets: MCQ, Root Cause Mapping (RCM), Vulnerability Severity Prediction (VSP), Attack Technique Extraction (ATE), Threat Actor Attribution (TAA)",
    "description": "A comprehensive benchmark suite designed to evaluate LLMs in Cyber Threat Intelligence (CTI). It assesses capabilities in understanding crucial CTI concepts, mapping vulnerabilities (CVE to CWE), predicting severity (CVSS), and attributing threats."
  },
  {
    "id": "saved-1769608044994-uch43",
    "title": "CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity",
    "source": "arXiv",
    "authors": [
      "Norbert Tihanyi",
      "Mohamed Amine Ferrag",
      "Merouane Debbah",
      "Thierry Lestable"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.07688",
    "githubLink": "https://github.com/Norbix/CyberMetric",
    "itemCount": "10,000 questions",
    "specs": "Multiple Choice Questions (MCQ) in varying dataset sizes (80, 500, 2000, 10000)",
    "description": "A benchmark dataset designed to evaluate the cybersecurity knowledge of LLMs. It contains questions sourced from standards, certifications, research papers, and books, verified by human experts."
  },
  {
    "id": "saved-1769608044994-piewp",
    "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity",
    "source": "arXiv",
    "authors": [
      "Xiangmin Shen",
      "Lingzhi Wang",
      "Zhenyuan Li",
      "Yan Chen",
      "Wencheng Zhao",
      "Dawei Sun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2412.20787",
    "githubLink": "https://github.com/CloudSecurityAlliance/SecBench",
    "itemCount": "44,823 MCQs and 3,087 SAQs",
    "specs": "Multiple Choice Questions (MCQs) and Short Answer Questions (SAQs)",
    "description": "A multi-dimensional benchmarking dataset for evaluating LLMs in cybersecurity, covering knowledge retention and logical reasoning across multiple languages (Chinese/English) and sub-domains."
  },
  {
    "id": "saved-1769608044994-hg3xc",
    "title": "LSPR23: A novel IDS dataset from the largest live-fire cybersecurity exercise",
    "source": "Scholar",
    "authors": [
      "Roland Meier",
      "Luca Gambazzi",
      "Vincent Lenders"
    ],
    "year": "2024",
    "paperLink": "https://doi.org/10.1016/j.jisa.2024.104245",
    "githubLink": "https://roland-meier.ch/",
    "itemCount": "400 GB network traffic, 6 million log entries",
    "specs": "PCAP network traffic and host-based event logs",
    "description": "A labelled Intrusion Detection System (IDS) dataset derived from the Locked Shields live-fire cyber defense exercise. It captures real-world red team attacks and blue team defenses."
  },
  {
    "id": "saved-1769608044994-ahiwj",
    "title": "Aya Dataset",
    "source": "arXiv",
    "authors": [
      "Shivalika Singh",
      "Freddie Vargus",
      "Daniel D'souza",
      "Börje F. Karlsson",
      "Abinaya Mahendiran",
      "Wei-Yin Ko",
      "Herumb Shandilya",
      "Jay Patel",
      "Deividas Mataciunas",
      "Laura O'Mahony",
      "Mike Zhang",
      "Ramith Hettiarachchi",
      "Joseph Wilson",
      "Marina Machado",
      "Luisa Souza Moura",
      "Dominik Krzemiński",
      "Hakimeh Fadaei",
      "Irem Ergun",
      "Ifeoma Okoh",
      "Aisha Alaagib",
      "Oshan Ivantha Mudannayake",
      "Zaid Alyafeai",
      "Vu Minh Chien",
      "Sebastian Ruder",
      "Surya Guthikonda",
      "Emad A. Alghamdi",
      "Sebastian Gehrmann",
      "Niklas Muennighoff",
      "Max Bartolo",
      "Julia Kreutzer",
      "Ahmet Üstün",
      "Marzieh Fadaee",
      "Sara Hooker"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.06619",
    "githubLink": "https://huggingface.co/datasets/CohereForAI/aya_dataset",
    "itemCount": "65 languages (human-curated), 114 languages (collection)",
    "specs": "Text; Instruction Tuning",
    "description": "An open-access collection for multilingual instruction tuning, including human-curated examples and a massive collection of templated instructions."
  },
  {
    "id": "saved-1769608044994-whdta",
    "title": "AgentHarm",
    "source": "Hugging Face",
    "authors": [
      "UK AI Safety Institute",
      "Gray Swan AI"
    ],
    "year": "2024",
    "paperLink": "https://huggingface.co/datasets/ai-safety-institute/AgentHarm",
    "githubLink": "https://github.com/grayswan-ai/AgentHarm",
    "itemCount": "44 base behaviors (176 augmented)",
    "specs": "Text (Agent tasks); Harmful behaviors",
    "description": "A benchmark for measuring the harmfulness of LLM agents. It includes a diverse set of behaviors designed to test agent safety in tool-use contexts."
  },
  {
    "id": "saved-1769608044994-mj245",
    "title": "PAD (Purple-teaming LLMs with Adversarial Defender training)",
    "source": "arXiv",
    "authors": [
      "Minda Hu",
      "Researchers at CatalyzeX/Other"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.01850",
    "githubLink": "https://github.com/NA",
    "itemCount": "Variable (Self-play generated conversational data)",
    "specs": "Conversational text data (multi-turn dialogues between attacker and defender models).",
    "description": "A framework and pipeline for safeguarding LLMs by incorporating red-teaming (attack) and blue-teaming (safety training) techniques in a self-play manner. It automatically collects conversational data covering specific safety risks where an attacker elicits unsafe responses and a defender generates safe ones."
  },
  {
    "id": "saved-1769608044994-niwtq",
    "title": "Ferret",
    "source": "Other",
    "authors": [
      "Declare Lab",
      "Soujanya Poria",
      "et al."
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.10647",
    "githubLink": "https://github.com/declare-lab/ferret",
    "itemCount": "Framework for generating infinite adversarial prompts",
    "specs": "Text (Adversarial prompts and model responses).",
    "description": "An automated red-teaming framework that uses reward-based scoring to generate effective adversarial prompts. While primarily a red-teaming tool, it is used in purple team contexts to iteratively improve model robustness (blue team defense) against evolving attacks."
  },
  {
    "id": "saved-1769608044994-iejuz",
    "title": "AudioMarkBench",
    "source": "arXiv",
    "authors": [
      "Hongbin Liu",
      "Moyang Guo",
      "Zhengyuan Jiang",
      "Lun Wang",
      "Neil Zhenqiang Gong"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2406.06979",
    "githubLink": "https://github.com/moyangkuo/AudioMarkBench",
    "itemCount": "Derived from Common Voice dataset",
    "specs": "Audio; Watermarking; Adversarial Perturbations",
    "description": "A systematic benchmark for evaluating the robustness of audio watermarking techniques against adversarial attacks, including watermark removal and forgery, under various settings (white-box, black-box, no-box)."
  },
  {
    "id": "saved-1769608044994-u8667",
    "title": "HARPER (Human from an Articulated Robot Perspective)",
    "source": "arXiv",
    "authors": [
      "Andrea Avogaro",
      "Andrea Toaiari",
      "Federico Cunico",
      "Xiangmin Xu",
      "Haralambos Dafas",
      "Alessandro Vinciarelli",
      "Emma Li",
      "Marco Cristani"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2403.00000",
    "githubLink": "https://github.com/Intelligo-Labs/HARPER",
    "itemCount": "15 actions, 17 participants, multiple views",
    "specs": "RGB-D, 3D skeletal poses, robot egocentric view, OptiTrack ground truth",
    "description": "A dataset for 3D human pose estimation and forecasting in dyadic interactions between users and a quadruped robot (Spot). It focuses on the robot's egocentric perspective during collaborative tasks."
  },
  {
    "id": "saved-1769608044994-46pvr",
    "title": "HAGS (Hand and Glove Segmentation Dataset)",
    "source": "arXiv",
    "authors": [
      "UT Nuclear Robotics Group"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2407.14649",
    "githubLink": "https://github.com/UTNuclearRoboticsPublic/assembly_glovebox_dataset",
    "itemCount": "191 videos, 1728 labeled frames",
    "specs": "Video, pixel-level segmentation masks, RGB images",
    "description": "A dataset designed for human-robot collaboration in industrial glovebox environments. It focuses on pixel-level segmentation of hands and gloves to support safe cooperative assembly tasks."
  },
  {
    "id": "saved-1769608044994-vqesb",
    "title": "CREW Platform and Benchmark",
    "source": "arXiv",
    "authors": [
      "Lingyu Zhang",
      "Zhengran Ji",
      "Boyuan Chen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2408.00170",
    "githubLink": "https://github.com/GeneralRoboticsLab/CREW",
    "itemCount": "Data from 50 human subject studies",
    "specs": "Unity environments, Python agents, physiological data (e.g., gaze, EEG support)",
    "description": "A platform and benchmark for facilitating human-AI teaming research in real-time decision-making scenarios. It includes pre-built tasks and supports multimodal physiological signal recording."
  },
  {
    "id": "saved-1769608044994-yrnc6",
    "title": "WorkBench",
    "source": "arXiv",
    "authors": [
      "Olly Styles",
      "Sam Miller",
      "Patricio Cerda-Mardini",
      "Tanaya Guha",
      "Victor Sanchez",
      "Bertie Vidgen"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2405.00823",
    "githubLink": "https://github.com/olly-styles/WorkBench",
    "itemCount": "690 tasks; 26 tools; 5 databases",
    "specs": "Text; Database interactions; Tool use; Sandbox environment",
    "description": "A benchmark dataset for evaluating agents' ability to execute common business activities such as sending emails and scheduling meetings in a realistic workplace setting."
  },
  {
    "id": "saved-1769608044994-kbyp8",
    "title": "TravelPlanner",
    "source": "arXiv",
    "authors": [
      "Jian Xie",
      "Kai Zhang",
      "Jiangjie Chen",
      "Ting-En Lin",
      "Lou Jie",
      "Zhanghui Kuang",
      "Furu Wei",
      "Li Yuan",
      "Yu Su"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.01622",
    "githubLink": "https://github.com/OSU-NLP-Group/TravelPlanner",
    "itemCount": "1,225 queries",
    "specs": "Text; Tool use; Planning constraints; Multi-turn",
    "description": "A benchmark for real-world planning with language agents, focusing on travel planning which requires tool use and complex planning within multiple constraints."
  },
  {
    "id": "saved-1769608044994-6n54t",
    "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning (SciToolBench)",
    "source": "arXiv",
    "authors": [
      "Ziru Chen",
      "Shijie Chen",
      "Yuting Ning",
      "Qianheng Zhang",
      "Boshi Wang",
      "Botao Yu",
      "Yifei Li",
      "Zeyi Liao",
      "Chen Wei",
      "Zitong Lu",
      "Vishal Dey",
      "Mingyi Xue",
      "Frazier N. Baker",
      "Benjamin Burns",
      "Daniel Adu-Ampratwum",
      "Xuhui Huang",
      "Xia Ning",
      "Song Gao",
      "Yu Su",
      "Huan Sun"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2402.11451",
    "githubLink": "https://github.com/OSU-NLP-Group/SciAgent",
    "itemCount": "856 questions; 2,446 functions",
    "specs": "Scientific domains (Math, Physics, Chemistry, etc.), tool-augmented reasoning tasks.",
    "description": "Evaluates tool-augmented LLMs on scientific reasoning tasks across 5 domains. The benchmark, SciToolBench, requires agents to retrieve, understand, and use specific functions to solve scientific problems."
  },
  {
    "id": "saved-1769608044994-zqk4n",
    "title": "Benchmarking the Sim-to-Real Gap in Cloth Manipulation",
    "source": "arXiv",
    "authors": [
      "David Blanco-Mulero",
      "Oriol Barbany",
      "Gokhan Alcan",
      "Adria Colome",
      "Carme Torras",
      "Ville Kyrki"
    ],
    "year": "2024",
    "paperLink": "https://arxiv.org/abs/2310.09543",
    "githubLink": "https://sites.google.com/view/cloth-sim2real-benchmark",
    "itemCount": "Not specified (Multiple trajectories across 3 fabric types)",
    "specs": "Point clouds, depth images, robot trajectories (dynamic & quasi-static tasks)",
    "description": "A benchmark dataset designed to evaluate the reality gap between deformable object simulators (MuJoCo, Bullet, Flex, SOFA) and real-world data. It involves dynamic and quasi-static cloth manipulation tasks."
  },
  {
    "id": "saved-1769608044994-0jlu0",
    "title": "SafetyBench",
    "source": "arXiv",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Lindong Wu",
      "Rui Sun",
      "Yongkang Huang",
      "Chong Long",
      "Xiao Liu",
      "Xuanyu Lei",
      "Jie Tang",
      "Minlie Huang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.07045",
    "githubLink": "https://github.com/thu-coai/SafetyBench",
    "itemCount": "11,435 questions",
    "specs": "Multiple-choice questions (Text), Chinese and English",
    "description": "A comprehensive benchmark for evaluating the safety of Large Language Models (LLMs) using multiple-choice questions across 7 distinct categories of safety concerns, available in both Chinese and English."
  },
  {
    "id": "saved-1769608044994-qo9ew",
    "title": "Do Not Answer",
    "source": "arXiv",
    "authors": [
      "Yuxia Wang",
      "Haonan Li",
      "Xudong Han",
      "Preslav Nakov",
      "Timothy Baldwin"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.13387",
    "githubLink": "https://github.com/Libr-AI/do-not-answer",
    "itemCount": "939 prompts",
    "specs": "Text prompts, 12 harm categories (3-level taxonomy)",
    "description": "An open-source dataset to evaluate LLM safeguards, consisting of prompts across various harm types that responsible models should refuse to answer."
  },
  {
    "id": "saved-1769608044994-d4s4g",
    "title": "BeaverTails",
    "source": "arXiv",
    "authors": [
      "Jiaming Ji",
      "Mickel Liu",
      "Juntao Dai",
      "Xuehai Pan",
      "Chi Zhang",
      "Ce Bian",
      "Boyuan Chen",
      "Ruiyang Sun",
      "Yizhou Wang",
      "Yaodong Yang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.04657",
    "githubLink": "https://github.com/PKU-Alignment/safe-rlhf",
    "itemCount": "330,000+ QA pairs",
    "specs": "Text QA pairs, Safety meta-labels, Human-preference data",
    "description": "A dataset fostering safety alignment research in LLMs, separating helpfulness and harmlessness annotations for question-answering pairs."
  },
  {
    "id": "saved-1769608044994-2pl0t",
    "title": "AdvBench",
    "source": "arXiv",
    "authors": [
      "Andy Zou",
      "Zifan Wang",
      "J. Zico Kolter",
      "Matt Fredrikson"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.15043",
    "githubLink": "https://github.com/llm-attacks/llm-attacks",
    "itemCount": "500 harmful behaviors",
    "specs": "Text strings/instructions",
    "description": "A dataset of harmful behaviors and instructions used to test adversarial attacks (jailbreaking) on aligned language models."
  },
  {
    "id": "saved-1769608044994-gp15l",
    "title": "AgentBench",
    "source": "arXiv",
    "authors": [
      "Xiao Liu",
      "Hao Yu",
      "Hanchen Zhang",
      "Yifan Xu",
      "Lei Lei",
      "Hanyu Lai",
      "Yu Gu",
      "Hangliang Ding",
      "Kaiwen Men",
      "Kejunjie Yang",
      "Shudan Zhang",
      "Xiang Deng",
      "Aohan Zeng",
      "Zhengxiao Du",
      "Chenhui Zhang",
      "Sheng Shen",
      "Tianjun Zhang",
      "Yu Su",
      "Huan Sun",
      "Minlie Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.03688",
    "githubLink": "https://github.com/THUDM/AgentBench",
    "itemCount": "8 environments",
    "specs": "Multi-turn open-ended generation; Environments include OS, Database, Knowledge Graph, Card Game, Householding, Web Shopping",
    "description": "A comprehensive benchmark designed to evaluate LLMs as autonomous agents across 8 distinct environments, including Operating Systems, Databases, Knowledge Graphs, Digital Card Games, Householding, and Web Shopping."
  },
  {
    "id": "saved-1769608044994-emdrt",
    "title": "WebArena",
    "source": "arXiv",
    "authors": [
      "Shuyan Zhou",
      "Frank F. Xu",
      "Hao Zhu",
      "Xuhui Zhou",
      "Robert Lo",
      "Abishek Sridhar",
      "Xianyi Cheng",
      "Tianyue Ou",
      "Yonatan Bisk",
      "Daniel Fried",
      "Uri Alon",
      "Graham Neubig"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.13854",
    "githubLink": "https://github.com/web-arena-x/webarena",
    "itemCount": "812 test tasks",
    "specs": "Web-based tasks, Text/HTML modality, Dockerized environments",
    "description": "A realistic and reproducible web environment for building and evaluating autonomous agents. It consists of fully functional websites from four common domains (e-commerce, social forums, software development, content management) to test agents on long-horizon web tasks."
  },
  {
    "id": "saved-1769608044994-upwrp",
    "title": "SWE-bench",
    "source": "arXiv",
    "authors": [
      "Carlos E. Jimenez",
      "John Yang",
      "Alexander Wettig",
      "Shunyu Yao",
      "Karthik Narasimhan",
      "Ofir Press"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.06770",
    "githubLink": "https://github.com/swe-bench/swe-bench",
    "itemCount": "2,294 task instances",
    "specs": "Python codebases, GitHub issues/PRs, Docker execution environment",
    "description": "A benchmark for evaluating large language models on real-world software engineering issues collected from GitHub. It tasks agents with resolving issues (bugs/features) in popular Python repositories by generating patches that pass new tests."
  },
  {
    "id": "saved-1769608044994-w099y",
    "title": "ToolBench",
    "source": "arXiv",
    "authors": [
      "Yujia Qin",
      "Shihao Liang",
      "Yining Ye",
      "Kunlun Zhu",
      "Lan Yan",
      "Yaxi Lu",
      "Yankai Lin",
      "Xin Cong",
      "Xiangru Tang",
      "Bill Qian",
      "Sihan Zhao",
      "Runchu Tian",
      "Ruobing Xie",
      "Jie Zhou",
      "Mark Gerstein",
      "Dahai Li",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.16789",
    "githubLink": "https://github.com/OpenBMB/ToolBench",
    "itemCount": "16,000+ real-world APIs",
    "specs": "Instruction tuning data, API calls, Text modality",
    "description": "An open platform for training, serving, and evaluating LLMs for tool learning. It includes a large-scale instruction-tuning dataset constructed automatically to help models master diverse real-world APIs."
  },
  {
    "id": "saved-1769608044994-cvzsw",
    "title": "GAIA",
    "source": "arXiv",
    "authors": [
      "Grégoire Mialon",
      "Clémentine Fourrier",
      "Craig Swift",
      "Thomas Wolf",
      "Yann LeCun",
      "Thomas Scialom"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.12983",
    "githubLink": "https://github.com/gaia-benchmark/gaia",
    "itemCount": "466 questions",
    "specs": "Text questions, varying modalities (images, files), Tool use required",
    "description": "A benchmark for General AI Assistants featuring conceptually simple questions for humans that are challenging for AI. It requires fundamental abilities such as reasoning, tool use, and multi-modality to solve real-world tasks."
  },
  {
    "id": "saved-1769608044994-cundn",
    "title": "MINT",
    "source": "arXiv",
    "authors": [
      "Zihan Wang",
      "Mao Xue",
      "Vicki Cheung",
      "Hang Ma",
      "Duarte Alves",
      "Sitao Xiang",
      "Lehman",
      "He",
      "Graham Neubig"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.10691",
    "githubLink": "https://github.com/zihanguai/MINT",
    "itemCount": "Derived from 8 datasets",
    "specs": "Multi-turn interaction, Tool use, Natural Language Feedback",
    "description": "A benchmark evaluating LLMs in multi-turn interactions with tools and natural language feedback. It repurposes established datasets to test reasoning, coding, and decision-making with simulated user feedback."
  },
  {
    "id": "saved-1769608044994-g2siq",
    "title": "SmartPlay",
    "source": "arXiv",
    "authors": [
      "Yue Wu",
      "Xuan Zhang",
      "Yue Zhang",
      "Pan Lu",
      "Steven Hoi",
      "Caiming Xiong",
      "Ran Xu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.01557",
    "githubLink": "https://github.com/microsoft/SmartPlay",
    "itemCount": "6 games",
    "specs": "Game environments, Multi-turn, Spatial/Strategic reasoning",
    "description": "A benchmark for LLMs as intelligent agents featuring 6 distinct games (including Minecraft, Rock-Paper-Scissors, Tower of Hanoi) that test capabilities like planning, spatial reasoning, and learning from history."
  },
  {
    "id": "saved-1769608044994-1zid3",
    "title": "DiverseVul",
    "source": "arXiv",
    "authors": [
      "Yizheng Chen",
      "Zhoujie Ding",
      "Lamya Alowain",
      "Xinyun Chen",
      "David Wagner"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.00409",
    "githubLink": "https://github.com/wagner-group/diversevul",
    "itemCount": "18,945 vulnerable functions, 330,492 non-vulnerable functions",
    "specs": "C/C++ source code",
    "description": "A large and diverse vulnerable source code dataset containing 18,945 vulnerable functions spanning 150 CWEs. It is designed to be more diverse and significantly larger than previous datasets like ReVeal and CVEFixes."
  },
  {
    "id": "saved-1769608044994-5ekr2",
    "title": "CyberSecEval (Purple Llama)",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Cyrus Nikolaidis",
      "Shengye Wan",
      "Ivan Evtimov",
      "Dominik Gabi",
      "Daniel Song",
      "Faizan Ahmad",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.04724",
    "githubLink": "https://github.com/facebookresearch/PurpleLlama",
    "itemCount": "Multiple test suites (thousands of test cases)",
    "specs": "Text/Code (LLM prompts and responses)",
    "description": "A comprehensive benchmark to evaluate the cybersecurity risks of Large Language Models (LLMs), focusing on their propensity to generate insecure code and their compliance with assisting in cyberattacks."
  },
  {
    "id": "saved-1769608044994-c8l1i",
    "title": "FormAI",
    "source": "arXiv",
    "authors": [
      "Norbert Tihanyi",
      "Tamas Bisztray",
      "Ridhi Jain",
      "Mohamed Amine Ferrag",
      "Lucas Cordeiro",
      "Vasileios Mavroeidis"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.02192",
    "githubLink": "https://github.com/FormAI-Dataset/FormAI-Dataset",
    "itemCount": "112,000 (v1) to 331,000 (v2) programs",
    "specs": "AI-generated C programs",
    "description": "A large collection of AI-generated C programs with vulnerability classifications labeled using formal verification (ESBMC). It aims to study vulnerabilities introduced by generative AI."
  },
  {
    "id": "saved-1769608044994-mdmkl",
    "title": "KoSBi (Korean Social Bias Dataset)",
    "source": "arXiv",
    "authors": [
      "Hwaran Lee",
      "Seokhee Hong",
      "Joonsuk Park",
      "Takyoung Kim",
      "Gunhee Kim",
      "Jung-Woo Ha"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.17701",
    "githubLink": "https://github.com/naver-ai/korean-safety-benchmarks",
    "itemCount": "34,000 pairs",
    "specs": "Text, Social Bias pairs",
    "description": "A large-scale dataset for mitigating social bias risks in Large Language Models (LLMs) for Korean. It includes context-sentence pairs covering diverse demographic groups and categories to help train safe sentence classifiers."
  },
  {
    "id": "saved-1769608044994-30y7t",
    "title": "CMMLU",
    "source": "arXiv",
    "authors": [
      "Haonan Li",
      "Yixuan Zhang",
      "Fajri Koto",
      "Yifei Yang",
      "Hai Zhao",
      "Yeyun Gong",
      "Nan Duan",
      "Timothy Baldwin"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.09212",
    "githubLink": "https://github.com/haonan-li/CMMLU",
    "itemCount": "11,528 questions",
    "specs": "Chinese, Multiple-choice, Text, 67 subjects",
    "description": "A comprehensive Chinese benchmark covering 67 subjects across natural science, social sciences, engineering, and humanities, designed to evaluate the knowledge and reasoning capabilities of LLMs in a Chinese context."
  },
  {
    "id": "saved-1769608044995-ql0iq",
    "title": "MMMU (Massive Multi-discipline Multimodal Understanding)",
    "source": "arXiv",
    "authors": [
      "Xiang Yue",
      "Yuansheng Ni",
      "Kai Zhang",
      "Tianyu Zheng",
      "Ruiyu Liu",
      "Ge Zhang",
      "Samuel Stevens",
      "Dongfu Jiang",
      "Weiming Ren",
      "Yuxuan Sun",
      "Cong Wei",
      "Botao Yu",
      "Ruicheng Yuan",
      "Renliang Sun",
      "Ming Yin",
      "Boyuan Zheng",
      "Zhenzhu Yang",
      "Yibo Liu",
      "Wenhu Chen",
      "Yu Su"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.16502",
    "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
    "itemCount": "11,500 questions",
    "specs": "Multimodal (Text + Images: charts, diagrams, maps, chemical structures, etc.). Covers 30 subjects and 183 subfields.",
    "description": "Designed to evaluate 'Expert AGI', this benchmark includes college-level multimodal questions requiring domain-specific knowledge and deliberate reasoning across six core disciplines (e.g., Science, Health, Art)."
  },
  {
    "id": "saved-1769608044995-g2vqx",
    "title": "AGIEval",
    "source": "arXiv",
    "authors": [
      "Wanjun Zhong",
      "Ruixiang Cui",
      "Yiduo Guo",
      "Yaobo Liang",
      "Shuai Lu",
      "Yanlin Wang",
      "Amin Saied",
      "Weizhu Chen",
      "Nan Duan"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.06364",
    "githubLink": "https://github.com/ruixiangcui/AGIEval",
    "itemCount": "~8,000 questions",
    "specs": "Text-based (mostly), Multiple Choice, Cloze. Bilingual (English and Chinese).",
    "description": "A human-centric benchmark derived from official, high-standard admission and qualification exams (e.g., SAT, Gaokao, LSAT) intended for human test-takers, assessing general abilities pertinent to human cognition."
  },
  {
    "id": "saved-1769608044995-sozmc",
    "title": "Violent Non-State Actor (VNSA) CBRN Event Database",
    "source": "Semantic Scholar",
    "authors": [
      "Derrick Tin",
      "Lenard Cheng",
      "Heejun Shin",
      "Ryan Hata",
      "Fredrik Granholm",
      "George Braitberg",
      "Gregory Ciottone",
      "START Consortium"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1017/S1049023X23000481",
    "githubLink": "https://www.start.umd.edu/data-tools/cbrn-data-portal",
    "itemCount": "565 events",
    "specs": "Tabular/Structured Data; Coverage: 1990-2020",
    "description": "A descriptive database analyzing the use of CBRN weapons by violent non-state actors globally, recording specific agents, locations, and outcomes of events."
  },
  {
    "id": "saved-1769608044995-b9lov",
    "title": "Foundation Model Transparency Index (FMTI)",
    "source": "arXiv",
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Betty Xiong",
      "Nestor Maslej",
      "Percy Liang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.12941",
    "githubLink": "https://github.com/stanford-crfm/fmti",
    "itemCount": "100 indicators evaluated across 13-14 major developers",
    "specs": "Tabular data (scores), PDF reports, transparency indicators",
    "description": "A comprehensive assessment of the transparency of foundation model developers. It scores major AI companies (e.g., OpenAI, Google, Meta) on 100 indicators across three domains: upstream resources (data, labor, compute), the model itself (capabilities, risks), and downstream use (distribution, impact). The project provides a dataset of these scores and detailed transparency reports."
  },
  {
    "id": "saved-1769608044995-eygt3",
    "title": "The Data Provenance Collection",
    "source": "arXiv",
    "authors": [
      "Shayne Longpre",
      "Robert Mahari",
      "Anthony Chen",
      "Naana Obeng-Marnu",
      "Damien Sileo",
      "William Brannon",
      "Niklas Muennighoff",
      "Nathan Nathan",
      "Alex Pentland",
      "Sara Hooker",
      "Jad Kabbara"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.16787",
    "githubLink": "https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection",
    "itemCount": "44 data collections comprising 1800+ datasets",
    "specs": "Text/Metadata (licenses, sources, creators, provenance traces)",
    "description": "A large-scale audit and dataset of licensing and attribution for AI training datasets. The initiative traces the lineage of popular fine-tuning data collections, documenting their sources, licenses, creators, and other metadata to improve data transparency in the AI ecosystem."
  },
  {
    "id": "saved-1769608044995-a7oaj",
    "title": "HaluEval",
    "source": "arXiv",
    "authors": [
      "Junyi Li",
      "Xiaoxue Cheng",
      "Wayne Xin Zhao",
      "Jian-Yun Nie",
      "Ji-Rong Wen"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.11747",
    "githubLink": "https://github.com/RUCAIBox/HaluEval",
    "itemCount": "35,000 samples (30k generated, 5k human-annotated)",
    "specs": "Text (QA, Dialogue, Summarization)",
    "description": "A large-scale hallucination evaluation benchmark for LLMs. It includes both automatically generated and human-annotated hallucinated samples to evaluate the ability of models to recognize hallucinations."
  },
  {
    "id": "saved-1769608044995-wto1f",
    "title": "HallusionBench",
    "source": "arXiv",
    "authors": [
      "Tianrui Guan",
      "Fuxiao Liu",
      "Xiyang Wu",
      "Ruiqi Xian",
      "Zongxia Li",
      "Xiaoyu Liu",
      "Xijun Wang",
      "Lichang Chen",
      "Furong Huang",
      "Yaser Yacoob",
      "Dinesh Manocha",
      "Tianyi Zhou"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.14566",
    "githubLink": "https://github.com/tianyi-lab/HallusionBench",
    "itemCount": "346 images, 1,129 questions",
    "specs": "Image/Text (Visual QA), Control groups for consistency analysis",
    "description": "An image-context reasoning benchmark designed to challenge Large Vision-Language Models (LVLMs) by focusing on language hallucinations and visual illusions."
  },
  {
    "id": "saved-1769608044995-m25la",
    "title": "Head-to-Tail",
    "source": "arXiv",
    "authors": [
      "Kai Sun",
      "Yifan Ethan Xu",
      "Hanwen Zha",
      "Yue Liu",
      "Xin Dong"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.10168",
    "githubLink": "https://github.com/facebookresearch/head-to-tail",
    "itemCount": "18,000 QA pairs",
    "specs": "Text (QA), Split by entity popularity",
    "description": "A benchmark for evaluating the factual knowledge of LLMs across facts of varying popularity (Head, Torso, Tail) to better understand hallucination rates in less common knowledge areas."
  },
  {
    "id": "saved-1769608044995-nd4ig",
    "title": "DOLOS",
    "source": "Other",
    "authors": [
      "Dongyang Guo",
      "Yinping Nie",
      "Kangkang Chen",
      "Haonan Luo",
      "Zhiyuan Ji",
      "Yi Yang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.00494",
    "githubLink": "https://github.com/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning",
    "itemCount": "1,675 video clips",
    "specs": "Audio, Visual; MUMIN feature annotations; 213 subjects",
    "description": "A large-scale audio-visual dataset sourced from reality TV game shows (like 'Golden Balls'). It focuses on conversational deception in dynamic, rich environments with fine-grained annotations."
  },
  {
    "id": "saved-1769608044995-yosl5",
    "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
    "source": "arXiv",
    "authors": [
      "David Rein",
      "Betty Li Hou",
      "Asa Cooper Stickland",
      "Jackson Petty",
      "Richard Yuanzhe Pang",
      "Julien Dirani",
      "Julian Michael",
      "Samuel R. Bowman"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.12022",
    "githubLink": "https://github.com/idavidrein/gpqa",
    "itemCount": "448 questions (main set)",
    "specs": "Text (Multiple-choice Questions)",
    "description": "GPQA is a challenging multiple-choice question answering dataset written by domain experts in biology, physics, and chemistry. It is designed to be difficult even for highly skilled non-experts with unrestricted access to the internet (Google-proof), while domain experts can answer them with high accuracy."
  },
  {
    "id": "saved-1769608044995-6hn3t",
    "title": "ConceptARC",
    "source": "arXiv",
    "authors": [
      "Arseny Moskvichev",
      "Victor Vikram Odouard",
      "Melanie Mitchell"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.07141",
    "githubLink": "https://github.com/victorvikram/ConceptARC",
    "itemCount": "160 tasks (16 concept groups × 10 tasks)",
    "specs": "JSON format, 2D grid pairs",
    "description": "A benchmark organized around specific 'concept groups' (e.g., center, same-different, inside-outside) to systematically assess abstraction and generalization capabilities. It aims to test if models have truly grasped specific spatial and semantic concepts."
  },
  {
    "id": "saved-1769608044995-ly6yi",
    "title": "1D-ARC",
    "source": "arXiv",
    "authors": [
      "Yudong Xu",
      "Wenhao Li",
      "Pashootan Vaezipoor",
      "Scott Sanner",
      "Elias B. Khalil"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.18354",
    "githubLink": "https://github.com/khalil-research/LLM4ARC",
    "itemCount": "Variable (generated via configurable parameters)",
    "specs": "JSON format, 1D arrays",
    "description": "A simplified version of ARC consisting of one-dimensional (array-like) tasks. It was designed to investigate the reasoning capabilities of Large Language Models (LLMs) and the importance of object-based representations in a simpler domain."
  },
  {
    "id": "saved-1769608044995-be27z",
    "title": "InterCode",
    "source": "arXiv",
    "authors": [
      "John Yang",
      "Akshara Prabhakar",
      "Karthik Narasimhan",
      "Shunyu Yao"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.14898",
    "githubLink": "https://github.com/princeton-nlp/intercode",
    "itemCount": "3 environments (Bash, SQL, Python)",
    "specs": "Interactive code environments, Docker containers, RL interface",
    "description": "A lightweight, flexible framework for designing interactive code environments to evaluate language agents. It treats coding as a standard reinforcement learning environment with code as actions and execution feedback as observations, supporting Bash, SQL, and Python tasks."
  },
  {
    "id": "saved-1769608044995-3zgvh",
    "title": "Mind2Web",
    "source": "arXiv",
    "authors": [
      "Xiang Deng",
      "Yu Gu",
      "Boyuan Zheng",
      "Shijie Chen",
      "Samuel Stevens",
      "Boshi Wang",
      "Huan Sun",
      "Yu Su"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.06070",
    "githubLink": "https://github.com/OSU-NLP-Group/Mind2Web",
    "itemCount": "2,350 tasks",
    "specs": "Text, HTML, DOM Snapshots, Action Sequences",
    "description": "A dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. It contains over 2,000 open-ended tasks collected from 137 real-world websites spanning 31 domains."
  },
  {
    "id": "saved-1769608044995-4r9im",
    "title": "CBBQ: Chinese Bias Benchmark Dataset",
    "source": "arXiv",
    "authors": [
      "Yufei Huang",
      "Deyi Xiong"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.16244",
    "githubLink": "https://github.com/YFHuangxxxx/CBBQ",
    "itemCount": "106,588 generated instances",
    "specs": "Chinese text, Multiple Choice QA, over 3,000 templates",
    "description": "A Chinese bias benchmark dataset co-developed by human experts and generative language models. It covers stereotypes and societal biases across 14 social dimensions pertinent to Chinese culture and values."
  },
  {
    "id": "saved-1769608044995-6qci2",
    "title": "HEIM (Holistic Evaluation of Text-to-Image Models)",
    "source": "arXiv",
    "authors": [
      "Tony Lee",
      "Michihiro Yasunaga",
      "Chenlin Meng",
      "Yifan Mai",
      "Joon Sung Park",
      "Agrim Gupta",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.04287",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "62 scenarios",
    "specs": "Text-to-Image; 12 evaluation aspects",
    "description": "An extension of the HELM framework focused on text-to-image models. It evaluates models across 12 aspects including alignment, quality, aesthetics, originality, reasoning, knowledge, bias, and toxicity."
  },
  {
    "id": "saved-1769608044995-psos8",
    "title": "Tensor Trust",
    "source": "arXiv",
    "authors": [
      "Sam Toyer",
      "Olivia Watkins",
      "Ethan Adrian Mendes",
      "Justin Svegliato",
      "Luke Bailey",
      "Tiffany Wang",
      "Isaac Ong",
      "Karim Elmaaroufi",
      "Pieter Abbeel",
      "Trevor Darrell",
      "Alan Ritter",
      "Stuart Russell"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.01011",
    "githubLink": "https://github.com/HumanCompatibleAI/tensor-trust",
    "itemCount": "126,000+ attacks, 46,000+ defenses",
    "specs": "Text (English), JSON/Parquet",
    "description": "A large-scale dataset of human-generated prompt injection attacks and defenses collected from an online game where players attempt to trick an LLM into revealing a password."
  },
  {
    "id": "saved-1769608044995-7uaw6",
    "title": "BIPIA (Benchmark for Indirect Prompt Injection Attacks)",
    "source": "arXiv",
    "authors": [
      "Jian Yi",
      "Huiya Feng",
      "Zhenxiang Xiao",
      "Wei Tan",
      "Qun Xi",
      "Shengjie Zhang",
      "Yuan Ni",
      "Guotong Xie",
      "Zheng Zhang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.14197",
    "githubLink": "https://github.com/microsoft/BIPIA",
    "itemCount": "35,000+ malicious instances (expandable)",
    "specs": "Text, JSON",
    "description": "The first benchmark designed to evaluate the robustness of Large Language Models against indirect prompt injection attacks, covering tasks like email QA, web QA, and summarization."
  },
  {
    "id": "saved-1769608044995-w5wkt",
    "title": "deepset/prompt-injections",
    "source": "Hugging Face",
    "authors": [
      "deepset"
    ],
    "year": "2023",
    "paperLink": "https://huggingface.co/datasets/deepset/prompt-injections",
    "githubLink": "https://huggingface.co/datasets/deepset/prompt-injections",
    "itemCount": "662 samples",
    "specs": "Text, Parquet",
    "description": "A widely used small-scale dataset combining benign instructions and adversarial prompt injections, often used for training and evaluating basic injection classifiers."
  },
  {
    "id": "saved-1769608044995-u8vqn",
    "title": "XSTest",
    "source": "arXiv",
    "authors": [
      "Paul Röttger",
      "Hannah Rose Kirk",
      "Bertie Vidgen",
      "Giuseppe Attanasio",
      "Federico Bianchi",
      "Dirk Hovy"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.01263",
    "githubLink": "https://github.com/paul-rottger/xstest",
    "itemCount": "450 prompts (250 safe, 200 unsafe)",
    "specs": "Text prompts in CSV format; Includes 10 types of safe prompts and contrasting unsafe prompts",
    "description": "A test suite designed to identify exaggerated safety behaviors in Large Language Models (LLMs), specifically measuring false refusals where models refuse safe prompts that superficially resemble unsafe ones."
  },
  {
    "id": "saved-1769608044995-v6geb",
    "title": "EHRXQA",
    "source": "arXiv",
    "authors": [
      "Seongsu Bae",
      "Kyubok Lee",
      "Woncheol Shin",
      "Sang-Wook Kim",
      "Edward Choi"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.18652",
    "githubLink": "https://github.com/baeseongsu/ehrxqa",
    "itemCount": "46,152 samples",
    "specs": "Multi-modal (Structured Tables + Chest X-ray Images); Text, Image, SQL",
    "description": "A multi-modal question answering dataset that combines structured Electronic Health Records (EHR) from MIMIC-IV with chest X-ray images from MIMIC-CXR. It requires models to perform joint reasoning across both tabular and imaging modalities to answer clinical questions."
  },
  {
    "id": "saved-1769608044995-s57vs",
    "title": "NYU Breast Pathology Reports Dataset",
    "source": "Other",
    "authors": [
      "K. T. Nguyen",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10319853/",
    "githubLink": "https://github.com/nyukat/pathology_extraction",
    "itemCount": "1,438 reports",
    "specs": "Text (Clinical Reports)",
    "description": "A dataset of breast pathology reports annotated for Named Entity Recognition (NER) tasks. It focuses on extracting key diagnostic elements such as cancer subtype, cancer grade, and lesion position from free-text clinical reports."
  },
  {
    "id": "saved-1769608044995-pzidz",
    "title": "M4Raw",
    "source": "Scholar",
    "authors": [
      "Mengye Lyu",
      "Lifeng Mei",
      "Shoujin Huang",
      "Sixing Liu",
      "Yi Li"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1038/s41597-023-02181-4",
    "githubLink": "https://github.com/mylyu/M4Raw",
    "itemCount": "183 subjects, >25k slices",
    "specs": "Raw k-space (0.3T), T1w, T2w, FLAIR",
    "description": "A multi-contrast, multi-repetition, multi-channel MRI k-space dataset for low-field MRI research and reconstruction."
  },
  {
    "id": "saved-1769608044995-lv3ip",
    "title": "RadImageNet-VQA",
    "source": "Hugging Face",
    "authors": [
      "Raidium"
    ],
    "year": "2023",
    "paperLink": "https://huggingface.co/datasets/raidium/RadImageNet-VQA",
    "githubLink": "https://huggingface.co/datasets/raidium/RadImageNet-VQA",
    "itemCount": "750,000 images, 6.75M QA pairs",
    "specs": "Images (CT, MRI), Text (Open-ended, Closed-ended, Multiple-choice)",
    "description": "A large-scale synthetic dataset designed for training and benchmarking radiologic VQA on CT and MRI exams. It is built from the RadImageNet dataset with generated QA pairs to challenge multimodal models."
  },
  {
    "id": "saved-1769608044995-jahzp",
    "title": "DEEP-VOICE",
    "source": "Other",
    "authors": [
      "Bird",
      "J. J.",
      "Lotfi",
      "A."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.12734",
    "githubLink": "https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition",
    "itemCount": "~11,778 audio samples (5,889 real, 5,889 fake)",
    "specs": "Audio (.wav), Real vs. AI-Generated (RVC)",
    "description": "A dataset designed for the detection of AI-generated speech (voice cloning). It contains real human speech from public figures and counterparts generated using Retrieval-based Voice Conversion (RVC)."
  },
  {
    "id": "saved-1769608044995-cs958",
    "title": "GPTCloneBench",
    "source": "arXiv",
    "authors": [
      "Alam",
      "A. I.",
      "Roy",
      "P. R.",
      "Al-Omari",
      "F.",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.14088",
    "githubLink": "https://github.com/yh1105/datasetforTBCCD",
    "itemCount": "~37k true semantic pairs, ~20k cross-language pairs",
    "specs": "Source Code (Java, C, C#, Python), Semantic Clones",
    "description": "A comprehensive benchmark for evaluating semantic and cross-language code clones generated by Large Language Models (specifically GPT-3). It focuses on assessing how well detectors identify AI-generated semantic equivalents."
  },
  {
    "id": "saved-1769608044995-2n1ho",
    "title": "Genomic Benchmarks",
    "source": "Other",
    "authors": [
      "Katarína Grešová",
      "Vlastimil Martinek",
      "David Čechák",
      "Petr Šimeček",
      "Panagiotis Alexiou"
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1186/s12863-023-01123-8",
    "githubLink": "https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks",
    "itemCount": "9 datasets",
    "specs": "Genomic sequences (text/DNA), classification tasks, Python package",
    "description": "A collection of curated and easily accessible sequence classification datasets for genomics, covering regulatory elements like promoters and enhancers across human, mouse, and roundworm genomes."
  },
  {
    "id": "saved-1769608044995-4ap2z",
    "title": "Genome Understanding Evaluation (GUE)",
    "source": "arXiv",
    "authors": [
      "Zhihan Zhou",
      "Yanrong Ji",
      "Weijian Li",
      "Pratik Dutta",
      "Ramana Davuluri",
      "Han Liu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.15581",
    "githubLink": "https://github.com/MAGICS-LAB/DNABERT_2",
    "itemCount": "28-36 datasets across 7-9 tasks",
    "specs": "Multi-species (Human, Mouse, Virus, Yeast) DNA sequences; input lengths ranging from 70 to 10,000 base pairs.",
    "description": "A comprehensive multi-species genome classification benchmark designed to evaluate DNA language models. It amalgamates diverse datasets to test capabilities in core promoter detection, transcription factor prediction, splice site prediction, and more."
  },
  {
    "id": "saved-1769608044995-3fs05",
    "title": "Plant Genomic Benchmark (PGB)",
    "source": "Hugging Face",
    "authors": [
      "Guillaume Sala",
      "Odile Moreira",
      "Pauline Vicas",
      "Thomas Pierrot",
      "Karim Beguir",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1101/2023.10.27.564408",
    "githubLink": "https://huggingface.co/datasets/InstaDeepAI/plant-genomic-benchmark",
    "itemCount": "28 datasets across 8 tasks",
    "specs": "Plant DNA sequences (up to 6,000 bp); tasks include gene expression prediction (regression) and regulatory element classification.",
    "description": "A large-scale DNA benchmark suite designed to evaluate foundation models on plant biology tasks. It covers single and multi-output regression and classification tasks such as promoter strength prediction, gene expression, and chromatin accessibility in various plant species."
  },
  {
    "id": "saved-1769608044995-0gjy7",
    "title": "SimpleSafetyTests",
    "source": "Hugging Face",
    "authors": [
      "Bertie Vidgen",
      "Hannah Rose Kirk",
      "Paul Röttger",
      "Scott Hale"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.08370",
    "githubLink": "https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests",
    "itemCount": "100 test prompts",
    "specs": "Text prompts; 5 primary harm areas",
    "description": "A lightweight test suite for rapidly identifying critical safety risks. It contains 100 clear-cut unsafe prompts across five severe harm areas (e.g., suicide, child abuse) that models should definitively refuse."
  },
  {
    "id": "saved-1769608044995-gmm63",
    "title": "SafetyBench",
    "source": "arXiv",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Lindong Wu",
      "Rui Sun",
      "Yongkang Huang",
      "Chong Long",
      "Xiao Liu",
      "Xuanyu Lei",
      "Jie Tang",
      "Minlie Huang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.13788",
    "githubLink": "https://github.com/thu-coai/SafetyBench",
    "itemCount": "11,435 questions",
    "specs": "Text (Multiple Choice Questions), Chinese and English",
    "description": "A comprehensive benchmark for evaluating the safety of LLMs comprising diverse multiple-choice questions across 7 distinct categories of safety concerns."
  },
  {
    "id": "saved-1769608044995-szgqe",
    "title": "ToxicChat",
    "source": "arXiv",
    "authors": [
      "Zi Lin",
      "Zihan Wang",
      "Yongqi Tong",
      "Yangkun Wang",
      "Yuxin Guo",
      "Yujia Wang",
      "Jingbo Shang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.17389",
    "githubLink": "https://github.com/lmsys/toxic-chat",
    "itemCount": "10,166 conversation pairs",
    "specs": "Text (User-AI conversation pairs), annotated for toxicity and jailbreaking",
    "description": "A benchmark based on real-world user-AI conversations from an open-source chatbot, designed to evaluate toxicity detection and jailbreaking attempts in realistic interactive settings."
  },
  {
    "id": "saved-1769608044995-mwwf0",
    "title": "MITweet (Multifaceted Ideology Detection)",
    "source": "Scholar",
    "authors": [
      "Songtao Liu",
      "Ziling Luo",
      "Minghua Xu",
      "LiXiao Wei",
      "Ziyao Wei",
      "Han Yu",
      "Wei Xiang",
      "Bang Wang"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.emnlp-main.865/",
    "githubLink": "https://github.com/LST1836/MITweet",
    "itemCount": "12,594 tweets",
    "specs": "English Tweets, 12 Facet Labels (Relevance & Ideology)",
    "description": "A high-quality dataset for multifaceted ideology detection, moving beyond binary left-right classification to capture ideological stances across 12 specific domains/facets."
  },
  {
    "id": "saved-1769608044995-spi10",
    "title": "MBIB (Media Bias Identification Benchmark)",
    "source": "arXiv",
    "authors": [
      "Martin Wessel",
      "Tomáš Horych",
      "Terry Ruas",
      "Akiko Aizawa",
      "Bela Gipp",
      "Timo Spinde"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.13148",
    "githubLink": "https://github.com/media-bias-group/mbib-base",
    "itemCount": "22 datasets (aggregated)",
    "specs": "Diverse text formats, Multiple bias types",
    "description": "A comprehensive benchmark that groups different types of media bias (linguistic, cognitive, and political) under a common framework, distilled from a review of over 100 datasets."
  },
  {
    "id": "saved-1769608044995-bswu5",
    "title": "SycophancyEval",
    "source": "Hugging Face",
    "authors": [
      "Mrinank Sharma",
      "Meg Tong",
      "Tomasz Korbak",
      "David Duvenaud",
      "Amanda Askell",
      "Samuel R. Bowman",
      "Newton Cheng",
      "Esin Durmus",
      "Zac Hatfield-Dodds",
      "Scott R. Johnston",
      "Shauna Kravec",
      "Timothy Maxwell",
      "Sam McCandlish",
      "Kamal Ndousse",
      "Oliver Rausch",
      "Nicholas Schiefer",
      "Da Yan",
      "Miranda Zhang",
      "Ethan Perez"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.13548",
    "githubLink": "https://github.com/meg-tong/sycophancy-eval",
    "itemCount": "~21,000 prompts",
    "specs": "Text (JSONL format); Tasks: Answer, Argument, Feedback, Are You Sure?",
    "description": "A benchmark designed to evaluate sycophantic behavior in language models across varied free-form text-generation tasks, including answering questions with user opinions, providing feedback on arguments, and responding to challenges (e.g., 'Are you sure?')."
  },
  {
    "id": "saved-1769608044995-1zr0l",
    "title": "OpinionQA",
    "source": "arXiv",
    "authors": [
      "Shibani Santurkar",
      "Esin Durmus",
      "Faisal Ladhak",
      "Cinoo Lee",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2303.17548",
    "githubLink": "https://github.com/tatsu-lab/opinions_qa",
    "itemCount": "1,498 multiple-choice questions",
    "specs": "Text; Multiple-choice questions derived from public opinion surveys",
    "description": "Evaluates the alignment of Large Language Model opinions with those of 60 US demographic groups using questions from Pew Research Center's American Trends Panel surveys."
  },
  {
    "id": "saved-1769608044995-dyxv0",
    "title": "MACHIAVELLI Benchmark",
    "source": "arXiv",
    "authors": [
      "Alexander Pan",
      "Jun Shern Chan",
      "Andy Zou",
      "Nathaniel Li",
      "Steven Basart",
      "Thomas Woodside",
      "Jonathan Ng",
      "Hanlin Zhang",
      "Scott Emmons",
      "Dan Hendrycks"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2304.03279",
    "githubLink": "https://github.com/aypan17/machiavelli",
    "itemCount": "134 games (572k scenes, 3M annotations)",
    "specs": "Text-based Reinforcement Learning environment; Annotated with behavioral metrics.",
    "description": "A benchmark consisting of 134 text-based 'Choose-Your-Own-Adventure' games designed to evaluate artificial agents on their tendency to engage in power-seeking, deceptive, and unethical behaviors (Machiavellianism) while attempting to maximize rewards. It tracks trade-offs between reward maximization and ethical constraints."
  },
  {
    "id": "saved-1769608044995-f5r9j",
    "title": "ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code",
    "source": "arXiv",
    "authors": [
      "Yuliang Liu",
      "Xiangru Tang",
      "Zefan Cai",
      "Junjie Lu",
      "Yichi Zhang",
      "Yanjun Shao",
      "Zexuan Deng",
      "Helan Hu",
      "Zengxian Yang",
      "Kaikai An",
      "Ruijun Huang",
      "Shuzheng Si",
      "Sheng Chen",
      "Haozhe Zhao",
      "Zhengliang Li",
      "Liang Chen",
      "Zhiwei Jiang",
      "Baobao Chang",
      "Yujia Qin",
      "Wangchunshu Zhou",
      "Yilun Zhao",
      "Arman Cohan",
      "Mark Gerstein"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.09835",
    "githubLink": "https://github.com/gersteinlab/ML-Bench",
    "itemCount": "9,641 examples",
    "specs": "Annotated examples across 18 GitHub repositories; Repository-level code tasks",
    "description": "A benchmark rooted in real-world programming applications that leverages existing code repositories to perform ML tasks. It assesses the capability of LLMs to generate executable scripts and autonomous agents to perform complex coding tasks in a Linux sandbox environment."
  },
  {
    "id": "saved-1769608044995-w078d",
    "title": "AI Village Capture the Flag @ DEFCON31",
    "source": "Other",
    "authors": [
      "AI Village"
    ],
    "year": "2023",
    "paperLink": "https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31",
    "githubLink": "https://github.com/adv-ml-notebooks/ai-village-ctf-defcon31",
    "itemCount": "27 challenges",
    "specs": "Diverse ML challenges involving adversarial examples, model inversion, and other AI security concepts.",
    "description": "A collection of machine learning security challenges designed for the AI Village CTF at DEFCON 31. Participants interact with ML models to find flags by exploiting vulnerabilities."
  },
  {
    "id": "saved-1769608044995-d4tca",
    "title": "HackAPrompt Dataset",
    "source": "Hugging Face",
    "authors": [
      "Sander Schulhoff",
      "Jeremy Pinto",
      "Anaum Khan",
      "Louis-François Bouchard",
      "Chenglei Si",
      "Jordan Boyd-Graber"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.16119",
    "githubLink": "https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset",
    "itemCount": "600,000+ samples",
    "specs": "Text (User inputs, model completions, expected outputs)",
    "description": "A large-scale dataset of over 600,000 adversarial prompts generated during a global prompt hacking competition. It covers varying levels of difficulty and multiple attack types, including prompt leaking and jailbreaking, designed to expose vulnerabilities in LLMs."
  },
  {
    "id": "saved-1769608044995-gt3tu",
    "title": "BeaverTails",
    "source": "Hugging Face",
    "authors": [
      "Jiaming Ji",
      "Mickel Liu",
      "Josef Dai",
      "Xuehai Pan",
      "Chi Zhang",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2311.00861",
    "githubLink": "https://github.com/PKU-Alignment/beavertails",
    "itemCount": "~333,963 QA pairs; ~361,903 comparison pairs",
    "specs": "Text (QA pairs), Safety meta-labels, Preference data",
    "description": "A large-scale dataset designed to foster research on safety alignment. It uniquely separates annotations for helpfulness and harmlessness, providing safety meta-labels and expert comparison data."
  },
  {
    "id": "saved-1769608044995-8x3q6",
    "title": "UltraFeedback",
    "source": "Hugging Face",
    "authors": [
      "Ganqu Cui",
      "Lifan Yuan",
      "Ning Ding",
      "Guanming Yao",
      "Wei Zhu",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.01377",
    "githubLink": "https://github.com/OpenBMB/UltraFeedback",
    "itemCount": "~64,000 prompts; ~256,000 samples",
    "specs": "Text, Scalar ratings, Textual feedback",
    "description": "A large-scale, fine-grained, diverse preference dataset used for training reward models. It uses GPT-4 to generate detailed feedback and numerical ratings for model responses."
  },
  {
    "id": "saved-1769608044995-n48pp",
    "title": "Psych8k",
    "source": "arXiv",
    "authors": [
      "June M. Liu",
      "Donghao Li",
      "He Cao",
      "Tianhe Ren",
      "Zeyi Liao",
      "Jiamin Wu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.15461",
    "githubLink": "https://github.com/EmoCareAI/ChatPsychiatrist",
    "itemCount": "8,187 QA pairs",
    "specs": "Text (English, Counseling Dialogues)",
    "description": "A dataset constructed from 260 in-depth real counseling interviews, processed into single-turn query-answer pairs to fine-tune LLMs (like ChatPsychiatrist) for mental health support."
  },
  {
    "id": "saved-1769608044995-b5xg9",
    "title": "HEx-PHI (Harmful Examples - Prohibited, Harmful Instructions)",
    "source": "Hugging Face",
    "authors": [
      "Xiangyu Qi",
      "Yi Zeng",
      "Tinghao Xie",
      "Pin-Yu Chen",
      "Ruoxi Jia",
      "Prateek Mittal",
      "Peter Henderson"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.05914",
    "githubLink": "https://huggingface.co/datasets/LLM-Tuning-Safety/HEx-PHI",
    "itemCount": "330 harmful instructions",
    "specs": "Text-only; 11 harmful categories (e.g., illegal activity, violence, malware)",
    "description": "A dataset of harmful instructions used to evaluate safety risks in fine-tuned LLMs. It is categorized according to prohibited content policies similar to those of Meta and OpenAI."
  },
  {
    "id": "saved-1769608044996-tkrku",
    "title": "FalseQA",
    "source": "arXiv",
    "authors": [
      "Shengding Hu",
      "Yifan Luo",
      "Huadong Wang",
      "Xingyi Cheng",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.acl-long.309/",
    "githubLink": "https://github.com/thunlp/FalseQA",
    "itemCount": "2,365 questions",
    "specs": "CSV format with columns for question, answer (rebuttal), and binary label (0/1).",
    "description": "A specialized dataset of human-written False Premise Questions (FPQs) designed to evaluate whether pre-trained language models can discriminate and rebut tricky questions with false premises."
  },
  {
    "id": "saved-1769608044996-fh0c6",
    "title": "(QA)2",
    "source": "arXiv",
    "authors": [
      "Najoung Kim",
      "Phu Mon Htut",
      "Samuel R. Bowman",
      "Jackson Petty"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.acl-long.471/",
    "githubLink": "https://github.com/najoungkim/qa2",
    "itemCount": "~602 examples",
    "specs": "Real-world search queries with annotations for questionable assumptions.",
    "description": "An open-domain evaluation dataset consisting of naturally occurring search engine queries that contain questionable assumptions (false or unverifiable), designed to test if models can detect them."
  },
  {
    "id": "saved-1769608044996-zvxr4",
    "title": "FreshQA",
    "source": "arXiv",
    "authors": [
      "Tu Vu",
      "Mohit Iyyer",
      "Xuezhi Wang",
      "Noah Constant",
      "Jerry Wei",
      "Jason Wei",
      "Chris Tar",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.03214",
    "githubLink": "https://github.com/freshllms/freshqa",
    "itemCount": "600 questions",
    "specs": "QA pairs categorized by valid/false premise and fast-changing/slow-changing status.",
    "description": "A dynamic QA benchmark that includes a category for questions with false premises, designed to test LLMs on fast-changing world knowledge and their ability to debunk false premises."
  },
  {
    "id": "saved-1769608044996-6lp1s",
    "title": "FActScore",
    "source": "arXiv",
    "authors": [
      "Sewon Min",
      "Kalpesh Krishna",
      "Xinxi Lyu",
      "Mike Lewis",
      "Wen-tau Yih",
      "Pang Wei Koh",
      "Mohit Iyyer",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.14251",
    "githubLink": "https://github.com/kalpeshk2011/FActScore",
    "itemCount": "183 labeled entities / 500 unlabeled entities",
    "specs": "Text (biographies); Atomic fact decomposition; Wikipedia verification",
    "description": "FActScore (Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation) is a benchmark and metric that breaks down long-form text generations (specifically people biographies) into atomic facts and verifies them against a knowledge source like Wikipedia to measure factual precision."
  },
  {
    "id": "saved-1769608044996-op3yj",
    "title": "FacTool",
    "source": "arXiv",
    "authors": [
      "I. Chern",
      "Steffi Chern",
      "Shuyue Hu",
      "William Yang Wang",
      "Pengfei Liu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.13528",
    "githubLink": "https://github.com/GAIR-NLP/factool",
    "itemCount": "Variable (Multi-task evaluation set)",
    "specs": "Text, Code, Math; Tool-augmented verification",
    "description": "A task-augmented framework and benchmark for detecting factual errors in generative AI. It covers multiple domains including knowledge-based QA, code generation, mathematical reasoning, and scientific literature review."
  },
  {
    "id": "saved-1769608044996-az0xk",
    "title": "MT-Bench",
    "source": "arXiv",
    "authors": [
      "Lianmin Zheng",
      "Wei-Lin Chiang",
      "Ying Sheng",
      "Siyuan Zhuang",
      "Zhanghao Wu",
      "Yonghao Zhuang",
      "Zi Lin",
      "Zhuohan Li",
      "Dacheng Li",
      "Eric. P Xing",
      "Hao Zhang",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.05685",
    "githubLink": "https://github.com/lm-sys/FastChat",
    "itemCount": "80 prompts (160 turns)",
    "specs": "Multi-turn chat, Text, Pairwise comparison",
    "description": "A set of 80 high-quality multi-turn questions designed to test the conversation flow and instruction-following capabilities of LLMs. It uses GPT-4 as a judge to evaluate model responses against a reference."
  },
  {
    "id": "saved-1769608044996-ua8p2",
    "title": "JudgeLM-100K",
    "source": "arXiv",
    "authors": [
      "Lianghui Zhu",
      "Xinggang Wang",
      "Xinlong Wang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.17631",
    "githubLink": "https://github.com/baaivision/JudgeLM",
    "itemCount": "100,000 samples (training)",
    "specs": "Text, Instruction following, Judging pairs",
    "description": "A large-scale dataset used to fine-tune and evaluate LLMs as scalable judges. It contains judge samples with high-quality annotations to train models that can perform judge tasks efficiently."
  },
  {
    "id": "saved-1769608044996-8s5h9",
    "title": "PandaLM",
    "source": "arXiv",
    "authors": [
      "Yidong Wang",
      "Zhuohao Yu",
      "Zhengran Zeng",
      "Linyi Yang",
      "Cunxiang Wang",
      "Hao Chen",
      "Chaoya Jiang",
      "Rui Xie",
      "Jindong Wang",
      "Xing Xie",
      "Wei Ye",
      "Shikun Zhang",
      "Yue Zhang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.05087",
    "githubLink": "https://github.com/WeOpenML/PandaLM",
    "itemCount": "~1,000 test samples",
    "specs": "Text, Instruction tuning optimization, Pairwise comparison",
    "description": "A benchmark and judge model focused on automating the evaluation of instruction-tuned models. It includes a human-annotated test set to ensure the reliability of the automated judge."
  },
  {
    "id": "saved-1769608044996-pem2z",
    "title": "VisAlign",
    "source": "arXiv",
    "authors": [
      "Jiyoung Lee",
      "Seungho Kim",
      "Seunghyun Won",
      "Joonseok Lee",
      "Marzyeh Ghassemi",
      "James Thorne",
      "Jaeseok Choi",
      "O-Kil Kwon",
      "Edward Choi"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.01525",
    "githubLink": "https://github.com/jiyounglee-0523/VisAlign",
    "itemCount": "Not specified (Multiple categories)",
    "specs": "Image classification, Visual alignment",
    "description": "A dataset for measuring the degree of alignment between AI models and human visual perception. It categorizes images based on whether a model must classify, abstain, or is uncertain, mirroring human visual capabilities."
  },
  {
    "id": "saved-1769608044996-kbp1y",
    "title": "FNS 2023 Dataset",
    "source": "Other",
    "authors": [
      "Mahmoud El-Haj",
      "Paul Rayson",
      "Nadhem Zmandar",
      "George Giannakopoulos",
      "Nikiforos Pittaras",
      "Marina Litvak",
      "Natalia Vanetik",
      "Ahmed AbuRa'ed"
    ],
    "year": "2023",
    "paperLink": "https://wp.lancs.ac.uk/cfie/fns2023/",
    "githubLink": "https://github.com/iit-Demokritos/FNS2023_data",
    "itemCount": "~4,000 annual reports",
    "specs": "PDF/Text annual reports, Gold standard summaries",
    "description": "Dataset for the Financial Narrative Summarisation shared task, focusing on generating summaries from UK, Spanish, and Greek annual financial reports."
  },
  {
    "id": "saved-1769608044996-ohuhx",
    "title": "RadGraph2",
    "source": "Other",
    "authors": [
      "Sameer Khanna",
      "Adam Dejl",
      "Kibo Yoon",
      "Steven QH Truong",
      "Hanh Duong",
      "Agustina Saenz",
      "Pranav Rajpurkar"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.05046",
    "githubLink": "https://physionet.org/content/radgraph2",
    "itemCount": "800 annotated reports",
    "specs": "Text (Radiology Reports), Hierarchical Entity-Relation Schema",
    "description": "A dataset extending RadGraph to model disease progression in radiology reports. It utilizes a hierarchical schema to capture entities and relations, specifically focusing on changes in disease state and device placement over time compared to prior reports."
  },
  {
    "id": "saved-1769608044996-w4e3i",
    "title": "MeetingBank",
    "source": "Other",
    "authors": [
      "Yebowen Hu",
      "Tim Ganter",
      "Hanieh Deilamsalehy",
      "Franck Dernoncourt",
      "Hassan Foroosh",
      "Fei Liu"
    ],
    "year": "2023",
    "paperLink": "https://aclanthology.org/2023.acl-long.906/",
    "githubLink": "https://github.com/YebowenHu/MeetingBank-utils",
    "itemCount": "1,366 meetings (6,892 segments)",
    "specs": "Text, Audio, Video (City Council Meetings)",
    "description": "A benchmark dataset for meeting summarization derived from city council meetings of major U.S. cities. It includes video, audio, transcripts, and meeting minutes, enabling research into multi-modal and long-form meeting summarization."
  },
  {
    "id": "saved-1769608044996-siz99",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "source": "arXiv",
    "authors": [
      "Boxin Wang",
      "Weixin Chen",
      "Hengzhi Pei",
      "Chulin Xie",
      "Mintong Kang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.11698",
    "githubLink": "https://github.com/AI-secure/DecodingTrust",
    "itemCount": "Hundreds of thousands of prompts",
    "specs": "Text; 8 trustworthiness perspectives (toxicity, bias, robustness, privacy, etc.)",
    "description": "A comprehensive trustworthiness evaluation platform for large language models, specifically GPT-4 and GPT-3.5, covering diverse perspectives including toxicity, stereotype bias, adversarial robustness, OOD robustness, privacy, machine ethics, and fairness."
  },
  {
    "id": "saved-1769608044996-qq5ap",
    "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models",
    "source": "arXiv",
    "authors": [
      "Yue Huang",
      "Qihui Zhang",
      "Philip S. Yu",
      "Lichao Sun"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.11507",
    "githubLink": "https://github.com/HowieHwong/TrustGPT",
    "itemCount": "Uses Social Chemistry 101 (292k norms)",
    "specs": "Text; Toxicity, bias, value-alignment evaluation",
    "description": "A benchmark designed to evaluate LLMs from three ethical perspectives: toxicity, bias, and value-alignment, utilizing the Social Chemistry 101 dataset."
  },
  {
    "id": "saved-1769608044996-p4e3q",
    "title": "XIMAGENET-12",
    "source": "arXiv",
    "authors": [
      "Qiang Li",
      "Dan Zhang",
      "Shengzhao Lei",
      "Xun Zhao",
      "WeiWei Li",
      "Porawit Kamnoedboon",
      "Junhao Dong",
      "Shuyan Li"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.08182",
    "githubLink": "https://sites.google.com/view/ximagenet-12/home",
    "itemCount": "200,000+ images; 15,410 manual annotations",
    "specs": "Images; Semantic annotations, Segmentation masks",
    "description": "An explainable visual benchmark dataset designed to evaluate the robustness of visual models. It consists of images from 12 ImageNet categories with manual semantic annotations and simulates diverse real-world scenarios (e.g., blurring, background changes) to assess model reliance on specific features."
  },
  {
    "id": "saved-1769608044996-5a33d",
    "title": "MMBench",
    "source": "arXiv",
    "authors": [
      "Yuan Liu",
      "Haodong Duan",
      "Yuanhan Zhang",
      "Bo Li",
      "Songyang Zhang",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.06281",
    "githubLink": "https://github.com/open-compass/MMBench",
    "itemCount": "2,974 questions",
    "specs": "Multiple-choice questions covering 20 ability dimensions arranged in a hierarchy (Perception, Reasoning, etc.); uses CircularEval to test consistency.",
    "description": "A comprehensive evaluation pipeline for large vision-language models using 'CircularEval' to assess robustness across perception and reasoning abilities."
  },
  {
    "id": "saved-1769608044996-edror",
    "title": "SEED-Bench",
    "source": "arXiv",
    "authors": [
      "Bohao Li",
      "Rui Wang",
      "Guangzhi Wang",
      "Yuying Ge",
      "Yixiao Ge",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.16125",
    "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
    "itemCount": "19,000 questions (v1)",
    "specs": "Multiple-choice questions covering 12 dimensions; includes both image and video modalities for spatial and temporal reasoning.",
    "description": "A multimodal benchmark for evaluating Large Language Models on spatial and temporal understanding capabilities using multiple-choice questions."
  },
  {
    "id": "saved-1769608044996-6tipk",
    "title": "MME (Multimodal Evaluation)",
    "source": "arXiv",
    "authors": [
      "Chaoyou Fu",
      "Peixian Chen",
      "Yunhang Shen",
      "Yulei Qin",
      "Menglin Zhang",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.13394",
    "githubLink": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation",
    "itemCount": "14 subtasks",
    "specs": "Yes/No questions to avoid prompt engineering bias; covers coarse/fine-grained perception, OCR, logic, and numerical reasoning.",
    "description": "A comprehensive evaluation suite measuring both perception and cognition abilities of Multimodal Large Language Models (MLLMs)."
  },
  {
    "id": "saved-1769608044996-qngd1",
    "title": "POPE (Polling on Object Existence)",
    "source": "arXiv",
    "authors": [
      "Yifan Li",
      "Yifan Du",
      "Kun Zhou",
      "Jinpeng Wang",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.10355",
    "githubLink": "https://github.com/RUCAIBox/POPE",
    "itemCount": "3,000 visual-object pairs",
    "specs": "Binary (Yes/No) questions about object existence; includes random, popular, and adversarial sampling settings.",
    "description": "A polling-based evaluation method specifically designed to detect and evaluate object hallucination in Large Vision-Language Models."
  },
  {
    "id": "saved-1769608044996-ppd02",
    "title": "Do Anything Now (JailbreakHub)",
    "source": "arXiv",
    "authors": [
      "Xinyue Shen",
      "Zeyuan Chen",
      "Michael Backes",
      "Yun Shen",
      "Yang Zhang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.03825",
    "githubLink": "https://github.com/verazuo/jailbreak_llms",
    "itemCount": "1,405 jailbreak prompts, 107,250 samples",
    "specs": "Text",
    "description": "A comprehensive analysis of 'in-the-wild' jailbreak prompts, specifically the 'Do Anything Now' (DAN) type. The authors collected a large dataset of jailbreak prompts from online communities to evaluate LLM safeguards."
  },
  {
    "id": "saved-1769608044996-2sbig",
    "title": "SciEval",
    "source": "arXiv",
    "authors": [
      "Liangtai Sun",
      "Yang Han",
      "Zihan Zhao",
      "Da Ma",
      "Zhennan Shen",
      "Baocai Chen",
      "Lu Chen",
      "Kai Yu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.13149",
    "githubLink": "https://github.com/OpenDFM/SciEval",
    "itemCount": "~18,000 questions",
    "specs": "Text, Objective and Subjective Questions",
    "description": "A comprehensive multi-disciplinary evaluation benchmark for scientific research abilities of Large Language Models. It covers four dimensions based on Bloom's taxonomy: basic knowledge, knowledge application, scientific calculation, and research ability."
  },
  {
    "id": "saved-1769608044996-i1ww0",
    "title": "SciBench",
    "source": "arXiv",
    "authors": [
      "Xiaoxuan Wang",
      "Ziniu Hu",
      "Pan Lu",
      "Yanqiao Zhu",
      "Jieyu Zhang",
      "Satyen Subramaniam",
      "Arjun R. Loomba",
      "Shichang Zhang",
      "Yizhou Sun",
      "Wei Wang"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2307.10635",
    "githubLink": "https://github.com/mandarin-wang/SciBench",
    "itemCount": "869 open-ended problems, 103 closed-set problems",
    "specs": "Text + some Multimodal (Images in 177 problems), Free-response",
    "description": "A benchmark for evaluating college-level scientific problem-solving abilities of LLMs. It features open-ended, free-response questions from mathematics, chemistry, and physics textbooks, requiring multi-step reasoning and complex calculations."
  },
  {
    "id": "saved-1769608044996-53fx3",
    "title": "SuperBench",
    "source": "arXiv",
    "authors": [
      "Pu Ren",
      "N. Benjamin Erichson",
      "Junyi Guo",
      "Shashank Subramanian",
      "Omer San",
      "Zarija Lukić",
      "Michael W. Mahoney"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.13596",
    "githubLink": "https://github.com/erichson/SuperBench",
    "itemCount": "439 GB (Storage size)",
    "specs": "Image/Grid Data; Super-resolution",
    "description": "A high-resolution benchmark dataset for scientific machine learning (SciML) tasks, specifically super-resolution. It includes data from fluid flows, cosmology, and weather simulations to validate spatial reconstruction performance."
  },
  {
    "id": "saved-1769608044996-u3phk",
    "title": "Mol-Instructions",
    "source": "arXiv",
    "authors": [
      "Yin Fang",
      "Xiaozhuan Liang",
      "Ningyu Zhang",
      "Kangwei Liu",
      "Rui Huang",
      "Zhuo Chen",
      "Xiaohui Fan",
      "Huajun Chen"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.08018",
    "githubLink": "https://github.com/zjunlp/Mol-Instructions",
    "itemCount": "~148.4K molecule instructions, ~505K protein instructions",
    "specs": "Text and Sequence-based (SMILES, Protein Sequences); Instruction Tuning format",
    "description": "A comprehensive instruction tuning dataset for the biomolecular domain, designed to improve LLMs' understanding of biomolecules. It includes tasks related to small molecules, proteins, and biomolecular text."
  },
  {
    "id": "saved-1769608044996-9j9pi",
    "title": "MatSci-NLP",
    "source": "arXiv",
    "authors": [
      "Yu Song",
      "Santiago Miret",
      "Bang Liu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2305.08264",
    "githubLink": "https://github.com/IntelLabs/MatSci-NLP",
    "itemCount": "7 tasks",
    "specs": "Text-based; NLP tasks (NER, Relation Classification, etc.); Materials Science domain",
    "description": "A natural language benchmark for evaluating NLP models on materials science text. It encompasses seven tasks ranging from named entity recognition to synthesis action retrieval, specifically tailored for the materials science domain."
  },
  {
    "id": "saved-1769608044996-y9asf",
    "title": "Belebele",
    "source": "arXiv",
    "authors": [
      "Lucas Bandarkar",
      "Davis Liang",
      "Benjamin Muller",
      "Mikel Artetxe",
      "Satya Narayan Shukla",
      "Donald Husa",
      "Naman Goyal",
      "Abhinandan Krishnan",
      "Luke Zettlemoyer",
      "Madian Khabsa"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2308.16884",
    "githubLink": "https://github.com/facebookresearch/belebele",
    "itemCount": "122 languages, ~900 questions per language",
    "specs": "Text; Multiple-choice Reading Comprehension",
    "description": "A massively multilingual reading comprehension dataset spanning 122 language variants, with questions linked to FLORES-200 passages."
  },
  {
    "id": "saved-1769608044996-4g0b8",
    "title": "CyberSecEval / Purple Llama",
    "source": "arXiv",
    "authors": [
      "Manish Bhatt",
      "Sahana Chennabasappa",
      "Cyrus Nikolaidis",
      "Shengye Wan",
      "Ivan Evtimov",
      "Joshua Saxe",
      "Meta AI"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.03262",
    "githubLink": "https://github.com/meta-llama/PurpleLlama",
    "itemCount": "Multiple subsets: AutoPatchBench (142 test cases), Instruct/Autocomplete (Thousands of prompts)",
    "specs": "Text prompts, code snippets, and containerized test environments for C/C++ vulnerability patching.",
    "description": "A comprehensive benchmark suite from Meta designed to evaluate the cybersecurity risks of Large Language Models (LLMs) as coding assistants and offensive helpers. It includes tests for insecure code generation, compliance with cyberattack requests, and automated patching capabilities (AutoPatchBench)."
  },
  {
    "id": "saved-1769608044996-mpf8b",
    "title": "MultiPL-E",
    "source": "arXiv",
    "authors": [
      "Federico Cassano",
      "John Gouwar",
      "Daniel Nguyen",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2208.08227",
    "githubLink": "https://github.com/nuprl/MultiPL-E",
    "itemCount": "Variable (translations of HumanEval/MBPP)",
    "specs": "18+ languages (C++, Java, Rust, TypeScript, etc.)",
    "description": "A system for translating unit test-driven neural code generation benchmarks (like HumanEval and MBPP) to 18+ other programming languages to evaluate multilingual performance."
  },
  {
    "id": "saved-1769608044996-k75c4",
    "title": "RepoBench",
    "source": "arXiv",
    "authors": [
      "Tianyang Liu",
      "Canwen Xu",
      "Julian McAuley"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.03091",
    "githubLink": "https://github.com/Leolty/repobench",
    "itemCount": "Variable (subsets for Retrieval/Completion)",
    "specs": "Python, Java, Repository-level context",
    "description": "A benchmark designed for evaluating repository-level code auto-completion systems. It evaluates retrieval, code completion, and pipeline tasks with cross-file context."
  },
  {
    "id": "saved-1769608044996-wuygb",
    "title": "CrossCodeEval",
    "source": "arXiv",
    "authors": [
      "Yangruibo Ding",
      "Zijian Wang",
      "Wasi Uddin Ahmad",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.11248",
    "githubLink": "https://github.com/amazon-science/cceval",
    "itemCount": "Variable (diverse real-world samples)",
    "specs": "Python, Java, TypeScript, C#",
    "description": "A diverse and multilingual benchmark for cross-file code completion. It strictly requires cross-file context for accurate code completion, built on real-world repositories."
  },
  {
    "id": "saved-1769608044996-ab7gf",
    "title": "Skill-Mix",
    "source": "arXiv",
    "authors": [
      "Dingli Yu",
      "Simran Kaur",
      "Arushi Gupta",
      "Jonah Brown-Cohen",
      "Anirudh Goyal",
      "Sanjeev Arora"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2310.17567",
    "githubLink": "https://github.com/google-deepmind/skill-mix",
    "itemCount": "Configurable (N skills, k subsets)",
    "specs": "Text generation; Skill combination; Randomized prompts",
    "description": "An evaluation to measure an AI agent's ability to flexibly combine basic learned skills. The evaluator picks random subsets of skills and asks the LLM to produce text combining that subset."
  },
  {
    "id": "saved-1769608044996-ax0f9",
    "title": "A-Lab Experimental Dataset",
    "source": "Scholar",
    "authors": [
      "Nathan J. Szymanski",
      "Bernardus Rendy",
      "Yuxing Fei",
      "Gerbrand Ceder",
      "et al."
    ],
    "year": "2023",
    "paperLink": "https://doi.org/10.1038/s41586-023-06734-w",
    "githubLink": "https://github.com/CederGroupHub/A-Lab",
    "itemCount": "58 target compounds (41 synthesized)",
    "specs": "Experimental logs; XRD data; Synthesis recipes; Success/failure labels",
    "description": "A dataset of autonomous solid-state synthesis experiments generated by the A-Lab. It contains data from 17 days of continuous operation, including synthesis recipes, X-ray diffraction patterns, and outcomes for 58 target materials."
  },
  {
    "id": "saved-1769608044996-8105a",
    "title": "AppAgent: Multimodal Agents as Smartphone Users",
    "source": "arXiv",
    "authors": [
      "Chi Zhang",
      "Zhao Yang",
      "Jiaxuan Liu",
      "Yucheng Han",
      "Xin Chen",
      "Zebiao Huang",
      "Bin Fu",
      "Gang Yu"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2312.13771",
    "githubLink": "https://github.com/TencentQQGYLab/AppAgent",
    "itemCount": "50 tasks across 10 apps",
    "specs": "Multimodal (Text + Visual), interacts with smartphone apps via a simplified action space (tap, swipe).",
    "description": "Enables LLM-based multimodal agents to operate smartphone applications like humans, learning from exploration or demonstration. The benchmark evaluates the agent's proficiency in handling high-level tasks across diverse apps."
  },
  {
    "id": "saved-1769608044996-oaasc",
    "title": "ToolBench: An Instruction-Tuning Benchmark for Tool Learning",
    "source": "arXiv",
    "authors": [
      "Yujia Qin",
      "Shihao Liang",
      "Yining Ye",
      "Kunlun Zhu",
      "Lan Yan",
      "Yaxi Lu",
      "Yankai Lin",
      "Xin Cong",
      "Xiangru Tang",
      "Bill Qian",
      "Sihan Zhao",
      "Lauren Hong",
      "Runchu Tian",
      "Ruobing Xie",
      "Jie Zhou",
      "Mark Gerstein",
      "Dahai Li",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2309.17438",
    "githubLink": "https://github.com/OpenBMB/ToolBench",
    "itemCount": "16,464 real-world RESTful APIs; 126k instruction pairs",
    "specs": "JSON-based API calls, multi-turn instructions. Covers 49 categories of tools.",
    "description": "A large-scale benchmark for tool learning, covering diverse real-world APIs and scenarios. It is designed to facilitate the construction of open-source LLMs with general tool-use capabilities."
  },
  {
    "id": "saved-1769608044996-bvt5a",
    "title": "Handover-Sim2Real",
    "source": "Other",
    "authors": [
      "Sammy Christen",
      "Wei Yang",
      "Claudia Pérez-D'Arpino",
      "Otmar Hilliges",
      "Dieter Fox",
      "Yu-Wei Chao"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2211.16167",
    "githubLink": "https://github.com/NVlabs/handover-sim2real",
    "itemCount": "Not specified (Procedural generation scenes)",
    "specs": "3D Point clouds, simulation environments (PyBullet)",
    "description": "Official code and benchmark for learning human-to-robot handovers from point clouds, evaluating the transfer of policies trained in simulation to real-world setups."
  },
  {
    "id": "saved-1769608044996-opksz",
    "title": "EISOST (Endoscopic Images Sim-to-Real Oropharyngeal Segmentation)",
    "source": "Other",
    "authors": [
      "Guankun Wang",
      "Tian-Ao Ren",
      "Jiewen Lai",
      "Long Bai",
      "Hongliang Ren"
    ],
    "year": "2023",
    "paperLink": "https://github.com/gkw0010/EISOST-Sim2Real-Dataset-Release",
    "githubLink": "https://github.com/gkw0010/EISOST-Sim2Real-Dataset-Release",
    "itemCount": "1,397 images (1,194 sim, 203 real)",
    "specs": "RGB images, pixel-level segmentation masks",
    "description": "A dataset for Sim-to-Real domain adaptation in medical imaging, specifically for the segmentation of oropharyngeal organs (uvula, epiglottis, glottis)."
  },
  {
    "id": "saved-1769608044996-jgu44",
    "title": "PromptBench",
    "source": "arXiv",
    "authors": [
      "Kaijie Zhu",
      "Jindong Wang",
      "Jiaheng Zhou",
      "Zichen Wang",
      "Hao Chen",
      "Yidong Wang",
      "Linyi Yang",
      "Wei Ye",
      "Neil Zhenqiang Gong",
      "Yue Zhang",
      "Xie Xing"
    ],
    "year": "2023",
    "paperLink": "https://arxiv.org/abs/2306.04528",
    "githubLink": "https://github.com/microsoft/promptbench",
    "itemCount": "4,032 adversarial prompts",
    "specs": "Text (Adversarial prompts across 13 tasks)",
    "description": "A unified library and benchmark for evaluating the robustness of Large Language Models against adversarial prompts. It covers various levels of attacks including character, word, sentence, and semantic perturbations."
  },
  {
    "id": "saved-1769608044996-rl3pf",
    "title": "ChartQA",
    "source": "arXiv",
    "authors": [
      "Ahmed Masry",
      "Do Long",
      "Jia Tan",
      "Shafiq Joty",
      "Enamul Hoque"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.10244",
    "githubLink": "https://github.com/vis-nlp/ChartQA",
    "itemCount": "32.7k QA pairs",
    "specs": "Images (Charts), Text (QA pairs), Data Tables",
    "description": "A benchmark for Question Answering about charts requiring visual and logical reasoning. It combines human-authored question-answer pairs with machine-generated ones to cover a wide range of visual and logical reasoning tasks."
  },
  {
    "id": "saved-1769608044996-kki8e",
    "title": "Chart-to-Text",
    "source": "Scholar",
    "authors": [
      "Shankar Kantharaj",
      "Rixie Tiffany Leong",
      "Xiang Lin",
      "Ahmed Masry",
      "Megh Thakkar",
      "Enamul Hoque",
      "Shafiq Joty"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.acl-long.277/",
    "githubLink": "https://github.com/vis-nlp/Chart-to-text",
    "itemCount": "44k Charts",
    "specs": "Images, Data Tables, Text (Summaries)",
    "description": "A large-scale benchmark for chart summarization and captioning. It contains charts from real-world sources (Pew, Statista) paired with data tables and summaries."
  },
  {
    "id": "saved-1769608044996-8vs7i",
    "title": "MultiMedQA",
    "source": "arXiv",
    "authors": [
      "Karan Singhal",
      "Shekoofeh Azizi",
      "Tao Tu",
      "S. Sara Mahdavi",
      "Jason Wei",
      "Hyung Won Chung",
      "Nathan Scales",
      "Ajay Tanwani",
      "Heather Cole-Lewis",
      "Stephen Pfohl",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2212.13138",
    "githubLink": "https://github.com/google-research/google-research/tree/master/medpalm",
    "itemCount": "7 datasets total (includes MedQA, MedMCQA, PubMedQA, LiveQA, MedicationQA, MMLU-Clinical, HealthSearchQA)",
    "specs": "Text (Question Answering)",
    "description": "A benchmark suite combining six existing open question answering datasets (including MedQA, MedMCQA, PubMedQA) and a new dataset 'HealthSearchQA' of consumer medical questions, designed to evaluate Large Language Models in the medical domain."
  },
  {
    "id": "saved-1769608044996-jukmx",
    "title": "ToxiGen",
    "source": "arXiv",
    "authors": [
      "Thomas Hartvigsen",
      "Saadia Gabriel",
      "Hamid Palangi",
      "Maarten Sap",
      "Dipankar Ray",
      "Ece Kamar"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.09509",
    "githubLink": "https://github.com/microsoft/ToxiGen",
    "itemCount": "274,186 statements",
    "specs": "Text statements, generated by GPT-3",
    "description": "A large-scale machine-generated dataset for adversarial and implicit hate speech detection, containing toxic and benign statements about minority groups."
  },
  {
    "id": "saved-1769608044997-43119",
    "title": "HOPE",
    "source": "Scholar",
    "authors": [
      "Karan Malhotra",
      "Aseem Srivastava",
      "Mansi Agarwal",
      "Tanmoy Chakraborty"
    ],
    "year": "2022",
    "paperLink": "https://dl.acm.org/doi/10.1145/3488560.3498422",
    "githubLink": "https://github.com/LCS2-IIITD/SPARTA_WSDM2022",
    "itemCount": "212 sessions (approx. 12,900 utterances)",
    "specs": "Text (English), Dyadic Conversations, Annotated Dialogue Acts",
    "description": "A dataset of dyadic counseling conversation transcripts sourced from public counseling videos. It is annotated with fine-grained dialogue acts tailored for counseling conversations, designed to support tasks like dialogue act classification and response generation."
  },
  {
    "id": "saved-1769608044997-nvpdp",
    "title": "MEMO",
    "source": "Scholar",
    "authors": [
      "Aseem Srivastava",
      "Mansi Agarwal",
      "Karan Malhotra",
      "Tanmoy Chakraborty"
    ],
    "year": "2022",
    "paperLink": "https://doi.org/10.1007/978-3-031-20353-4_33",
    "githubLink": "https://github.com/LCS2-IIITD/MEMO",
    "itemCount": "212 sessions with summaries",
    "specs": "Text (English), Dialogue Transcripts, Summaries (Counseling Notes)",
    "description": "A dataset specifically designed for counseling summarization. It extends the HOPE dataset by providing human-written summaries (counseling notes) for the conversation transcripts, enabling research into domain-specific summarization."
  },
  {
    "id": "saved-1769608044997-4mscp",
    "title": "KoBEST (Korean Balanced Evaluation of Significant Tasks)",
    "source": "arXiv",
    "authors": [
      "Dohyeong Kim",
      "Myeongjun Jang",
      "Deuk Sin Kwon",
      "Eric Davis"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.04541",
    "githubLink": "https://huggingface.co/datasets/skt/kobest_v1",
    "itemCount": "5 tasks",
    "specs": "Text, Logic/Reasoning/QA",
    "description": "A benchmark designed to evaluate advanced Korean linguistic knowledge and reasoning. It consists of five downstream tasks: Boolean Question Answering (BoolQ), Choice of Plausible Alternatives (COPA), HellaSwag, Word-in-Context (WiC), and Sentiment Negation (SentiNeg)."
  },
  {
    "id": "saved-1769608044997-59f00",
    "title": "K-MHaS (Korean Multi-label Hate Speech Dataset)",
    "source": "Hugging Face",
    "authors": [
      "Jean Lee",
      "Taejun Lim",
      "Heejun Lee",
      "Bogeun Jo",
      "Yangsok Kim",
      "Heegeun Yoon",
      "Soyeon Caren Han"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.coling-1.311/",
    "githubLink": "https://huggingface.co/datasets/jeanlee/kmhas_korean_hate_speech",
    "itemCount": "109,692 utterances",
    "specs": "Text, Multi-label Classification",
    "description": "A large-scale multi-label hate speech detection dataset consisting of comments from Korean online news platforms. It provides fine-grained labels for different types of hate speech (e.g., Politics, Gender, Region) to handle the subjectivity and intersectionality of hate speech."
  },
  {
    "id": "saved-1769608044997-y2265",
    "title": "BIG-bench (Beyond the Imitation Game Benchmark)",
    "source": "arXiv",
    "authors": [
      "Aarohi Srivastava",
      "Abhinav Rastogi",
      "Abhishek Rao",
      "Abu Awal Md Shoeb",
      "Abubakar Abid",
      "Adam Fisch",
      "Adam R. Brown",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.04615",
    "githubLink": "https://github.com/google/BIG-bench",
    "itemCount": "200+ tasks",
    "specs": "JSON task format. Primarily text-based but includes diverse reasoning, logic, and knowledge domains.",
    "description": "A massive collaborative benchmark intended to probe large language models on diverse tasks believed to be beyond the capabilities of current models, aiming to extrapolate future capabilities."
  },
  {
    "id": "saved-1769608044997-5lxrk",
    "title": "FaithDial",
    "source": "arXiv",
    "authors": [
      "Nouha Dziri",
      "Ehsan Kamalloo",
      "Sivan Milton",
      "Osmar Zaiane",
      "Mo Yu",
      "Edoardo M. Ponti",
      "Siva Reddy"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.10757",
    "githubLink": "https://github.com/McGill-NLP/FaithDial",
    "itemCount": "5,649 dialogues, 50,761 utterances",
    "specs": "Text (Dialogue), Knowledge-grounded",
    "description": "A benchmark for faithful information-seeking dialogue. It was created by letting humans edit hallucinated responses in the Wizard of Wikipedia dataset to ensure faithfulness to knowledge sources."
  },
  {
    "id": "saved-1769608044997-dujlv",
    "title": "FactualityPrompts",
    "source": "arXiv",
    "authors": [
      "Nayeon Lee",
      "Wei Ping",
      "Peng Xu",
      "Mostofa Patwary",
      "Pascale Fung",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.04624",
    "githubLink": "https://github.com/nayeon7lee/FactualityPrompt",
    "itemCount": "Unknown (Test prompts set)",
    "specs": "Text (Open-ended generation)",
    "description": "A benchmark of prompts designed to trigger factual errors in open-ended text generation, coupled with a pipeline for evaluating the factuality of the generated text."
  },
  {
    "id": "saved-1769608044997-7bfsk",
    "title": "Mini-ARC",
    "source": "Other",
    "authors": [
      "Subin Kim",
      "Prin Phunyaphibarn",
      "Donghyun Ahn",
      "Sundong Kim"
    ],
    "year": "2022",
    "paperLink": "https://openreview.net/forum?id=pCKWb4yE1i",
    "githubLink": "https://github.com/KSB21ST/MINI-ARC",
    "itemCount": "150 tasks",
    "specs": "JSON format, fixed 5x5 grids",
    "description": "A compact version of the ARC dataset where all grids are fixed to a 5x5 size. It is designed to reduce the computational modeling budget while maintaining the core challenge of abductive reasoning."
  },
  {
    "id": "saved-1769608044997-5upki",
    "title": "HELM (Holistic Evaluation of Language Models)",
    "source": "arXiv",
    "authors": [
      "Percy Liang",
      "Rishi Bommasani",
      "Tony Lee",
      "Dimitris Tsipras",
      "Dilara Soylu",
      "Michihiro Yasunaga",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2211.09110",
    "githubLink": "https://github.com/stanford-crfm/helm",
    "itemCount": "42 scenarios (initial release), expanded over time",
    "specs": "Text-only; Multi-metric evaluation (Accuracy, Calibration, Robustness, Fairness, Bias, Toxicity, Efficiency)",
    "description": "A comprehensive benchmark that evaluates language models across a vast taxonomy of scenarios and metrics to improve transparency. It measures capabilities (e.g., accuracy) and risks (e.g., bias, toxicity) across diverse domains."
  },
  {
    "id": "saved-1769608044997-h9vlh",
    "title": "DS-1000",
    "source": "arXiv",
    "authors": [
      "Yuhang Lai",
      "Chengxi Li",
      "Yiming Wang",
      "Tianyi Zhang",
      "Ruiqi Zhong",
      "Luke Zettlemoyer",
      "Scott Wen-tau Yih",
      "Daniel Fried",
      "Sida I. Wang",
      "Tao Yu"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2211.11501",
    "githubLink": "https://github.com/xlang-ai/DS-1000",
    "itemCount": "1000 problems",
    "specs": "Python code, 7 libraries (NumPy, Pandas, SciPy, Scikit-learn, TensorFlow, PyTorch, Matplotlib)",
    "description": "A code generation benchmark with 1,000 data science problems spanning seven Python libraries (NumPy, Pandas, etc.). It focuses on realistic and practical use cases collected from StackOverflow, with specific defense against memorization."
  },
  {
    "id": "saved-1769608044997-1uxmp",
    "title": "EHRSQL",
    "source": "arXiv",
    "authors": [
      "Gyubok Lee",
      "Hyeonji Hwang",
      "Seongsu Bae",
      "Yeonsu Kwon",
      "Woncheol Shin",
      "Seongjun Yang",
      "Minjoon Seo",
      "Jong-Yeup Kim",
      "Edward Choi"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2301.07695",
    "githubLink": "https://github.com/glee4810/EHRSQL",
    "itemCount": "~24,000 samples",
    "specs": "Structured Data (MIMIC-III and eICU); Text-to-SQL",
    "description": "A large-scale text-to-SQL dataset for Electronic Health Records, designed to test QA models on structured data. It includes questions collected from hospital staff and covers a wide range of SQL queries, including those requiring time expression understanding and unanswerable question detection."
  },
  {
    "id": "saved-1769608044997-fr9dl",
    "title": "DrugEHRQA",
    "source": "Scholar",
    "authors": [
      "Jayetri Bardhan",
      "Anthony Colas",
      "Kirk Roberts",
      "Daisy Zhe Wang"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.lrec-1.116/",
    "githubLink": "https://github.com/jayetri/DrugEHRQA",
    "itemCount": "70,000+ QA pairs",
    "specs": "Multi-modal (Structured Tables + Unstructured Text)",
    "description": "A multi-modal question answering dataset focusing on medication-related queries. It contains QA pairs derived from both structured tables and unstructured discharge summaries in MIMIC-III, designed to improve QA using cross-modal context."
  },
  {
    "id": "saved-1769608044997-9wilz",
    "title": "SKM-TEA",
    "source": "arXiv",
    "authors": [
      "Arjun D. Desai",
      "Andrew M. Schmidt",
      "Elka B. Rubin",
      "Christopher M. Sandino",
      "Marianne S. Black",
      "Valentina Mazzoli",
      "Akshay S. Chaudhari"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.06823",
    "githubLink": "https://github.com/StanfordMIMI/skm-tea",
    "itemCount": "155 patients (~25,000 slices)",
    "specs": "Raw k-space, DICOM, segmentation masks, bounding boxes",
    "description": "Stanford Knee MRI with Multi-Task Evaluation (SKM-TEA) is a dataset of quantitative knee MRI scans enabling end-to-end evaluation of reconstruction, segmentation, and detection."
  },
  {
    "id": "saved-1769608044997-gadey",
    "title": "ScienceQA",
    "source": "Hugging Face",
    "authors": [
      "Pan Lu",
      "Swaroop Mishra",
      "Tony Xia",
      "Liang Qiu",
      "Kai-Wei Chang",
      "Song-Chun Zhu",
      "Oyvind Tafjord",
      "Peter Clark",
      "Ashwin Kalyan"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2209.09513",
    "githubLink": "https://github.com/lupantech/ScienceQA",
    "itemCount": "21,208 questions",
    "specs": "Text & Image, Multiple Choice, Science Domain",
    "description": "A multimodal science question answering dataset with annotated lectures and explanations, covering natural, language, and social sciences."
  },
  {
    "id": "saved-1769608044997-2btqy",
    "title": "MedMCQA",
    "source": "Hugging Face",
    "authors": [
      "Ankit Pal",
      "Logesh Kumar Umapathi",
      "Malaikannan Sankarasubbu"
    ],
    "year": "2022",
    "paperLink": "https://proceedings.mlr.press/v174/pal22a.html",
    "githubLink": "https://github.com/medmcqa/medmcqa",
    "itemCount": "194,000+ MCQs",
    "specs": "Text (Multiple Choice Questions)",
    "description": "A large-scale multiple-choice question answering dataset covering 21 medical subjects, including a significant number of dermatology questions. It is designed to address real-world medical entrance exam questions."
  },
  {
    "id": "saved-1769608044997-p3o53",
    "title": "Fig-QA (Figurative Language Question Answering)",
    "source": "Hugging Face",
    "authors": [
      "Emmy Liu",
      "Chenxuan Cui",
      "Kenneth Zheng",
      "Graham Neubig"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.12632",
    "githubLink": "https://github.com/nightingal3/fig-qa",
    "itemCount": "10,256 examples",
    "specs": "Text-only; Winograd schema format",
    "description": "A Winograd-style benchmark designed to test the ability of language models to interpret figurative language. It consists of paired figurative phrases (metaphors) with divergent meanings to evaluate non-literal reasoning capabilities."
  },
  {
    "id": "saved-1769608044997-a5o9w",
    "title": "Diamond Images Dataset",
    "source": "Other",
    "authors": [
      "Aayush Purswani"
    ],
    "year": "2022",
    "paperLink": "https://www.kaggle.com/datasets/harshitlakhani/natural-diamonds-prices-images",
    "githubLink": "https://www.kaggle.com/datasets/harshitlakhani/natural-diamonds-prices-images",
    "itemCount": "~21,000 images",
    "specs": "Images (JPG), Labeled folders",
    "description": "A collection of images of diamonds classified by their shape (e.g., Round, Emerald, Heart). This dataset is used for computer vision tasks such as shape identification and cut classification."
  },
  {
    "id": "saved-1769608044997-kzn88",
    "title": "CDC SARS-CoV-2 Surveillance Benchmarks",
    "source": "Scholar",
    "authors": [
      "Lingzi Xiaoli",
      "Jill V. Hagey",
      "Lee S. Katz",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://peerj.com/articles/13821/",
    "githubLink": "https://github.com/CDCgov/datasets-sars-cov-2",
    "itemCount": "6 datasets",
    "specs": "Genomic sequencing data (Illumina short reads, Oxford Nanopore long reads)",
    "description": "Standardized benchmark datasets for testing bioinformatics pipelines used in SARS-CoV-2 surveillance, including variant calling and lineage inference."
  },
  {
    "id": "saved-1769608044997-hts4f",
    "title": "Instagram-Based Benchmark Dataset for Cyberbullying Detection",
    "source": "Semantic Scholar",
    "authors": [
      "Rawan ALBayari",
      "Salim Abdallah"
    ],
    "year": "2022",
    "paperLink": "https://www.mdpi.com/2306-5729/7/7/88",
    "githubLink": "https://www.mdpi.com/2306-5729/7/7/88",
    "itemCount": "200,000 comments (46,898 annotated)",
    "specs": "Text (Arabic, Cyberbullying detection)",
    "description": "A benchmark dataset collected from Instagram focusing on cyberbullying in Arabic text. It provides a multi-class categorization of comments, including bullying, which is highly relevant to child safety on social media."
  },
  {
    "id": "saved-1769608044997-o1i26",
    "title": "PJZC Dataset",
    "source": "Semantic Scholar",
    "authors": [
      "Daniela Fernanda Milon-Flores",
      "Robson Leonardo Ferreira Cordeiro"
    ],
    "year": "2022",
    "paperLink": "https://doi.org/10.1016/j.knosys.2021.108017",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "Binary annotated chat logs (variable size)",
    "specs": "Text (Chat Logs)",
    "description": "An updated dataset aimed at mitigating the data scarcity in grooming detection. It combines newer IRC logs (2013–2022) as true negatives with the last set of grooming chats from the Perverted Justice archives (2013–2014) as true positives."
  },
  {
    "id": "saved-1769608044997-d4xel",
    "title": "MaXM (Multilingual Visual Question Answering)",
    "source": "arXiv",
    "authors": [
      "Soravit Changpinyo",
      "Linting Xue",
      "Yara Rizk",
      "Hamid Palangi",
      "Praveen Krishnan",
      "Aaron Sarna"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2209.05401",
    "githubLink": "https://github.com/google-research-datasets/maxm",
    "itemCount": "Contains questions in 7 languages corresponding to Crossmodal-3600 images",
    "specs": "Multimodal (Image + Multilingual Text), 7 languages",
    "description": "A large-scale benchmark for the Multilingual Visual Question Answering (often abbreviated as VMQA or mVQA) task. It provides a test-only benchmark in 7 diverse languages derived from the Crossmodal-3600 dataset to evaluate multilingual multimodal models."
  },
  {
    "id": "saved-1769608044997-gahh6",
    "title": "PACS (Physical Audiovisual CommonSense)",
    "source": "arXiv",
    "authors": [
      "Samuel Yu",
      "Peter Wu",
      "Paul Pu Liang",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2203.11130",
    "githubLink": "https://github.com/samuelyu2002/PACS",
    "itemCount": "13,400 QA pairs, 1,526 videos",
    "specs": "Video, Audio, Text (Question-Answer pairs); 1,377 unique physical commonsense questions",
    "description": "A multimodal benchmark for physical commonsense reasoning. It contains question-answer pairs about physical properties and interactions (e.g., material types, object affordances) in videos, requiring both visual and acoustic reasoning."
  },
  {
    "id": "saved-1769608044997-r5bp6",
    "title": "Anthropic HH-RLHF",
    "source": "Hugging Face",
    "authors": [
      "Yuntao Bai",
      "Andy Jones",
      "Kamal Ndousse",
      "Amanda Askell",
      "Anna Chen",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.05862",
    "githubLink": "https://github.com/anthropics/hh-rlhf",
    "itemCount": "~161,000 conversations",
    "specs": "Text pairs (chosen/rejected), JSONL format",
    "description": "A dataset consisting of human preference data for training helpful and harmless AI assistants. It includes pairs of model responses to user prompts, with one response labeled as 'chosen' and the other as 'rejected' based on human feedback."
  },
  {
    "id": "saved-1769608044997-hbfl8",
    "title": "LexGLUE (Legal General Language Understanding Evaluation)",
    "source": "Hugging Face",
    "authors": [
      "Ilias Chalkidis",
      "Abhik Jana",
      "Dirk Hartung",
      "Michael Bommarito",
      "Ion Androutsopoulos",
      "Daniel Katz",
      "Nikolaos Aletras"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.acl-long.297/",
    "githubLink": "https://github.com/coastalcph/lex-glue",
    "itemCount": "Aggregates 7 datasets (~200k+ total samples)",
    "specs": "Text (English); Multi-task (Classification, QA, etc.)",
    "description": "A benchmark dataset to evaluate the performance of NLP methods in legal tasks, including legal judgment prediction. It aggregates seven existing legal NLP datasets (including ECtHR, SCOTUS, LEDGAR, etc.) to provide a standardized evaluation framework."
  },
  {
    "id": "saved-1769608044997-on883",
    "title": "CXR-PRO",
    "source": "Other",
    "authors": [
      "Vignav Ramesh",
      "Nathan A. Chi",
      "Pranav Rajpurkar"
    ],
    "year": "2022",
    "paperLink": "https://physionet.org/content/cxr-pro/1.0.0/",
    "githubLink": "https://physionet.org/content/cxr-pro/",
    "itemCount": "374,139 reports",
    "specs": "Text radiology reports (references omitted), Linked to MIMIC-CXR images",
    "description": "An adaptation of the MIMIC-CXR dataset that omits references to prior radiology reports to address the issue of hallucinated references in generated reports."
  },
  {
    "id": "saved-1769608044997-60y9i",
    "title": "ECTSum",
    "source": "Other",
    "authors": [
      "Rajdeep Mukherjee",
      "Abhinav Bohra",
      "Akash Banerjee",
      "Soumya Sharma",
      "Manjunath Hegde",
      "Afreen Shaikh",
      "Shivani Shrivastava",
      "Koustuv Dasgupta",
      "Niloy Ganguly",
      "Saptarshi Ghosh",
      "Pawan Goyal"
    ],
    "year": "2022",
    "paperLink": "https://aclanthology.org/2022.emnlp-main.742/",
    "githubLink": "https://github.com/rajdeep345/ECTSum",
    "itemCount": "2,425 transcripts",
    "specs": "Text (Financial Earnings Calls)",
    "description": "A large-scale dataset for bullet point summarization of long earnings call transcripts (ECTs). It consists of transcripts from publicly traded companies and expert-written telegram-style bullet point summaries derived from Reuters articles. The dataset is designed to benchmark financial document summarization."
  },
  {
    "id": "saved-1769608044997-7jdp7",
    "title": "Multi-LexSum",
    "source": "arXiv",
    "authors": [
      "Zejiang Shen",
      "Kyle Lo",
      "Lauren Yu",
      "Nathan Dahlberg",
      "Margo Schlanger",
      "Doug Downey"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.10883",
    "githubLink": "https://github.com/multilexsum/dataset",
    "itemCount": "9,280 summaries (from ~4,500 cases)",
    "specs": "Text (Legal Case Summaries)",
    "description": "A dataset of civil rights lawsuit summaries at multiple granularities (tiny, short, long). It is sourced from the Civil Rights Litigation Clearinghouse and features expert-authored summaries, making it suitable for multi-document and variable-granularity summarization tasks."
  },
  {
    "id": "saved-1769608044997-urkoi",
    "title": "OpenXAI",
    "source": "arXiv",
    "authors": [
      "Chirag Agarwal",
      "Satyapriya Krishna",
      "Eshika Saxena",
      "Martin Pawelczyk",
      "Nari Johnson",
      "Isha Puri",
      "Marinka Zitnik",
      "Himabindu Lakkaraju"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.11104",
    "githubLink": "https://github.com/wcventure/OpenXAI",
    "itemCount": "7 real-world datasets + synthetic generator",
    "specs": "Tabular data, Synthetic data; Metrics for Faithfulness, Stability, Fairness",
    "description": "A comprehensive and extensible open-source framework for evaluating and benchmarking post-hoc explanation methods. It includes a collection of real-world high-stakes datasets (finance, healthcare, etc.) and a synthetic data generator to create ground-truth explanations."
  },
  {
    "id": "saved-1769608044997-72g0i",
    "title": "Big-Bench Hard (BBH)",
    "source": "arXiv",
    "authors": [
      "Mirac Suzgun",
      "Nathan Scales",
      "Nathanael Schärli",
      "Sebastian Gehrmann",
      "Yi Tay",
      "Hyung Won Chung",
      "Aakanksha Chowdhery",
      "Quoc V. Le",
      "Ed H. Chi",
      "Denny Zhou",
      "Jason Wei"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2210.09261",
    "githubLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
    "itemCount": "6,511 examples (23 tasks)",
    "specs": "Text (Diverse Reasoning Tasks)",
    "description": "A subset of the BIG-Bench suite, focusing on 23 challenging tasks where prior language models did not outperform the average human-rater. It emphasizes multi-step reasoning capabilities."
  },
  {
    "id": "saved-1769608044997-u507m",
    "title": "SciRepEval",
    "source": "arXiv",
    "authors": [
      "Amanpreet Singh",
      "Mike D'Arcy",
      "Arman Cohan",
      "Doug Downey",
      "Sergey Feldman"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2211.13308",
    "githubLink": "https://github.com/allenai/scirepeval",
    "itemCount": "24 tasks",
    "specs": "Text-based; Document embeddings; Formats: Classification, Regression, Ranking, Search",
    "description": "A large-scale benchmark for training and evaluating scientific document representations. It covers a wide range of tasks including classification, regression, ranking, and search to test generalization across formats."
  },
  {
    "id": "saved-1769608044997-ow1ff",
    "title": "FLORES-200",
    "source": "arXiv",
    "authors": [
      "NLLB Team",
      "Marta R. Costa-jussà",
      "James Cross",
      "Onur Çelebi",
      "Maha Elbayad",
      "Kenneth Heafield",
      "Kevin Heffernan",
      "Elahe Kalbassi",
      "Janice Lam",
      "Daniel Licht",
      "Jean Maillard",
      "Anna Sun",
      "Skyler Wang",
      "Guillaume Wenzek",
      "Al Youngblood",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2207.04672",
    "githubLink": "https://github.com/facebookresearch/flores",
    "itemCount": "200 languages, 3001 sentences per language",
    "specs": "Text; Machine Translation",
    "description": "A many-to-many multilingual translation benchmark dataset covering 200 languages, doubling the coverage of its predecessor FLORES-101."
  },
  {
    "id": "saved-1769608044997-bpe7e",
    "title": "IGLUE",
    "source": "arXiv",
    "authors": [
      "Emanuele Bugliarello",
      "Fangyu Liu",
      "Jonas Pfeiffer",
      "Siva Reddy",
      "Desmond Elliott",
      "Edoardo Maria Ponti",
      "Ivan Vulić"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2201.11732",
    "githubLink": "https://github.com/e-bug/iglue",
    "itemCount": "20 languages, 5 datasets",
    "specs": "Multimodal (Image + Text); Visual QA, Retrieval, Reasoning",
    "description": "The Image-Grounded Language Understanding Evaluation benchmark, aggregating visual QA, retrieval, and reasoning tasks across 20 languages."
  },
  {
    "id": "saved-1769608044997-ewene",
    "title": "MASSIVE",
    "source": "arXiv",
    "authors": [
      "Jack FitzGerald",
      "Christopher Hench",
      "Charith Peris",
      "Scott Mackie",
      "Kay Rottmann",
      "Ana Sanchez",
      "Aaron Nash",
      "Liam Urbach",
      "Vishesh Kakarala",
      "Richa Singh",
      "Swetha Ranganath",
      "Laurie Crist",
      "Misha Britan",
      "Wouter Leeuwis",
      "Gokhan Tur",
      "Prem Natarajan"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.08582",
    "githubLink": "https://github.com/alexa/massive",
    "itemCount": "1,000,000 utterances, 51 languages",
    "specs": "Text; Intent Classification, Slot Filling",
    "description": "A multilingual natural language understanding dataset for intent classification and slot filling, containing 1M utterances across 51 languages."
  },
  {
    "id": "saved-1769608044997-dm41h",
    "title": "FLEURS",
    "source": "Hugging Face",
    "authors": [
      "Alexis Conneau",
      "Min Ma",
      "Simran Khanuja",
      "Yu Zhang",
      "Vera Axelrod",
      "Siddharth Dalmia",
      "Jason Riesa",
      "Clara Rivera",
      "Ankur Bapna"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2205.12446",
    "githubLink": "https://huggingface.co/datasets/google/fleurs",
    "itemCount": "102 languages, ~12 hours per language",
    "specs": "Audio/Speech; ASR, Speech Translation",
    "description": "A parallel speech dataset in 102 languages built on top of FLORES-101, designed for ASR and speech translation evaluation."
  },
  {
    "id": "saved-1769608044997-9799n",
    "title": "Anthropic Red Team Dataset",
    "source": "arXiv",
    "authors": [
      "Deep Ganguli",
      "Liane Lovitt",
      "John Kernion",
      "Amanda Askell",
      "Yuntao Bai",
      "et al."
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2209.07858",
    "githubLink": "https://github.com/anthropics/hh-rlhf",
    "itemCount": "38,961 dialogues",
    "specs": "Text (Multi-turn dialogues); Red-teaming transcripts",
    "description": "A dataset of adversarial conversations where humans attempt to elicit harmful responses from an AI assistant. Useful for training and evaluating safety mitigation."
  },
  {
    "id": "saved-1769608044997-pe00q",
    "title": "PMO: Sample Efficiency Matters",
    "source": "Other",
    "authors": [
      "Wenhao Gao",
      "Tianfan Fu",
      "Jimeng Sun",
      "Connor W. Coley"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2206.12411",
    "githubLink": "https://github.com/wenhao-gao/mol_opt",
    "itemCount": "23 optimization tasks",
    "specs": "Molecular optimization; SMILES representation; Oracle budget limit (10k)",
    "description": "A benchmark for practical molecular optimization focused on sample efficiency. It evaluates 25 molecular design algorithms on 23 single-objective optimization tasks with a limited oracle budget, simulating realistic resource constraints in autonomous discovery."
  },
  {
    "id": "saved-1769608044997-o1ikm",
    "title": "ObjectFolder 2.0",
    "source": "Scholar",
    "authors": [
      "Ruohan Gao",
      "Zilin Si",
      "Yen-Yu Chang",
      "Samuel Clarke",
      "Jeannette Bohg",
      "Li Fei-Fei",
      "Wenzhen Yuan",
      "Jiajun Wu"
    ],
    "year": "2022",
    "paperLink": "https://arxiv.org/abs/2204.02389",
    "githubLink": "https://github.com/rhgao/ObjectFolder",
    "itemCount": "1,000 neural objects; 100 real-world objects",
    "specs": "Implicit neural representations, 3D meshes, videos, impact sounds, tactile readings",
    "description": "A large-scale multisensory dataset of 1,000 objects modeled with implicit neural representations (VisionNet, AudioNet, TouchNet) and 100 real-world objects with corresponding visual, acoustic, and tactile measurements for Sim2Real transfer."
  },
  {
    "id": "saved-1769608044997-dtesz",
    "title": "TruthfulQA",
    "source": "arXiv",
    "authors": [
      "Stephanie Lin",
      "Jacob Hilton",
      "Owain Evans"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2109.07958",
    "githubLink": "https://github.com/sylinrl/TruthfulQA",
    "itemCount": "817 questions",
    "specs": "Text questions (Multiple choice and Generation), CSV",
    "description": "A benchmark to measure whether language models generate truthful answers to questions, focusing on imitative falsehoods and misconceptions."
  },
  {
    "id": "saved-1769608044997-n2au9",
    "title": "ESConv (Emotional Support Conversation)",
    "source": "arXiv",
    "authors": [
      "Siyang Liu",
      "Chujie Zheng",
      "Orianna Demasi",
      "Sahand Sabour",
      "Yu Li",
      "Zhou Yu",
      "Yong Jiang",
      "Minlie Huang"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.269/",
    "githubLink": "https://github.com/thu-coai/Emotional-Support-Conversation",
    "itemCount": "1,300 conversations",
    "specs": "Text (English), Dialogue, Annotated Support Strategies",
    "description": "A dataset constructed for Emotional Support Conversation (ESC) tasks. It contains conversations between help-seekers and supporters, where supporters follow a specific emotional support framework (exploration, comforting, action)."
  },
  {
    "id": "saved-1769608044997-p4zb4",
    "title": "PsyQA",
    "source": "arXiv",
    "authors": [
      "Hao Sun",
      "Zhenru Lin",
      "Chujie Zheng",
      "Siyang Liu",
      "Minlie Huang"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.findings-acl.130/",
    "githubLink": "https://github.com/thu-coai/PsyQA",
    "itemCount": "22,346 questions; 56,063 answers",
    "specs": "Text (Chinese), Q&A pairs, Long-form answers",
    "description": "A large-scale Chinese dataset for generating long counseling text. It consists of question and answer pairs crawled from a Chinese mental health service platform, with a portion annotated for support strategies."
  },
  {
    "id": "saved-1769608044997-4ogtp",
    "title": "ReVeal",
    "source": "arXiv",
    "authors": [
      "Saikat Chakraborty",
      "Rahul Krishna",
      "Yangruibo Ding",
      "Baishakhi Ray"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2009.07235",
    "githubLink": "https://github.com/VulDetProject/ReVeal",
    "itemCount": "~18k functions (approx. 1.6k vulnerable)",
    "specs": "C/C++ code",
    "description": "A dataset created to evaluate deep learning-based vulnerability detection. It contains labeled vulnerabilities from Chromium and Debian issue trackers, highlighting the importance of realistic data distributions."
  },
  {
    "id": "saved-1769608044997-9ja7j",
    "title": "KLUE (Korean Language Understanding Evaluation)",
    "source": "arXiv",
    "authors": [
      "Sungjoon Park",
      "Jihyung Moon",
      "Sungdong Kim",
      "Won Ik Cho",
      "Jiyoon Han",
      "Jangwon Park",
      "Chisung Song",
      "Jun Seok Kim",
      "Yongsi Song",
      "Taehwan Oh",
      "Joohong Lee",
      "Juhyun Oh",
      "Sungwon Lyu",
      "Younghoon Jeong",
      "Inkwon Lee",
      "Sangwoo Seo",
      "Dongjun Lee",
      "Hyunwoo Kim",
      "Myeonghwa Lee",
      "Seongbo Jang",
      "Seungwon Do",
      "Sunkyu Kim",
      "Kyungtae Lim",
      "Jongwon Lee",
      "Kyumin Park",
      "Jamin Shin",
      "Seonghyun Kim",
      "Lucy Park",
      "Alice Oh",
      "Jung-Woo Ha",
      "Kyunghyun Cho"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2105.09680",
    "githubLink": "https://github.com/KLUE-benchmark/KLUE",
    "itemCount": "8 tasks (size varies, e.g., ~13k STS pairs, ~30k NER sentences)",
    "specs": "Text (JSON/TSV), Multiple NLU tasks",
    "description": "A comprehensive benchmark suite for evaluating Korean natural language understanding models. It consists of 8 diverse tasks including Topic Classification, Semantic Textual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking."
  },
  {
    "id": "saved-1769608044997-8xu6b",
    "title": "LARC (Language-complete ARC)",
    "source": "arXiv",
    "authors": [
      "Samuel Acquaviva",
      "Yewen Pu",
      "Marta Kryven",
      "Catherine Wong",
      "Gabrielle E. Ecanow",
      "Maxwell Nye",
      "Theodoros Sechopoulos",
      "Michael Henry Tessler",
      "Joshua B. Tenenbaum"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2106.07824",
    "githubLink": "https://github.com/samacqua/LARC",
    "itemCount": "Annotations for ARC tasks (354 successful)",
    "specs": "Text descriptions linked to ARC tasks",
    "description": "An extension of ARC that includes natural language descriptions of the underlying transformation rules. Collected via a communication game where one human describes the task to another, verifying that the language is sufficient to solve the puzzle."
  },
  {
    "id": "saved-1769608044998-3ptwa",
    "title": "BBQ: Bias Benchmark for QA",
    "source": "arXiv",
    "authors": [
      "Alicia Parrish",
      "Angelica Chen",
      "Nikita Nangia",
      "Vishakh Padmakumar",
      "Jason Phang",
      "Jana Thompson",
      "Phu Mon Htut",
      "Samuel R. Bowman"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.08193",
    "githubLink": "https://github.com/nyu-mll/BBQ",
    "itemCount": "58,492 samples",
    "specs": "English text, Multiple Choice (Trinary: Target, Non-target, Unknown), JSONL format",
    "description": "A dataset of question sets constructed to highlight attested social biases against people belonging to protected classes along nine social dimensions (e.g., age, gender, race, socioeconomic status). It evaluates model responses in both under-informative (ambiguous) and adequately informative (disambiguated) contexts."
  },
  {
    "id": "saved-1769608044998-6pfip",
    "title": "emrKBQA",
    "source": "Scholar",
    "authors": [
      "Preethi Raghavan",
      "Jennifer J. Liang",
      "Diwakar Mahajan",
      "Rachita Chandra",
      "Peter Szolovits"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.bionlp-1.7/",
    "githubLink": "https://github.com/emrKBQA/emrKBQA",
    "itemCount": "~940,000 samples",
    "specs": "Structured Data (MIMIC-III); Logical Forms",
    "description": "A dataset for answering physician questions from structured patient records (MIMIC-III). It serves as a structured counterpart to emrQA, focusing on retrieving answers by mapping natural language questions to logical forms over the database."
  },
  {
    "id": "saved-1769608044998-od86l",
    "title": "MIMIC-SPARQL",
    "source": "arXiv",
    "authors": [
      "Junwoo Park",
      "Youngwoo Cho",
      "Haneol Lee",
      "Jaegul Choo",
      "Edward Choi"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2107.13904",
    "githubLink": "https://github.com/junwoopark92/mimic-sparql",
    "itemCount": "10,000 questions (derived from MIMIC-SQL)",
    "specs": "Knowledge Graph; SPARQL",
    "description": "A knowledge graph-based question answering dataset for EHRs. It is a counterpart to MIMIC-SQL, where the relational database is converted into a directed acyclic graph, and questions are mapped to SPARQL queries."
  },
  {
    "id": "saved-1769608044998-eubat",
    "title": "fastMRI+",
    "source": "arXiv",
    "authors": [
      "Ruiya Zhao",
      "Malcolm J. Gantenbein",
      "Kezhou Shi",
      "Erich Kobler",
      "Daniel Sodickson"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2109.03612",
    "githubLink": "https://github.com/microsoft/fastmri-plus",
    "itemCount": "23,724 bounding box annotations",
    "specs": "Bounding box annotations, study-level labels",
    "description": "An extension of the fastMRI dataset containing clinical pathology annotations (bounding boxes and labels) for knee and brain data."
  },
  {
    "id": "saved-1769608044998-q92pc",
    "title": "SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical Visual Question Answering",
    "source": "arXiv",
    "authors": [
      "Bo Liu",
      "Li-Ming Zhan",
      "Li Xu",
      "Lin Ma",
      "Yan Yang",
      "Xiao-Ming Wu"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2102.09542",
    "githubLink": "https://github.com/BoKelvin/SLAKE",
    "itemCount": "642 images, 14,028 QA pairs",
    "specs": "Bilingual (English/Chinese), Modalities: Image (CT, MRI, X-Ray) and Text. Formats: JSON, JPG. Includes semantic segmentation masks and bounding boxes.",
    "description": "A large-scale bilingual (English and Chinese) benchmark dataset for Medical Visual Question Answering (Med-VQA). It contains radiology images (CT, MRI, X-Ray) with comprehensive semantic annotations (masks, bounding boxes) and a structured medical knowledge graph to facilitate both visual and knowledge-based reasoning."
  },
  {
    "id": "saved-1769608044998-tw6np",
    "title": "C-SLAKE (Consistent SLAKE)",
    "source": "Other",
    "authors": [
      "OpenMICG"
    ],
    "year": "2021",
    "paperLink": "https://github.com/OpenMICG/CSLAKE",
    "githubLink": "https://github.com/OpenMICG/CSLAKE",
    "itemCount": "Derived from SLAKE (approx. same scale/subset)",
    "specs": "Modalities: Image (CT, MRI, X-Ray) and Text. Focused on consistency assessment.",
    "description": "An extension of the SLAKE dataset designed to assess the consistency of Med-VQA models. It encompasses a diverse array of medical materials including CT, MRI, and X-Ray images, focusing on testing model robustness and consistency."
  },
  {
    "id": "saved-1769608044998-6l1vk",
    "title": "DeepViral",
    "source": "Scholar",
    "authors": [
      "Wang Liu-Wei",
      "Robert Hoehndorf",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://doi.org/10.1093/bioinformatics/btab147",
    "githubLink": "https://github.com/bio-ontology-research-group/DeepViral",
    "itemCount": "24,678 positive interactions; 1,066 viral proteins",
    "specs": "Protein sequences, phenotype ontology embeddings, functional annotations",
    "description": "A deep learning method and dataset for predicting novel virus–host interactions (protein-protein interactions). It leverages protein sequences and infectious disease phenotypes embedded in a shared space."
  },
  {
    "id": "saved-1769608044998-p5dho",
    "title": "PANC (PAN12 + ChatCoder2)",
    "source": "arXiv",
    "authors": [
      "Matthias Vogt",
      "Ulf Leser",
      "Alan Akbik"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.387/",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "Combined from PAN12 and ChatCoder2 sources",
    "specs": "Text (Chat Streams), CSV/Datapack",
    "description": "A combined dataset designed for the task of Early Sexual Predator Detection (eSPD). It merges PAN12 non-grooming chats with ChatCoder2 grooming chats to create a realistic stream of messages for real-time detection analysis, addressing the limitations of PAN12's segmented format."
  },
  {
    "id": "saved-1769608044998-1tzws",
    "title": "P-Stance",
    "source": "Scholar",
    "authors": [
      "Yingjie Li",
      "Tiberiu Sosea",
      "Aditya Sawant",
      "Ajith Jayaraman Nair",
      "Diana Inkpen",
      "Cornelia Caragea"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.findings-acl.208/",
    "githubLink": "https://github.com/chuchun8/PStance",
    "itemCount": "21,574 tweets",
    "specs": "English Tweets, Stance Labels (Favor/Against/Neutral)",
    "description": "A large-scale stance detection dataset in the political domain, specifically focused on the 2020 U.S. Presidential election candidates (Trump, Biden, Sanders)."
  },
  {
    "id": "saved-1769608044998-rx21m",
    "title": "ETHICS",
    "source": "Hugging Face",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart",
      "Andrew Critch",
      "Jerry Li",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2008.02275",
    "githubLink": "https://github.com/hendrycks/ethics",
    "itemCount": "~130,000 examples",
    "specs": "Text (English); Binary classification",
    "description": "A benchmark that assesses a language model's knowledge of basic concepts of morality. It spans concepts in justice, well-being, duties, virtues, and commonsense morality, requiring models to predict moral judgments about diverse text scenarios."
  },
  {
    "id": "saved-1769608044998-0medi",
    "title": "Swiss-Judgment-Prediction (SJP)",
    "source": "Hugging Face",
    "authors": [
      "Joel Niklaus",
      "Ilias Chalkidis",
      "Matthias Stürmer"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.00806",
    "githubLink": "https://github.com/rcds/swiss_judgment_prediction",
    "itemCount": "85,000 cases",
    "specs": "Text (German, French, Italian); Binary classification",
    "description": "A multilingual, diachronic dataset of 85K Swiss Federal Supreme Court cases annotated with the respective judgment outcome (approval/dismissal). It covers cases in German, French, and Italian."
  },
  {
    "id": "saved-1769608044998-irt9p",
    "title": "ILDC (Indian Legal Documents Corpus)",
    "source": "arXiv",
    "authors": [
      "Vijit Malik",
      "Rishabh Sanjay",
      "Shubham Kumar Nigam",
      "Kripabandhu Ghosh",
      "Shouvik Kumar Guha",
      "Arnab Bhattacharya",
      "Ashutosh Modi"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.313/",
    "githubLink": "https://github.com/Exploration-Lab/CJPE",
    "itemCount": "35,000 cases",
    "specs": "Text (English); Judgment prediction, Explanation generation",
    "description": "A corpus of Indian Supreme Court cases annotated with court decisions and explanations. It introduces the task of Court Judgment Prediction and Explanation (CJPE) for the Indian legal system."
  },
  {
    "id": "saved-1769608044998-zop0d",
    "title": "JUSTICE",
    "source": "arXiv",
    "authors": [
      "Mohammad Alali",
      "Shaayan Syed",
      "Mohammed Alsayed",
      "Smit Patel",
      "Hemanth Bodala"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2112.03414",
    "githubLink": "https://github.com/MohammadAlali/JUSTICE",
    "itemCount": "~4,500 cases",
    "specs": "Text (English); Judgment prediction",
    "description": "A benchmark dataset for predicting the judgment of the Supreme Court of the United States (SCOTUS). It aims to help models identify patterns influencing court decisions and predict outcomes based on case facts."
  },
  {
    "id": "saved-1769608044998-1d9wi",
    "title": "PsyQA",
    "source": "Semantic Scholar",
    "authors": [
      "Hao Sun",
      "Zhenru Lin",
      "Chuanqi Tan",
      "Xiaancheng Zheng",
      "Ruobing Xie",
      "Jianan Wang",
      "Yanwen Wu",
      "Zhiguo Gong",
      "Wei Wang",
      "Mengyue Wu"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.acl-long.118/",
    "githubLink": "https://github.com/qiuhuachuan/PsyDial",
    "itemCount": "22,000 questions, 56,000 answers",
    "specs": "Text (Chinese, QA pairs)",
    "description": "A Chinese dataset of psychological health support in the form of question-and-answer pairs, crawled from a mental health service platform, with a portion annotated for support strategies."
  },
  {
    "id": "saved-1769608044998-7atjb",
    "title": "FFA-IR",
    "source": "arXiv",
    "authors": [
      "Mingjie Li",
      "Wenjia Cai",
      "Rui Liu",
      "Yuetian Weng",
      "Xiaoyun Zhao",
      "Cong Wang",
      "Xin Chen",
      "Zhong Liu",
      "Caineng Pan",
      "Mengke Li",
      "Yingfeng Zheng",
      "Yizhi Liu",
      "Flora D. Salim",
      "Karin Verspoor",
      "Xiaodan Liang",
      "Xiaojun Chang"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2108.02023",
    "githubLink": "https://github.com/mlii0117/FFA-IR",
    "itemCount": "1,048,584 images, 10,790 reports",
    "specs": "Fundus Fluorescein Angiography images, Bilingual reports (En/Zh), Lesion annotations",
    "description": "A large-scale benchmark for explainable and reliable medical report generation based on Fundus Fluorescein Angiography (FFA) images, featuring bilingual (Chinese/English) reports."
  },
  {
    "id": "saved-1769608044998-xgncu",
    "title": "GovReport",
    "source": "Hugging Face",
    "authors": [
      "Luyang Huang",
      "Shuyang Cao",
      "Nikolaus Parulian",
      "Heng Ji",
      "Lu Wang"
    ],
    "year": "2021",
    "paperLink": "https://aclanthology.org/2021.naacl-main.112/",
    "githubLink": "https://github.com/luyang-huang96/LongDocSum",
    "itemCount": "19,466 reports",
    "specs": "Text (Government Reports)",
    "description": "A large-scale dataset for long document summarization containing reports from the U.S. Government Accountability Office (GAO) and Congressional Research Service (CRS). The dataset features significantly longer documents and summaries compared to previous benchmarks."
  },
  {
    "id": "saved-1769608044998-02wv1",
    "title": "GSM8K (Grade School Math 8K)",
    "source": "arXiv",
    "authors": [
      "Karl Cobbe",
      "Vineet Kosaraju",
      "Mohammad Bavarian",
      "Mark Chen",
      "Heewoo Jun",
      "Lukasz Kaiser",
      "Matthias Plappert",
      "Jerry Tworek",
      "Jacob Hilton",
      "Reiichiro Nakano",
      "Christopher Hesse",
      "John Schulman"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2110.14168",
    "githubLink": "https://github.com/openai/grade-school-math",
    "itemCount": "~8.5k problems",
    "specs": "Text (Math Word Problems)",
    "description": "A dataset of high-quality linguistically diverse grade school math word problems. It is designed to support the task of question answering on basic mathematical problems that require multi-step reasoning."
  },
  {
    "id": "saved-1769608044998-kvfth",
    "title": "XTREME-R",
    "source": "arXiv",
    "authors": [
      "Sebastian Ruder",
      "Noah Constant",
      "Jan A. Botha",
      "Aditya Siddhant",
      "Orhan Firat",
      "Jinlan Fu",
      "Pengfei Liu",
      "Junjie Hu",
      "Dan Garrette",
      "Graham Neubig",
      "Melvin Johnson"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2104.07412",
    "githubLink": "https://github.com/google-research/xtreme",
    "itemCount": "10 tasks, 50 languages",
    "specs": "Text; Tasks include reasoning, retrieval, and QA",
    "description": "An improved version of the XTREME benchmark that covers 50 typologically diverse languages and 10 challenging tasks, including language-agnostic retrieval."
  },
  {
    "id": "saved-1769608044998-l34t8",
    "title": "AdvGLUE",
    "source": "Hugging Face",
    "authors": [
      "Boxin Wang",
      "Chejian Xu",
      "Shuohang Wang",
      "Zhe Gan",
      "Yu Cheng",
      "Jianfeng Gao",
      "Ahmed Hassan Awadallah",
      "Bo Li"
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2111.02840",
    "githubLink": "https://github.com/ai-secure/adv_glue",
    "itemCount": "~4,000 adversarial examples (across dev/test sets)",
    "specs": "Text (Natural Language Understanding); Word/Sentence-level perturbations",
    "description": "A multi-task benchmark for evaluating the robustness of language models. It applies 14 textual adversarial attack methods to GLUE tasks (like SST-2, MNLI, QNLI) to create valid, human-verified adversarial examples."
  },
  {
    "id": "saved-1769608044998-38qeu",
    "title": "HumanEval",
    "source": "arXiv",
    "authors": [
      "Mark Chen",
      "Jerry Tworek",
      "Heewoo Jun",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2107.03374",
    "githubLink": "https://github.com/openai/human-eval",
    "itemCount": "164 problems",
    "specs": "Python functions with docstrings and unit tests",
    "description": "A benchmark for evaluating the functional correctness of code generated by Large Language Models. It consists of programming problems that include a function signature, docstring, body, and unit tests."
  },
  {
    "id": "saved-1769608044998-kl9ej",
    "title": "MBPP (Mostly Basic Programming Problems)",
    "source": "arXiv",
    "authors": [
      "Jacob Austin",
      "Augustus Odena",
      "Maxwell Nye",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2108.07732",
    "githubLink": "https://github.com/google-research/mbpp",
    "itemCount": "~974 problems",
    "specs": "Python code, natural language descriptions, test cases",
    "description": "A dataset containing entry-level programming problems, designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases."
  },
  {
    "id": "saved-1769608044998-731o5",
    "title": "CodeXGLUE",
    "source": "arXiv",
    "authors": [
      "Shuai Lu",
      "Daya Guo",
      "Shuo Ren",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2102.04664",
    "githubLink": "https://github.com/microsoft/CodeXGLUE",
    "itemCount": "14 datasets (variable sizes)",
    "specs": "Multilingual (Python, Java, etc.), diverse tasks",
    "description": "A benchmark dataset for code understanding and generation. It includes a collection of 14 datasets for 10 diversified code intelligence tasks, including code completion, translation, and refinement."
  },
  {
    "id": "saved-1769608044998-gir4c",
    "title": "APPS (Automated Programming Progress Standard)",
    "source": "arXiv",
    "authors": [
      "Dan Hendrycks",
      "Steven Basart",
      "Saurav Kadavath",
      "et al."
    ],
    "year": "2021",
    "paperLink": "https://arxiv.org/abs/2105.09938",
    "githubLink": "https://github.com/hendrycks/apps",
    "itemCount": "10,000 problems",
    "specs": "Python, natural language specifications, test cases",
    "description": "A benchmark for code generation with problems ranging from simple one-line solutions to substantial algorithmic challenges, similar to those found on coding competition websites."
  },
  {
    "id": "saved-1769608044998-62ki6",
    "title": "Hanabi Open Agent Dataset (HOAD)",
    "source": "Scholar",
    "authors": [
      "Aron Sarmasi",
      "Timothy Zhang",
      "Graham Todd",
      "Sam Earle",
      "Julian Togelius",
      "Ahmed Khalifa"
    ],
    "year": "2021",
    "paperLink": "https://dl.acm.org/doi/10.5555/3463952.3464177",
    "githubLink": "https://github.com/aronsar/hoad",
    "itemCount": "500,000+ games per agent",
    "specs": "Game logs, agent policies, cross-play matrices",
    "description": "A comprehensive collection of existing Hanabi playing agents ported to the Hanabi Learning Environment (HLE), along with cross-play performance data. It is designed to support research in ad-hoc teaming and human-AI cooperation in imperfect information games."
  },
  {
    "id": "saved-1769608044998-4eqb9",
    "title": "Olympus",
    "source": "Scholar",
    "authors": [
      "Florian Häse",
      "Matteo Aldeghi",
      "Riley J. Hickman",
      "Alán Aspuru-Guzik"
    ],
    "year": "2021",
    "paperLink": "https://doi.org/10.1088/2632-2153/ac020d",
    "githubLink": "https://github.com/aspuru-guzik-group/olympus",
    "itemCount": "33 benchmark datasets",
    "specs": "Python package; Single and multi-objective optimization; Mixed parameter spaces (continuous/categorical)",
    "description": "A benchmarking framework for noisy optimization and experiment planning in chemistry and materials science. It provides a consistent interface to evaluate optimization algorithms against realistic experimental emulators and datasets."
  },
  {
    "id": "saved-1769608044998-gocel",
    "title": "Summit: Benchmarking Machine Learning Methods for Reaction Optimisation",
    "source": "Scholar",
    "authors": [
      "Kobi C. Felton",
      "Jan G. Rittig",
      "Alexei A. Lapkin"
    ],
    "year": "2021",
    "paperLink": "https://doi.org/10.1002/cmtd.202000051",
    "githubLink": "https://github.com/sustainable-processes/summit",
    "itemCount": "2 primary reaction benchmarks",
    "specs": "Python package; Reaction optimization simulations; continuous and categorical variables",
    "description": "A benchmark suite for reaction optimization algorithms. It includes two in-silico benchmarks based on real chemical reactions (nucleophilic aromatic substitution and C-N cross-coupling) to compare machine learning strategies like Bayesian optimization."
  },
  {
    "id": "saved-1769608044998-51pcn",
    "title": "PlotQA",
    "source": "arXiv",
    "authors": [
      "Nitesh Methani",
      "Pritha Ganguly",
      "Mitesh M. Khapra",
      "Pratyush Kumar"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/1909.00931",
    "githubLink": "https://github.com/NiteshMethani/PlotQA",
    "itemCount": "224k Plots, 28M QA pairs",
    "specs": "Images (Plots), Text (QA pairs)",
    "description": "A large-scale dataset for reasoning over scientific plots. It is designed to test models on complex reasoning tasks involving data extraction and mathematical operations over plot data."
  },
  {
    "id": "saved-1769608044998-4wozz",
    "title": "BLURB",
    "source": "arXiv",
    "authors": [
      "Yu Gu",
      "Robert Tinn",
      "Hao Cheng",
      "Michael Lucas",
      "Naoto Usuyama",
      "Xiaodong Liu",
      "Tristan Naumann",
      "Jianfeng Gao",
      "Hoifung Poon"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2007.15779",
    "githubLink": "https://microsoft.github.io/BLURB/",
    "itemCount": "13 datasets (NER, PICO, Relation Extraction, etc.)",
    "specs": "Text (Various NLP tasks including NER, RE, Sentence Similarity)",
    "description": "The Biomedical Language Understanding and Reasoning Benchmark is a collection of resources for biomedical NLP. It comprises 13 publicly available datasets across 6 diverse tasks, focusing on PubMed-based applications."
  },
  {
    "id": "saved-1769608044998-5yv6p",
    "title": "RealToxicityPrompts",
    "source": "arXiv",
    "authors": [
      "Samuel Gehman",
      "Suchin Gururangan",
      "Maarten Sap",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.06367",
    "githubLink": "https://github.com/allenai/real-toxicity-prompts",
    "itemCount": "100,000+ prompts",
    "specs": "Text prompts (English), JSONL format",
    "description": "A dataset of naturally occurring sentence-level prompts derived from web text, paired with toxicity scores, designed to evaluate neural toxic degeneration in language models."
  },
  {
    "id": "saved-1769608044998-ey11q",
    "title": "Counsel Chat",
    "source": "Hugging Face",
    "authors": [
      "Nicolas Bertagnolli"
    ],
    "year": "2020",
    "paperLink": "https://towardsdatascience.com/counsel-chat-bootstrapping-high-quality-therapy-data-971b419f33da",
    "githubLink": "https://github.com/nbertagnolli/counsel-chat",
    "itemCount": "3,602 questions and answers (approx.)",
    "specs": "Text (English), Q&A pairs",
    "description": "A collection of high-quality counseling questions and answers scraped from an online counseling platform where verified therapists respond to user queries. It is widely used for training empathetic dialogue systems."
  },
  {
    "id": "saved-1769608044998-2ub4t",
    "title": "Big-Vul",
    "source": "Other",
    "authors": [
      "Jiahao Fan",
      "Yi Li",
      "Shaohua Wang",
      "Tien N. Nguyen"
    ],
    "year": "2020",
    "paperLink": "https://conf.researchr.org/details/msr-2020/msr-2020-technical-papers/11/A-C-C-Code-Vulnerability-Dataset-with-Code-Changes-and-CVE-Summaries",
    "githubLink": "https://github.com/ZeoVan/MSR_20_Code_vulnerability_CSV_Dataset",
    "itemCount": "217,007 rows",
    "specs": "C/C++ code, CSV format",
    "description": "A large-scale C/C++ code vulnerability dataset constructed from open-source GitHub projects. It includes code changes and CVE summaries to address issues like low volume and lack of real-world complexity in previous datasets."
  },
  {
    "id": "saved-1769608044998-vjx04",
    "title": "KorNLI and KorSTS",
    "source": "arXiv",
    "authors": [
      "Jiyeon Ham",
      "Yo Joong Choe",
      "Kyubyong Park",
      "Ilji Choi",
      "Hyungjoon Soh"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2004.03289",
    "githubLink": "https://github.com/kakaobrain/KorNLUDatasets",
    "itemCount": "KorNLI: ~570k pairs, KorSTS: 8,628 pairs",
    "specs": "Text, NLI, STS",
    "description": "Benchmark datasets for Korean Natural Language Inference (NLI) and Semantic Textual Similarity (STS). KorNLI includes machine-translated (from MNLI/SNLI) and human-translated (XNLI) data. KorSTS is a translation of the STS-B dataset."
  },
  {
    "id": "saved-1769608044998-3evdz",
    "title": "MMLU (Massive Multitask Language Understanding)",
    "source": "arXiv",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart",
      "Andy Zou",
      "Mantas Mazeika",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.03300",
    "githubLink": "https://github.com/hendrycks/test",
    "itemCount": "15,908 questions",
    "specs": "Multiple-choice (4 options), Text, 57 subjects",
    "description": "A massive benchmark that covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to a professional level, testing both world knowledge and problem-solving ability."
  },
  {
    "id": "saved-1769608044998-h6t81",
    "title": "ORNL Radiation Detection in Urban Environments Dataset",
    "source": "Scholar",
    "authors": [
      "James M. Ghawaly",
      "Andrew D. Nicholson",
      "Douglas E. Peplow",
      "Christine M. Anderson-Cook",
      "Kary L. Myers",
      "Daniel E. Archer",
      "Michael J. Willis",
      "Brian J. Quiter"
    ],
    "year": "2020",
    "paperLink": "https://doi.org/10.1038/s41597-020-00672-2",
    "githubLink": "https://github.com/ORNL/ARDIDSA-Data",
    "itemCount": "Thousands of synthetic list mode data files",
    "specs": "Synthetic signal data (List mode gamma-ray spectra); Modality: Sensor/Signal",
    "description": "A synthetic dataset representing the response of a mobile NaI(Tl) detector in a mid-sized urban city, created to test and train radiation detection and identification algorithms against dynamic backgrounds."
  },
  {
    "id": "saved-1769608044998-qmkt5",
    "title": "MIMIC-SQL",
    "source": "Scholar",
    "authors": [
      "Ping Wang",
      "Tian Shi",
      "Chandan K. Reddy"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.acl-main.123/",
    "githubLink": "https://github.com/wangpinggl/TREQS",
    "itemCount": "10,000 questions",
    "specs": "Structured Data (MIMIC-III); Text-to-SQL",
    "description": "A dataset for text-to-SQL generation on Electronic Medical Records. It consists of two subsets: machine-generated template questions and human-annotated natural language questions, based on MIMIC-III data."
  },
  {
    "id": "saved-1769608044998-0j9hw",
    "title": "MIMIC-IV (Lab Events)",
    "source": "Other",
    "authors": [
      "Alistair Johnson",
      "Lucas Bulgarelli",
      "Lu Shen",
      "Alvin Gayles",
      "Ayad Shammout",
      "Steven Horng",
      "Leo Anthony Celi",
      "Roger Mark"
    ],
    "year": "2020",
    "paperLink": "https://physionet.org/content/mimiciv/",
    "githubLink": "https://github.com/MIT-LCP/mimic-code",
    "itemCount": "Millions of events (covering 65k+ patients)",
    "specs": "Structured Tabular Data (CSV/Parquet)",
    "description": "A module within the massive MIMIC-IV critical care dataset containing laboratory measurements for patients. The 'labevents' table captures routine pathology results (e.g., blood work, chemistry) for hospital admissions, widely used for benchmarking clinical time-series prediction and phenotype classification."
  },
  {
    "id": "saved-1769608044998-vjpfy",
    "title": "VinDr-CXR",
    "source": "Other",
    "authors": [
      "Ha Q. Nguyen",
      "Khanh Lam",
      "Linh T. Le",
      "Hieu H. Pham",
      "Dat Q. Tran",
      "Dung B. Nguyen"
    ],
    "year": "2020",
    "paperLink": "https://physionet.org/content/vindr-cxr/1.0.0/",
    "githubLink": "https://github.com/vinbigdata-medical/vindr-cxr",
    "itemCount": "18,000 images",
    "specs": "DICOM images, bounding box annotations for 22 local findings, 6 global disease labels",
    "description": "An open dataset of chest X-rays with radiologist annotations for the classification of common thoracic diseases and localization of critical findings."
  },
  {
    "id": "saved-1769608044998-rwg4u",
    "title": "Object-CXR",
    "source": "Other",
    "authors": [
      "JF Healthcare",
      "MIDL Organizers"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.11597",
    "githubLink": "https://github.com/hlk-1135/object-CXR",
    "itemCount": "10,000 images",
    "specs": "5000 positive and 5000 negative samples, bounding boxes/masks",
    "description": "A benchmark dataset designed for the automatic detection of foreign objects in chest X-rays."
  },
  {
    "id": "saved-1769608044998-3yduk",
    "title": "PathVQA",
    "source": "arXiv",
    "authors": [
      "Xuehai He",
      "Pathwan Zhang",
      "Qian Wang",
      "Luntian Mou"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.10286",
    "githubLink": "https://github.com/flaviagiammarino/path-vqa",
    "itemCount": "32,799 QA pairs; 4,998 images",
    "specs": "Visual Question Answering (VQA), Text + Image",
    "description": "A dataset designed for medical visual question answering (VQA) in pathology. It contains question-answer pairs generated from pathology textbooks and digital libraries, including both open-ended and binary (yes/no) questions."
  },
  {
    "id": "saved-1769608044998-ta4qv",
    "title": "US-DermMCQA (Internal / Liu et al. 2020)",
    "source": "Other",
    "authors": [
      "Yuan Liu",
      "Aravind Klempner",
      "et al."
    ],
    "year": "2020",
    "paperLink": "https://www.nature.com/articles/s41591-020-0842-3",
    "githubLink": "https://github.com/google-health/dermatology-research",
    "itemCount": "1,996 cases (derived)",
    "specs": "Image + Text (Multiple Choice Questions)",
    "description": "A dermatology multiple-choice question assessment benchmark often referenced in Google/DeepMind research (e.g., MedGemma). It is an internal dataset derived from the work of Liu et al. (2020), consisting of de-identified teledermatology cases with 136 skin conditions transformed into 4-way MCQs."
  },
  {
    "id": "saved-1769608044999-hltwt",
    "title": "PAD-UFES-20",
    "source": "Other",
    "authors": [
      "André G. C. Pacheco",
      "Gustavo R. Lima",
      "Amanda S. Salomão",
      "Breno Krohling",
      "Igor P. Biral",
      "Gabriel G. de Angelo",
      "Felipe C. Alves Jr.",
      "José G. M. Esgario",
      "Alana C. Simora",
      "Pedro B. C. Castro",
      "Felipe B. Rodrigues",
      "Patricia H. L. Frasson",
      "Renato A. Krohling",
      "Helder Knidel",
      "Maria C. S. Santos",
      "Rachel B. Spirandelli",
      "Luíz F. S. de Barros"
    ],
    "year": "2020",
    "paperLink": "https://doi.org/10.1016/j.dib.2020.106221",
    "githubLink": "https://github.com/labcin-ufes/PAD-UFES-20",
    "itemCount": "2,298 images",
    "specs": "Images (PNG), Metadata (CSV). 6 diagnostic classes: Basal Cell Carcinoma (BCC), Squamous Cell Carcinoma (SCC), Actinic Keratosis (ACK), Seborrheic Keratosis (SEK), Melanoma (MEL), and Nevus (NEV). Metadata includes up to 26 features.",
    "description": "A skin lesion benchmark dataset composed of clinical images collected from smartphones and patient clinical data. Unlike many other datasets that focus on dermoscopy, this dataset provides standard camera images which are more representative of a primary care or teledermatology setting. It includes rich metadata such as patient age, gender, body region, and skin cancer history."
  },
  {
    "id": "saved-1769608044999-5yfwy",
    "title": "SOREL-20M (Sophos-ReversingLabs 20 Million)",
    "source": "arXiv",
    "authors": [
      "Richard Harang",
      "Ethan M. Rudd"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2012.07634",
    "githubLink": "https://github.com/sophos-ai/SOREL-20M",
    "itemCount": "20 million samples (10M benign, 10M malicious)",
    "specs": "EMBERv2 feature format, SQLite metadata, raw PE binaries (disarmed)",
    "description": "A massive scale dataset for malware detection consisting of nearly 20 million files. It includes pre-extracted features and metadata for both benign and malicious PE files, aiming to improve the scalability and robustness of malware classifiers."
  },
  {
    "id": "saved-1769608044999-5c5my",
    "title": "ISIC 2020 Challenge Dataset (SIIM-ISIC Melanoma Classification)",
    "source": "Semantic Scholar",
    "authors": [
      "Veronica Rotemberg",
      "Nicholas Kurtansky",
      "Betina Betz-Stablein",
      "et al."
    ],
    "year": "2020",
    "paperLink": "https://doi.org/10.34970/2020-ds01",
    "githubLink": "https://github.com/ISIC-Archive",
    "itemCount": "33,126 images",
    "specs": "DICOM/JPEG images, CSV metadata (patient ID, anatomical site, diagnosis)",
    "description": "A large collection of dermoscopic images of skin lesions, used to benchmark algorithms for melanoma detection. The dataset includes a significant number of benign lesions to challenge models in distinguishing them from malignant cases."
  },
  {
    "id": "saved-1769608044999-3yl88",
    "title": "X-Stance",
    "source": "arXiv",
    "authors": [
      "Jannis Vamvas",
      "Rico Sennrich"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.08385",
    "githubLink": "https://github.com/ZurichNLP/xstance",
    "itemCount": "67,000 comments",
    "specs": "Multilingual Text (DE, FR, IT), 150+ Political Targets",
    "description": "A multilingual, multi-target stance detection dataset extracted from a Swiss voting advice platform, covering German, French, and Italian comments on various political issues."
  },
  {
    "id": "saved-1769608044999-knj96",
    "title": "DeepMind Specification Gaming Examples",
    "source": "Scholar",
    "authors": [
      "Victoria Krakovna",
      "Jonathan Uesato",
      "Vladimir Mikulik",
      "Matthew Rahtz",
      "Tom Everitt",
      "Ramana Kumar",
      "Zac Kenton",
      "Jan Leike",
      "Shane Legg"
    ],
    "year": "2020",
    "paperLink": "https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/",
    "githubLink": "https://docs.google.com/spreadsheets/d/1_s4cW-zYvI5fYPbArvQvcI4SA0UaT8s6DENpNB_o_gM",
    "itemCount": "~60+ documented examples",
    "specs": "List of environments (Atari, MuJoCo, etc.) and video/descriptions of failures.",
    "description": "A curated list and collection of examples demonstrating specification gaming (reward hacking) across various AI environments. While not a single downloadable dataset in the traditional sense, it serves as a foundational reference and collection of environments (like CoastRunners, Traffic, etc.) where reward hacking has been documented."
  },
  {
    "id": "saved-1769608044999-4iq2e",
    "title": "Social Chemistry 101",
    "source": "Semantic Scholar",
    "authors": [
      "Maxwell Forbes",
      "Jena D. Hwang",
      "Vered Shwartz",
      "Maarten Sap",
      "Yejin Choi"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.emnlp-main.65/",
    "githubLink": "https://github.com/mbforbes/social-chemistry-101",
    "itemCount": "292,000 rules-of-thumb, 4.5M annotations",
    "specs": "Text (English); Structure (Situation, Rule, Judgment)",
    "description": "A large-scale dataset for reasoning about social and moral norms. It contains rules-of-thumb, situations, and judgment attributes to train models in understanding social norms and moral judgments."
  },
  {
    "id": "saved-1769608044999-1e21p",
    "title": "RealToxicityPrompts",
    "source": "arXiv",
    "authors": [
      "Samuel Gehman",
      "Suchin Gururangan",
      "Maarten Sap",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2009.11462",
    "githubLink": "https://github.com/allenai/real-toxicity-prompts",
    "itemCount": "100,000 prompts",
    "specs": "Text-only; Prompts with toxicity scores (using Perspective API)",
    "description": "A large-scale dataset of prompts designed to evaluate the risk of neural toxic degeneration in language models. It pairs prompts with toxicity scores to measure how often models generate toxic continuations."
  },
  {
    "id": "saved-1769608044999-wiqd9",
    "title": "FNS 2020 (Financial Narrative Summarisation)",
    "source": "Other",
    "authors": [
      "Mahmoud El-Haj",
      "Ahmed AbuRa'ed",
      "Marina Litvak",
      "Natalia Vanetik",
      "George Giannakopoulos"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.fnp-1.1/",
    "githubLink": "https://github.com/ignatiuszek/FNS-2020",
    "itemCount": "3,863 annual reports",
    "specs": "Text (Financial Annual Reports)",
    "description": "A dataset created for the Financial Narrative Summarisation shared task. It contains UK annual reports published in PDF format, focusing on summarizing the narrative sections of financial documents."
  },
  {
    "id": "saved-1769608044999-911y9",
    "title": "ERASER (Evaluating Rationales And Simple English Reasoning)",
    "source": "Scholar",
    "authors": [
      "Jay DeYoung",
      "Sarthak Jain",
      "Nazneen Fatema Rajani",
      "Eric Lehman",
      "Caiming Xiong",
      "Richard Socher",
      "Byron C. Wallace"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.acl-main.408/",
    "githubLink": "http://www.eraserbenchmark.com/",
    "itemCount": "7 datasets (Movies, FEVER, MultiRC, etc.)",
    "specs": "Text (NLP); Rationales (span selection)",
    "description": "A benchmark to evaluate rationalized NLP models. It unifies multiple datasets where the task is to predict a label and provide a 'rationale' (supporting evidence) from the input text. It includes metrics for measuring how well model rationales align with human annotations."
  },
  {
    "id": "saved-1769608044999-dmn1q",
    "title": "LogiQA",
    "source": "arXiv",
    "authors": [
      "Jian Liu",
      "Leyang Cui",
      "Hanmeng Liu",
      "Dandan Huang",
      "Yile Wang",
      "Yue Zhang"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2007.08124",
    "githubLink": "https://github.com/lgw863/LogiQA-dataset",
    "itemCount": "8,678 QA instances",
    "specs": "Text (Reading Comprehension)",
    "description": "A challenge dataset for machine reading comprehension with logical reasoning. The questions are sourced from the Chinese Civil Service Examination and cover various types of logical reasoning."
  },
  {
    "id": "saved-1769608044999-jx6tg",
    "title": "XTREME",
    "source": "arXiv",
    "authors": [
      "Junjie Hu",
      "Sebastian Ruder",
      "Aditya Siddhant",
      "Graham Neubig",
      "Orhan Firat",
      "Melvin Johnson"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.11080",
    "githubLink": "https://github.com/google-research/xtreme",
    "itemCount": "9 tasks, 40 languages",
    "specs": "Text; Tasks include classification, structured prediction, QA, and retrieval",
    "description": "A massively multilingual multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks."
  },
  {
    "id": "saved-1769608044999-nmy4i",
    "title": "XGLUE",
    "source": "arXiv",
    "authors": [
      "Yaobo Liang",
      "Nan Duan",
      "Yeyun Gong",
      "Ning Wu",
      "Fenfei Guo",
      "Weizhen Qi",
      "Ming Gong",
      "Linjun Shou",
      "Daxin Jiang",
      "Guihong Cao",
      "Xiaodong Fan",
      "Ruofei Zhang",
      "Rahul Agrawal",
      "Edward Cui",
      "Sining Wei",
      "Taroon Bharti",
      "Ying Qiao",
      "Jiun-Hung Chen",
      "Winnie Wu",
      "Shuguang Liu",
      "Fan Yang",
      "Daniel Campos",
      "Rangan Majumder",
      "Ming Zhou"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2004.01401",
    "githubLink": "https://github.com/microsoft/XGLUE",
    "itemCount": "11 tasks, 19 languages",
    "specs": "Text; Tasks include NER, POS, QA, News Classification, Query-Ad Matching",
    "description": "A benchmark dataset for evaluating cross-lingual pre-trained models across 11 tasks and 19 languages, covering both natural language understanding and generation."
  },
  {
    "id": "saved-1769608044999-r86hc",
    "title": "TyDi QA",
    "source": "arXiv",
    "authors": [
      "Jonathan H. Clark",
      "Eunsol Choi",
      "Michael Collins",
      "Dan Garrette",
      "Tom Kwiatkowski",
      "Vitaly Nikolaev",
      "Jennimaria Palomaki"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2003.05002",
    "githubLink": "https://github.com/google-research-datasets/tydiqa",
    "itemCount": "204,000 QA pairs, 11 languages",
    "specs": "Text; Question Answering (Gold Passage, Primary Task)",
    "description": "A question answering benchmark covering 11 typologically diverse languages with 204K question-answer pairs, designed to avoid translation-based artifacts."
  },
  {
    "id": "saved-1769608044999-2xisx",
    "title": "CoVoST 2",
    "source": "arXiv",
    "authors": [
      "Changhan Wang",
      "Anne Wu",
      "Juan Pino"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2007.10310",
    "githubLink": "https://github.com/facebookresearch/covost",
    "itemCount": "2,880 hours of speech, 21 languages",
    "specs": "Audio/Speech; Speech Translation",
    "description": "A large-scale multilingual speech-to-text translation corpus covering 21 languages into English and English into 15 languages."
  },
  {
    "id": "saved-1769608044999-rx9u1",
    "title": "CrowS-Pairs",
    "source": "Semantic Scholar",
    "authors": [
      "Nikita Nangia",
      "Clara Vania",
      "Rasika Bhalerao",
      "Samuel R. Bowman"
    ],
    "year": "2020",
    "paperLink": "https://aclanthology.org/2020.emnlp-main.154/",
    "githubLink": "https://github.com/nyu-mll/crows-pairs",
    "itemCount": "1,508 sentence pairs",
    "specs": "Text; 9 bias categories (race, gender, religion, etc.)",
    "description": "A challenge dataset for measuring social bias in masked language models. It uses minimal pairs of sentences to test for stereotypical biases."
  },
  {
    "id": "saved-1769608044999-k9lc4",
    "title": "RobustBench",
    "source": "arXiv",
    "authors": [
      "Francesco Croce",
      "Maksym Andriushchenko",
      "Vikash Sehwag",
      "Edoardo Debenedetti",
      "Nicolas Flammarion",
      "Mihailo Chiang",
      "Pratyush Maini",
      "Tolga Ergen",
      "Nicolas Papernot",
      "Matthias Hein"
    ],
    "year": "2020",
    "paperLink": "https://arxiv.org/abs/2010.09670",
    "githubLink": "https://github.com/RobustBench/robustbench",
    "itemCount": "Tracks 120+ models; uses full CIFAR/ImageNet test sets",
    "specs": "Image classification; L_inf, L_2 threats; Corruptions",
    "description": "A standardized benchmark for adversarial robustness in image classification. It tracks the progress of robust models on CIFAR-10, CIFAR-100, and ImageNet using a unified evaluation protocol (AutoAttack) to prevent overestimated robustness."
  },
  {
    "id": "saved-1769608044999-buj4o",
    "title": "BLUE",
    "source": "Scholar",
    "authors": [
      "Yifan Peng",
      "Shankai Yan",
      "Zhiyong Lu"
    ],
    "year": "2019",
    "paperLink": "https://doi.org/10.18653/v1/W19-5044",
    "githubLink": "https://github.com/ncbi-nlp/BLUE_Benchmark",
    "itemCount": "10 datasets",
    "specs": "Text (NER, Relation Extraction, Sentence Similarity, Inference)",
    "description": "The Biomedical Language Understanding Evaluation benchmark consists of five different biomedicine text-mining tasks with ten corpora, covering both biomedical literature and clinical notes."
  },
  {
    "id": "saved-1769608044999-apmfd",
    "title": "Devign",
    "source": "arXiv",
    "authors": [
      "Yaqin Zhou",
      "Shangqing Liu",
      "Jingkai Siow",
      "Xiaoning Du",
      "Yang Liu"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1909.03496",
    "githubLink": "https://github.com/epicosy/devign",
    "itemCount": "Approx. 27,000 functions (from 4 projects)",
    "specs": "C code, Graph-based representations",
    "description": "A graph neural network-based model and dataset for vulnerability identification. The dataset is manually labeled and built from four large-scale open-source C projects: Linux, FFmpeg, Qemu, and Wireshark."
  },
  {
    "id": "saved-1769608044999-wdqnz",
    "title": "KorQuAD 1.0 (Korean Question Answering Dataset)",
    "source": "arXiv",
    "authors": [
      "Seungyoung Lim",
      "Myungji Kim",
      "Jooyoul Lee"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1909.07005",
    "githubLink": "https://korquad.github.io/",
    "itemCount": "70,000+ pairs (10,165 validation)",
    "specs": "Text, Machine Reading Comprehension (QA)",
    "description": "A large-scale Korean dataset for extractive machine reading comprehension, derived from Wikipedia. It is designed to evaluate the reading comprehension capabilities of Korean language models, similar to SQuAD for English."
  },
  {
    "id": "saved-1769608044999-v6t4c",
    "title": "ARC-AGI (Abstraction and Reasoning Corpus)",
    "source": "arXiv",
    "authors": [
      "François Chollet"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.01547",
    "githubLink": "https://github.com/fchollet/ARC",
    "itemCount": "1,000 tasks (400 training, 400 evaluation, 200 private test)",
    "specs": "JSON format containing 2D grid pairs (integers 0-9 representing colors).",
    "description": "A benchmark measuring fluid intelligence and skill acquisition efficiency. It consists of unique training and evaluation tasks that require solving novel grid-based logic puzzles with minimal prior knowledge, aiming to test 'General Intelligence' rather than memorization."
  },
  {
    "id": "saved-1769608044999-ng3p1",
    "title": "Miami University Deception Detection Database (MU3D)",
    "source": "Other",
    "authors": [
      "E. Paige Lloyd",
      "Jason C. Deska",
      "Kurt Hugenberg",
      "Allen R. McConnell",
      "Katherine T. Humphrey",
      "Jonathan W. Kunstman"
    ],
    "year": "2019",
    "paperLink": "https://link.springer.com/article/10.3758/s13428-018-1061-4",
    "githubLink": "http://hdl.handle.net/2374.MIA/6067",
    "itemCount": "320 videos",
    "specs": "Video, Audio, Transcripts; 80 unique targets",
    "description": "A free resource containing videos of diverse targets (Black/White, Male/Female) telling truths and lies about their social relationships. Designed to cross-examine race, gender, and deception."
  },
  {
    "id": "saved-1769608044999-4wl4s",
    "title": "Bag-of-Lies (BOL)",
    "source": "Other",
    "authors": [
      "Vipul Gupta",
      "Mridula Agarwal",
      "Manik Arora",
      "Tanmoy Chakraborty",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "year": "2019",
    "paperLink": "https://openaccess.thecvf.com/content_CVPRW_2019/papers/CV-COPS/Gupta_Bag-of-Lies_A_Multimodal_Dataset_for_Deception_Detection_CVPRW_2019_paper.pdf",
    "githubLink": "http://iab-rubric.org/resources/bag-of-lies",
    "itemCount": "325 annotated data points",
    "specs": "Video, Audio, EEG, Gaze data; 35 subjects",
    "description": "A multimodal dataset collected in a realistic scenario where subjects describe images. It is unique for including gaze data and EEG signals alongside standard audio-visual feeds."
  },
  {
    "id": "saved-1769608044999-gidil",
    "title": "Box of Lies",
    "source": "Other",
    "authors": [
      "Felix Soldner",
      "Verónica Pérez-Rosas",
      "Rada Mihalcea"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/N19-1175/",
    "githubLink": "https://github.com/lit-umich/BoxOfLies",
    "itemCount": "25 videos / ~1,049 utterances",
    "specs": "Video, Audio, Text (Dialogue); 26 unique participants",
    "description": "A multimodal dataset derived from the 'Box of Lies' game on 'The Tonight Show Starring Jimmy Fallon'. It features dialogue-heavy deception in a game-like setting."
  },
  {
    "id": "saved-1769608044999-oet4r",
    "title": "PIQA (Physical Interaction Question Answering)",
    "source": "arXiv",
    "authors": [
      "Yonatan Bisk",
      "Rowan Zellers",
      "Ronan Le Bras",
      "Jianfeng Gao",
      "Yejin Choi"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.11641",
    "githubLink": "https://github.com/ybisk/piqa",
    "itemCount": "21,035 QA pairs (16,113 Train, 1,838 Dev, 3,084 Test)",
    "specs": "Text-based multiple choice (2 options); Physical commonsense reasoning task",
    "description": "A benchmark dataset for evaluating physical commonsense reasoning in natural language. The task involves choosing the most plausible solution to a goal regarding everyday physical interactions from two multiple-choice options. It focuses on affordances and physical properties often omitted in text due to reporting bias."
  },
  {
    "id": "saved-1769608044999-u1kx6",
    "title": "MIMIC-CXR",
    "source": "Other",
    "authors": [
      "Alistair E. W. Johnson",
      "Tom J. Pollard",
      "Seth J. Berkowitz",
      "Nathaniel R. Greenbaum",
      "Matthew P. Lungren",
      "Chih-ying Deng",
      "Roger G. Mark",
      "Steven Horng"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1901.07042",
    "githubLink": "https://github.com/MIT-LCP/mimic-code",
    "itemCount": "377,110 images; 227,827 reports",
    "specs": "DICOM/JPG images, free-text radiology reports, 14 CheXpert-derived labels",
    "description": "A large publicly available dataset of chest radiographs with free-text radiology reports, de-identified and sourced from the Beth Israel Deaconess Medical Center."
  },
  {
    "id": "saved-1769608044999-z2i1j",
    "title": "CheXpert",
    "source": "Other",
    "authors": [
      "Jeremy Irvin",
      "Pranav Rajpurkar",
      "Michael Ko",
      "Yifan Yu",
      "Silviana Ciurea-Ilcus",
      "Chris Chute",
      "Henrik Marklund",
      "Behzad Haghgoo",
      "Robyn Ball",
      "Katie Shpanskaya"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1901.07031",
    "githubLink": "https://github.com/stanfordmlgroup/chexpert",
    "itemCount": "224,316 images",
    "specs": "Frontal and lateral chest radiographs, 14 observations with uncertainty labels",
    "description": "A large dataset of chest radiographs with uncertainty labels and radiologist-labeled reference standard evaluation sets, designed for automated interpretation."
  },
  {
    "id": "saved-1769608044999-xlhy3",
    "title": "PadChest",
    "source": "Other",
    "authors": [
      "Aurelia Bustos",
      "Antonio Pertusa",
      "Jose-Maria Salinas",
      "Maria de la Iglesia-Vaya"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1901.07441",
    "githubLink": "https://github.com/auriml/Rx-thorax-automatic-captioning",
    "itemCount": "160,868 images",
    "specs": "High-resolution X-ray images, Spanish radiology reports, 174 radiographic findings",
    "description": "A large-scale, high-resolution chest X-ray dataset from Spain with multi-label annotated reports, including hierarchical labels mapped to UMLS."
  },
  {
    "id": "saved-1769608044999-ejxnc",
    "title": "OASIS-3",
    "source": "Semantic Scholar",
    "authors": [
      "Pamela J. LaMontagne",
      "Tammie L. S. Benzinger",
      "John C. Morris",
      "Sarah Keefe"
    ],
    "year": "2019",
    "paperLink": "https://www.medrxiv.org/content/10.1101/2019.12.13.19014902v1",
    "githubLink": "https://www.oasis-brains.org/",
    "itemCount": "1,098 participants, >2,000 MR sessions",
    "specs": "T1w, T2w, FLAIR, ASL, SWI, BOLD, DTI, PET",
    "description": "A longitudinal neuroimaging, clinical, cognitive, and biomarker dataset for normal aging and Alzheimer's Disease."
  },
  {
    "id": "saved-1769608044999-ds1n2",
    "title": "HellaSwag",
    "source": "arXiv",
    "authors": [
      "Rowan Zellers",
      "Ari Holtzman",
      "Yonatan Bisk",
      "Ali Farhadi",
      "Yejin Choi"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1905.07830",
    "githubLink": "https://github.com/rowanzellers/hellaswag",
    "itemCount": "70,000+ samples",
    "specs": "Text, Multiple Choice, Commonsense NLI",
    "description": "An adversarial dataset for commonsense natural language inference where models must select the best ending to a context, designed to be hard for state-of-the-art models."
  },
  {
    "id": "saved-1769608044999-0fotx",
    "title": "VQA-Med 2019",
    "source": "Scholar",
    "authors": [
      "Asma Ben Abacha",
      "Sadid A. Hasan",
      "Vivek V. Datla",
      "Joey Liu",
      "Dina Demner-Fushman",
      "Henning Müller"
    ],
    "year": "2019",
    "paperLink": "https://ceur-ws.org/Vol-2380/paper_272.pdf",
    "githubLink": "https://github.com/abachaa/VQA-Med-2019",
    "itemCount": "4,200 images, 15,292 QA pairs",
    "specs": "Images (Radiology), Text (QA pairs)",
    "description": "A dataset created for the ImageCLEF 2019 VQA-Med task. It focuses on four categories of questions: Modality, Plane, Organ System, and Abnormality, derived from radiology images."
  },
  {
    "id": "saved-1769608044999-3r27p",
    "title": "EyeQ (Eye Quality Assessment Dataset)",
    "source": "arXiv",
    "authors": [
      "Huazhu Fu",
      "Boyang Wang",
      "Jianbing Shen",
      "Shanshan Cui",
      "Yanwu Xu",
      "Jiang Liu",
      "Ling Shao"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1907.05345",
    "githubLink": "https://github.com/HzFu/EyeQ",
    "itemCount": "28,792 images",
    "specs": "JPEG images, 3-level quality labels (Good, Usable, Reject)",
    "description": "A re-annotated subset of the EyePACS dataset focused on retinal image quality assessment. It provides a three-level quality grading system (Good, Usable, Reject) to help develop robust quality control algorithms for medical imaging."
  },
  {
    "id": "saved-1769608044999-kqge8",
    "title": "TAPE (Tasks Assessing Protein Embeddings)",
    "source": "arXiv",
    "authors": [
      "Roshan Rao",
      "Nicholas Bhattacharya",
      "Neil Thomas",
      "Yan Duan",
      "Xi Chen",
      "John Canny",
      "Pieter Abbeel",
      "Yun S. Song"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1906.08230",
    "githubLink": "https://github.com/songlab-cal/tape",
    "itemCount": "5 supervised tasks (plus Pfam pretraining)",
    "specs": "Protein sequences (text), structure prediction, classification, regression",
    "description": "A set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology (structure prediction, remote homology, protein engineering) to evaluate protein embeddings."
  },
  {
    "id": "saved-1769608044999-kcdmr",
    "title": "ViraMiner",
    "source": "Other",
    "authors": [
      "Ardi Tampuu",
      "Zurab Bzhalava",
      "Joakim Dillner",
      "Raul Vicente"
    ],
    "year": "2019",
    "paperLink": "https://doi.org/10.1371/journal.pone.0222271",
    "githubLink": "https://github.com/NeuroCSUT/ViraMiner",
    "itemCount": "Sequences from 19 metagenomic experiments",
    "specs": "Raw DNA sequences (300 bp contigs), labeled by BLAST",
    "description": "A deep learning-based benchmark for identifying viral genomes in human samples. It contains two branches of Convolutional Neural Networks designed to detect both patterns and pattern-frequencies on raw metagenomics contigs."
  },
  {
    "id": "saved-1769608044999-6jxrb",
    "title": "Dreaddit",
    "source": "arXiv",
    "authors": [
      "Elsbeth Turcan",
      "Kathleen McKeown"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.00133",
    "githubLink": "http://www.cs.columbia.edu/~eturcan/data/dreaddit.zip",
    "itemCount": "190,000 posts (3,553 labelled segments)",
    "specs": "Text (English, Reddit posts)",
    "description": "A dataset of lengthy social media posts from five different Reddit categories, annotated for the presence of stress, used for stress analysis and identification in text."
  },
  {
    "id": "saved-1769608044999-oexdy",
    "title": "BillSum",
    "source": "Other",
    "authors": [
      "Anastassia Kornilova",
      "Vladimir Eidelman"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/D19-5406/",
    "githubLink": "https://github.com/FiscalNote/BillSum",
    "itemCount": "22,218 bills",
    "specs": "Text (Legislative Bills)",
    "description": "The first dataset focused on the summarization of US Congressional and California state bills. It contains legislative text and human-written summaries, designed to benchmark summarization models on technical legislative language."
  },
  {
    "id": "saved-1769608044999-h6b4e",
    "title": "MIMIC-CXR",
    "source": "Other",
    "authors": [
      "Alistair E.W. Johnson",
      "Tom J. Pollard",
      "Seth J. Berkowitz",
      "Nathaniel R. Greenbaum",
      "Matthew P. Lungren",
      "Chih-ying Deng",
      "Roger G. Mark",
      "Steven Horng"
    ],
    "year": "2019",
    "paperLink": "https://physionet.org/content/mimic-cxr/2.0.0/",
    "githubLink": "https://github.com/MIT-LCP/mimic-cxr",
    "itemCount": "227,835 report studies",
    "specs": "Text, Image (Radiology Reports)",
    "description": "A large database of chest radiographs with associated free-text radiology reports. While primarily for medical imaging, it is a standard benchmark for radiology report summarization (generating the 'Impression' section from 'Findings')."
  },
  {
    "id": "saved-1769608045000-qenha",
    "title": "PharmaCoNER",
    "source": "Semantic Scholar",
    "authors": [
      "Aitor Gonzalez-Agirre",
      "Montserrat Marimon",
      "Ander Intxaurrondo",
      "Ona Rabal",
      "Marta Villegas",
      "Martin Krallinger"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/D19-5701/",
    "githubLink": "https://github.com/PlanTL-SANIDAD/SPACCC",
    "itemCount": "1,000 clinical cases (396,988 words)",
    "specs": "Text (Spanish clinical case reports with NER annotations)",
    "description": "A benchmark track for Named Entity Recognition (NER) of pharmacological substances, compounds, and proteins in Spanish clinical cases, essential for pharmacovigilance and patient safety monitoring in non-English texts."
  },
  {
    "id": "saved-1769608045000-mymxl",
    "title": "CoS-E (Common Sense Explanations)",
    "source": "Hugging Face",
    "authors": [
      "Nazneen Fatema Rajani",
      "Bryan McCann",
      "Caiming Xiong",
      "Richard Socher"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/P19-1487/",
    "githubLink": "https://github.com/salesforce/cos-e",
    "itemCount": "Explanations for CQA v1.0 and v1.11",
    "specs": "Text; Free-form explanations and selected spans",
    "description": "A dataset of human-annotated commonsense explanations for the Commonsense Question Answering (CQA) dataset. It is designed to train and evaluate models on their ability to generate clear, human-like justifications for commonsense reasoning tasks."
  },
  {
    "id": "saved-1769608045000-ol3pb",
    "title": "ERASER (Evaluating Rationales And Simple English Reasoning)",
    "source": "arXiv",
    "authors": [
      "Jay DeYoung",
      "Sarthak Jain",
      "Nazneen Fatema Rajani",
      "Eric Lehman",
      "Caiming Xiong",
      "Richard Socher",
      "Byron C. Wallace"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1911.03429",
    "githubLink": "https://github.com/jaydeyoung/eraserbenchmark",
    "itemCount": "7 datasets (approx. 20k+ total samples)",
    "specs": "Text data, classification labels, human-annotated rationales (span indices)",
    "description": "A benchmark for evaluating the interpretability of NLP models. It comprises multiple datasets (like Movies, FEVER, MultiRC) with human-annotated \"rationales\" (supporting evidence snippets) that justify the classification labels, enabling the measurement of model transparency via explanation alignment."
  },
  {
    "id": "saved-1769608045000-j7p7z",
    "title": "SciCite",
    "source": "Semantic Scholar",
    "authors": [
      "Arman Cohan",
      "Waleed Ammar",
      "Madeleine van Zuylen",
      "Field Cady"
    ],
    "year": "2019",
    "paperLink": "https://aclanthology.org/N19-1011/",
    "githubLink": "https://github.com/allenai/scicite",
    "itemCount": "~11,000 citation contexts",
    "specs": "Text; Classification (Citation Intent)",
    "description": "A dataset for citation intent classification. It annotates citations in scientific papers with their rhetorical function (e.g., Background, Method, Result), helping in the analysis of scientific literature structure."
  },
  {
    "id": "saved-1769608045000-h20ym",
    "title": "MLQA",
    "source": "arXiv",
    "authors": [
      "Patrick Lewis",
      "Barlas Oğuz",
      "Ruty Rinott",
      "Sebastian Riedel",
      "Holger Schwenk"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1910.07475",
    "githubLink": "https://github.com/facebookresearch/mlqa",
    "itemCount": "5K+ QA instances, 7 languages",
    "specs": "Text; Extractive Question Answering",
    "description": "A multi-way parallel extractive question answering evaluation benchmark intended to be used for zero-shot cross-lingual transfer."
  },
  {
    "id": "saved-1769608045000-rx72r",
    "title": "APRICOT",
    "source": "arXiv",
    "authors": [
      "A. Braunegg",
      "Amartya Chakraborty",
      "Michael Krumdick",
      "Nicole Lape",
      "Sara Leary",
      "Keith Manville",
      "Elizabeth Merkhofer",
      "Laura Strickhart",
      "Matthew Walmer"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1912.08166",
    "githubLink": "https://github.com/mitre/apricot",
    "itemCount": "1,011 images",
    "specs": "Images; Physical patches; Object Detection annotations (COCO format)",
    "description": "A dataset of physical adversarial attacks on object detection systems. It contains photos of printed adversarial patches placed in real-world scenes, annotated with bounding boxes, designed to test robustness in uncontrolled settings."
  },
  {
    "id": "saved-1769608045000-sw9z3",
    "title": "Overcooked-AI",
    "source": "arXiv",
    "authors": [
      "Micah Carroll",
      "Rohin Shah",
      "Mark K. Ho",
      "Thomas L. Griffiths",
      "Sanjit A. Seshia",
      "Pieter Abbeel",
      "Anca Dragan"
    ],
    "year": "2019",
    "paperLink": "https://arxiv.org/abs/1910.05789",
    "githubLink": "https://github.com/HumanCompatibleAI/overcooked_ai",
    "itemCount": "Multiple datasets (e.g., human-human trajectories, 45+ validation episodes)",
    "specs": "Grid-world game states, player actions, trajectories, collaborative tasks",
    "description": "A benchmark environment and dataset for fully cooperative human-AI task performance based on the video game Overcooked. It includes collected human-human and human-AI gameplay trajectories to evaluate coordination and collaboration."
  },
  {
    "id": "saved-1769608045000-y62j0",
    "title": "FigureQA",
    "source": "arXiv",
    "authors": [
      "Samira Ebrahimi Kahou",
      "Vincent Michalski",
      "Adam Atkinson",
      "Akos Kadar",
      "Adam Trischler",
      "Yoshua Bengio"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1710.07300",
    "githubLink": "https://github.com/Maluuba/FigureQA",
    "itemCount": "100k Images, 1.3M QA pairs",
    "specs": "Images (Synthetic Charts), Text (Binary QA)",
    "description": "A visual reasoning dataset consisting of synthetic figures (bar, line, pie charts) and yes/no questions. It focuses on relational reasoning capability of models."
  },
  {
    "id": "saved-1769608045000-xn2e7",
    "title": "DVQA",
    "source": "arXiv",
    "authors": [
      "Kushal Kafle",
      "Brian Price",
      "Scott Cohen",
      "Christopher Kanan"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1801.08163",
    "githubLink": "https://github.com/kushalkafle/DVQA_dataset",
    "itemCount": "307k Images, 2.3M QA pairs",
    "specs": "Images (Bar Charts), Text (QA pairs)",
    "description": "Understanding Data Visualization via Question Answering. A large-scale dataset of bar charts and QA pairs designed to test algorithms' ability to understand and extract information from bar charts."
  },
  {
    "id": "saved-1769608045000-zabp9",
    "title": "Phrase-Indexed Question Answering (PIQA)",
    "source": "Semantic Scholar",
    "authors": [
      "Minjoon Seo",
      "Tom Kwiatkowski",
      "Ankur P. Parikh",
      "Ali Farhadi",
      "Hannaneh Hajishirzi"
    ],
    "year": "2018",
    "paperLink": "https://aclanthology.org/D18-1455/",
    "githubLink": "https://github.com/google-research/google-research/tree/master/phrase_indexed_qa",
    "itemCount": "Based on SQuAD/TriviaQA scales (exact count varies by base dataset used)",
    "specs": "Extractive QA; Independent encoding formulation",
    "description": "A modular variant of extractive question answering that enforces complete independence between document and question encoders. This formulation addresses scalability in machine comprehension by allowing answer candidate phrases to be pre-computed and indexed offline."
  },
  {
    "id": "saved-1769608045000-jnxyq",
    "title": "emrQA",
    "source": "Scholar",
    "authors": [
      "Anusri Pampari",
      "Preethi Raghavan",
      "Jennifer Liang",
      "Jian Peng"
    ],
    "year": "2018",
    "paperLink": "https://aclanthology.org/D18-1258/",
    "githubLink": "https://github.com/panusri/emrQA",
    "itemCount": "~1,000,000 question-logical form pairs; 400,000+ QA pairs",
    "specs": "Unstructured Text (Clinical Notes); Slot-filled templates",
    "description": "A large-scale corpus for question answering on electronic medical records, generated by repurposing existing expert annotations from i2b2 clinical NLP challenges. It focuses on reading comprehension over clinical notes."
  },
  {
    "id": "saved-1769608045000-zff7t",
    "title": "fastMRI",
    "source": "arXiv",
    "authors": [
      "Jure Zbontar",
      "Florian Knoll",
      "Anuroop Sriram",
      "Tullie Murrell",
      "Zhengnan Huang",
      "Matthew J. Muckley",
      "Aaron Defazio"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1811.08839",
    "githubLink": "https://github.com/facebookresearch/fastMRI",
    "itemCount": "8,344 raw volumes, >1.57 million slices",
    "specs": "Raw k-space, DICOM images (Knee, Brain)",
    "description": "A large-scale collection of both raw MRI measurements (k-space) and clinical MRI images (DICOM) for investigating AI-based MRI reconstruction."
  },
  {
    "id": "saved-1769608045000-9vckv",
    "title": "Calgary-Campinas (CC-359)",
    "source": "Scholar",
    "authors": [
      "Roberto Souza",
      "Oeslle Lucena",
      "Julia Garrafa",
      "David Gobbi"
    ],
    "year": "2018",
    "paperLink": "https://pubmed.ncbi.nlm.nih.gov/29408420/",
    "githubLink": "https://sites.google.com/view/calgary-campinas-dataset/home",
    "itemCount": "359 volumes",
    "specs": "3D T1-weighted MRI, 1.5T and 3T",
    "description": "A multi-vendor, multi-field strength brain MR dataset primarily used for benchmarking skull stripping and brain MRI analysis."
  },
  {
    "id": "saved-1769608045000-ehgbs",
    "title": "ARC (AI2 Reasoning Challenge)",
    "source": "Semantic Scholar",
    "authors": [
      "Peter Clark",
      "Isaac Cowhey",
      "Oren Etzioni",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Carissa Schoenick",
      "Oyvind Tafjord"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1803.05457",
    "githubLink": "https://github.com/allenai/arc",
    "itemCount": "7,787 questions",
    "specs": "Text, Multiple Choice, Science Domain",
    "description": "A dataset of grade-school science questions consisting of a 'Challenge Set' (hard questions for retrieval methods) and an 'Easy Set'."
  },
  {
    "id": "saved-1769608045000-a7x09",
    "title": "CommonsenseQA",
    "source": "arXiv",
    "authors": [
      "Alon Talmor",
      "Jonathan Herzig",
      "Nicholas Lourie",
      "Jonathan Berant"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1811.00937",
    "githubLink": "https://github.com/jonathanherzig/commonsenseqa",
    "itemCount": "12,247 questions",
    "specs": "Text, 5-option Multiple Choice, Commonsense",
    "description": "A dataset for commonsense question answering constructed from ConceptNet, requiring background knowledge to solve questions with complex semantics."
  },
  {
    "id": "saved-1769608045000-herzu",
    "title": "SWAG (Situations With Adversarial Generations)",
    "source": "Scholar",
    "authors": [
      "Rowan Zellers",
      "Yonatan Bisk",
      "Roy Schwartz",
      "Yejin Choi"
    ],
    "year": "2018",
    "paperLink": "https://aclanthology.org/D18-1009/",
    "githubLink": "https://github.com/rowanzellers/swagaf",
    "itemCount": "113,000 questions",
    "specs": "Text (Video captions), Multiple Choice",
    "description": "A large-scale dataset for grounded commonsense inference, where models predict what happens next in a video scene description from multiple choices."
  },
  {
    "id": "saved-1769608045000-tnh4c",
    "title": "VQA-RAD",
    "source": "Hugging Face",
    "authors": [
      "Jason J. Lau",
      "Soumya Gayen",
      "Asma Ben Abacha",
      "Dina Demner-Fushman"
    ],
    "year": "2018",
    "paperLink": "https://www.nature.com/articles/sdata2018251",
    "githubLink": "https://github.com/openmedlab/Awesome-Medical-Dataset",
    "itemCount": "315 images, 3,515 QA pairs",
    "specs": "Images (CT, MRI, X-ray), Text (Open-ended, Yes/No questions)",
    "description": "A manually curated dataset of clinically generated visual questions and answers about radiology images. It is designed to evaluate VQA models on real-world clinical questions, distinguishing it from larger, automatically generated datasets."
  },
  {
    "id": "saved-1769608045000-0xidb",
    "title": "Region-based annotated Child Pornography Dataset (RCPD)",
    "source": "Other",
    "authors": [
      "Camila Laranjeira da Silva",
      "Jefersson A. dos Santos",
      "et al."
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/2204.14110",
    "githubLink": "http://bioinfo.dcc.ufmg.br/propedia2/download",
    "itemCount": "2,138 samples (836 CSAM, 285 Adult Porn, 1,017 Normal)",
    "specs": "Image Metadata, Bounding Boxes, Feature Vectors (No raw CSAM images)",
    "description": "A research dataset containing metadata and region-based annotations (bounding boxes) for images related to CSAM detection. The dataset includes feature vectors and annotations for body parts and scene context to aid in the development of privacy-preserving detection algorithms without distributing the illegal image content itself."
  },
  {
    "id": "saved-1769608045000-lkjxd",
    "title": "EMBER (Endgame Malware BEnchmark for Research)",
    "source": "arXiv",
    "authors": [
      "Hyrum S. Anderson",
      "Phil Roth",
      "Robert J. Joyce",
      "et al."
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1804.04637",
    "githubLink": "https://github.com/elastic/ember",
    "itemCount": "1.1 million samples (2018 version); 3.2 million samples (2024 version)",
    "specs": "JSON format features (byte histograms, string info, imports/exports), SHA256 hashes, Windows PE format",
    "description": "A large-scale benchmark dataset for static analysis of Windows Portable Executable (PE) files. It is designed to train and evaluate machine learning models for malware detection, containing labeled benign and malicious samples with raw features extracted using LIEF."
  },
  {
    "id": "saved-1769608045000-wu6e4",
    "title": "CAIL2018 (Chinese AI and Law 2018)",
    "source": "arXiv",
    "authors": [
      "Chaojun Xiao",
      "Haoxi Zhong",
      "Zhipeng Guo",
      "Cunchao Tu",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Yansong Feng",
      "Xianpei Han",
      "Zhen Hu",
      "Heng Wang",
      "Jianfeng Xu"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1807.02478",
    "githubLink": "https://github.com/thunlp/CAIL",
    "itemCount": "2.6 million cases",
    "specs": "Text (Chinese); Multi-label classification, Regression",
    "description": "The first large-scale Chinese legal dataset for judgment prediction, consisting of over 2.6 million criminal cases. It challenges models to predict applicable law articles, charges, and prison terms based on fact descriptions."
  },
  {
    "id": "saved-1769608045000-csgr3",
    "title": "e-SNLI",
    "source": "arXiv",
    "authors": [
      "Oana-Maria Camburu",
      "Tim Rocktäschel",
      "Thomas Lukasiewicz",
      "Phil Blunsom"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1812.01193",
    "githubLink": "https://github.com/OanaMariaCamburu/e-SNLI",
    "itemCount": "570,000 sentence pairs",
    "specs": "Text; Natural Language Explanations",
    "description": "An extension of the Stanford Natural Language Inference (SNLI) dataset that includes human-annotated natural language explanations for the entailment relations. It serves as a benchmark for models that generate textual explanations for their decisions."
  },
  {
    "id": "saved-1769608045000-yr5e3",
    "title": "SciTail",
    "source": "Hugging Face",
    "authors": [
      "Tushar Khot",
      "Ashish Sabharwal",
      "Peter Clark"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1802.02324",
    "githubLink": "https://github.com/allenai/scitail",
    "itemCount": "27,026 examples",
    "specs": "Text; Natural Language Inference (Entailment/Neutral)",
    "description": "A textual entailment dataset created from multiple-choice science exams and web sentences. It is designed to evaluate a model's ability to determine if a hypothesis follows from a premise in a scientific context."
  },
  {
    "id": "saved-1769608045000-89427",
    "title": "SciERC",
    "source": "Semantic Scholar",
    "authors": [
      "Yi Luan",
      "Luheng He",
      "Mari Ostendorf",
      "Hannaneh Hajishirzi"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1808.09602",
    "githubLink": "https://github.com/dwadden/dygiepp",
    "itemCount": "500 annotated abstracts",
    "specs": "Text; Named Entity Recognition (NER), Relation Extraction",
    "description": "A benchmark for scientific entity and relation extraction. It consists of annotated abstracts from AI conference proceedings, labeling scientific entities (e.g., Task, Method, Metric) and their relations."
  },
  {
    "id": "saved-1769608045000-jedlw",
    "title": "XNLI",
    "source": "arXiv",
    "authors": [
      "Alexis Conneau",
      "Ruty Rinott",
      "Guillaume Lample",
      "Adina Williams",
      "Samuel R. Bowman",
      "Holger Schwenk",
      "Veselin Stoyanov"
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1809.05053",
    "githubLink": "https://github.com/facebookresearch/XNLI",
    "itemCount": "112.5k annotated pairs, 15 languages",
    "specs": "Text; Natural Language Inference (Classification)",
    "description": "A benchmark for cross-lingual sentence understanding, consisting of Natural Language Inference data in 15 languages."
  },
  {
    "id": "saved-1769608045000-cnltk",
    "title": "CoNaLa",
    "source": "arXiv",
    "authors": [
      "Pengcheng Yin",
      "Bowen Deng",
      "Edgar Chen",
      "et al."
    ],
    "year": "2018",
    "paperLink": "https://arxiv.org/abs/1805.08949",
    "githubLink": "https://github.com/conala-corpus/conala-corpus",
    "itemCount": "2,379 curated / ~600k mined pairs",
    "specs": "Natural Language to Python Code",
    "description": "The Code/Natural Language Challenge dataset. It contains pairs of natural language intents and code snippets, mined from Stack Overflow and manually curated."
  },
  {
    "id": "saved-1769608045000-g01mc",
    "title": "ChestX-ray14 (NIH Chest X-ray)",
    "source": "Other",
    "authors": [
      "Xiaosong Wang",
      "Yifan Peng",
      "Le Lu",
      "Zhiyong Lu",
      "Mohammadhadi Bagheri",
      "Ronald M. Summers"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1705.02315",
    "githubLink": "https://github.com/zoogzog/chexnet",
    "itemCount": "112,120 images",
    "specs": "Frontal-view X-ray images, 14 disease labels text-mined from reports",
    "description": "A hospital-scale chest X-ray database for weakly-supervised classification and localization of common thorax diseases."
  },
  {
    "id": "saved-1769608045000-lzwra",
    "title": "RACE",
    "source": "arXiv",
    "authors": [
      "Guokun Lai",
      "Qizhe Xie",
      "Hanxiao Liu",
      "Yiming Yang",
      "Eduard Hovy"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1704.04683",
    "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
    "itemCount": "~100,000 questions",
    "specs": "Text, Multiple Choice, Reading Comprehension",
    "description": "A reading comprehension dataset collected from English examinations in China for middle and high school students, focusing on reasoning capabilities."
  },
  {
    "id": "saved-1769608045000-5vcqw",
    "title": "SciQ",
    "source": "Semantic Scholar",
    "authors": [
      "Johannes Welbl",
      "Nelson F. Liu",
      "Matt Gardner"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1707.06209",
    "githubLink": "https://allenai.org/data/sciq",
    "itemCount": "13,679 questions",
    "specs": "Text, Multiple Choice, Science Domain",
    "description": "A dataset of crowdsourced science exam questions covering Biology, Chemistry, Earth Science, and Physics."
  },
  {
    "id": "saved-1769608045000-63k0n",
    "title": "MoleculeNet",
    "source": "arXiv",
    "authors": [
      "Zhenqin Wu",
      "Bharath Ramsundar",
      "Evan N. Feinberg",
      "Joseph Gomes",
      "Caleb Geniesse",
      "Aneesh S. Pappu",
      "Karl Leswing",
      "Vijay Pande"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1703.00564",
    "githubLink": "https://github.com/deepchem/deepchem",
    "itemCount": "Over 700,000 compounds",
    "specs": "Molecular structures (SMILES/graphs), regression and classification tasks",
    "description": "A benchmark for molecular machine learning, curating multiple public datasets to test methods on molecular properties including quantum mechanics, physical chemistry, biophysics, and physiology."
  },
  {
    "id": "saved-1769608045000-vqjxh",
    "title": "Visual Memory QA (MemexQA)",
    "source": "arXiv",
    "authors": [
      "Lu Jiang",
      "Liangliang Cao",
      "Yannis Kalantidis",
      "Sachin Farfade",
      "Alexander G. Hauptmann"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1708.01336",
    "githubLink": "https://github.com/JunweiLiang/FVTA_MemexQA",
    "itemCount": "Contains personal photos/videos and crowd-sourced QA pairs (Exact count varies by version, e.g., MemexQA v1.1)",
    "specs": "Multimodal (Images/Videos + Text Questions)",
    "description": "A dataset and task designed to answer questions about user's daily life based on their personal photo and video collections without relying on metadata. It focuses on recovering memories (e.g., 'When was the last time we went hiking?') using visual content."
  },
  {
    "id": "saved-1769608045000-io0zs",
    "title": "PACS (Photo, Art Painting, Cartoon, Sketch)",
    "source": "Scholar",
    "authors": [
      "Da Li",
      "Yongxin Yang",
      "Yi-Zhe Song",
      "Timothy M. Hospedales"
    ],
    "year": "2017",
    "paperLink": "https://arxiv.org/abs/1710.03077",
    "githubLink": "http://sketchx.eecs.qmul.ac.uk/downloads/",
    "itemCount": "9,991 images",
    "specs": "4 domains (Photo, Art Painting, Cartoon, Sketch), 7 object categories (Dog, Elephant, Giraffe, Guitar, Horse, House, Person)",
    "description": "A popular domain generalization benchmark consisting of images from four domains: Photo, Art Painting, Cartoon, and Sketch. It is designed to evaluate a model's ability to generalize to an unseen domain (e.g., training on three and testing on the fourth)."
  },
  {
    "id": "saved-1769608045000-zhxh3",
    "title": "OpenI (Indiana University Chest X-ray)",
    "source": "Other",
    "authors": [
      "Dina Demner-Fushman",
      "Marc D. Kohli",
      "Marc B. Rosenman",
      "S. E. Shooshan",
      "L. Rodriguez",
      "S. Antani",
      "G. R. Thoma",
      "C. J. McDonald"
    ],
    "year": "2016",
    "paperLink": "https://lhncbc.nlm.nih.gov/system/files/pub2012019.pdf",
    "githubLink": "https://huggingface.co/datasets/fuyu-8b/open-i",
    "itemCount": "7,470 images; 3,955 reports",
    "specs": "Frontal and lateral X-ray images, associated radiology reports (XML)",
    "description": "A public chest X-ray dataset collected from the Indiana University hospital network, widely used for report generation benchmarks."
  },
  {
    "id": "saved-1769608045000-zcun2",
    "title": "Pornography-2k",
    "source": "Other",
    "authors": [
      "Carlos H. A. Mello",
      "Sandra Avila",
      "Anderson Rocha"
    ],
    "year": "2016",
    "paperLink": "https://ieeexplore.ieee.org/document/7738016",
    "githubLink": "https://github.com/fffaded/SIEGuardian-Dataset",
    "itemCount": "2,000 videos (140 hours)",
    "specs": "Video (AVI/MP4), Frames",
    "description": "A widely used benchmark dataset for pornography detection, often utilized as a proxy or 'adult' class in CSAM research to test the separation between adult content and other categories. It contains diverse video footage of pornographic and non-pornographic content."
  },
  {
    "id": "saved-1769608045000-2pgsv",
    "title": "BreaKHis (Breast Cancer Histopathological Image Classification)",
    "source": "Scholar",
    "authors": [
      "Fabio A. Spanhol",
      "Luiz S. Oliveira",
      "Caroline Petitjean",
      "Laurent Heutte"
    ],
    "year": "2016",
    "paperLink": "http://web.inf.ufpr.br/vri/breast-cancer-database",
    "githubLink": "https://github.com/mrdvince/breast_cancer_detection",
    "itemCount": "7,909 images",
    "specs": "RGB PNG images, 4 magnification levels (40X, 100X, 200X, 400X)",
    "description": "A benchmark for the classification of breast cancer histopathology images. The dataset contains microscopic images of breast tumor tissue collected from patients, labeled as either benign or malignant."
  },
  {
    "id": "saved-1769608045000-e7j91",
    "title": "Real-life Trial (RLT) Deception Detection Dataset",
    "source": "Other",
    "authors": [
      "Verónica Pérez-Rosas",
      "Mohamed Abouelenien",
      "Rada Mihalcea",
      "Mihai Burzo"
    ],
    "year": "2015",
    "paperLink": "https://web.eecs.umich.edu/~zmohamed/PDFs/Trial.ICMI.pdf",
    "githubLink": "https://github.com/lit-umich/Real-life-Deception-Detection-2016",
    "itemCount": "121 video clips",
    "specs": "Video (visual), Audio, Text (transcriptions); High-stakes courtroom setting",
    "description": "A multimodal dataset consisting of 121 video clips (61 deceptive, 60 truthful) collected from public courtroom trials. It captures high-stakes deception scenarios involving defendants and witnesses."
  },
  {
    "id": "saved-1769608045000-nod0p",
    "title": "OWASP Benchmark Project",
    "source": "Other",
    "authors": [
      "OWASP Foundation",
      "Dave Wichers"
    ],
    "year": "2015",
    "paperLink": "https://owasp.org/www-project-benchmark/",
    "githubLink": "https://github.com/OWASP-Benchmark/BenchmarkUtils",
    "itemCount": "2,740 test cases (Java v1.2), 1,230 test cases (Python v0.1)",
    "specs": "Java (v1.2), Python (v0.1); Web application format; Vulnerabilities include SQLi, XSS, Command Injection, etc.",
    "description": "A fully runnable, open-source web application designed to evaluate the accuracy, coverage, and speed of automated software vulnerability detection tools (SAST, DAST, and IAST). It contains thousands of test cases mapped to specific CWEs to identify true positives and false positives."
  },
  {
    "id": "saved-1769608045000-yona1",
    "title": "BraTS (Multimodal Brain Tumor Image Segmentation Benchmark)",
    "source": "Scholar",
    "authors": [
      "Bjoern H. Menze",
      "Andras Jakab",
      "Stefan Bauer",
      "Jayashree Kalpathy-Cramer",
      "Keyvan Farahani"
    ],
    "year": "2015",
    "paperLink": "https://ieeexplore.ieee.org/document/6975210",
    "githubLink": "https://github.com/RAVIRAJAG/BraTS-2023-Challenge",
    "itemCount": "Varies (e.g., ~2000 patients in 2021)",
    "specs": "Multi-modal MRI (T1, T1ce, T2, FLAIR), NIfTI format",
    "description": "A long-running benchmark challenge for the segmentation of brain tumors (gliomas) in multimodal magnetic resonance imaging scans."
  },
  {
    "id": "saved-1769608045000-l4ra6",
    "title": "EyePACS (Kaggle Diabetic Retinopathy Detection)",
    "source": "Other",
    "authors": [
      "Jorge Cuadros",
      "George Breslauer",
      "Kaggle",
      "EyePACS"
    ],
    "year": "2015",
    "paperLink": "https://www.kaggle.com/c/diabetic-retinopathy-detection",
    "githubLink": "https://github.com/bumbledeep/eyepacs",
    "itemCount": "88,702 images (35,126 train, 53,576 test)",
    "specs": "JPEG images, 5-class grading (0-4), high-resolution fundus photography",
    "description": "A large-scale dataset of high-resolution retinal fundus images used for the automated detection of diabetic retinopathy. It was originally released as part of a Kaggle competition to identify the presence of diabetic retinopathy in images."
  },
  {
    "id": "saved-1769608045000-okyfq",
    "title": "DeepSEA Dataset",
    "source": "Scholar",
    "authors": [
      "Jian Zhou",
      "Olga G. Troyanskaya"
    ],
    "year": "2015",
    "paperLink": "https://doi.org/10.1038/nmeth.3547",
    "githubLink": "https://github.com/FunctionLab/DeepSEA",
    "itemCount": "~521 million bp (training), 919 chromatin features",
    "specs": "1,000 bp DNA sequences mapped to 919 binary chromatin feature labels (multi-label classification).",
    "description": "A foundational dataset originally created to train the DeepSEA model for predicting chromatin effects of sequence alterations. It serves as a standard benchmark for predicting chromatin profiles (transcription factor binding, DNase I sensitivity, histone marks) from DNA sequences."
  },
  {
    "id": "saved-1769608045000-mnfhn",
    "title": "IU X-Ray (OpenI)",
    "source": "Other",
    "authors": [
      "Dina Demner-Fushman",
      "Marc D. Kohli",
      "Marc B. Rosenman",
      "Sameer E. Shooshan",
      "Laritza Rodriguez",
      "Sameer Antani",
      "George R. Thoma",
      "Clement J. McDonald"
    ],
    "year": "2015",
    "paperLink": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4849245/",
    "githubLink": "https://openi.nlm.nih.gov/",
    "itemCount": "7,470 images, 3,955 reports",
    "specs": "Chest X-ray images, Radiology reports (XML)",
    "description": "A collection of chest X-ray images with associated radiology reports from the Indiana University hospital network. It is widely used as a smaller benchmark for report generation."
  },
  {
    "id": "saved-1769608045000-qlh94",
    "title": "BigCloneBench",
    "source": "Other",
    "authors": [
      "Svajlenko",
      "J.",
      "Roy",
      "C. K."
    ],
    "year": "2014 (Updated 2018/2020)",
    "paperLink": "https://ieeexplore.ieee.org/document/6936865",
    "githubLink": "https://github.com/jeffsvajlenko/BigCloneBench",
    "itemCount": "8 million+ clone pairs, 25,000+ projects",
    "specs": "Java Source Code, Clone Types 1-4",
    "description": "A large-scale benchmark for code clone detection, containing millions of manually validated clone pairs. It is widely used to evaluate tools for detecting Type-1 (exact) to Type-4 (semantic) code clones in Java."
  },
  {
    "id": "saved-1769608045001-ylxu0",
    "title": "PAN12 Sexual Predator Identification",
    "source": "Scholar",
    "authors": [
      "Giacomo Inches",
      "Fabio Crestani"
    ],
    "year": "2012",
    "paperLink": "https://aclanthology.org/2012.clef_conference-2012.39/",
    "githubLink": "https://webis.de/events/pan-12/pan12-papers-sexual-predator-identification.html",
    "itemCount": "222,000 chat segments (approx.)",
    "specs": "Text (Chat logs, grooming detection)",
    "description": "A classic benchmark dataset for the identification of sexual predators in chat logs. It contains anonymized chat conversations labeled for grooming behavior, derived from the Perverted Justice Foundation and other sources."
  },
  {
    "id": "saved-1769608045001-ppjha",
    "title": "PAN-2012 Sexual Predator Identification (PAN12)",
    "source": "Other",
    "authors": [
      "Giacomo Inches",
      "Fabio Crestani"
    ],
    "year": "2012",
    "paperLink": "https://ceur-ws.org/Vol-1178/CLEF2012wn-PAN-InchesEt2012.pdf",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-Datasets",
    "itemCount": "~66,900 chat logs (Training corpus)",
    "specs": "Text (XML format), Chat logs",
    "description": "A benchmark dataset consisting of chat logs designed to identify sexual predators in online conversations. It contains anonymized chat logs involving two or more people, with the goal of determining who is the predator. This is one of the most widely used datasets for text-based grooming detection research."
  },
  {
    "id": "saved-1769608045001-rsjp1",
    "title": "PAN 2012 Sexual Predator Identification (PAN12)",
    "source": "Scholar",
    "authors": [
      "Giacomo Inches",
      "Fabio Crestani"
    ],
    "year": "2012",
    "paperLink": "https://www.webis.de/publications.html#inches_2012",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "~669,000 messages (Training Corpus), ~222,000 chat segments",
    "specs": "Text (Chat Logs), XML format",
    "description": "The most widely used benchmark for grooming detection, created for the PAN 2012 competition. It consists of chat logs from Perverted Justice (predators) and non-predatory chats from Omegle and IRC. It supports tasks like identifying predatory users and specific grooming lines."
  },
  {
    "id": "saved-1769608045001-06gh4",
    "title": "Deceptive Opinion Spam Corpus (Ott Dataset)",
    "source": "Other",
    "authors": [
      "Myle Ott",
      "Yejin Choi",
      "Claire Cardie",
      "Jeffrey T. Hancock"
    ],
    "year": "2011",
    "paperLink": "https://aclanthology.org/P11-1032.pdf",
    "githubLink": "https://github.com/myleott/op_spam",
    "itemCount": "1,600 reviews",
    "specs": "Text (English); Positive and Negative sentiment; 400 truthful/400 deceptive per sentiment",
    "description": "A 'gold standard' text-based dataset for deception detection containing truthful and deceptive hotel reviews. Deceptive reviews were generated using Amazon Mechanical Turk, while truthful reviews were mined from TripAdvisor."
  },
  {
    "id": "saved-1769608045001-7nmw3",
    "title": "Virus Texture Dataset v.1.0",
    "source": "Scholar",
    "authors": [
      "Gustaf Kylberg",
      "Mats Uppström",
      "Ida-Maria Sintorn"
    ],
    "year": "2011",
    "paperLink": "https://kylberg.org/virus-texture-dataset/",
    "githubLink": "https://kylberg.org/virus-texture-dataset/",
    "itemCount": "1,500 images",
    "specs": "Greyscale TEM images (41x41 pixels patches), 15 virus classes, 100 samples per class",
    "description": "A classic computer vision benchmark for virus classification using Transmission Electron Microscopy (TEM) images. It is widely used to evaluate texture descriptors and image classification models in virology."
  },
  {
    "id": "saved-1769608045001-eif6w",
    "title": "ChatCoder2 (CC2)",
    "source": "Scholar",
    "authors": [
      "I. McGhee",
      "J. Bayzick",
      "A. Kontostathis",
      "L. Edwards",
      "A. McBride",
      "E. Jakubowski"
    ],
    "year": "2011",
    "paperLink": "https://www.tandfonline.com/doi/abs/10.2753/JEC1086-4415150305",
    "githubLink": "https://gitlab.com/early-sexual-predator-detection/eSPD-datasets",
    "itemCount": "497 complete predator chats",
    "specs": "Text (Complete Chat Logs)",
    "description": "A dataset containing complete predator chats collected from Perverted Justice. It is often used for studying the semantic segmentation of grooming chats into phases like 'Approach', 'Grooming', and 'Personal Information Exchange'."
  },
  {
    "id": "saved-1769608045001-di1jf",
    "title": "Diamonds",
    "source": "Hugging Face",
    "authors": [
      "Hadley Wickham"
    ],
    "year": "2007",
    "paperLink": "https://ggplot2.tidyverse.org/reference/diamonds.html",
    "githubLink": "https://github.com/tidyverse/ggplot2",
    "itemCount": "53,940 rows",
    "specs": "Tabular (10 columns: price, carat, cut, color, clarity, x, y, z, depth, table)",
    "description": "A classic dataset containing the prices and other attributes of nearly 54,000 diamonds. Widely used for data visualization and regression analysis tutorials, specifically for predicting diamond prices based on features like carat, cut, color, and clarity."
  },
  {
    "id": "saved-1769608045001-wiogg",
    "title": "Software Assurance Reference Dataset (SARD)",
    "source": "Scholar",
    "authors": [
      "NIST SAMATE Group"
    ],
    "year": "2005",
    "paperLink": "https://samate.nist.gov/SARD/",
    "itemCount": "500,000+ test cases",
    "specs": "C, C++, Java, PHP, C#; Synthetic and Production code",
    "description": "A massive collection of test cases (programs with known bugs/vulnerabilities) maintained by NIST to allow users, researchers, and software security tool developers to evaluate their tools."
  },
  {
    "id": "saved-1769608045001-dg480",
    "title": "IXI Dataset",
    "source": "Scholar",
    "authors": [
      "Biomedical Image Analysis Group (Imperial College London)"
    ],
    "year": "2005",
    "paperLink": "https://brain-development.org/ixi-dataset/",
    "githubLink": "https://brain-development.org/ixi-dataset/",
    "itemCount": "~600 subjects",
    "specs": "T1, T2, PD, MRA, DTI (NIfTI)",
    "description": "A collection of MR images from normal, healthy subjects collected at three different hospitals, widely used for registration and segmentation tasks."
  },
  {
    "title": "BAISBench",
    "paperLink": "https://arxiv.org/abs/2505.08341",
    "description": "Biological AI Scientist Benchmark designed to assess agents on cell type annotation and scientific discovery tasks using single-cell transcriptomic data.",
    "authors": [
      "Yinghua Luo",
      "et al."
    ],
    "githubLink": "https://github.com/EperLuo/BaisBench",
    "itemCount": "15 datasets (annotation), 193 MCQs (discovery)",
    "source": "arXiv",
    "specs": "Single-cell RNA-seq data, multiple-choice questions",
    "year": "2025",
    "id": "saved-1769638817151-t84d9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSU-2Kv5DvQ8Ic7f7gKJbLoezaqaaHYWwTf64mI7Jjv4CHNG8w8wcNOtgfsfXRu_6bW5JIqocNgDgHq_kYgSKfoc40UOBUnhcaJSA-dDXreAmLbNBkEAH-PJmBYx08_GbrG27L4eeg4cpMGIPbG5NL6227LZ5Be39ziqG-gDuHIscCe0_W6vz-Rjh149GFcuSRMpyYI-E0wwf02Gq_vZ-MIt2uL6W-QrxAD_lRKr0ginE6hcgsf2OLZOZW47DioapIT-JS7V5nUQ==",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8AuImro3mXR9fs6LHSuXMj3pnxJCZPIV4MyZp3q9uRp_yPAW6tAU2vWMyg-VpYXY-KzHdjckYcyQBRJUguE3ADPbBw3vjjkmm2xWT4pgLj7XF2heDM1sjdQm9UA==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUUuunrwR_Z9yb0X_pBu54vY6nEnYt_b0ZRMMSvCbtBUHi2ZPHq9fpGAhpr5hqIcoFT77sX10ZNQByIefDGazik22aHxo7DWbbl7IiVhfkyBX3fArbN6_egdF6zGKpVSy9V8dzxQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE20gXYrmQZT5A74KnL3cL-PyI0WDBCART5i4M3jgJuUFDFqokbhJjp_zSToYKkqffdON9LmwTT4IB2d0B4V0P1GudCpjR2DZdNAu2qy9CI1EN8azTHOTFikrWFsiDxBcKXRvU8RLwbdWZL4NPePY28vY52EQeqcU-qDQku76f7HYugvFlS",
        "title": "materialsproject.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaMIbUHrfd7x6wUuks51XwZ7qoTCyxMpkPl4GdyZl1hpc3NgwOz-rVeJhayMSMzqAd_3GPUVs19EI-kHWLiWa5k2KNifZpohP7cWJCLoIn33lutIEnw14DWEnMURcatt9E_-34VvWedj7-PSoQ6WxtlrpJjNdHiWX2nMRSsgZ9u3KsDVNlCIzaE8NP9BJ_jTTx7HVH-ocPJt7lPL7p4M10dRE0VPAncjWPouqhNNeiKOVJNQ2L5WnS-cm5pC47MFyjpBjIr3H9L3mTWySePHix_Z-1",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNwsa_SKRGVxJ4WmnUQ0hbPjjhbh4VieBOtLdOpXrT13u40J8qUP9eqTDF7zu5gjSeUf_Snh6i0kZ-x_rZQqtLcyvQ657WzUD9o8uqQI2ajv71rHR1cuvwFVJhQCSRrmwzmi8kjKhpaLAPqKhB2C2z",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdJ7-0LLL9o6TUPQWB5gM8T9F-hkOEDm3e88YTmYvTmLBqRGVHeBrWO3fHyWcretGUs6u_N6R3IJFceWpYYKp1lZ2MeB2OsXD6vKMzJRcEvnHWcgU9J55mxqQFxNinZwG3_P0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQrvzsZBT8flkqEWJGICZGepcItgRAPvk-8apZ21UU09l1ypPOSLWbPKvmOx7AYkfdedIq0DKXUdZuP7lAS17WvXoXoxvWWXLirF90Wu04stxq1sCIdfoBKfVOW6vo",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFErr9Fpbnp0s5SWT5ZMkO7fT6q8btY14vvE9AB7TRnsYSjqy7ygJN4hhf-xJsiZD1TH6EWBmPI8Iao50ejIHipp4JL4Tf7SNefSxD0ZrWG7aHjdzW_jOii37P7_ZNRatLZufyGS9YCnAtRHUS9sRyE_187I63Odoh0hXLsI9aXn4eU",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExkKxH6AEhY4aq7YJyDagm0xz2PKd2198OkHxlaP6Ce2XqldpbpeJjXlWCc0hzgRzaksIaVGxyehvPPjt22m3IUoKH0jtx2l-EvakNCO5h4tfpZDlzvtHEX80ytFBS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfr4f46UsESF47hg1A-mtFR54a7hTfpCCqtlGaRMXFe0RmqtcUvcdERWz9DT3Xb00jru_beGp1b3NrqQFcpHzoy_-fikAX3p3-F3CYU5cNvqqtvj-pKRBzUd-9U_MIqOvgjmBjU4_L9qyhIOTt",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgl0Cef2ekZ0eOEJsteWIY8N5_wfndupSCId3CKldrcYHCAAzq4-L61id_ceDWtO0EBHmIvJz892XN35H0K_CdX03bqu8N1JKZ3YEqQd5BTec6A_xdNXfR92KJ4PB_K7wDyZxX3A==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "NeuroDiscoveryBench",
    "paperLink": "https://allenai.org/research/neurodiscoverybench",
    "description": "A benchmark to test AI systems on answering questions grounded in real-world neuroscience data, requiring data manipulation, analysis, and visualization. Built on open datasets from the Allen Institute.",
    "authors": [
      "Allen Institute for AI"
    ],
    "githubLink": "https://github.com/allenai/neurodiscoverybench",
    "itemCount": "~70 question-answer pairs",
    "source": "Semantic Scholar",
    "specs": "Text questions, real-world neuroscience datasets",
    "year": "2025",
    "id": "saved-1769638817151-t2j00",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSU-2Kv5DvQ8Ic7f7gKJbLoezaqaaHYWwTf64mI7Jjv4CHNG8w8wcNOtgfsfXRu_6bW5JIqocNgDgHq_kYgSKfoc40UOBUnhcaJSA-dDXreAmLbNBkEAH-PJmBYx08_GbrG27L4eeg4cpMGIPbG5NL6227LZ5Be39ziqG-gDuHIscCe0_W6vz-Rjh149GFcuSRMpyYI-E0wwf02Gq_vZ-MIt2uL6W-QrxAD_lRKr0ginE6hcgsf2OLZOZW47DioapIT-JS7V5nUQ==",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8AuImro3mXR9fs6LHSuXMj3pnxJCZPIV4MyZp3q9uRp_yPAW6tAU2vWMyg-VpYXY-KzHdjckYcyQBRJUguE3ADPbBw3vjjkmm2xWT4pgLj7XF2heDM1sjdQm9UA==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUUuunrwR_Z9yb0X_pBu54vY6nEnYt_b0ZRMMSvCbtBUHi2ZPHq9fpGAhpr5hqIcoFT77sX10ZNQByIefDGazik22aHxo7DWbbl7IiVhfkyBX3fArbN6_egdF6zGKpVSy9V8dzxQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE20gXYrmQZT5A74KnL3cL-PyI0WDBCART5i4M3jgJuUFDFqokbhJjp_zSToYKkqffdON9LmwTT4IB2d0B4V0P1GudCpjR2DZdNAu2qy9CI1EN8azTHOTFikrWFsiDxBcKXRvU8RLwbdWZL4NPePY28vY52EQeqcU-qDQku76f7HYugvFlS",
        "title": "materialsproject.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaMIbUHrfd7x6wUuks51XwZ7qoTCyxMpkPl4GdyZl1hpc3NgwOz-rVeJhayMSMzqAd_3GPUVs19EI-kHWLiWa5k2KNifZpohP7cWJCLoIn33lutIEnw14DWEnMURcatt9E_-34VvWedj7-PSoQ6WxtlrpJjNdHiWX2nMRSsgZ9u3KsDVNlCIzaE8NP9BJ_jTTx7HVH-ocPJt7lPL7p4M10dRE0VPAncjWPouqhNNeiKOVJNQ2L5WnS-cm5pC47MFyjpBjIr3H9L3mTWySePHix_Z-1",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNwsa_SKRGVxJ4WmnUQ0hbPjjhbh4VieBOtLdOpXrT13u40J8qUP9eqTDF7zu5gjSeUf_Snh6i0kZ-x_rZQqtLcyvQ657WzUD9o8uqQI2ajv71rHR1cuvwFVJhQCSRrmwzmi8kjKhpaLAPqKhB2C2z",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdJ7-0LLL9o6TUPQWB5gM8T9F-hkOEDm3e88YTmYvTmLBqRGVHeBrWO3fHyWcretGUs6u_N6R3IJFceWpYYKp1lZ2MeB2OsXD6vKMzJRcEvnHWcgU9J55mxqQFxNinZwG3_P0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQrvzsZBT8flkqEWJGICZGepcItgRAPvk-8apZ21UU09l1ypPOSLWbPKvmOx7AYkfdedIq0DKXUdZuP7lAS17WvXoXoxvWWXLirF90Wu04stxq1sCIdfoBKfVOW6vo",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFErr9Fpbnp0s5SWT5ZMkO7fT6q8btY14vvE9AB7TRnsYSjqy7ygJN4hhf-xJsiZD1TH6EWBmPI8Iao50ejIHipp4JL4Tf7SNefSxD0ZrWG7aHjdzW_jOii37P7_ZNRatLZufyGS9YCnAtRHUS9sRyE_187I63Odoh0hXLsI9aXn4eU",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExkKxH6AEhY4aq7YJyDagm0xz2PKd2198OkHxlaP6Ce2XqldpbpeJjXlWCc0hzgRzaksIaVGxyehvPPjt22m3IUoKH0jtx2l-EvakNCO5h4tfpZDlzvtHEX80ytFBS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfr4f46UsESF47hg1A-mtFR54a7hTfpCCqtlGaRMXFe0RmqtcUvcdERWz9DT3Xb00jru_beGp1b3NrqQFcpHzoy_-fikAX3p3-F3CYU5cNvqqtvj-pKRBzUd-9U_MIqOvgjmBjU4_L9qyhIOTt",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgl0Cef2ekZ0eOEJsteWIY8N5_wfndupSCId3CKldrcYHCAAzq4-L61id_ceDWtO0EBHmIvJz892XN35H0K_CdX03bqu8N1JKZ3YEqQd5BTec6A_xdNXfR92KJ4PB_K7wDyZxX3A==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "ResearchBench",
    "paperLink": "https://arxiv.org/abs/2503.21248",
    "description": "A large-scale benchmark for evaluating LLMs on scientific discovery via task decomposition: inspiration retrieval, hypothesis composition, and hypothesis ranking, across 12 disciplines.",
    "authors": [
      "Yujie Liu",
      "Zonglin Yang",
      "Tong Xie",
      "Jinjie Ni",
      "Ben Gao",
      "Yuqiang Li",
      "Shixiang Tang",
      "Wanli Ouyang",
      "Erik Cambria",
      "Dongzhan Zhou"
    ],
    "githubLink": "",
    "itemCount": "Papers from 12 disciplines",
    "source": "arXiv",
    "specs": "Task decomposition, hypothesis generation/ranking",
    "year": "2025",
    "id": "saved-1769638817151-vcn9c",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSU-2Kv5DvQ8Ic7f7gKJbLoezaqaaHYWwTf64mI7Jjv4CHNG8w8wcNOtgfsfXRu_6bW5JIqocNgDgHq_kYgSKfoc40UOBUnhcaJSA-dDXreAmLbNBkEAH-PJmBYx08_GbrG27L4eeg4cpMGIPbG5NL6227LZ5Be39ziqG-gDuHIscCe0_W6vz-Rjh149GFcuSRMpyYI-E0wwf02Gq_vZ-MIt2uL6W-QrxAD_lRKr0ginE6hcgsf2OLZOZW47DioapIT-JS7V5nUQ==",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8AuImro3mXR9fs6LHSuXMj3pnxJCZPIV4MyZp3q9uRp_yPAW6tAU2vWMyg-VpYXY-KzHdjckYcyQBRJUguE3ADPbBw3vjjkmm2xWT4pgLj7XF2heDM1sjdQm9UA==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUUuunrwR_Z9yb0X_pBu54vY6nEnYt_b0ZRMMSvCbtBUHi2ZPHq9fpGAhpr5hqIcoFT77sX10ZNQByIefDGazik22aHxo7DWbbl7IiVhfkyBX3fArbN6_egdF6zGKpVSy9V8dzxQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE20gXYrmQZT5A74KnL3cL-PyI0WDBCART5i4M3jgJuUFDFqokbhJjp_zSToYKkqffdON9LmwTT4IB2d0B4V0P1GudCpjR2DZdNAu2qy9CI1EN8azTHOTFikrWFsiDxBcKXRvU8RLwbdWZL4NPePY28vY52EQeqcU-qDQku76f7HYugvFlS",
        "title": "materialsproject.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaMIbUHrfd7x6wUuks51XwZ7qoTCyxMpkPl4GdyZl1hpc3NgwOz-rVeJhayMSMzqAd_3GPUVs19EI-kHWLiWa5k2KNifZpohP7cWJCLoIn33lutIEnw14DWEnMURcatt9E_-34VvWedj7-PSoQ6WxtlrpJjNdHiWX2nMRSsgZ9u3KsDVNlCIzaE8NP9BJ_jTTx7HVH-ocPJt7lPL7p4M10dRE0VPAncjWPouqhNNeiKOVJNQ2L5WnS-cm5pC47MFyjpBjIr3H9L3mTWySePHix_Z-1",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNwsa_SKRGVxJ4WmnUQ0hbPjjhbh4VieBOtLdOpXrT13u40J8qUP9eqTDF7zu5gjSeUf_Snh6i0kZ-x_rZQqtLcyvQ657WzUD9o8uqQI2ajv71rHR1cuvwFVJhQCSRrmwzmi8kjKhpaLAPqKhB2C2z",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdJ7-0LLL9o6TUPQWB5gM8T9F-hkOEDm3e88YTmYvTmLBqRGVHeBrWO3fHyWcretGUs6u_N6R3IJFceWpYYKp1lZ2MeB2OsXD6vKMzJRcEvnHWcgU9J55mxqQFxNinZwG3_P0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQrvzsZBT8flkqEWJGICZGepcItgRAPvk-8apZ21UU09l1ypPOSLWbPKvmOx7AYkfdedIq0DKXUdZuP7lAS17WvXoXoxvWWXLirF90Wu04stxq1sCIdfoBKfVOW6vo",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFErr9Fpbnp0s5SWT5ZMkO7fT6q8btY14vvE9AB7TRnsYSjqy7ygJN4hhf-xJsiZD1TH6EWBmPI8Iao50ejIHipp4JL4Tf7SNefSxD0ZrWG7aHjdzW_jOii37P7_ZNRatLZufyGS9YCnAtRHUS9sRyE_187I63Odoh0hXLsI9aXn4eU",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExkKxH6AEhY4aq7YJyDagm0xz2PKd2198OkHxlaP6Ce2XqldpbpeJjXlWCc0hzgRzaksIaVGxyehvPPjt22m3IUoKH0jtx2l-EvakNCO5h4tfpZDlzvtHEX80ytFBS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfr4f46UsESF47hg1A-mtFR54a7hTfpCCqtlGaRMXFe0RmqtcUvcdERWz9DT3Xb00jru_beGp1b3NrqQFcpHzoy_-fikAX3p3-F3CYU5cNvqqtvj-pKRBzUd-9U_MIqOvgjmBjU4_L9qyhIOTt",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgl0Cef2ekZ0eOEJsteWIY8N5_wfndupSCId3CKldrcYHCAAzq4-L61id_ceDWtO0EBHmIvJz892XN35H0K_CdX03bqu8N1JKZ3YEqQd5BTec6A_xdNXfR92KJ4PB_K7wDyZxX3A==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "SciKnowEval",
    "paperLink": "https://arxiv.org/abs/2406.09096",
    "description": "A framework evaluating LLMs across five progressive levels of scientific knowledge: memory, comprehension, reasoning, discernment, and application. The dataset spans Biology, Chemistry, Physics, and Materials Science.",
    "authors": [
      "Kehua Feng",
      "Keyan Ding",
      "Weijie Wang",
      "Xiang Zhuang",
      "Zeyuan Wang",
      "Ming Qin",
      "Yu Zhao",
      "Jianhua Yao",
      "Qiang Zhang",
      "Huajun Chen"
    ],
    "githubLink": "https://github.com/HICAI-ZJU/SciKnowEval",
    "itemCount": "50,000+ samples",
    "source": "arXiv",
    "specs": "JSON format, multi-level QA",
    "year": "2024",
    "id": "saved-1769638817151-wwzzx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSU-2Kv5DvQ8Ic7f7gKJbLoezaqaaHYWwTf64mI7Jjv4CHNG8w8wcNOtgfsfXRu_6bW5JIqocNgDgHq_kYgSKfoc40UOBUnhcaJSA-dDXreAmLbNBkEAH-PJmBYx08_GbrG27L4eeg4cpMGIPbG5NL6227LZ5Be39ziqG-gDuHIscCe0_W6vz-Rjh149GFcuSRMpyYI-E0wwf02Gq_vZ-MIt2uL6W-QrxAD_lRKr0ginE6hcgsf2OLZOZW47DioapIT-JS7V5nUQ==",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8AuImro3mXR9fs6LHSuXMj3pnxJCZPIV4MyZp3q9uRp_yPAW6tAU2vWMyg-VpYXY-KzHdjckYcyQBRJUguE3ADPbBw3vjjkmm2xWT4pgLj7XF2heDM1sjdQm9UA==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUUuunrwR_Z9yb0X_pBu54vY6nEnYt_b0ZRMMSvCbtBUHi2ZPHq9fpGAhpr5hqIcoFT77sX10ZNQByIefDGazik22aHxo7DWbbl7IiVhfkyBX3fArbN6_egdF6zGKpVSy9V8dzxQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE20gXYrmQZT5A74KnL3cL-PyI0WDBCART5i4M3jgJuUFDFqokbhJjp_zSToYKkqffdON9LmwTT4IB2d0B4V0P1GudCpjR2DZdNAu2qy9CI1EN8azTHOTFikrWFsiDxBcKXRvU8RLwbdWZL4NPePY28vY52EQeqcU-qDQku76f7HYugvFlS",
        "title": "materialsproject.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaMIbUHrfd7x6wUuks51XwZ7qoTCyxMpkPl4GdyZl1hpc3NgwOz-rVeJhayMSMzqAd_3GPUVs19EI-kHWLiWa5k2KNifZpohP7cWJCLoIn33lutIEnw14DWEnMURcatt9E_-34VvWedj7-PSoQ6WxtlrpJjNdHiWX2nMRSsgZ9u3KsDVNlCIzaE8NP9BJ_jTTx7HVH-ocPJt7lPL7p4M10dRE0VPAncjWPouqhNNeiKOVJNQ2L5WnS-cm5pC47MFyjpBjIr3H9L3mTWySePHix_Z-1",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNwsa_SKRGVxJ4WmnUQ0hbPjjhbh4VieBOtLdOpXrT13u40J8qUP9eqTDF7zu5gjSeUf_Snh6i0kZ-x_rZQqtLcyvQ657WzUD9o8uqQI2ajv71rHR1cuvwFVJhQCSRrmwzmi8kjKhpaLAPqKhB2C2z",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdJ7-0LLL9o6TUPQWB5gM8T9F-hkOEDm3e88YTmYvTmLBqRGVHeBrWO3fHyWcretGUs6u_N6R3IJFceWpYYKp1lZ2MeB2OsXD6vKMzJRcEvnHWcgU9J55mxqQFxNinZwG3_P0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQrvzsZBT8flkqEWJGICZGepcItgRAPvk-8apZ21UU09l1ypPOSLWbPKvmOx7AYkfdedIq0DKXUdZuP7lAS17WvXoXoxvWWXLirF90Wu04stxq1sCIdfoBKfVOW6vo",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFErr9Fpbnp0s5SWT5ZMkO7fT6q8btY14vvE9AB7TRnsYSjqy7ygJN4hhf-xJsiZD1TH6EWBmPI8Iao50ejIHipp4JL4Tf7SNefSxD0ZrWG7aHjdzW_jOii37P7_ZNRatLZufyGS9YCnAtRHUS9sRyE_187I63Odoh0hXLsI9aXn4eU",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExkKxH6AEhY4aq7YJyDagm0xz2PKd2198OkHxlaP6Ce2XqldpbpeJjXlWCc0hzgRzaksIaVGxyehvPPjt22m3IUoKH0jtx2l-EvakNCO5h4tfpZDlzvtHEX80ytFBS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfr4f46UsESF47hg1A-mtFR54a7hTfpCCqtlGaRMXFe0RmqtcUvcdERWz9DT3Xb00jru_beGp1b3NrqQFcpHzoy_-fikAX3p3-F3CYU5cNvqqtvj-pKRBzUd-9U_MIqOvgjmBjU4_L9qyhIOTt",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgl0Cef2ekZ0eOEJsteWIY8N5_wfndupSCId3CKldrcYHCAAzq4-L61id_ceDWtO0EBHmIvJz892XN35H0K_CdX03bqu8N1JKZ3YEqQd5BTec6A_xdNXfR92KJ4PB_K7wDyZxX3A==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Matbench Discovery",
    "paperLink": "https://arxiv.org/abs/2308.14920",
    "description": "A framework to evaluate machine learning crystal stability predictions, simulating a high-throughput materials discovery campaign. It ranks models on their ability to predict the stability of unrelaxed crystal structures.",
    "authors": [
      "Janosh Riebesell",
      "Rhys E. A. Goodall",
      "Philipp Benner",
      "Yuan Chiang",
      "Bowen Deng",
      "Alpha A. Lee",
      "Anubhav Jain",
      "Kristin A. Persson"
    ],
    "githubLink": "https://github.com/janosh/matbench-discovery",
    "itemCount": "High-throughput simulation data",
    "source": "arXiv",
    "specs": "Crystal structures (CIF/JSON), stability metrics",
    "year": "2023",
    "id": "saved-1769638817151-gxc45",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSU-2Kv5DvQ8Ic7f7gKJbLoezaqaaHYWwTf64mI7Jjv4CHNG8w8wcNOtgfsfXRu_6bW5JIqocNgDgHq_kYgSKfoc40UOBUnhcaJSA-dDXreAmLbNBkEAH-PJmBYx08_GbrG27L4eeg4cpMGIPbG5NL6227LZ5Be39ziqG-gDuHIscCe0_W6vz-Rjh149GFcuSRMpyYI-E0wwf02Gq_vZ-MIt2uL6W-QrxAD_lRKr0ginE6hcgsf2OLZOZW47DioapIT-JS7V5nUQ==",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8AuImro3mXR9fs6LHSuXMj3pnxJCZPIV4MyZp3q9uRp_yPAW6tAU2vWMyg-VpYXY-KzHdjckYcyQBRJUguE3ADPbBw3vjjkmm2xWT4pgLj7XF2heDM1sjdQm9UA==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUUuunrwR_Z9yb0X_pBu54vY6nEnYt_b0ZRMMSvCbtBUHi2ZPHq9fpGAhpr5hqIcoFT77sX10ZNQByIefDGazik22aHxo7DWbbl7IiVhfkyBX3fArbN6_egdF6zGKpVSy9V8dzxQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE20gXYrmQZT5A74KnL3cL-PyI0WDBCART5i4M3jgJuUFDFqokbhJjp_zSToYKkqffdON9LmwTT4IB2d0B4V0P1GudCpjR2DZdNAu2qy9CI1EN8azTHOTFikrWFsiDxBcKXRvU8RLwbdWZL4NPePY28vY52EQeqcU-qDQku76f7HYugvFlS",
        "title": "materialsproject.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaMIbUHrfd7x6wUuks51XwZ7qoTCyxMpkPl4GdyZl1hpc3NgwOz-rVeJhayMSMzqAd_3GPUVs19EI-kHWLiWa5k2KNifZpohP7cWJCLoIn33lutIEnw14DWEnMURcatt9E_-34VvWedj7-PSoQ6WxtlrpJjNdHiWX2nMRSsgZ9u3KsDVNlCIzaE8NP9BJ_jTTx7HVH-ocPJt7lPL7p4M10dRE0VPAncjWPouqhNNeiKOVJNQ2L5WnS-cm5pC47MFyjpBjIr3H9L3mTWySePHix_Z-1",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNwsa_SKRGVxJ4WmnUQ0hbPjjhbh4VieBOtLdOpXrT13u40J8qUP9eqTDF7zu5gjSeUf_Snh6i0kZ-x_rZQqtLcyvQ657WzUD9o8uqQI2ajv71rHR1cuvwFVJhQCSRrmwzmi8kjKhpaLAPqKhB2C2z",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdJ7-0LLL9o6TUPQWB5gM8T9F-hkOEDm3e88YTmYvTmLBqRGVHeBrWO3fHyWcretGUs6u_N6R3IJFceWpYYKp1lZ2MeB2OsXD6vKMzJRcEvnHWcgU9J55mxqQFxNinZwG3_P0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQrvzsZBT8flkqEWJGICZGepcItgRAPvk-8apZ21UU09l1ypPOSLWbPKvmOx7AYkfdedIq0DKXUdZuP7lAS17WvXoXoxvWWXLirF90Wu04stxq1sCIdfoBKfVOW6vo",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFErr9Fpbnp0s5SWT5ZMkO7fT6q8btY14vvE9AB7TRnsYSjqy7ygJN4hhf-xJsiZD1TH6EWBmPI8Iao50ejIHipp4JL4Tf7SNefSxD0ZrWG7aHjdzW_jOii37P7_ZNRatLZufyGS9YCnAtRHUS9sRyE_187I63Odoh0hXLsI9aXn4eU",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExkKxH6AEhY4aq7YJyDagm0xz2PKd2198OkHxlaP6Ce2XqldpbpeJjXlWCc0hzgRzaksIaVGxyehvPPjt22m3IUoKH0jtx2l-EvakNCO5h4tfpZDlzvtHEX80ytFBS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfr4f46UsESF47hg1A-mtFR54a7hTfpCCqtlGaRMXFe0RmqtcUvcdERWz9DT3Xb00jru_beGp1b3NrqQFcpHzoy_-fikAX3p3-F3CYU5cNvqqtvj-pKRBzUd-9U_MIqOvgjmBjU4_L9qyhIOTt",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgl0Cef2ekZ0eOEJsteWIY8N5_wfndupSCId3CKldrcYHCAAzq4-L61id_ceDWtO0EBHmIvJz892XN35H0K_CdX03bqu8N1JKZ3YEqQd5BTec6A_xdNXfR92KJ4PB_K7wDyZxX3A==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "SRSD (Symbolic Regression for Scientific Discovery)",
    "paperLink": "https://arxiv.org/abs/2206.10540",
    "description": "A benchmark for symbolic regression focused on scientific discovery, containing 120 datasets based on Feynman lectures with physically meaningful sampling ranges.",
    "authors": [
      "Yoshitomo Matsubara",
      "Naoya Chiba",
      "Ryo Igarashi",
      "Yoshitaka Ushiku"
    ],
    "githubLink": "https://github.com/omron-sinicx/srsd-benchmark",
    "itemCount": "120 datasets",
    "source": "arXiv",
    "specs": "Numerical datasets, equation discovery",
    "year": "2022",
    "id": "saved-1769638817151-z35xh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSU-2Kv5DvQ8Ic7f7gKJbLoezaqaaHYWwTf64mI7Jjv4CHNG8w8wcNOtgfsfXRu_6bW5JIqocNgDgHq_kYgSKfoc40UOBUnhcaJSA-dDXreAmLbNBkEAH-PJmBYx08_GbrG27L4eeg4cpMGIPbG5NL6227LZ5Be39ziqG-gDuHIscCe0_W6vz-Rjh149GFcuSRMpyYI-E0wwf02Gq_vZ-MIt2uL6W-QrxAD_lRKr0ginE6hcgsf2OLZOZW47DioapIT-JS7V5nUQ==",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8AuImro3mXR9fs6LHSuXMj3pnxJCZPIV4MyZp3q9uRp_yPAW6tAU2vWMyg-VpYXY-KzHdjckYcyQBRJUguE3ADPbBw3vjjkmm2xWT4pgLj7XF2heDM1sjdQm9UA==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUUuunrwR_Z9yb0X_pBu54vY6nEnYt_b0ZRMMSvCbtBUHi2ZPHq9fpGAhpr5hqIcoFT77sX10ZNQByIefDGazik22aHxo7DWbbl7IiVhfkyBX3fArbN6_egdF6zGKpVSy9V8dzxQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE20gXYrmQZT5A74KnL3cL-PyI0WDBCART5i4M3jgJuUFDFqokbhJjp_zSToYKkqffdON9LmwTT4IB2d0B4V0P1GudCpjR2DZdNAu2qy9CI1EN8azTHOTFikrWFsiDxBcKXRvU8RLwbdWZL4NPePY28vY52EQeqcU-qDQku76f7HYugvFlS",
        "title": "materialsproject.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaMIbUHrfd7x6wUuks51XwZ7qoTCyxMpkPl4GdyZl1hpc3NgwOz-rVeJhayMSMzqAd_3GPUVs19EI-kHWLiWa5k2KNifZpohP7cWJCLoIn33lutIEnw14DWEnMURcatt9E_-34VvWedj7-PSoQ6WxtlrpJjNdHiWX2nMRSsgZ9u3KsDVNlCIzaE8NP9BJ_jTTx7HVH-ocPJt7lPL7p4M10dRE0VPAncjWPouqhNNeiKOVJNQ2L5WnS-cm5pC47MFyjpBjIr3H9L3mTWySePHix_Z-1",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNwsa_SKRGVxJ4WmnUQ0hbPjjhbh4VieBOtLdOpXrT13u40J8qUP9eqTDF7zu5gjSeUf_Snh6i0kZ-x_rZQqtLcyvQ657WzUD9o8uqQI2ajv71rHR1cuvwFVJhQCSRrmwzmi8kjKhpaLAPqKhB2C2z",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdJ7-0LLL9o6TUPQWB5gM8T9F-hkOEDm3e88YTmYvTmLBqRGVHeBrWO3fHyWcretGUs6u_N6R3IJFceWpYYKp1lZ2MeB2OsXD6vKMzJRcEvnHWcgU9J55mxqQFxNinZwG3_P0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQrvzsZBT8flkqEWJGICZGepcItgRAPvk-8apZ21UU09l1ypPOSLWbPKvmOx7AYkfdedIq0DKXUdZuP7lAS17WvXoXoxvWWXLirF90Wu04stxq1sCIdfoBKfVOW6vo",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFErr9Fpbnp0s5SWT5ZMkO7fT6q8btY14vvE9AB7TRnsYSjqy7ygJN4hhf-xJsiZD1TH6EWBmPI8Iao50ejIHipp4JL4Tf7SNefSxD0ZrWG7aHjdzW_jOii37P7_ZNRatLZufyGS9YCnAtRHUS9sRyE_187I63Odoh0hXLsI9aXn4eU",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExkKxH6AEhY4aq7YJyDagm0xz2PKd2198OkHxlaP6Ce2XqldpbpeJjXlWCc0hzgRzaksIaVGxyehvPPjt22m3IUoKH0jtx2l-EvakNCO5h4tfpZDlzvtHEX80ytFBS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfr4f46UsESF47hg1A-mtFR54a7hTfpCCqtlGaRMXFe0RmqtcUvcdERWz9DT3Xb00jru_beGp1b3NrqQFcpHzoy_-fikAX3p3-F3CYU5cNvqqtvj-pKRBzUd-9U_MIqOvgjmBjU4_L9qyhIOTt",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgl0Cef2ekZ0eOEJsteWIY8N5_wfndupSCId3CKldrcYHCAAzq4-L61id_ceDWtO0EBHmIvJz892XN35H0K_CdX03bqu8N1JKZ3YEqQd5BTec6A_xdNXfR92KJ4PB_K7wDyZxX3A==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Therapeutics Data Commons (TDC)",
    "paperLink": "https://arxiv.org/abs/2102.09548",
    "description": "A large collection of AI-ready datasets and tasks for drug discovery and development, covering small molecules, biologics, and gene editing across various stages of the therapeutic pipeline.",
    "authors": [
      "Kexin Huang",
      "Tianfan Fu",
      "Wenhao Gao",
      "Yue Zhao",
      "Yusuf Roohani",
      "Jure Leskovec",
      "Connor W. Coley",
      "Cao Xiao",
      "Jimeng Sun",
      "Marinka Zitnik"
    ],
    "githubLink": "https://github.com/mims-harvard/TDC",
    "itemCount": "66+ datasets",
    "source": "arXiv",
    "specs": "CSV/Pandas DataFrames, molecular graphs, sequences",
    "year": "2021",
    "id": "saved-1769638817151-b1e61",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSU-2Kv5DvQ8Ic7f7gKJbLoezaqaaHYWwTf64mI7Jjv4CHNG8w8wcNOtgfsfXRu_6bW5JIqocNgDgHq_kYgSKfoc40UOBUnhcaJSA-dDXreAmLbNBkEAH-PJmBYx08_GbrG27L4eeg4cpMGIPbG5NL6227LZ5Be39ziqG-gDuHIscCe0_W6vz-Rjh149GFcuSRMpyYI-E0wwf02Gq_vZ-MIt2uL6W-QrxAD_lRKr0ginE6hcgsf2OLZOZW47DioapIT-JS7V5nUQ==",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8AuImro3mXR9fs6LHSuXMj3pnxJCZPIV4MyZp3q9uRp_yPAW6tAU2vWMyg-VpYXY-KzHdjckYcyQBRJUguE3ADPbBw3vjjkmm2xWT4pgLj7XF2heDM1sjdQm9UA==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUUuunrwR_Z9yb0X_pBu54vY6nEnYt_b0ZRMMSvCbtBUHi2ZPHq9fpGAhpr5hqIcoFT77sX10ZNQByIefDGazik22aHxo7DWbbl7IiVhfkyBX3fArbN6_egdF6zGKpVSy9V8dzxQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE20gXYrmQZT5A74KnL3cL-PyI0WDBCART5i4M3jgJuUFDFqokbhJjp_zSToYKkqffdON9LmwTT4IB2d0B4V0P1GudCpjR2DZdNAu2qy9CI1EN8azTHOTFikrWFsiDxBcKXRvU8RLwbdWZL4NPePY28vY52EQeqcU-qDQku76f7HYugvFlS",
        "title": "materialsproject.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaMIbUHrfd7x6wUuks51XwZ7qoTCyxMpkPl4GdyZl1hpc3NgwOz-rVeJhayMSMzqAd_3GPUVs19EI-kHWLiWa5k2KNifZpohP7cWJCLoIn33lutIEnw14DWEnMURcatt9E_-34VvWedj7-PSoQ6WxtlrpJjNdHiWX2nMRSsgZ9u3KsDVNlCIzaE8NP9BJ_jTTx7HVH-ocPJt7lPL7p4M10dRE0VPAncjWPouqhNNeiKOVJNQ2L5WnS-cm5pC47MFyjpBjIr3H9L3mTWySePHix_Z-1",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNwsa_SKRGVxJ4WmnUQ0hbPjjhbh4VieBOtLdOpXrT13u40J8qUP9eqTDF7zu5gjSeUf_Snh6i0kZ-x_rZQqtLcyvQ657WzUD9o8uqQI2ajv71rHR1cuvwFVJhQCSRrmwzmi8kjKhpaLAPqKhB2C2z",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdJ7-0LLL9o6TUPQWB5gM8T9F-hkOEDm3e88YTmYvTmLBqRGVHeBrWO3fHyWcretGUs6u_N6R3IJFceWpYYKp1lZ2MeB2OsXD6vKMzJRcEvnHWcgU9J55mxqQFxNinZwG3_P0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQrvzsZBT8flkqEWJGICZGepcItgRAPvk-8apZ21UU09l1ypPOSLWbPKvmOx7AYkfdedIq0DKXUdZuP7lAS17WvXoXoxvWWXLirF90Wu04stxq1sCIdfoBKfVOW6vo",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFErr9Fpbnp0s5SWT5ZMkO7fT6q8btY14vvE9AB7TRnsYSjqy7ygJN4hhf-xJsiZD1TH6EWBmPI8Iao50ejIHipp4JL4Tf7SNefSxD0ZrWG7aHjdzW_jOii37P7_ZNRatLZufyGS9YCnAtRHUS9sRyE_187I63Odoh0hXLsI9aXn4eU",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExkKxH6AEhY4aq7YJyDagm0xz2PKd2198OkHxlaP6Ce2XqldpbpeJjXlWCc0hzgRzaksIaVGxyehvPPjt22m3IUoKH0jtx2l-EvakNCO5h4tfpZDlzvtHEX80ytFBS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfr4f46UsESF47hg1A-mtFR54a7hTfpCCqtlGaRMXFe0RmqtcUvcdERWz9DT3Xb00jru_beGp1b3NrqQFcpHzoy_-fikAX3p3-F3CYU5cNvqqtvj-pKRBzUd-9U_MIqOvgjmBjU4_L9qyhIOTt",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgl0Cef2ekZ0eOEJsteWIY8N5_wfndupSCId3CKldrcYHCAAzq4-L61id_ceDWtO0EBHmIvJz892XN35H0K_CdX03bqu8N1JKZ3YEqQd5BTec6A_xdNXfR92KJ4PB_K7wDyZxX3A==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "RSFAKE-1M",
    "paperLink": "https://arxiv.org/abs/2505.23283",
    "description": "A large-scale dataset designed for detecting diffusion-generated forgeries in remote sensing imagery. It contains 1 million images (500k real, 500k fake) covering diverse generation conditions such as text-to-image, structure-guided generation, and inpainting.",
    "authors": [
      "Zhihong Tan",
      "Jiayi Wang",
      "Huiying Shi",
      "Zhenzhong Chen"
    ],
    "githubLink": "https://huggingface.co/datasets/TZHSW/RSFAKE",
    "itemCount": "1,000,000 images",
    "source": "Hugging Face",
    "specs": "Remote sensing imagery, Diffusion-generated forgeries, Resolutions: 256x256, 512x512, 768x768",
    "year": "2025",
    "id": "imported-json-1769638825346-5-8kf5j",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "DocTamper",
    "paperLink": "https://openaccess.thecvf.com/content/CVPR2023/html/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2023_paper.html",
    "description": "A large-scale document manipulation dataset focused on tampered text detection. It provides pixel-level annotations for various tampering techniques like copy-move, splicing, and generation within document images.",
    "authors": [
      "Chenfan Qu",
      "Chongyu Liu",
      "Yuliang Liu",
      "Xinhong Chen",
      "Dezhi Peng",
      "Fengjun Guo",
      "Lianwen Jin"
    ],
    "githubLink": "https://github.com/qcf-568/DocTamper",
    "itemCount": "170,000 document images",
    "source": "Scholar",
    "specs": "Document images, Pixel-level annotations, Chinese and English text",
    "year": "2023",
    "id": "imported-json-1769638825346-6-0k8io",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "DF2023",
    "paperLink": "https://zenodo.org/records/7326540",
    "description": "A massive training and validation dataset for general image forgery detection. It encompasses four major forgery categories: splicing, copy-move, enhancement, and removal, aiming to standardize training for deep learning models.",
    "authors": [
      "David Fischinger",
      "Martin Boyer"
    ],
    "githubLink": "https://zenodo.org/records/7326540",
    "itemCount": "1,000,000 images",
    "source": "Scholar",
    "specs": "Images, Splicing, Copy-move, Enhancement, Removal",
    "year": "2023",
    "id": "imported-json-1769638825346-7-ucn5q",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "CocoGlide",
    "paperLink": "https://openaccess.thecvf.com/content/CVPR2023/html/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.html",
    "description": "A validation dataset created to evaluate forgery detection on AI-generated content. It consists of images manipulated using the GLIDE diffusion model, serving as a benchmark for modern generative attacks.",
    "authors": [
      "Fabrizio Guillaro",
      "Davide Cozzolino",
      "Avneesh Sud",
      "Nicholas Dufour",
      "Luisa Verdoliva"
    ],
    "githubLink": "https://github.com/grip-unina/TruFor",
    "itemCount": "512 images",
    "source": "GitHub",
    "specs": "Images, Diffusion-based manipulation (GLIDE)",
    "year": "2023",
    "id": "imported-json-1769638825346-8-obzpr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "DeeperForensics-1.0",
    "paperLink": "https://arxiv.org/abs/2001.03024",
    "description": "A large-scale dataset for real-world face forgery detection. It features extensive real-world perturbations and a hidden test set to better simulate wild scenarios compared to previous benchmarks.",
    "authors": [
      "Liming Jiang",
      "Ren Li",
      "Wayne Wu",
      "Chen Qian",
      "Chen Change Loy"
    ],
    "githubLink": "https://github.com/EndlessSora/DeeperForensics-1.0",
    "itemCount": "60,000 videos (17.6 million frames)",
    "source": "arXiv",
    "specs": "Video, Face swapping, Real-world perturbations",
    "year": "2020",
    "id": "imported-json-1769638825346-9-wrsdu",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "FaceForensics++",
    "paperLink": "https://arxiv.org/abs/1901.08971",
    "description": "A prominent benchmark for facial manipulation detection. It contains video sequences manipulated with four automated methods (Deepfakes, Face2Face, FaceSwap, NeuralTextures) at different compression quality levels.",
    "authors": [
      "Andreas Rössler",
      "Davide Cozzolino",
      "Luisa Verdoliva",
      "Christian Riess",
      "Justus Thies",
      "Matthias Nießner"
    ],
    "githubLink": "https://github.com/ondyari/FaceForensics",
    "itemCount": "4,000 manipulated videos (derived from 1,000 original sequences)",
    "source": "arXiv",
    "specs": "Video, Facial manipulation, C0/C23/C40 compression levels",
    "year": "2019",
    "id": "imported-json-1769638825346-10-vq9i7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "DEFACTO",
    "paperLink": "https://ieeexplore.ieee.org/document/8903178",
    "description": "A synthetic dataset automatically generated from MS-COCO to produce semantically meaningful forgeries. It covers four types of manipulations: splicing, copy-move, object removal, and morphing.",
    "authors": [
      "Gaël Mahfoudi",
      "Badr Tajini",
      "Florent Retraint",
      "Frédéric Morain-Nicolier",
      "Jean Luc Dugelay",
      "Marc Pic"
    ],
    "githubLink": "https://defactodataset.github.io/",
    "itemCount": "~200,000 images",
    "source": "Scholar",
    "specs": "Images, Splicing, Copy-move, Removal, Morphing, MS-COCO base",
    "year": "2019",
    "id": "imported-json-1769638825346-11-l5p9b",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "CASIA v2.0",
    "paperLink": "https://ieeexplore.ieee.org/document/6637622",
    "description": "A classic and widely used benchmark dataset for image tampering detection. It includes a balanced set of authentic and tampered images focusing on splicing and copy-move forgeries.",
    "authors": [
      "Jing Dong",
      "Wei Wang",
      "Tieniu Tan"
    ],
    "githubLink": "https://github.com/namtpham/casia2groundtruth",
    "itemCount": "12,614 images",
    "source": "Scholar",
    "specs": "Images (JPEG, TIFF, BMP), Splicing, Copy-move",
    "year": "2013",
    "id": "imported-json-1769638825346-12-8x3i0",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGeWbGmzDq3k9B75-5KrtqmL9ZhLBpp3ptUi6c_KxoVdejNKhtR83oe3YbLDJ1euRfx4JkVqC_SSdF6Qef5KlOp9MZJC7aVeSw07bWt1atM2EZYoS-0uId0PSB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwoSz1IQHZTTWKIe41tzGdMgqs6Iaq-kbgxp1uUCG5sHQcZWzX0x1EL1vOxWIi3REI9SYc2pUg9W2MyxwsYSK-_w0vxHfbLITLs9_LjneEIa8LkaYYcaX_xwaKK0A_b87CkmLsIt1SdlvQn0lI7EbMEWwHHD7kae1Wl0Gv5njM8gehzOOupK_mNv_in9pCdY6sU8Q7c-m8uIC481DxfuShjfmK54_BT-mOFTqmyj76dZnb7SaOmToOSA0Xqcm_dfTvzeY=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpaBhE5bZREbKizHyXVxBghZ66Jdk5cd6GWRzTh5NAB2qOXfoXTTvod9yShLhj3LjKIyXGJlbzczrjcPJdGGi1GvZyE5EmdRY6BW-SJ4NJUMF54jjrn81VJtjq2TtFZHZ9JD8goOdBgUWnWQDE3cenO2O2nGgt1oCEUacsJyRqLLY7kCExPjl7ykr_k4dmeqZNm7w8H1juusxjIirz9xGN8tuAFUqH1slH0nwXUjDA5-3Vym7mES1RBGZaJg==",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP3aCXWPEyJwqaGTUXjVh0Lil11vW3iBG3w6Vl6XfUXHyiFA-QkNDL1s9SNXbZcf-O46w5S3XNVfNl581Q3t_s2yHDYkYPJ9qZm3fgciYkpSxHctqQKEnw40CEwiXFlXOXo8CJpao=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGneGJ5zkdb6Rn0jddm9xPdJvayo3_htqv6lFjOmLvr-QQVWU-2roD5eAUapBpMy2eHeRB0lZtsi2Abu9zbu0wUTX9O9QWU8Kj1WFbmEdKHqbPkbSECjyOzdsnHIZeZWKzaIIKsPEIPAVQWfE0=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFgyeubQk_Cm3CmxjHxipMiLM35YdDsHd9EHPX25ZHlkrLJbYUA7FnjLy6GcZGysEXPdSISo-rbHjYyKVXWph6tqduH5oE-affR5ieFpMGkwEUKzUalXRLoFYayl5IHiiLDcW05YkXW-1F5rXmyw0Swi9AEDOibsgQoTqYs16PCQGNDzqEOodxR-7QytxdMUBsoz9zYdoqiXmU55hgdgIggqjMsqpjMEbabpDbkdwHP9wxhlgiE9cBKGrGQce7piw_SsCmVq7VD0F7ey_7",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRpHIMLEtHh9OLtFPEwlruiFd44CIR6PoCF_BmFsSGIBymHdLvEi2fuoQ8bau5NwVV1gi2zADzg0XFTl4rnhR23lSqgb8Lc0NfnPHqU8rcch06A45WtcqWjxFQVMZ3dlg023avA9058rHqouoDYXozrA4zTTq3GFlXW7iSy369zO8NBFTA_Try7J5sp3a9JfF5VgOj2FklmgsSc-xcqQpsUKTBF_dxCiTxfn7t5utd8j1j0AVaZ6M_ZcJTao2OqAkvFCrC-6cR0A==",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJ-xy0m4qIha8yUVSPyNOokzwgRB-C-dTPwYayuoB6kJbx0oaHEgx8_bX23321SUr8Uefo3VVVXuGWQa55XtF71dA1UJFhiyEfgw2YJJ2g4wVlPESrHm7LgffLQFox0J-F4gHhkCQ_znsH2AOZpewmBK6gRxLXI4-nKjSK40BUDRowcySW1dj5UXZP1o4NlqxGijYlxeQKwNTLurB0QyV_SkYoGA==",
        "title": "ait.ac.at"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPaR5XJd_gdSa7_2n889VN25BSYIv_1ZGFY3KiOVu-1mAMJLwAQCJjV-zNu7fWxqlZ5I9wlSdt5iuKZfsow4Nu-JykgAhufabm6PCrazhZf7bDZVzEk2e02e4eHkvqBJN3vUj46lPt9sq42u5_c5XMEJ6EEHivNzKZIrUYxQ==",
        "title": "eurecom.fr"
      }
    ]
  },
  {
    "title": "Deepfake-Eval-2024",
    "paperLink": "https://arxiv.org/abs/2503.00000",
    "description": "A multi-modal in-the-wild benchmark consisting of deepfakes collected from social media and detection platforms in 2024. It covers diverse media content from 88 different websites in 52 languages, designed to evaluate the robustness of detection models against contemporary manipulation technologies.",
    "authors": [
      "Nuria Chandra",
      "et al."
    ],
    "githubLink": "https://github.com/nuriachandra/Deepfake-Eval-2024",
    "itemCount": "44 hours video, 56.5 hours audio, 1,975 images",
    "source": "Hugging Face",
    "specs": "Video, Audio, Image (Multi-modal)",
    "year": "2025",
    "id": "imported-json-1769638825347-590-p7ft9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2Frml8YUFL-WMwPW96KVHk3BHjJs7bouE3Yq6HGra8tvLimUW_bEvfy-3e1Rgrp1Ja04YQaGWHibKcsS9r2CNQ52aOI4y8fEtP6sEIMsqnK7rAIIxMfnXqkOrb_GjiUFyvdpgRsc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEuqLpenXU65vT5Sd_ezjkzNsc0rWYMG7nWgdvbdVUI8pXOKWLttX5mVhufAnER2L8eDRvKxAy83upt-3Q_M3XA3kAo22zvqK1i6HTbj_RxuDAPjNSbppgqGebwQOEjZbTZC27SP7pO5BgcTF-H3dBCJkmFCeEQaTl1lTn2KTfFLem_RRhxKtvUg77SxnXFXHLzzlJMtlHQfCDIv4VLjg=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8cP0IyfL01vrkfDx2LNSN_iY3CZRIjcYq1raONI9VFIJ1LyO9OMAETL6LTsjYP_M8rJd4HKUcc08geTQzBwEnZGVI_pOXTEagd_GzWToWFdJ4DD0XdyqzaHM07BB51-qq2cCjc4O2_PzpdQvwiXEw18lRWI4lW5gtNGzAEXHeGY4hpAQ7eSEKZu6h-P-Gi3X_HsY=",
        "title": "indiaai.gov.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKBY3FskiAevh0ZMlpd6mbAwqBLCsV4wz5HM1wLDtHX5luToIJiCABGZfsSMK8xqIqr98QnQzllz6Kk40uZw4nyOMk2yj7Yzk1fcP7OqQdYCDml_MOwPx1oNOdTDM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTse7XbKGKWKWXp85kPDuQO2jyHzwoTG8WcyVttTmEmHUU-JnBzpUpMPsdrE4ZpS_nFxQ648qmIViI8O4ZdRPOOEcCrY3DyASqlQhcm0qlLVQPAfeg50uCyteb8X3Ci9Mb9UGB9g==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EsTNKFYKI-PZmvS5DqxbzK6NTXuqQj9Twh0NYMK7v1SmJABw9MdEmIrd-2o9ctasgrun2IPiWw2y4wdDNluiTOFQUgnYEQkg7nE0HQE88Se5oaK075wFY3Rz8K8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Osz-GIINizZgbGZ41tczVNW9auUtV2dSRvjptAbmYt4J9VFIOU0LUowuqN3stnSOt4nQx4qGVeUqyuRqywKdfUXxXNAdHRhnfWEBDQikuraFT_CcEXz0MBLG88KnRwtKXraWdK-hy5Z70M6Wvhn-7NFH6JthYU0rv_UDdPLHuBz5PpGLLFlOGKHyatogEuk=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaixA4YmOOJfJ7TJ_pfmbFS-oBqlrS3Qt8UcafSTxDhGR5C1fQ0z3rlleKtEjt-nBkpbuel0iFKie3yLOQrRXGDluDEhRaiLkQTXmyZ_o5yAOtpI2dsh4DiGQ7jXKEaDgiHAl1Znm8yedKyOGcc997D5PbcFJSUDufXVFi_DA02A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuL-Kvi9VHEN4YF3H2MrGkrj8f8VTIMTAMtEqAtoH1mtP02lv6hXoO6ZGjj3EwoPXygwk1UI2lKGBp4_OVszBjN73-3FCS6l5KH8DnHfPelUOPlYPQOP07rJsg9rPLmevUsNZN2vmZQrzilFOFwzDPAmUdCIyxuvDzBDRSy8QGcWY=",
        "title": "emergentmind.com"
      }
    ]
  },
  {
    "title": "GenImage",
    "paperLink": "https://arxiv.org/abs/2306.08571",
    "description": "A million-scale benchmark for detecting AI-generated images. It includes over one million pairs of real and fake images, with fakes generated by state-of-the-art diffusion models (Stable Diffusion, Midjourney) and GANs (BigGAN, StyleGAN), utilizing ImageNet classes.",
    "authors": [
      "Mingjian Zhu",
      "Hanting Chen",
      "Qiangyu Yan",
      "Xudong Huang",
      "Guanyu Lin",
      "Wei Li",
      "Zhijun Tu",
      "Hailin Hu",
      "Jie Hu",
      "Yunhe Wang"
    ],
    "githubLink": "https://github.com/GenImage-Dataset/GenImage",
    "itemCount": "1,331,167 real / 1,350,000 fake images",
    "source": "arXiv",
    "specs": "Image (Diffusion, GAN)",
    "year": "2023",
    "id": "imported-json-1769638825347-591-3urny",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2Frml8YUFL-WMwPW96KVHk3BHjJs7bouE3Yq6HGra8tvLimUW_bEvfy-3e1Rgrp1Ja04YQaGWHibKcsS9r2CNQ52aOI4y8fEtP6sEIMsqnK7rAIIxMfnXqkOrb_GjiUFyvdpgRsc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEuqLpenXU65vT5Sd_ezjkzNsc0rWYMG7nWgdvbdVUI8pXOKWLttX5mVhufAnER2L8eDRvKxAy83upt-3Q_M3XA3kAo22zvqK1i6HTbj_RxuDAPjNSbppgqGebwQOEjZbTZC27SP7pO5BgcTF-H3dBCJkmFCeEQaTl1lTn2KTfFLem_RRhxKtvUg77SxnXFXHLzzlJMtlHQfCDIv4VLjg=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8cP0IyfL01vrkfDx2LNSN_iY3CZRIjcYq1raONI9VFIJ1LyO9OMAETL6LTsjYP_M8rJd4HKUcc08geTQzBwEnZGVI_pOXTEagd_GzWToWFdJ4DD0XdyqzaHM07BB51-qq2cCjc4O2_PzpdQvwiXEw18lRWI4lW5gtNGzAEXHeGY4hpAQ7eSEKZu6h-P-Gi3X_HsY=",
        "title": "indiaai.gov.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKBY3FskiAevh0ZMlpd6mbAwqBLCsV4wz5HM1wLDtHX5luToIJiCABGZfsSMK8xqIqr98QnQzllz6Kk40uZw4nyOMk2yj7Yzk1fcP7OqQdYCDml_MOwPx1oNOdTDM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTse7XbKGKWKWXp85kPDuQO2jyHzwoTG8WcyVttTmEmHUU-JnBzpUpMPsdrE4ZpS_nFxQ648qmIViI8O4ZdRPOOEcCrY3DyASqlQhcm0qlLVQPAfeg50uCyteb8X3Ci9Mb9UGB9g==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EsTNKFYKI-PZmvS5DqxbzK6NTXuqQj9Twh0NYMK7v1SmJABw9MdEmIrd-2o9ctasgrun2IPiWw2y4wdDNluiTOFQUgnYEQkg7nE0HQE88Se5oaK075wFY3Rz8K8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Osz-GIINizZgbGZ41tczVNW9auUtV2dSRvjptAbmYt4J9VFIOU0LUowuqN3stnSOt4nQx4qGVeUqyuRqywKdfUXxXNAdHRhnfWEBDQikuraFT_CcEXz0MBLG88KnRwtKXraWdK-hy5Z70M6Wvhn-7NFH6JthYU0rv_UDdPLHuBz5PpGLLFlOGKHyatogEuk=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaixA4YmOOJfJ7TJ_pfmbFS-oBqlrS3Qt8UcafSTxDhGR5C1fQ0z3rlleKtEjt-nBkpbuel0iFKie3yLOQrRXGDluDEhRaiLkQTXmyZ_o5yAOtpI2dsh4DiGQ7jXKEaDgiHAl1Znm8yedKyOGcc997D5PbcFJSUDufXVFi_DA02A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuL-Kvi9VHEN4YF3H2MrGkrj8f8VTIMTAMtEqAtoH1mtP02lv6hXoO6ZGjj3EwoPXygwk1UI2lKGBp4_OVszBjN73-3FCS6l5KH8DnHfPelUOPlYPQOP07rJsg9rPLmevUsNZN2vmZQrzilFOFwzDPAmUdCIyxuvDzBDRSy8QGcWY=",
        "title": "emergentmind.com"
      }
    ]
  },
  {
    "title": "DeepFakeFace (DFF)",
    "paperLink": "https://arxiv.org/abs/2309.02218",
    "description": "A dataset focused on the robustness and generalizability of deepfake detection against diffusion models. It contains high-quality faces generated by Stable Diffusion, Inpainting, and InsightFace, structured similarly to the IMDB-WIKI dataset.",
    "authors": [
      "Haixu Song",
      "Shiyu Huang",
      "Yinpeng Dong",
      "Wei-Wei Tu"
    ],
    "githubLink": "https://github.com/OpenRL-Lab/DeepFakeFace",
    "itemCount": "120,000 images",
    "source": "Hugging Face",
    "specs": "Image (Diffusion, Inpainting)",
    "year": "2023",
    "id": "imported-json-1769638825347-592-7k4o3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2Frml8YUFL-WMwPW96KVHk3BHjJs7bouE3Yq6HGra8tvLimUW_bEvfy-3e1Rgrp1Ja04YQaGWHibKcsS9r2CNQ52aOI4y8fEtP6sEIMsqnK7rAIIxMfnXqkOrb_GjiUFyvdpgRsc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEuqLpenXU65vT5Sd_ezjkzNsc0rWYMG7nWgdvbdVUI8pXOKWLttX5mVhufAnER2L8eDRvKxAy83upt-3Q_M3XA3kAo22zvqK1i6HTbj_RxuDAPjNSbppgqGebwQOEjZbTZC27SP7pO5BgcTF-H3dBCJkmFCeEQaTl1lTn2KTfFLem_RRhxKtvUg77SxnXFXHLzzlJMtlHQfCDIv4VLjg=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8cP0IyfL01vrkfDx2LNSN_iY3CZRIjcYq1raONI9VFIJ1LyO9OMAETL6LTsjYP_M8rJd4HKUcc08geTQzBwEnZGVI_pOXTEagd_GzWToWFdJ4DD0XdyqzaHM07BB51-qq2cCjc4O2_PzpdQvwiXEw18lRWI4lW5gtNGzAEXHeGY4hpAQ7eSEKZu6h-P-Gi3X_HsY=",
        "title": "indiaai.gov.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKBY3FskiAevh0ZMlpd6mbAwqBLCsV4wz5HM1wLDtHX5luToIJiCABGZfsSMK8xqIqr98QnQzllz6Kk40uZw4nyOMk2yj7Yzk1fcP7OqQdYCDml_MOwPx1oNOdTDM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTse7XbKGKWKWXp85kPDuQO2jyHzwoTG8WcyVttTmEmHUU-JnBzpUpMPsdrE4ZpS_nFxQ648qmIViI8O4ZdRPOOEcCrY3DyASqlQhcm0qlLVQPAfeg50uCyteb8X3Ci9Mb9UGB9g==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EsTNKFYKI-PZmvS5DqxbzK6NTXuqQj9Twh0NYMK7v1SmJABw9MdEmIrd-2o9ctasgrun2IPiWw2y4wdDNluiTOFQUgnYEQkg7nE0HQE88Se5oaK075wFY3Rz8K8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Osz-GIINizZgbGZ41tczVNW9auUtV2dSRvjptAbmYt4J9VFIOU0LUowuqN3stnSOt4nQx4qGVeUqyuRqywKdfUXxXNAdHRhnfWEBDQikuraFT_CcEXz0MBLG88KnRwtKXraWdK-hy5Z70M6Wvhn-7NFH6JthYU0rv_UDdPLHuBz5PpGLLFlOGKHyatogEuk=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaixA4YmOOJfJ7TJ_pfmbFS-oBqlrS3Qt8UcafSTxDhGR5C1fQ0z3rlleKtEjt-nBkpbuel0iFKie3yLOQrRXGDluDEhRaiLkQTXmyZ_o5yAOtpI2dsh4DiGQ7jXKEaDgiHAl1Znm8yedKyOGcc997D5PbcFJSUDufXVFi_DA02A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuL-Kvi9VHEN4YF3H2MrGkrj8f8VTIMTAMtEqAtoH1mtP02lv6hXoO6ZGjj3EwoPXygwk1UI2lKGBp4_OVszBjN73-3FCS6l5KH8DnHfPelUOPlYPQOP07rJsg9rPLmevUsNZN2vmZQrzilFOFwzDPAmUdCIyxuvDzBDRSy8QGcWY=",
        "title": "emergentmind.com"
      }
    ]
  },
  {
    "title": "In-the-Wild Audio Deepfake Dataset",
    "paperLink": "https://arxiv.org/abs/2210.02437",
    "description": "A dataset of real and synthetic speech recordings of 58 celebrities and politicians collected from online videos. It is designed to test the generalization of audio deepfake detection models beyond laboratory conditions.",
    "authors": [
      "Nicolas M. Müller",
      "Peter Czempin",
      "Franziska Dieckmann",
      "Adam Froghyar",
      "Konstantin Böttinger"
    ],
    "githubLink": "https://huggingface.co/datasets/mueller91/In-The-Wild",
    "itemCount": "31,779 samples (20.8h real / 17.2h fake)",
    "source": "Hugging Face",
    "specs": "Audio (wav)",
    "year": "2022",
    "id": "imported-json-1769638825347-593-qnq8s",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2Frml8YUFL-WMwPW96KVHk3BHjJs7bouE3Yq6HGra8tvLimUW_bEvfy-3e1Rgrp1Ja04YQaGWHibKcsS9r2CNQ52aOI4y8fEtP6sEIMsqnK7rAIIxMfnXqkOrb_GjiUFyvdpgRsc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEuqLpenXU65vT5Sd_ezjkzNsc0rWYMG7nWgdvbdVUI8pXOKWLttX5mVhufAnER2L8eDRvKxAy83upt-3Q_M3XA3kAo22zvqK1i6HTbj_RxuDAPjNSbppgqGebwQOEjZbTZC27SP7pO5BgcTF-H3dBCJkmFCeEQaTl1lTn2KTfFLem_RRhxKtvUg77SxnXFXHLzzlJMtlHQfCDIv4VLjg=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8cP0IyfL01vrkfDx2LNSN_iY3CZRIjcYq1raONI9VFIJ1LyO9OMAETL6LTsjYP_M8rJd4HKUcc08geTQzBwEnZGVI_pOXTEagd_GzWToWFdJ4DD0XdyqzaHM07BB51-qq2cCjc4O2_PzpdQvwiXEw18lRWI4lW5gtNGzAEXHeGY4hpAQ7eSEKZu6h-P-Gi3X_HsY=",
        "title": "indiaai.gov.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKBY3FskiAevh0ZMlpd6mbAwqBLCsV4wz5HM1wLDtHX5luToIJiCABGZfsSMK8xqIqr98QnQzllz6Kk40uZw4nyOMk2yj7Yzk1fcP7OqQdYCDml_MOwPx1oNOdTDM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTse7XbKGKWKWXp85kPDuQO2jyHzwoTG8WcyVttTmEmHUU-JnBzpUpMPsdrE4ZpS_nFxQ648qmIViI8O4ZdRPOOEcCrY3DyASqlQhcm0qlLVQPAfeg50uCyteb8X3Ci9Mb9UGB9g==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EsTNKFYKI-PZmvS5DqxbzK6NTXuqQj9Twh0NYMK7v1SmJABw9MdEmIrd-2o9ctasgrun2IPiWw2y4wdDNluiTOFQUgnYEQkg7nE0HQE88Se5oaK075wFY3Rz8K8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Osz-GIINizZgbGZ41tczVNW9auUtV2dSRvjptAbmYt4J9VFIOU0LUowuqN3stnSOt4nQx4qGVeUqyuRqywKdfUXxXNAdHRhnfWEBDQikuraFT_CcEXz0MBLG88KnRwtKXraWdK-hy5Z70M6Wvhn-7NFH6JthYU0rv_UDdPLHuBz5PpGLLFlOGKHyatogEuk=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaixA4YmOOJfJ7TJ_pfmbFS-oBqlrS3Qt8UcafSTxDhGR5C1fQ0z3rlleKtEjt-nBkpbuel0iFKie3yLOQrRXGDluDEhRaiLkQTXmyZ_o5yAOtpI2dsh4DiGQ7jXKEaDgiHAl1Znm8yedKyOGcc997D5PbcFJSUDufXVFi_DA02A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuL-Kvi9VHEN4YF3H2MrGkrj8f8VTIMTAMtEqAtoH1mtP02lv6hXoO6ZGjj3EwoPXygwk1UI2lKGBp4_OVszBjN73-3FCS6l5KH8DnHfPelUOPlYPQOP07rJsg9rPLmevUsNZN2vmZQrzilFOFwzDPAmUdCIyxuvDzBDRSy8QGcWY=",
        "title": "emergentmind.com"
      }
    ]
  },
  {
    "title": "Celeb-DF (v2)",
    "paperLink": "https://arxiv.org/abs/1909.12963",
    "description": "A challenging large-scale dataset for deepfake forensics, containing high-quality synthesized videos. It aims to bridge the gap in visual quality compared to earlier datasets, featuring reduced visual artifacts.",
    "authors": [
      "Yuezun Li",
      "Xin Yang",
      "Pu Sun",
      "Honggang Qi",
      "Siwei Lyu"
    ],
    "githubLink": "https://github.com/yuezunli/Celeb-DF-v2",
    "itemCount": "590 real / 5,639 fake videos",
    "source": "arXiv",
    "specs": "Video (MPEG4)",
    "year": "2020",
    "id": "imported-json-1769638825347-594-dmmij",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2Frml8YUFL-WMwPW96KVHk3BHjJs7bouE3Yq6HGra8tvLimUW_bEvfy-3e1Rgrp1Ja04YQaGWHibKcsS9r2CNQ52aOI4y8fEtP6sEIMsqnK7rAIIxMfnXqkOrb_GjiUFyvdpgRsc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEuqLpenXU65vT5Sd_ezjkzNsc0rWYMG7nWgdvbdVUI8pXOKWLttX5mVhufAnER2L8eDRvKxAy83upt-3Q_M3XA3kAo22zvqK1i6HTbj_RxuDAPjNSbppgqGebwQOEjZbTZC27SP7pO5BgcTF-H3dBCJkmFCeEQaTl1lTn2KTfFLem_RRhxKtvUg77SxnXFXHLzzlJMtlHQfCDIv4VLjg=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8cP0IyfL01vrkfDx2LNSN_iY3CZRIjcYq1raONI9VFIJ1LyO9OMAETL6LTsjYP_M8rJd4HKUcc08geTQzBwEnZGVI_pOXTEagd_GzWToWFdJ4DD0XdyqzaHM07BB51-qq2cCjc4O2_PzpdQvwiXEw18lRWI4lW5gtNGzAEXHeGY4hpAQ7eSEKZu6h-P-Gi3X_HsY=",
        "title": "indiaai.gov.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKBY3FskiAevh0ZMlpd6mbAwqBLCsV4wz5HM1wLDtHX5luToIJiCABGZfsSMK8xqIqr98QnQzllz6Kk40uZw4nyOMk2yj7Yzk1fcP7OqQdYCDml_MOwPx1oNOdTDM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTse7XbKGKWKWXp85kPDuQO2jyHzwoTG8WcyVttTmEmHUU-JnBzpUpMPsdrE4ZpS_nFxQ648qmIViI8O4ZdRPOOEcCrY3DyASqlQhcm0qlLVQPAfeg50uCyteb8X3Ci9Mb9UGB9g==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EsTNKFYKI-PZmvS5DqxbzK6NTXuqQj9Twh0NYMK7v1SmJABw9MdEmIrd-2o9ctasgrun2IPiWw2y4wdDNluiTOFQUgnYEQkg7nE0HQE88Se5oaK075wFY3Rz8K8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Osz-GIINizZgbGZ41tczVNW9auUtV2dSRvjptAbmYt4J9VFIOU0LUowuqN3stnSOt4nQx4qGVeUqyuRqywKdfUXxXNAdHRhnfWEBDQikuraFT_CcEXz0MBLG88KnRwtKXraWdK-hy5Z70M6Wvhn-7NFH6JthYU0rv_UDdPLHuBz5PpGLLFlOGKHyatogEuk=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaixA4YmOOJfJ7TJ_pfmbFS-oBqlrS3Qt8UcafSTxDhGR5C1fQ0z3rlleKtEjt-nBkpbuel0iFKie3yLOQrRXGDluDEhRaiLkQTXmyZ_o5yAOtpI2dsh4DiGQ7jXKEaDgiHAl1Znm8yedKyOGcc997D5PbcFJSUDufXVFi_DA02A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuL-Kvi9VHEN4YF3H2MrGkrj8f8VTIMTAMtEqAtoH1mtP02lv6hXoO6ZGjj3EwoPXygwk1UI2lKGBp4_OVszBjN73-3FCS6l5KH8DnHfPelUOPlYPQOP07rJsg9rPLmevUsNZN2vmZQrzilFOFwzDPAmUdCIyxuvDzBDRSy8QGcWY=",
        "title": "emergentmind.com"
      }
    ]
  },
  {
    "title": "DeepFake Detection Challenge (DFDC)",
    "paperLink": "https://arxiv.org/abs/2006.07397",
    "description": "A massive dataset created for the DFDC Kaggle competition. It contains over 100,000 clips featuring paid actors in diverse lighting and environments, manipulated with various deepfake methods.",
    "authors": [
      "Brian Dolhansky",
      "Joanna Bitton",
      "Ben Pflaum",
      "Jikuo Lu",
      "Russ Howes",
      "Mike Wang",
      "Cristian Canton Ferrer"
    ],
    "githubLink": "https://www.kaggle.com/c/deepfake-detection-challenge",
    "itemCount": "100,000+ clips (470 GB+)",
    "source": "arXiv",
    "specs": "Video",
    "year": "2020",
    "id": "imported-json-1769638825347-595-6wwx8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2Frml8YUFL-WMwPW96KVHk3BHjJs7bouE3Yq6HGra8tvLimUW_bEvfy-3e1Rgrp1Ja04YQaGWHibKcsS9r2CNQ52aOI4y8fEtP6sEIMsqnK7rAIIxMfnXqkOrb_GjiUFyvdpgRsc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEuqLpenXU65vT5Sd_ezjkzNsc0rWYMG7nWgdvbdVUI8pXOKWLttX5mVhufAnER2L8eDRvKxAy83upt-3Q_M3XA3kAo22zvqK1i6HTbj_RxuDAPjNSbppgqGebwQOEjZbTZC27SP7pO5BgcTF-H3dBCJkmFCeEQaTl1lTn2KTfFLem_RRhxKtvUg77SxnXFXHLzzlJMtlHQfCDIv4VLjg=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8cP0IyfL01vrkfDx2LNSN_iY3CZRIjcYq1raONI9VFIJ1LyO9OMAETL6LTsjYP_M8rJd4HKUcc08geTQzBwEnZGVI_pOXTEagd_GzWToWFdJ4DD0XdyqzaHM07BB51-qq2cCjc4O2_PzpdQvwiXEw18lRWI4lW5gtNGzAEXHeGY4hpAQ7eSEKZu6h-P-Gi3X_HsY=",
        "title": "indiaai.gov.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKBY3FskiAevh0ZMlpd6mbAwqBLCsV4wz5HM1wLDtHX5luToIJiCABGZfsSMK8xqIqr98QnQzllz6Kk40uZw4nyOMk2yj7Yzk1fcP7OqQdYCDml_MOwPx1oNOdTDM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTse7XbKGKWKWXp85kPDuQO2jyHzwoTG8WcyVttTmEmHUU-JnBzpUpMPsdrE4ZpS_nFxQ648qmIViI8O4ZdRPOOEcCrY3DyASqlQhcm0qlLVQPAfeg50uCyteb8X3Ci9Mb9UGB9g==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EsTNKFYKI-PZmvS5DqxbzK6NTXuqQj9Twh0NYMK7v1SmJABw9MdEmIrd-2o9ctasgrun2IPiWw2y4wdDNluiTOFQUgnYEQkg7nE0HQE88Se5oaK075wFY3Rz8K8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Osz-GIINizZgbGZ41tczVNW9auUtV2dSRvjptAbmYt4J9VFIOU0LUowuqN3stnSOt4nQx4qGVeUqyuRqywKdfUXxXNAdHRhnfWEBDQikuraFT_CcEXz0MBLG88KnRwtKXraWdK-hy5Z70M6Wvhn-7NFH6JthYU0rv_UDdPLHuBz5PpGLLFlOGKHyatogEuk=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaixA4YmOOJfJ7TJ_pfmbFS-oBqlrS3Qt8UcafSTxDhGR5C1fQ0z3rlleKtEjt-nBkpbuel0iFKie3yLOQrRXGDluDEhRaiLkQTXmyZ_o5yAOtpI2dsh4DiGQ7jXKEaDgiHAl1Znm8yedKyOGcc997D5PbcFJSUDufXVFi_DA02A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuL-Kvi9VHEN4YF3H2MrGkrj8f8VTIMTAMtEqAtoH1mtP02lv6hXoO6ZGjj3EwoPXygwk1UI2lKGBp4_OVszBjN73-3FCS6l5KH8DnHfPelUOPlYPQOP07rJsg9rPLmevUsNZN2vmZQrzilFOFwzDPAmUdCIyxuvDzBDRSy8QGcWY=",
        "title": "emergentmind.com"
      }
    ]
  },
  {
    "title": "WildDeepfake",
    "paperLink": "https://doi.org/10.1145/3394171.3413769",
    "description": "A dataset consisting of deepfake videos collected entirely from the internet to reflect real-world scenarios. It focuses on the diversity of scenes and manipulation techniques found 'in the wild'.",
    "authors": [
      "Bojia Zi",
      "Minghao Chang",
      "Jingjing Chen",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "githubLink": "https://github.com/OpenTAI/wild-deepfake",
    "itemCount": "7,314 face sequences (from 707 videos)",
    "source": "Semantic Scholar",
    "specs": "Video",
    "year": "2020",
    "id": "imported-json-1769638825347-596-r10al",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2Frml8YUFL-WMwPW96KVHk3BHjJs7bouE3Yq6HGra8tvLimUW_bEvfy-3e1Rgrp1Ja04YQaGWHibKcsS9r2CNQ52aOI4y8fEtP6sEIMsqnK7rAIIxMfnXqkOrb_GjiUFyvdpgRsc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEuqLpenXU65vT5Sd_ezjkzNsc0rWYMG7nWgdvbdVUI8pXOKWLttX5mVhufAnER2L8eDRvKxAy83upt-3Q_M3XA3kAo22zvqK1i6HTbj_RxuDAPjNSbppgqGebwQOEjZbTZC27SP7pO5BgcTF-H3dBCJkmFCeEQaTl1lTn2KTfFLem_RRhxKtvUg77SxnXFXHLzzlJMtlHQfCDIv4VLjg=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8cP0IyfL01vrkfDx2LNSN_iY3CZRIjcYq1raONI9VFIJ1LyO9OMAETL6LTsjYP_M8rJd4HKUcc08geTQzBwEnZGVI_pOXTEagd_GzWToWFdJ4DD0XdyqzaHM07BB51-qq2cCjc4O2_PzpdQvwiXEw18lRWI4lW5gtNGzAEXHeGY4hpAQ7eSEKZu6h-P-Gi3X_HsY=",
        "title": "indiaai.gov.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKBY3FskiAevh0ZMlpd6mbAwqBLCsV4wz5HM1wLDtHX5luToIJiCABGZfsSMK8xqIqr98QnQzllz6Kk40uZw4nyOMk2yj7Yzk1fcP7OqQdYCDml_MOwPx1oNOdTDM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTse7XbKGKWKWXp85kPDuQO2jyHzwoTG8WcyVttTmEmHUU-JnBzpUpMPsdrE4ZpS_nFxQ648qmIViI8O4ZdRPOOEcCrY3DyASqlQhcm0qlLVQPAfeg50uCyteb8X3Ci9Mb9UGB9g==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EsTNKFYKI-PZmvS5DqxbzK6NTXuqQj9Twh0NYMK7v1SmJABw9MdEmIrd-2o9ctasgrun2IPiWw2y4wdDNluiTOFQUgnYEQkg7nE0HQE88Se5oaK075wFY3Rz8K8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Osz-GIINizZgbGZ41tczVNW9auUtV2dSRvjptAbmYt4J9VFIOU0LUowuqN3stnSOt4nQx4qGVeUqyuRqywKdfUXxXNAdHRhnfWEBDQikuraFT_CcEXz0MBLG88KnRwtKXraWdK-hy5Z70M6Wvhn-7NFH6JthYU0rv_UDdPLHuBz5PpGLLFlOGKHyatogEuk=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaixA4YmOOJfJ7TJ_pfmbFS-oBqlrS3Qt8UcafSTxDhGR5C1fQ0z3rlleKtEjt-nBkpbuel0iFKie3yLOQrRXGDluDEhRaiLkQTXmyZ_o5yAOtpI2dsh4DiGQ7jXKEaDgiHAl1Znm8yedKyOGcc997D5PbcFJSUDufXVFi_DA02A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuL-Kvi9VHEN4YF3H2MrGkrj8f8VTIMTAMtEqAtoH1mtP02lv6hXoO6ZGjj3EwoPXygwk1UI2lKGBp4_OVszBjN73-3FCS6l5KH8DnHfPelUOPlYPQOP07rJsg9rPLmevUsNZN2vmZQrzilFOFwzDPAmUdCIyxuvDzBDRSy8QGcWY=",
        "title": "emergentmind.com"
      }
    ]
  },
  {
    "title": "H-Voice (Fake Voice Histograms)",
    "paperLink": "https://data.mendeley.com/datasets/k47yd3m28w/4",
    "description": "A dataset of histograms derived from original and fake voice recordings. The fake recordings were generated using algorithms like Deep Voice and Imitation. It is intended for training machine learning systems to classify authentic vs. synthetic voice recordings.",
    "authors": [
      "Dora Maria Ballesteros L",
      "Yohanna Patricia Rodriguez",
      "Diego Renza"
    ],
    "githubLink": "https://github.com/yohannarodriguez/Deep4SNet",
    "itemCount": "3,700+ histograms",
    "source": "Mendeley Data",
    "specs": "Histograms (features derived from Audio)",
    "year": "2020",
    "id": "imported-json-1769638825347-597-poif8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlQoBWi7YitQ5xhABeVeWHetTtYAItr8WcnE8m3MAECMNrMBYmeNGwdooItxMkC_un2BPLf4mJzG8XKLr3ba1xU--vu1yOjK38OrXXbTlRC-pB_KpH4tGoUwWnAffrElMATh0OYItLCfUeIH0laW6SirE55511ZLu33WVb39nkoWIu8_8b8b-votWj2Gh5N_SIv8N94PAM9hyoXxeOzlhpWmSNCFXHeQX98ucWS-ppMQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBY3osa2wUXI1FKVAjrcnJf9yWC9yXS_DBXMTONhFuQxA0E6lzgZqZFGG2wWGrwf7Ft7Od241UNp90kp4nJ1tYa3rsNYRoaHEBOU192OvcoGmVLN20HDHAdLMqfssZRZqGylL4JNioMtDewVWVtlECH3LQdUaEsbCdI2Rk7qw73iF_NGGijc_iNh2VKfbXlAhVpmqw9By3CmnGUD7dmG-k6df-MznqtOLfz0lVCoueZ924eX_9Bg==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFL2eoEyCeUodcBfHO4AHqvFleSi8nP7O6JH3wd-HUILzNgYAEk9jJnaDzzcRgGErt6t3s726KGdpKn_CGITXo9Az2svKozDimGTG6tSyVyd11GTTYEeNgsqVGuRI2lhTZi5JbDwVlAkzz_-MqAj8TpIyoG5Zvtow1FFDANb3TE6vE0RxBugAlrv-mORS_sLaMqWtcuARfszVUtbBDB1Ks=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEwcJqFiiLfuf9oXFkFjIDOIdqWcY6OoYig5aqMuLQW80_UqHQ7ZkqnIqAg8QqhnGAusQg_s-ktC0vOGdO1N_ydcFKMwKLc3W9-IqAOVCSzr3apKxBmukxtwgVHh5aRAU72ZVhKHwaa2zDD3yVg6__B8iln20abERtd297UTdij2ZrUIuwwZyIveQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuoNnBgne9otzRxsf_xgCfJLVa2kpiZcYX1wRbvkYSdFhGzEsF_3JzLgcFeQlChXy02lfHyjqsCl8Eofz_rsg_oU51s9ZLWRjF4WfthBFgKrw5sLwAvaRdE6wb-gsnNU24Z_R9mFJ7wHcDucFFgcsmd7tLBO60eYU44DTpvmaMs4F7U0K_Dw==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZuYcMmU0didHeE8L7HLjDoJSFovMOQzgPFKctMyZxaOkdN_MFOLs_jm_3vQvO0LbGFxNGe2ut36fv4Rz7HjJXAou01V_fVRWBlRBse5C7tmLTwbiTpIyd9T6Ww3TkkROBImgyLqKc",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8y2Bw6zso5fgQDjDgDXtmrt0Xcvm0Giq8wlXKU2yDuH879hdv0-BtLVSwVyrg5QdPC8Mo3F3XlOE2LHYXYZinVpUFkEseBqHopo5lMLymSJLKuUr0juky_DYfci4btjilrySIkVBDPry2",
        "title": "mendeley.com"
      }
    ]
  },
  {
    "title": "The Fake-or-Real (FoR) Dataset",
    "paperLink": "https://ieeexplore.ieee.org/document/8895628",
    "description": "A large dataset containing over 195,000 utterances, combining real human speech with synthetic speech generated by state-of-the-art TTS systems, including Baidu's Deep Voice 3 and Google's WaveNet. It is designed to train classifiers for synthetic speech detection.",
    "authors": [
      "Ricardo Reimao",
      "Vassilios Tzerpos"
    ],
    "githubLink": "https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-for-dataset",
    "itemCount": "195,000+ utterances (17 GB)",
    "source": "IEEE Xplore / Kaggle",
    "specs": "Audio (16kHz, mono), Balanced and Norm versions available",
    "year": "2019",
    "id": "imported-json-1769638825347-598-mcmy5",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlQoBWi7YitQ5xhABeVeWHetTtYAItr8WcnE8m3MAECMNrMBYmeNGwdooItxMkC_un2BPLf4mJzG8XKLr3ba1xU--vu1yOjK38OrXXbTlRC-pB_KpH4tGoUwWnAffrElMATh0OYItLCfUeIH0laW6SirE55511ZLu33WVb39nkoWIu8_8b8b-votWj2Gh5N_SIv8N94PAM9hyoXxeOzlhpWmSNCFXHeQX98ucWS-ppMQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBY3osa2wUXI1FKVAjrcnJf9yWC9yXS_DBXMTONhFuQxA0E6lzgZqZFGG2wWGrwf7Ft7Od241UNp90kp4nJ1tYa3rsNYRoaHEBOU192OvcoGmVLN20HDHAdLMqfssZRZqGylL4JNioMtDewVWVtlECH3LQdUaEsbCdI2Rk7qw73iF_NGGijc_iNh2VKfbXlAhVpmqw9By3CmnGUD7dmG-k6df-MznqtOLfz0lVCoueZ924eX_9Bg==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFL2eoEyCeUodcBfHO4AHqvFleSi8nP7O6JH3wd-HUILzNgYAEk9jJnaDzzcRgGErt6t3s726KGdpKn_CGITXo9Az2svKozDimGTG6tSyVyd11GTTYEeNgsqVGuRI2lhTZi5JbDwVlAkzz_-MqAj8TpIyoG5Zvtow1FFDANb3TE6vE0RxBugAlrv-mORS_sLaMqWtcuARfszVUtbBDB1Ks=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEwcJqFiiLfuf9oXFkFjIDOIdqWcY6OoYig5aqMuLQW80_UqHQ7ZkqnIqAg8QqhnGAusQg_s-ktC0vOGdO1N_ydcFKMwKLc3W9-IqAOVCSzr3apKxBmukxtwgVHh5aRAU72ZVhKHwaa2zDD3yVg6__B8iln20abERtd297UTdij2ZrUIuwwZyIveQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuoNnBgne9otzRxsf_xgCfJLVa2kpiZcYX1wRbvkYSdFhGzEsF_3JzLgcFeQlChXy02lfHyjqsCl8Eofz_rsg_oU51s9ZLWRjF4WfthBFgKrw5sLwAvaRdE6wb-gsnNU24Z_R9mFJ7wHcDucFFgcsmd7tLBO60eYU44DTpvmaMs4F7U0K_Dw==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZuYcMmU0didHeE8L7HLjDoJSFovMOQzgPFKctMyZxaOkdN_MFOLs_jm_3vQvO0LbGFxNGe2ut36fv4Rz7HjJXAou01V_fVRWBlRBse5C7tmLTwbiTpIyd9T6Ww3TkkROBImgyLqKc",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8y2Bw6zso5fgQDjDgDXtmrt0Xcvm0Giq8wlXKU2yDuH879hdv0-BtLVSwVyrg5QdPC8Mo3F3XlOE2LHYXYZinVpUFkEseBqHopo5lMLymSJLKuUr0juky_DYfci4btjilrySIkVBDPry2",
        "title": "mendeley.com"
      }
    ]
  },
  {
    "title": "LJ Speech Dataset",
    "paperLink": "https://keithito.com/LJ-Speech-Dataset/",
    "description": "A public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. It was the primary benchmark dataset used to train and evaluate the Baidu Deep Voice 3 model.",
    "authors": [
      "Keith Ito",
      "Linda Johnson"
    ],
    "githubLink": "https://keithito.com/LJ-Speech-Dataset/",
    "itemCount": "13,100 clips (~24 hours)",
    "source": "Web",
    "specs": "Audio (16-bit PCM, 22050Hz), Text Transcriptions",
    "year": "2017",
    "id": "imported-json-1769638825347-599-ozjfe",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlQoBWi7YitQ5xhABeVeWHetTtYAItr8WcnE8m3MAECMNrMBYmeNGwdooItxMkC_un2BPLf4mJzG8XKLr3ba1xU--vu1yOjK38OrXXbTlRC-pB_KpH4tGoUwWnAffrElMATh0OYItLCfUeIH0laW6SirE55511ZLu33WVb39nkoWIu8_8b8b-votWj2Gh5N_SIv8N94PAM9hyoXxeOzlhpWmSNCFXHeQX98ucWS-ppMQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBY3osa2wUXI1FKVAjrcnJf9yWC9yXS_DBXMTONhFuQxA0E6lzgZqZFGG2wWGrwf7Ft7Od241UNp90kp4nJ1tYa3rsNYRoaHEBOU192OvcoGmVLN20HDHAdLMqfssZRZqGylL4JNioMtDewVWVtlECH3LQdUaEsbCdI2Rk7qw73iF_NGGijc_iNh2VKfbXlAhVpmqw9By3CmnGUD7dmG-k6df-MznqtOLfz0lVCoueZ924eX_9Bg==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFL2eoEyCeUodcBfHO4AHqvFleSi8nP7O6JH3wd-HUILzNgYAEk9jJnaDzzcRgGErt6t3s726KGdpKn_CGITXo9Az2svKozDimGTG6tSyVyd11GTTYEeNgsqVGuRI2lhTZi5JbDwVlAkzz_-MqAj8TpIyoG5Zvtow1FFDANb3TE6vE0RxBugAlrv-mORS_sLaMqWtcuARfszVUtbBDB1Ks=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEwcJqFiiLfuf9oXFkFjIDOIdqWcY6OoYig5aqMuLQW80_UqHQ7ZkqnIqAg8QqhnGAusQg_s-ktC0vOGdO1N_ydcFKMwKLc3W9-IqAOVCSzr3apKxBmukxtwgVHh5aRAU72ZVhKHwaa2zDD3yVg6__B8iln20abERtd297UTdij2ZrUIuwwZyIveQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuoNnBgne9otzRxsf_xgCfJLVa2kpiZcYX1wRbvkYSdFhGzEsF_3JzLgcFeQlChXy02lfHyjqsCl8Eofz_rsg_oU51s9ZLWRjF4WfthBFgKrw5sLwAvaRdE6wb-gsnNU24Z_R9mFJ7wHcDucFFgcsmd7tLBO60eYU44DTpvmaMs4F7U0K_Dw==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZuYcMmU0didHeE8L7HLjDoJSFovMOQzgPFKctMyZxaOkdN_MFOLs_jm_3vQvO0LbGFxNGe2ut36fv4Rz7HjJXAou01V_fVRWBlRBse5C7tmLTwbiTpIyd9T6Ww3TkkROBImgyLqKc",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8y2Bw6zso5fgQDjDgDXtmrt0Xcvm0Giq8wlXKU2yDuH879hdv0-BtLVSwVyrg5QdPC8Mo3F3XlOE2LHYXYZinVpUFkEseBqHopo5lMLymSJLKuUr0juky_DYfci4btjilrySIkVBDPry2",
        "title": "mendeley.com"
      }
    ]
  },
  {
    "title": "CSTR VCTK Corpus",
    "paperLink": "https://datashare.ed.ac.uk/handle/10283/3443",
    "description": "A multi-speaker English speech corpus containing data from 110 English speakers with various accents. It is a standard benchmark for multi-speaker text-to-speech synthesis and was used to evaluate Baidu's Deep Voice 2 and Deep Voice 3 models.",
    "authors": [
      "Christophe Veaux",
      "Junichi Yamagishi",
      "Kirsten MacDonald"
    ],
    "githubLink": "https://datashare.ed.ac.uk/handle/10283/3443",
    "itemCount": "~44 hours (109 speakers)",
    "source": "University of Edinburgh",
    "specs": "Audio (48kHz), Text Transcriptions",
    "year": "2017",
    "id": "imported-json-1769638825347-600-ezrnn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlQoBWi7YitQ5xhABeVeWHetTtYAItr8WcnE8m3MAECMNrMBYmeNGwdooItxMkC_un2BPLf4mJzG8XKLr3ba1xU--vu1yOjK38OrXXbTlRC-pB_KpH4tGoUwWnAffrElMATh0OYItLCfUeIH0laW6SirE55511ZLu33WVb39nkoWIu8_8b8b-votWj2Gh5N_SIv8N94PAM9hyoXxeOzlhpWmSNCFXHeQX98ucWS-ppMQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBY3osa2wUXI1FKVAjrcnJf9yWC9yXS_DBXMTONhFuQxA0E6lzgZqZFGG2wWGrwf7Ft7Od241UNp90kp4nJ1tYa3rsNYRoaHEBOU192OvcoGmVLN20HDHAdLMqfssZRZqGylL4JNioMtDewVWVtlECH3LQdUaEsbCdI2Rk7qw73iF_NGGijc_iNh2VKfbXlAhVpmqw9By3CmnGUD7dmG-k6df-MznqtOLfz0lVCoueZ924eX_9Bg==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFL2eoEyCeUodcBfHO4AHqvFleSi8nP7O6JH3wd-HUILzNgYAEk9jJnaDzzcRgGErt6t3s726KGdpKn_CGITXo9Az2svKozDimGTG6tSyVyd11GTTYEeNgsqVGuRI2lhTZi5JbDwVlAkzz_-MqAj8TpIyoG5Zvtow1FFDANb3TE6vE0RxBugAlrv-mORS_sLaMqWtcuARfszVUtbBDB1Ks=",
        "title": "frontiersin.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEwcJqFiiLfuf9oXFkFjIDOIdqWcY6OoYig5aqMuLQW80_UqHQ7ZkqnIqAg8QqhnGAusQg_s-ktC0vOGdO1N_ydcFKMwKLc3W9-IqAOVCSzr3apKxBmukxtwgVHh5aRAU72ZVhKHwaa2zDD3yVg6__B8iln20abERtd297UTdij2ZrUIuwwZyIveQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuoNnBgne9otzRxsf_xgCfJLVa2kpiZcYX1wRbvkYSdFhGzEsF_3JzLgcFeQlChXy02lfHyjqsCl8Eofz_rsg_oU51s9ZLWRjF4WfthBFgKrw5sLwAvaRdE6wb-gsnNU24Z_R9mFJ7wHcDucFFgcsmd7tLBO60eYU44DTpvmaMs4F7U0K_Dw==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZuYcMmU0didHeE8L7HLjDoJSFovMOQzgPFKctMyZxaOkdN_MFOLs_jm_3vQvO0LbGFxNGe2ut36fv4Rz7HjJXAou01V_fVRWBlRBse5C7tmLTwbiTpIyd9T6Ww3TkkROBImgyLqKc",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8y2Bw6zso5fgQDjDgDXtmrt0Xcvm0Giq8wlXKU2yDuH879hdv0-BtLVSwVyrg5QdPC8Mo3F3XlOE2LHYXYZinVpUFkEseBqHopo5lMLymSJLKuUr0juky_DYfci4btjilrySIkVBDPry2",
        "title": "mendeley.com"
      }
    ]
  },
  {
    "title": "AVFakeBench",
    "paperLink": "https://arxiv.org/abs/2511.21251",
    "description": "A comprehensive audio-video forgery detection benchmark specifically designed for Audio-Video Large Language Models (AV-LMMs). It comprises 12,000 carefully curated audio-video questions covering seven forgery types (including both human and general subjects) and provides four levels of annotations (binary judgment, forgery type classification, detail selection, and explanatory reasoning).",
    "authors": [
      "Shuhan Xia",
      "et al."
    ],
    "githubLink": "https://github.com/shuhanxia/MMFakeBench",
    "itemCount": "12,000 questions",
    "source": "arXiv",
    "specs": "Audio-Video; 7 forgery types; 4-level annotations",
    "year": "2025",
    "id": "imported-json-1769638825347-601-e8fmd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnow-jzrE0ASpdVEWPt1WEyUYRl6q0eyB_s-wA-8Z9aG2D_Uw03Zuskemi_5s-I29bsx0MudfHkOOlTgA6fz3a1TePTZigrA-X4loKHAI2J7X1JNpvNrQ0wNo94KGcOul4QMpzTo=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGc0rTy67Xg4VTaNSLIwr4rwC_sC9GvHFl7P6Z9l_uMU7ZnHishzfimIsm_0CwKvII0h80cgpK2ZWP0kGCnNoDpUPzyrgniLDD1L3ghK2TkVrcNYGnmdvLFmPpSou7cgQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHL93DqZrGBk9a8hnqZZE2bz1knWiOKRPFXZyo8G9nC8MVNezveM7lZPhBuJjz2JyQWJfv20_kQh78sbbsazDd1TwotDhlrtvzgyuAYXWvHgjq6LxKC5EeISHFpZc6GZxzBUV6VGw2ZIUMyZXiPHp5_4veTcQQfRc3KCXnE",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP7KfFstgYLsg9IDMW9lYDfzX36BD3TafOcw4gPXY6GVZUEHShgSrG9ts7pYRynPhZqbLyX0knU2dNg1ZFMJXQyEAYCTUaumMdYGPzD3gBUQVWf9rRDqa2LEfEs1YhgPQTHI8brY99Bo3jVFo495Yviy8cp5YTAQEivqfeDHzbMAbmqy_zWrpLQPkfJHYcLt2enkG41196dcoCTTDEIvMY-UcSXazcM14aWBw48s3SVwl9aOmkP6XL",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFhBSOv8JgKbTmfJjFAJC9eytuvrD_tEEX7OmK-ClqKOO8Smkhtc27s8T8SZLsjwN5L9AHzIaRjjqpE_6pWbX8vswgJojyhzUT9_KnsKkt70LuNI9J9WWzymREzmIV2EWEqDt3gL0nkyST0QO8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSgG4QELQmBSZoGJeGwXO42ttIJfq00j9tMOnQVAJ9uj2Ck4d5a8uX0lTSBaaIZUing_gG1PU5O9SaE2IQ0LywKofu3GrmRHCIMRPXPXGh9qfeEzQD88ruy5cR_6TPnOth",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8MmknG3O3L3UAaKkXt4IP9lOBiOUdCFKgSv-7Oq2as9pz6kqo80vbhP8LhSFCVpBrbHc6MnZWOa9mnGAD2KfLxouGWz3Gw_pF4wLuuI870wtEKSI7Obdkfv6z0aYRSJDpsATFRif_bflR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPwVpjSLzWjhaIrj7D6HLdSsqGzKxlylE2WowQhm6BJ4I7XStLrs94u91CPri28uAY_P55frVEyuhLe0k5Wv-Dj8BlRUPfDGJL6j2cOzMnKXlah70kMhkPToxzJyF4tbWn",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "AV-DeepFake1M",
    "paperLink": "https://arxiv.org/abs/2311.15243",
    "description": "A large-scale audio-visual deepfake dataset designed for temporal localization of forged segments. It employs LLMs to generate content-driven manipulations, including video, audio, and joint audio-visual edits. The dataset features over 1 million videos from more than 2,000 subjects.",
    "authors": [
      "Zhixi Cai",
      "Kalin Stefanov",
      "Abhinav Dhall",
      "Munawar Hayat"
    ],
    "githubLink": "https://github.com/ControlNet/AV-Deepfake1M",
    "itemCount": "> 1,000,000 videos",
    "source": "arXiv",
    "specs": "Audio-Video; LLM-driven content manipulations; Temporal localization",
    "year": "2023",
    "id": "imported-json-1769638825347-602-oqxit",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnow-jzrE0ASpdVEWPt1WEyUYRl6q0eyB_s-wA-8Z9aG2D_Uw03Zuskemi_5s-I29bsx0MudfHkOOlTgA6fz3a1TePTZigrA-X4loKHAI2J7X1JNpvNrQ0wNo94KGcOul4QMpzTo=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGc0rTy67Xg4VTaNSLIwr4rwC_sC9GvHFl7P6Z9l_uMU7ZnHishzfimIsm_0CwKvII0h80cgpK2ZWP0kGCnNoDpUPzyrgniLDD1L3ghK2TkVrcNYGnmdvLFmPpSou7cgQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHL93DqZrGBk9a8hnqZZE2bz1knWiOKRPFXZyo8G9nC8MVNezveM7lZPhBuJjz2JyQWJfv20_kQh78sbbsazDd1TwotDhlrtvzgyuAYXWvHgjq6LxKC5EeISHFpZc6GZxzBUV6VGw2ZIUMyZXiPHp5_4veTcQQfRc3KCXnE",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP7KfFstgYLsg9IDMW9lYDfzX36BD3TafOcw4gPXY6GVZUEHShgSrG9ts7pYRynPhZqbLyX0knU2dNg1ZFMJXQyEAYCTUaumMdYGPzD3gBUQVWf9rRDqa2LEfEs1YhgPQTHI8brY99Bo3jVFo495Yviy8cp5YTAQEivqfeDHzbMAbmqy_zWrpLQPkfJHYcLt2enkG41196dcoCTTDEIvMY-UcSXazcM14aWBw48s3SVwl9aOmkP6XL",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFhBSOv8JgKbTmfJjFAJC9eytuvrD_tEEX7OmK-ClqKOO8Smkhtc27s8T8SZLsjwN5L9AHzIaRjjqpE_6pWbX8vswgJojyhzUT9_KnsKkt70LuNI9J9WWzymREzmIV2EWEqDt3gL0nkyST0QO8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSgG4QELQmBSZoGJeGwXO42ttIJfq00j9tMOnQVAJ9uj2Ck4d5a8uX0lTSBaaIZUing_gG1PU5O9SaE2IQ0LywKofu3GrmRHCIMRPXPXGh9qfeEzQD88ruy5cR_6TPnOth",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8MmknG3O3L3UAaKkXt4IP9lOBiOUdCFKgSv-7Oq2as9pz6kqo80vbhP8LhSFCVpBrbHc6MnZWOa9mnGAD2KfLxouGWz3Gw_pF4wLuuI870wtEKSI7Obdkfv6z0aYRSJDpsATFRif_bflR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPwVpjSLzWjhaIrj7D6HLdSsqGzKxlylE2WowQhm6BJ4I7XStLrs94u91CPri28uAY_P55frVEyuhLe0k5Wv-Dj8BlRUPfDGJL6j2cOzMnKXlah70kMhkPToxzJyF4tbWn",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "LAV-DF",
    "paperLink": "https://arxiv.org/abs/2305.01979",
    "description": "Localized Audio Visual DeepFake (LAV-DF) is a benchmark for content-driven audio-visual forgery detection and localization. It focuses on temporal edits where visual and audio content are manipulated to alter meaning, rather than just identity swapping.",
    "authors": [
      "Zhixi Cai",
      "Shreya Ghosh",
      "Abhinav Dhall",
      "Tom Gedeon",
      "Kalin Stefanov",
      "Munawar Hayat"
    ],
    "githubLink": "https://github.com/ControlNet/LAV-DF",
    "itemCount": "~136,000 videos",
    "source": "arXiv",
    "specs": "Audio-Video; Content-driven manipulation; Temporal localization",
    "year": "2023",
    "id": "imported-json-1769638825347-603-lbq0v",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnow-jzrE0ASpdVEWPt1WEyUYRl6q0eyB_s-wA-8Z9aG2D_Uw03Zuskemi_5s-I29bsx0MudfHkOOlTgA6fz3a1TePTZigrA-X4loKHAI2J7X1JNpvNrQ0wNo94KGcOul4QMpzTo=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGc0rTy67Xg4VTaNSLIwr4rwC_sC9GvHFl7P6Z9l_uMU7ZnHishzfimIsm_0CwKvII0h80cgpK2ZWP0kGCnNoDpUPzyrgniLDD1L3ghK2TkVrcNYGnmdvLFmPpSou7cgQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHL93DqZrGBk9a8hnqZZE2bz1knWiOKRPFXZyo8G9nC8MVNezveM7lZPhBuJjz2JyQWJfv20_kQh78sbbsazDd1TwotDhlrtvzgyuAYXWvHgjq6LxKC5EeISHFpZc6GZxzBUV6VGw2ZIUMyZXiPHp5_4veTcQQfRc3KCXnE",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP7KfFstgYLsg9IDMW9lYDfzX36BD3TafOcw4gPXY6GVZUEHShgSrG9ts7pYRynPhZqbLyX0knU2dNg1ZFMJXQyEAYCTUaumMdYGPzD3gBUQVWf9rRDqa2LEfEs1YhgPQTHI8brY99Bo3jVFo495Yviy8cp5YTAQEivqfeDHzbMAbmqy_zWrpLQPkfJHYcLt2enkG41196dcoCTTDEIvMY-UcSXazcM14aWBw48s3SVwl9aOmkP6XL",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFhBSOv8JgKbTmfJjFAJC9eytuvrD_tEEX7OmK-ClqKOO8Smkhtc27s8T8SZLsjwN5L9AHzIaRjjqpE_6pWbX8vswgJojyhzUT9_KnsKkt70LuNI9J9WWzymREzmIV2EWEqDt3gL0nkyST0QO8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSgG4QELQmBSZoGJeGwXO42ttIJfq00j9tMOnQVAJ9uj2Ck4d5a8uX0lTSBaaIZUing_gG1PU5O9SaE2IQ0LywKofu3GrmRHCIMRPXPXGh9qfeEzQD88ruy5cR_6TPnOth",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8MmknG3O3L3UAaKkXt4IP9lOBiOUdCFKgSv-7Oq2as9pz6kqo80vbhP8LhSFCVpBrbHc6MnZWOa9mnGAD2KfLxouGWz3Gw_pF4wLuuI870wtEKSI7Obdkfv6z0aYRSJDpsATFRif_bflR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPwVpjSLzWjhaIrj7D6HLdSsqGzKxlylE2WowQhm6BJ4I7XStLrs94u91CPri28uAY_P55frVEyuhLe0k5Wv-Dj8BlRUPfDGJL6j2cOzMnKXlah70kMhkPToxzJyF4tbWn",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "FakeAVCeleb",
    "paperLink": "https://arxiv.org/abs/2108.05080",
    "description": "A multimodal deepfake dataset containing both deepfake videos and synthesized lip-synced audios. It addresses racial bias by including subjects from four major ethnic backgrounds and utilizes popular generation methods (like Faceswap and SV2TTS) to create realistic multimodal forgeries.",
    "authors": [
      "Hasam Khalid",
      "Minha Kim",
      "Shahroz Tariq",
      "Simon S. Woo"
    ],
    "githubLink": "https://github.com/DASH-Lab/FakeAVCeleb",
    "itemCount": "~20,000 videos",
    "source": "arXiv",
    "specs": "Audio-Video; 500 subjects; Multimodal (Audio & Video) deepfakes",
    "year": "2021",
    "id": "imported-json-1769638825347-604-x34i2",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnow-jzrE0ASpdVEWPt1WEyUYRl6q0eyB_s-wA-8Z9aG2D_Uw03Zuskemi_5s-I29bsx0MudfHkOOlTgA6fz3a1TePTZigrA-X4loKHAI2J7X1JNpvNrQ0wNo94KGcOul4QMpzTo=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGc0rTy67Xg4VTaNSLIwr4rwC_sC9GvHFl7P6Z9l_uMU7ZnHishzfimIsm_0CwKvII0h80cgpK2ZWP0kGCnNoDpUPzyrgniLDD1L3ghK2TkVrcNYGnmdvLFmPpSou7cgQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHL93DqZrGBk9a8hnqZZE2bz1knWiOKRPFXZyo8G9nC8MVNezveM7lZPhBuJjz2JyQWJfv20_kQh78sbbsazDd1TwotDhlrtvzgyuAYXWvHgjq6LxKC5EeISHFpZc6GZxzBUV6VGw2ZIUMyZXiPHp5_4veTcQQfRc3KCXnE",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGP7KfFstgYLsg9IDMW9lYDfzX36BD3TafOcw4gPXY6GVZUEHShgSrG9ts7pYRynPhZqbLyX0knU2dNg1ZFMJXQyEAYCTUaumMdYGPzD3gBUQVWf9rRDqa2LEfEs1YhgPQTHI8brY99Bo3jVFo495Yviy8cp5YTAQEivqfeDHzbMAbmqy_zWrpLQPkfJHYcLt2enkG41196dcoCTTDEIvMY-UcSXazcM14aWBw48s3SVwl9aOmkP6XL",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFhBSOv8JgKbTmfJjFAJC9eytuvrD_tEEX7OmK-ClqKOO8Smkhtc27s8T8SZLsjwN5L9AHzIaRjjqpE_6pWbX8vswgJojyhzUT9_KnsKkt70LuNI9J9WWzymREzmIV2EWEqDt3gL0nkyST0QO8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSgG4QELQmBSZoGJeGwXO42ttIJfq00j9tMOnQVAJ9uj2Ck4d5a8uX0lTSBaaIZUing_gG1PU5O9SaE2IQ0LywKofu3GrmRHCIMRPXPXGh9qfeEzQD88ruy5cR_6TPnOth",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8MmknG3O3L3UAaKkXt4IP9lOBiOUdCFKgSv-7Oq2as9pz6kqo80vbhP8LhSFCVpBrbHc6MnZWOa9mnGAD2KfLxouGWz3Gw_pF4wLuuI870wtEKSI7Obdkfv6z0aYRSJDpsATFRif_bflR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPwVpjSLzWjhaIrj7D6HLdSsqGzKxlylE2WowQhm6BJ4I7XStLrs94u91CPri28uAY_P55frVEyuhLe0k5Wv-Dj8BlRUPfDGJL6j2cOzMnKXlah70kMhkPToxzJyF4tbWn",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "MLAAD (Multi-Language Audio Anti-Spoofing Dataset)",
    "paperLink": "https://arxiv.org/abs/2404.10359",
    "description": "A large-scale multilingual dataset designed to evaluate out-of-domain generalization. It includes synthesized speech from a wide variety of TTS models and architectures across numerous languages.",
    "authors": [
      "Nicolas M. Müller",
      "Piotr Kawa",
      "Wei Herng Choong",
      "Edresson Casanova",
      "Eren Gölge",
      "Thorsten Müller",
      "Piotr Syga",
      "Philip Sperl",
      "Konstantin Böttinger"
    ],
    "githubLink": "https://deepfake-total.com/mlaad",
    "itemCount": "~678 hours (Version 9)",
    "source": "arXiv",
    "specs": "Audio; 51 languages; 140 TTS models; 78 architectures",
    "year": "2024",
    "id": "imported-json-1769638825347-605-6mgb7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5d_SCGJK5sTiBSPKN2cI97UeeRJ0aD6H2bqRdpGrvnSbKnr1LktWJNN92B0im072BPmCqRwd0JO5MlqK-dJbJyBBk6z54QypIA-pXRnreflzDOh9_Kz-GVqAmW4m_oJzYVU_8k035KPR7INpX7NVzNGatAOwogmgklQ==",
        "title": "google.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvFq14RgtUJzlK1HxFgUoity35yF-kL1j0NNcWBT9tQlRYfUz22GAGlwXBcrlK3Kha2xPejm-a240-ITY8P2tAxGysZr7Vq9EyX431rmD3su0ijox4VqLcuoRKTCzJpOGTXmWA4s8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnKAmRTRl1pfn9V8SSNhbnMH2ZOnG9sgWuRvPrEzpPVDhIAfUgStMdz_b79qR1Cyl1lOHHl_1EYPMe6TB0-0d7uhnTMTI6R1bRprCQ4cAhRb-bM-G7XgiCZhV0nW7TiFObpwlWfuuNCg8Db53CUGu5JabmNNZLMFMOdZ9j7lG7R_r-R8ih2AarlUMVCUO-h3m6hAkJfiaC0WxYNAH3",
        "title": "skku.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpKe9zoQvlwxWHEpGiiD0pTcaWSXlaNRQKDZlbr_lwvo3hboL_iR1j-Mp0gW_fDNiUeF16mw9ekksQ2ixzGI--3ioe5ze3t2jJXKtHpM7rOOIzg6LiKS36-Btl",
        "title": "deepfake-total.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkcAiXicrveUiobJNRng9_F8nIxSwsNWvOdCvenLPn_v_y6mV42OEDkDut2cVBwa5uZaFhIyMfE2Cuee3ldaKdsLAwWh54T6XLt2r4NnrUkGkeiOUTATcxoDuG1b58",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEUYM3i12NIDathtDa3J2BxbigQAzfm_z39k6jDitOyrf491xjCBAXvdu8li3Ewrj620e4Jw2oCEpwqZvqlYZZAxP6QG-TgFEM8emmuFOFeuHilfZn8Hg5H",
        "title": "addchallenge.cn"
      }
    ]
  },
  {
    "title": "In-the-Wild",
    "paperLink": "https://arxiv.org/abs/2205.05579",
    "description": "A dataset of audio deepfakes and corresponding bona-fide audio for 58 politicians and public figures, collected from social networks and video streaming platforms to evaluate generalization to real-world conditions.",
    "authors": [
      "Nicolas M. Müller",
      "Pavel Czempin",
      "Franziska Dieckmann",
      "Adam Froghyar",
      "Konstantin Böttinger"
    ],
    "githubLink": "https://huggingface.co/datasets/mueller91/In-The-Wild",
    "itemCount": "31,779 samples (20.8 hours real, 17.2 hours fake)",
    "source": "Hugging Face",
    "specs": "Audio; Real-world social media samples; 58 target speakers",
    "year": "2022",
    "id": "imported-json-1769638825347-606-rizly",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5d_SCGJK5sTiBSPKN2cI97UeeRJ0aD6H2bqRdpGrvnSbKnr1LktWJNN92B0im072BPmCqRwd0JO5MlqK-dJbJyBBk6z54QypIA-pXRnreflzDOh9_Kz-GVqAmW4m_oJzYVU_8k035KPR7INpX7NVzNGatAOwogmgklQ==",
        "title": "google.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvFq14RgtUJzlK1HxFgUoity35yF-kL1j0NNcWBT9tQlRYfUz22GAGlwXBcrlK3Kha2xPejm-a240-ITY8P2tAxGysZr7Vq9EyX431rmD3su0ijox4VqLcuoRKTCzJpOGTXmWA4s8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnKAmRTRl1pfn9V8SSNhbnMH2ZOnG9sgWuRvPrEzpPVDhIAfUgStMdz_b79qR1Cyl1lOHHl_1EYPMe6TB0-0d7uhnTMTI6R1bRprCQ4cAhRb-bM-G7XgiCZhV0nW7TiFObpwlWfuuNCg8Db53CUGu5JabmNNZLMFMOdZ9j7lG7R_r-R8ih2AarlUMVCUO-h3m6hAkJfiaC0WxYNAH3",
        "title": "skku.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpKe9zoQvlwxWHEpGiiD0pTcaWSXlaNRQKDZlbr_lwvo3hboL_iR1j-Mp0gW_fDNiUeF16mw9ekksQ2ixzGI--3ioe5ze3t2jJXKtHpM7rOOIzg6LiKS36-Btl",
        "title": "deepfake-total.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkcAiXicrveUiobJNRng9_F8nIxSwsNWvOdCvenLPn_v_y6mV42OEDkDut2cVBwa5uZaFhIyMfE2Cuee3ldaKdsLAwWh54T6XLt2r4NnrUkGkeiOUTATcxoDuG1b58",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEUYM3i12NIDathtDa3J2BxbigQAzfm_z39k6jDitOyrf491xjCBAXvdu8li3Ewrj620e4Jw2oCEpwqZvqlYZZAxP6QG-TgFEM8emmuFOFeuHilfZn8Hg5H",
        "title": "addchallenge.cn"
      }
    ]
  },
  {
    "title": "ADD 2022 (Audio Deep Synthesis Detection)",
    "paperLink": "https://arxiv.org/abs/2202.08562",
    "description": "The dataset for the first Audio Deep Synthesis Detection Challenge, featuring three tracks: Low-quality Fake Audio Detection (LF), Partially Fake Audio Detection (PF), and Audio Fake Game (FG). It aims to simulate challenging real-world scenarios.",
    "authors": [
      "Jiangyan Yi",
      "Ruibo Fu",
      "Jianhua Tao",
      "Shuai Nie",
      "Haoxin Ma",
      "Chenglong Wang",
      "Tao Wang",
      "Zhengkun Tian",
      "Ye Bai",
      "Cunhang Fan",
      "Shan Liang",
      "Shiming Wang",
      "Shuai Zhang",
      "Xinrui Yan",
      "Le Xu",
      "Zhengqi Wen",
      "Haizhou Li",
      "Zheng Lian",
      "Bin Liu"
    ],
    "githubLink": "http://addchallenge.cn/add2022",
    "itemCount": "~100,000+ samples (across all tracks)",
    "source": "arXiv",
    "specs": "Audio; Chinese and English; Tracks: Low-quality, Partially Fake, Fake Game",
    "year": "2022",
    "id": "imported-json-1769638825347-607-q5m8g",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5d_SCGJK5sTiBSPKN2cI97UeeRJ0aD6H2bqRdpGrvnSbKnr1LktWJNN92B0im072BPmCqRwd0JO5MlqK-dJbJyBBk6z54QypIA-pXRnreflzDOh9_Kz-GVqAmW4m_oJzYVU_8k035KPR7INpX7NVzNGatAOwogmgklQ==",
        "title": "google.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvFq14RgtUJzlK1HxFgUoity35yF-kL1j0NNcWBT9tQlRYfUz22GAGlwXBcrlK3Kha2xPejm-a240-ITY8P2tAxGysZr7Vq9EyX431rmD3su0ijox4VqLcuoRKTCzJpOGTXmWA4s8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnKAmRTRl1pfn9V8SSNhbnMH2ZOnG9sgWuRvPrEzpPVDhIAfUgStMdz_b79qR1Cyl1lOHHl_1EYPMe6TB0-0d7uhnTMTI6R1bRprCQ4cAhRb-bM-G7XgiCZhV0nW7TiFObpwlWfuuNCg8Db53CUGu5JabmNNZLMFMOdZ9j7lG7R_r-R8ih2AarlUMVCUO-h3m6hAkJfiaC0WxYNAH3",
        "title": "skku.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpKe9zoQvlwxWHEpGiiD0pTcaWSXlaNRQKDZlbr_lwvo3hboL_iR1j-Mp0gW_fDNiUeF16mw9ekksQ2ixzGI--3ioe5ze3t2jJXKtHpM7rOOIzg6LiKS36-Btl",
        "title": "deepfake-total.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkcAiXicrveUiobJNRng9_F8nIxSwsNWvOdCvenLPn_v_y6mV42OEDkDut2cVBwa5uZaFhIyMfE2Cuee3ldaKdsLAwWh54T6XLt2r4NnrUkGkeiOUTATcxoDuG1b58",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEUYM3i12NIDathtDa3J2BxbigQAzfm_z39k6jDitOyrf491xjCBAXvdu8li3Ewrj620e4Jw2oCEpwqZvqlYZZAxP6QG-TgFEM8emmuFOFeuHilfZn8Hg5H",
        "title": "addchallenge.cn"
      }
    ]
  },
  {
    "title": "WaveFake",
    "paperLink": "https://arxiv.org/abs/2111.02813",
    "description": "A dataset designed to facilitate audio deepfake detection research, consisting of audio clips generated by six different deep generative model architectures. It covers both English (LJSpeech) and Japanese (JSUT) languages.",
    "authors": [
      "Joel Frank",
      "Lea Schönherr"
    ],
    "githubLink": "https://github.com/RUB-SysSec/WaveFake",
    "itemCount": "104,885 clips (~175 hours)",
    "source": "arXiv",
    "specs": "Audio (16-bit PCM WAV); English and Japanese; Architectures: MelGAN, Parallel WaveGAN, Multi-Band MelGAN, Full-Band MelGAN, HiFi-GAN, WaveGlow",
    "year": "2021",
    "id": "imported-json-1769638825347-608-luznx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5d_SCGJK5sTiBSPKN2cI97UeeRJ0aD6H2bqRdpGrvnSbKnr1LktWJNN92B0im072BPmCqRwd0JO5MlqK-dJbJyBBk6z54QypIA-pXRnreflzDOh9_Kz-GVqAmW4m_oJzYVU_8k035KPR7INpX7NVzNGatAOwogmgklQ==",
        "title": "google.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvFq14RgtUJzlK1HxFgUoity35yF-kL1j0NNcWBT9tQlRYfUz22GAGlwXBcrlK3Kha2xPejm-a240-ITY8P2tAxGysZr7Vq9EyX431rmD3su0ijox4VqLcuoRKTCzJpOGTXmWA4s8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnKAmRTRl1pfn9V8SSNhbnMH2ZOnG9sgWuRvPrEzpPVDhIAfUgStMdz_b79qR1Cyl1lOHHl_1EYPMe6TB0-0d7uhnTMTI6R1bRprCQ4cAhRb-bM-G7XgiCZhV0nW7TiFObpwlWfuuNCg8Db53CUGu5JabmNNZLMFMOdZ9j7lG7R_r-R8ih2AarlUMVCUO-h3m6hAkJfiaC0WxYNAH3",
        "title": "skku.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpKe9zoQvlwxWHEpGiiD0pTcaWSXlaNRQKDZlbr_lwvo3hboL_iR1j-Mp0gW_fDNiUeF16mw9ekksQ2ixzGI--3ioe5ze3t2jJXKtHpM7rOOIzg6LiKS36-Btl",
        "title": "deepfake-total.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkcAiXicrveUiobJNRng9_F8nIxSwsNWvOdCvenLPn_v_y6mV42OEDkDut2cVBwa5uZaFhIyMfE2Cuee3ldaKdsLAwWh54T6XLt2r4NnrUkGkeiOUTATcxoDuG1b58",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEUYM3i12NIDathtDa3J2BxbigQAzfm_z39k6jDitOyrf491xjCBAXvdu8li3Ewrj620e4Jw2oCEpwqZvqlYZZAxP6QG-TgFEM8emmuFOFeuHilfZn8Hg5H",
        "title": "addchallenge.cn"
      }
    ]
  },
  {
    "title": "DeepfakeBench-MM",
    "paperLink": "https://arxiv.org/abs/2510.22622",
    "description": "The first unified benchmark designed to standardize protocols for multimodal deepfake detection (MM-DFD). It addresses the lack of uniformity in data processing and evaluation by providing a comprehensive framework that supports multiple datasets and detectors. It includes a standardized preprocessing pipeline for audio-visual stream separation and face handling.",
    "authors": [
      "Kangran Zhao",
      "Yupeng Chen",
      "Xiaoyu Zhang",
      "Yize Chen",
      "Weinan Guan",
      "Baicheng Chen",
      "Chengzhe Sun",
      "Soumyya Kanti Datta",
      "Qingshan Liu",
      "Siwei Lyu",
      "Baoyuan Wu"
    ],
    "githubLink": "https://github.com/AnonymousDeepfakeBench-MM/DeepfakeBench-MM",
    "itemCount": "Supports 5 datasets (including Mega-MMDF, FakeAVCeleb, LAV-DF, AVDeepfake1M, IDForge)",
    "source": "arXiv",
    "specs": "Multimodal (Audio and Video); Standardized preprocessing (16kHz audio, 25fps video); MP4/NPZ formats",
    "year": "2025",
    "id": "imported-json-1769638825347-609-50b5k",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHUakZ2Ah6-I50yGa8OVuXvDbzE3mBumS1IxqbvoQ32U8nMti--3tKl8z47INv_tvmra6QYphAPyroPaA26znSaV1oniLJtuZsGdPcIQpFdN865mFjjxiMrUlea",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQo9ZUjPf9Uh-kT6lcCHpk5AykbPfoHVskCi0N3KQEFEGkNk7H8Ax4g0xmfPL13vJcCoNAx9CIB5wiIiMdvvr1pNLBsT_JyPYiWULMjamvEO5qrgQfHgEuExDGS8jWQeTVGEv-Amf2-iejCbun5T8vnAZ_98SqrOIhP97mMAbXucEXURd1dAWZj_WRut-Rtl_CtYA6lYpG5eYqpbmWDqEtBTuWOR9OmuNMreL8ceKJL_YgeCt8PdyGlRQ=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKGyJzuZ5vs5K02-mmKgsImRuE9v5ENG7JG8H8XC9tCIPXQkyThtj9j3rIPqolP4Bp-xusg_W5VBldnxwYnc3BZWoe0-sBDTr-8xGFi2doPReNPnBAgLSXnDcehRvz",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFv44us7pY-SqT8D70QL-bX0Lv5yuDZaOj82NbouCoJYJvYJF9cAKOds5GGhflXaP8KTjNJrzqF4OGibrCQ0FLeOgbH00W_32spa4yJzzGMmGPW0MGjPLFk_Xqour_Brv69",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4giMVSGS0CO2S4KMddCtDphGzT5uYsXn6yqNPN79et9x4iyB-NgaqwESUlAj0WTVZDjze4zPSGOIW9kP2ikc9FzW_8D0c23zIzBw3nO5tgy62ZUMgngCgmVGUp6y-YYs3LLCbXFr5wL9hSF2soC6Nc8BuB3zrdExx1j3EZn2uGXmwtEFS_9mKrptPYOrisHa0VOR75hkcJRtH06OhD_NI4S5EooIXPyDz30XUNYbC-LC2DVVwTmA6",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFO5HS5XEemI-vefUFTRHbUSxtTzAk0vi4NC_-Ki6h3EdWDa0qok1hhqgdq45EFRVYuLt9AHEILWxb5FAu_Wbd4343Bv2-pFl2M-bts7jbzDbGv90S5uqLTFwostSayj7GdWHnT_g==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "MMFakeBench",
    "paperLink": "https://arxiv.org/abs/2406.08772",
    "description": "A comprehensive benchmark for mixed-source multimodal misinformation detection, specifically evaluating Large Vision-Language Models (LVLMs). It focuses on detecting inconsistencies and forgeries across text and visual modalities, including textual veracity distortion, visual veracity distortion, and cross-modal consistency distortion.",
    "authors": [
      "Xuannan Liu",
      "Zekun Li",
      "Peipei Li",
      "Huaibo Huang",
      "Shuhan Xia",
      "Xing Cui",
      "Linzhi Huang"
    ],
    "githubLink": "https://github.com/liuxuannan/MMFakeBench",
    "itemCount": "Contains 12 sub-categories of misinformation forgery types",
    "source": "arXiv",
    "specs": "Multimodal (Text and Image); Evaluation for LVLMs",
    "year": "2024",
    "id": "imported-json-1769638825347-610-m8vxb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHUakZ2Ah6-I50yGa8OVuXvDbzE3mBumS1IxqbvoQ32U8nMti--3tKl8z47INv_tvmra6QYphAPyroPaA26znSaV1oniLJtuZsGdPcIQpFdN865mFjjxiMrUlea",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQo9ZUjPf9Uh-kT6lcCHpk5AykbPfoHVskCi0N3KQEFEGkNk7H8Ax4g0xmfPL13vJcCoNAx9CIB5wiIiMdvvr1pNLBsT_JyPYiWULMjamvEO5qrgQfHgEuExDGS8jWQeTVGEv-Amf2-iejCbun5T8vnAZ_98SqrOIhP97mMAbXucEXURd1dAWZj_WRut-Rtl_CtYA6lYpG5eYqpbmWDqEtBTuWOR9OmuNMreL8ceKJL_YgeCt8PdyGlRQ=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKGyJzuZ5vs5K02-mmKgsImRuE9v5ENG7JG8H8XC9tCIPXQkyThtj9j3rIPqolP4Bp-xusg_W5VBldnxwYnc3BZWoe0-sBDTr-8xGFi2doPReNPnBAgLSXnDcehRvz",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFv44us7pY-SqT8D70QL-bX0Lv5yuDZaOj82NbouCoJYJvYJF9cAKOds5GGhflXaP8KTjNJrzqF4OGibrCQ0FLeOgbH00W_32spa4yJzzGMmGPW0MGjPLFk_Xqour_Brv69",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4giMVSGS0CO2S4KMddCtDphGzT5uYsXn6yqNPN79et9x4iyB-NgaqwESUlAj0WTVZDjze4zPSGOIW9kP2ikc9FzW_8D0c23zIzBw3nO5tgy62ZUMgngCgmVGUp6y-YYs3LLCbXFr5wL9hSF2soC6Nc8BuB3zrdExx1j3EZn2uGXmwtEFS_9mKrptPYOrisHa0VOR75hkcJRtH06OhD_NI4S5EooIXPyDz30XUNYbC-LC2DVVwTmA6",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFO5HS5XEemI-vefUFTRHbUSxtTzAk0vi4NC_-Ki6h3EdWDa0qok1hhqgdq45EFRVYuLt9AHEILWxb5FAu_Wbd4343Bv2-pFl2M-bts7jbzDbGv90S5uqLTFwostSayj7GdWHnT_g==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Deepfake-Eval-2024",
    "paperLink": "https://arxiv.org/abs/2503.02857",
    "description": "An in-the-wild multimodal benchmark consisting of deepfakes collected from social media and detection platforms in 2024. It addresses the gap between academic datasets and real-world threats, containing diverse media content across many languages.",
    "authors": [
      "Nuria Alina Chandra",
      "Ryan Murtfeldt",
      "Lin Qiu",
      "Arnab Karmakar",
      "et al."
    ],
    "githubLink": "https://huggingface.co/datasets/nuriachandra/Deepfake-Eval-2024",
    "itemCount": "45h video, 56.5h audio, 1,975 images",
    "source": "Hugging Face",
    "specs": "Video, Audio, Image",
    "year": "2025",
    "id": "imported-json-1769638825347-611-bejlh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG0iu2RySq35ZydVm9sekS1K5eTDAOmD0nTC48j-AoPlpb1IeErnGJMKK8RgrWznAaqFh5Fk-TVW9DrhKHQ2qLSuvlB4r-3zcoQ_U-gvtqcFmzYV4nt5oMzpplhX69K",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7BJfxriBDxz_2G4QPx3Gw7mXWYEf1F6PSSxjPrvEtJJCwrhVy0Q9JiE4tBJrdlzO-9qjEaYjB4mAY9O3kbz9YIa1XPq1_mYaEyql3W26gAib0YrD-ZMXgis-cwLf31dy11H4LUB54A13VKZjNJ-eWkJBMDt5K71WTqGvRuF-dASBhclXWwFLrYg7dergkoQCAC5SuuzFbKjhnak1E5msjM66yHtT5YaajEbAs2khccG9K0ZBPh794S04=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_-sCFtvoTZaoivsEkNSVy5S60tKKjoVUaIGFZJ8oTzuyaBmEhzpX0Nrd7hD1QL5t34f2Ag_j0eKNkkylyPhOJQbdW975kK-6FGRLwz8bTyjYxqZRxuft8Hpy8FPWy6PcgOkEojQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhX40UjaMDW8pfD4BiCJql4Jv3hxJKhUfq7Wt0leBTVtugg30TrLtHdyHsVyVoO0Qc9mRrMEcYbNmbGCe1u2N0pz4omhcY2afn9Zul7n7a4ZVR65Av3Hee62F7D2zpEEzR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCBCnH4S_0TaZlOgEOIWCKeh8fH9flj0TGYj629tcTbzXSexKEvabPYIc3FQgZJqfuyusS3Ccvltk_xJdhYlbn6ETAatvn-bGw6kUS85flkfe9QN5KUOymJRaNzIRw",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2_aoEWv1tBz2Dv_cIvFcbLYd7t3m4DUbhw8gMqmsZ1LcehELsO8q4MXukMlKUuxINszq8om-7aLpP2S4SoMvI71LHgH29jBd90aF6Arm0Yp7Zbr5bb9nx-sWfFXaWP-gTzDrQnVrkqBniOTHXf3L3qqNlX_XLCHTEenbv",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7Rrr4tH57Z31lwBgNPCDJrFdd-5HTO3JBaVDwdDL2Fvshc1Rt9dDF8E42XvZ-tBV8RlwzO78CmTrkcs74qYmyRIiRT-rL9QM6_LbXlJ57s-yAQ56PBZYNTK_x",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHeT6XRhsNsGv7wVbzgNlT3yD7BghQOBaaEK5SCqNF6gPEPxA3Nztm-HUlfq05Ep9-oqQleei1hKfQXZWYrFcLnUe73ZE1fKWtgt5AIW8WVuJSpwneclirmhtWFhqQ-s9eK9ZZ_ArUmtLCSwxWMCqyQhTels1Ne_1Gig9-CoVGHHOAkWTlHFpdHctwF_mhZocvOsYHJ_KWSwxTUXOug6utDGsA-Tu6JhxA4utdy6nR3gOBisGrtDWRt_kFq16ufPSfB",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_Ru4rhbgtn4Ze2gi3XhQ79BcSxeni-B-c5YsC0eBJ2vaMjxqIs76m8yoUBTo4KIKIN0udc1aD0X_7N5w9JflcfDc0GvFj-2ned_eJkV-2AN6qric4v3xRqBn-xi3b4VexF_SbN_yYS8Mo3lqO3RXp1EzsSn3buJH15z1uZpgfMx781gOtcrszSuyFXpKuvugvQ0b2v5o61YTKXg==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AV-Deepfake1M",
    "paperLink": "https://arxiv.org/abs/2311.17655",
    "description": "A large-scale dataset focusing on content-driven audio-visual deepfakes. It is designed to benchmark detection and localization of manipulated segments in high-fidelity videos.",
    "authors": [
      "Zhixi Cai",
      "et al."
    ],
    "githubLink": "https://github.com/ControlNet/AV-Deepfake1M",
    "itemCount": "1M+ videos",
    "source": "arXiv",
    "specs": "Video, Audio",
    "year": "2023",
    "id": "imported-json-1769638825347-612-xxyx8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG0iu2RySq35ZydVm9sekS1K5eTDAOmD0nTC48j-AoPlpb1IeErnGJMKK8RgrWznAaqFh5Fk-TVW9DrhKHQ2qLSuvlB4r-3zcoQ_U-gvtqcFmzYV4nt5oMzpplhX69K",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7BJfxriBDxz_2G4QPx3Gw7mXWYEf1F6PSSxjPrvEtJJCwrhVy0Q9JiE4tBJrdlzO-9qjEaYjB4mAY9O3kbz9YIa1XPq1_mYaEyql3W26gAib0YrD-ZMXgis-cwLf31dy11H4LUB54A13VKZjNJ-eWkJBMDt5K71WTqGvRuF-dASBhclXWwFLrYg7dergkoQCAC5SuuzFbKjhnak1E5msjM66yHtT5YaajEbAs2khccG9K0ZBPh794S04=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_-sCFtvoTZaoivsEkNSVy5S60tKKjoVUaIGFZJ8oTzuyaBmEhzpX0Nrd7hD1QL5t34f2Ag_j0eKNkkylyPhOJQbdW975kK-6FGRLwz8bTyjYxqZRxuft8Hpy8FPWy6PcgOkEojQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhX40UjaMDW8pfD4BiCJql4Jv3hxJKhUfq7Wt0leBTVtugg30TrLtHdyHsVyVoO0Qc9mRrMEcYbNmbGCe1u2N0pz4omhcY2afn9Zul7n7a4ZVR65Av3Hee62F7D2zpEEzR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCBCnH4S_0TaZlOgEOIWCKeh8fH9flj0TGYj629tcTbzXSexKEvabPYIc3FQgZJqfuyusS3Ccvltk_xJdhYlbn6ETAatvn-bGw6kUS85flkfe9QN5KUOymJRaNzIRw",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2_aoEWv1tBz2Dv_cIvFcbLYd7t3m4DUbhw8gMqmsZ1LcehELsO8q4MXukMlKUuxINszq8om-7aLpP2S4SoMvI71LHgH29jBd90aF6Arm0Yp7Zbr5bb9nx-sWfFXaWP-gTzDrQnVrkqBniOTHXf3L3qqNlX_XLCHTEenbv",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7Rrr4tH57Z31lwBgNPCDJrFdd-5HTO3JBaVDwdDL2Fvshc1Rt9dDF8E42XvZ-tBV8RlwzO78CmTrkcs74qYmyRIiRT-rL9QM6_LbXlJ57s-yAQ56PBZYNTK_x",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHeT6XRhsNsGv7wVbzgNlT3yD7BghQOBaaEK5SCqNF6gPEPxA3Nztm-HUlfq05Ep9-oqQleei1hKfQXZWYrFcLnUe73ZE1fKWtgt5AIW8WVuJSpwneclirmhtWFhqQ-s9eK9ZZ_ArUmtLCSwxWMCqyQhTels1Ne_1Gig9-CoVGHHOAkWTlHFpdHctwF_mhZocvOsYHJ_KWSwxTUXOug6utDGsA-Tu6JhxA4utdy6nR3gOBisGrtDWRt_kFq16ufPSfB",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_Ru4rhbgtn4Ze2gi3XhQ79BcSxeni-B-c5YsC0eBJ2vaMjxqIs76m8yoUBTo4KIKIN0udc1aD0X_7N5w9JflcfDc0GvFj-2ned_eJkV-2AN6qric4v3xRqBn-xi3b4VexF_SbN_yYS8Mo3lqO3RXp1EzsSn3buJH15z1uZpgfMx781gOtcrszSuyFXpKuvugvQ0b2v5o61YTKXg==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "DGM^4",
    "paperLink": "https://arxiv.org/abs/2304.02556",
    "description": "A dataset for Detecting and Grounding Multi-Modal Media Manipulation. It focuses on human-centric news misinformation, containing image-text pairs with fine-grained manipulation annotations.",
    "authors": [
      "Rui Shao",
      "Tianxing Wu",
      "Ziwei Liu"
    ],
    "githubLink": "https://github.com/rshaojimmy/MultiModal-DeepFake",
    "itemCount": "230,000 image-text pairs",
    "source": "arXiv",
    "specs": "Image, Text",
    "year": "2023",
    "id": "imported-json-1769638825347-613-gwged",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG0iu2RySq35ZydVm9sekS1K5eTDAOmD0nTC48j-AoPlpb1IeErnGJMKK8RgrWznAaqFh5Fk-TVW9DrhKHQ2qLSuvlB4r-3zcoQ_U-gvtqcFmzYV4nt5oMzpplhX69K",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7BJfxriBDxz_2G4QPx3Gw7mXWYEf1F6PSSxjPrvEtJJCwrhVy0Q9JiE4tBJrdlzO-9qjEaYjB4mAY9O3kbz9YIa1XPq1_mYaEyql3W26gAib0YrD-ZMXgis-cwLf31dy11H4LUB54A13VKZjNJ-eWkJBMDt5K71WTqGvRuF-dASBhclXWwFLrYg7dergkoQCAC5SuuzFbKjhnak1E5msjM66yHtT5YaajEbAs2khccG9K0ZBPh794S04=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_-sCFtvoTZaoivsEkNSVy5S60tKKjoVUaIGFZJ8oTzuyaBmEhzpX0Nrd7hD1QL5t34f2Ag_j0eKNkkylyPhOJQbdW975kK-6FGRLwz8bTyjYxqZRxuft8Hpy8FPWy6PcgOkEojQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhX40UjaMDW8pfD4BiCJql4Jv3hxJKhUfq7Wt0leBTVtugg30TrLtHdyHsVyVoO0Qc9mRrMEcYbNmbGCe1u2N0pz4omhcY2afn9Zul7n7a4ZVR65Av3Hee62F7D2zpEEzR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCBCnH4S_0TaZlOgEOIWCKeh8fH9flj0TGYj629tcTbzXSexKEvabPYIc3FQgZJqfuyusS3Ccvltk_xJdhYlbn6ETAatvn-bGw6kUS85flkfe9QN5KUOymJRaNzIRw",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2_aoEWv1tBz2Dv_cIvFcbLYd7t3m4DUbhw8gMqmsZ1LcehELsO8q4MXukMlKUuxINszq8om-7aLpP2S4SoMvI71LHgH29jBd90aF6Arm0Yp7Zbr5bb9nx-sWfFXaWP-gTzDrQnVrkqBniOTHXf3L3qqNlX_XLCHTEenbv",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7Rrr4tH57Z31lwBgNPCDJrFdd-5HTO3JBaVDwdDL2Fvshc1Rt9dDF8E42XvZ-tBV8RlwzO78CmTrkcs74qYmyRIiRT-rL9QM6_LbXlJ57s-yAQ56PBZYNTK_x",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHeT6XRhsNsGv7wVbzgNlT3yD7BghQOBaaEK5SCqNF6gPEPxA3Nztm-HUlfq05Ep9-oqQleei1hKfQXZWYrFcLnUe73ZE1fKWtgt5AIW8WVuJSpwneclirmhtWFhqQ-s9eK9ZZ_ArUmtLCSwxWMCqyQhTels1Ne_1Gig9-CoVGHHOAkWTlHFpdHctwF_mhZocvOsYHJ_KWSwxTUXOug6utDGsA-Tu6JhxA4utdy6nR3gOBisGrtDWRt_kFq16ufPSfB",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_Ru4rhbgtn4Ze2gi3XhQ79BcSxeni-B-c5YsC0eBJ2vaMjxqIs76m8yoUBTo4KIKIN0udc1aD0X_7N5w9JflcfDc0GvFj-2ned_eJkV-2AN6qric4v3xRqBn-xi3b4VexF_SbN_yYS8Mo3lqO3RXp1EzsSn3buJH15z1uZpgfMx781gOtcrszSuyFXpKuvugvQ0b2v5o61YTKXg==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "LAV-DF",
    "paperLink": "https://arxiv.org/abs/2204.06228",
    "description": "Localized Audio Visual DeepFake dataset explicitly designed for temporal forgery localization. It includes content-driven manipulations where audio and visual elements are altered to change the sentiment or meaning.",
    "authors": [
      "Zhixi Cai",
      "Kalin Stefanov",
      "Abhinav Dhall",
      "Munawar Hayat"
    ],
    "githubLink": "https://github.com/ControlNet/LAV-DF",
    "itemCount": "136,000 clips",
    "source": "arXiv",
    "specs": "Video, Audio",
    "year": "2022",
    "id": "imported-json-1769638825347-614-g1qf2",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG0iu2RySq35ZydVm9sekS1K5eTDAOmD0nTC48j-AoPlpb1IeErnGJMKK8RgrWznAaqFh5Fk-TVW9DrhKHQ2qLSuvlB4r-3zcoQ_U-gvtqcFmzYV4nt5oMzpplhX69K",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7BJfxriBDxz_2G4QPx3Gw7mXWYEf1F6PSSxjPrvEtJJCwrhVy0Q9JiE4tBJrdlzO-9qjEaYjB4mAY9O3kbz9YIa1XPq1_mYaEyql3W26gAib0YrD-ZMXgis-cwLf31dy11H4LUB54A13VKZjNJ-eWkJBMDt5K71WTqGvRuF-dASBhclXWwFLrYg7dergkoQCAC5SuuzFbKjhnak1E5msjM66yHtT5YaajEbAs2khccG9K0ZBPh794S04=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_-sCFtvoTZaoivsEkNSVy5S60tKKjoVUaIGFZJ8oTzuyaBmEhzpX0Nrd7hD1QL5t34f2Ag_j0eKNkkylyPhOJQbdW975kK-6FGRLwz8bTyjYxqZRxuft8Hpy8FPWy6PcgOkEojQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhX40UjaMDW8pfD4BiCJql4Jv3hxJKhUfq7Wt0leBTVtugg30TrLtHdyHsVyVoO0Qc9mRrMEcYbNmbGCe1u2N0pz4omhcY2afn9Zul7n7a4ZVR65Av3Hee62F7D2zpEEzR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCBCnH4S_0TaZlOgEOIWCKeh8fH9flj0TGYj629tcTbzXSexKEvabPYIc3FQgZJqfuyusS3Ccvltk_xJdhYlbn6ETAatvn-bGw6kUS85flkfe9QN5KUOymJRaNzIRw",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2_aoEWv1tBz2Dv_cIvFcbLYd7t3m4DUbhw8gMqmsZ1LcehELsO8q4MXukMlKUuxINszq8om-7aLpP2S4SoMvI71LHgH29jBd90aF6Arm0Yp7Zbr5bb9nx-sWfFXaWP-gTzDrQnVrkqBniOTHXf3L3qqNlX_XLCHTEenbv",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7Rrr4tH57Z31lwBgNPCDJrFdd-5HTO3JBaVDwdDL2Fvshc1Rt9dDF8E42XvZ-tBV8RlwzO78CmTrkcs74qYmyRIiRT-rL9QM6_LbXlJ57s-yAQ56PBZYNTK_x",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHeT6XRhsNsGv7wVbzgNlT3yD7BghQOBaaEK5SCqNF6gPEPxA3Nztm-HUlfq05Ep9-oqQleei1hKfQXZWYrFcLnUe73ZE1fKWtgt5AIW8WVuJSpwneclirmhtWFhqQ-s9eK9ZZ_ArUmtLCSwxWMCqyQhTels1Ne_1Gig9-CoVGHHOAkWTlHFpdHctwF_mhZocvOsYHJ_KWSwxTUXOug6utDGsA-Tu6JhxA4utdy6nR3gOBisGrtDWRt_kFq16ufPSfB",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_Ru4rhbgtn4Ze2gi3XhQ79BcSxeni-B-c5YsC0eBJ2vaMjxqIs76m8yoUBTo4KIKIN0udc1aD0X_7N5w9JflcfDc0GvFj-2ned_eJkV-2AN6qric4v3xRqBn-xi3b4VexF_SbN_yYS8Mo3lqO3RXp1EzsSn3buJH15z1uZpgfMx781gOtcrszSuyFXpKuvugvQ0b2v5o61YTKXg==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "KoDF",
    "paperLink": "https://moneybrain-research.github.io/kodf/",
    "description": "A large-scale Korean DeepFake Detection Dataset constructed to provide a large collection of synthesized and real videos focused on Korean subjects, addressing the lack of ethnic diversity in previous datasets.",
    "authors": [
      "Patrick Kwon",
      "et al."
    ],
    "githubLink": "https://github.com/moneybrain-research/kodf",
    "itemCount": "237,942 videos (62,166 real, 175,776 fake)",
    "source": "GitHub",
    "specs": "Video, Audio",
    "year": "2021",
    "id": "imported-json-1769638825347-615-hop66",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG0iu2RySq35ZydVm9sekS1K5eTDAOmD0nTC48j-AoPlpb1IeErnGJMKK8RgrWznAaqFh5Fk-TVW9DrhKHQ2qLSuvlB4r-3zcoQ_U-gvtqcFmzYV4nt5oMzpplhX69K",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7BJfxriBDxz_2G4QPx3Gw7mXWYEf1F6PSSxjPrvEtJJCwrhVy0Q9JiE4tBJrdlzO-9qjEaYjB4mAY9O3kbz9YIa1XPq1_mYaEyql3W26gAib0YrD-ZMXgis-cwLf31dy11H4LUB54A13VKZjNJ-eWkJBMDt5K71WTqGvRuF-dASBhclXWwFLrYg7dergkoQCAC5SuuzFbKjhnak1E5msjM66yHtT5YaajEbAs2khccG9K0ZBPh794S04=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_-sCFtvoTZaoivsEkNSVy5S60tKKjoVUaIGFZJ8oTzuyaBmEhzpX0Nrd7hD1QL5t34f2Ag_j0eKNkkylyPhOJQbdW975kK-6FGRLwz8bTyjYxqZRxuft8Hpy8FPWy6PcgOkEojQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhX40UjaMDW8pfD4BiCJql4Jv3hxJKhUfq7Wt0leBTVtugg30TrLtHdyHsVyVoO0Qc9mRrMEcYbNmbGCe1u2N0pz4omhcY2afn9Zul7n7a4ZVR65Av3Hee62F7D2zpEEzR",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCBCnH4S_0TaZlOgEOIWCKeh8fH9flj0TGYj629tcTbzXSexKEvabPYIc3FQgZJqfuyusS3Ccvltk_xJdhYlbn6ETAatvn-bGw6kUS85flkfe9QN5KUOymJRaNzIRw",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2_aoEWv1tBz2Dv_cIvFcbLYd7t3m4DUbhw8gMqmsZ1LcehELsO8q4MXukMlKUuxINszq8om-7aLpP2S4SoMvI71LHgH29jBd90aF6Arm0Yp7Zbr5bb9nx-sWfFXaWP-gTzDrQnVrkqBniOTHXf3L3qqNlX_XLCHTEenbv",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7Rrr4tH57Z31lwBgNPCDJrFdd-5HTO3JBaVDwdDL2Fvshc1Rt9dDF8E42XvZ-tBV8RlwzO78CmTrkcs74qYmyRIiRT-rL9QM6_LbXlJ57s-yAQ56PBZYNTK_x",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHeT6XRhsNsGv7wVbzgNlT3yD7BghQOBaaEK5SCqNF6gPEPxA3Nztm-HUlfq05Ep9-oqQleei1hKfQXZWYrFcLnUe73ZE1fKWtgt5AIW8WVuJSpwneclirmhtWFhqQ-s9eK9ZZ_ArUmtLCSwxWMCqyQhTels1Ne_1Gig9-CoVGHHOAkWTlHFpdHctwF_mhZocvOsYHJ_KWSwxTUXOug6utDGsA-Tu6JhxA4utdy6nR3gOBisGrtDWRt_kFq16ufPSfB",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_Ru4rhbgtn4Ze2gi3XhQ79BcSxeni-B-c5YsC0eBJ2vaMjxqIs76m8yoUBTo4KIKIN0udc1aD0X_7N5w9JflcfDc0GvFj-2ned_eJkV-2AN6qric4v3xRqBn-xi3b4VexF_SbN_yYS8Mo3lqO3RXp1EzsSn3buJH15z1uZpgfMx781gOtcrszSuyFXpKuvugvQ0b2v5o61YTKXg==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "DREAM: A Benchmark Study for Deepfake REalism AssessMent",
    "paperLink": "https://arxiv.org/abs/2510.10053",
    "description": "A comprehensive benchmark specifically designed for the visual realism assessment of deepfakes, rather than just binary detection. It evaluates how realistic generated content looks to human observers, bridging the gap between detection and perception.",
    "authors": [
      "Bo Peng",
      "Zichuan Wang",
      "Sheng Yu",
      "Xiaochuan Jin",
      "Wei Wang",
      "Jing Dong"
    ],
    "githubLink": "Not available",
    "itemCount": "140,000 realism scores from 3,500 annotators",
    "source": "arXiv",
    "specs": "Video, Text (descriptions), Realism Scores",
    "year": "2025",
    "id": "imported-json-1769638825347-616-6s5nc",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCqitrKQOgC7yscIvndul9SzIvr6DryIY85tQlWvyQBZlCnbaw5_Ls6cKjWnpmUUEcWOLMGEZ6U3UHLJYQoWHVVRn1HehgLx6DAfUgY1gomFccVSJXy0YNcM-V",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIUFn-QP4iZdVcjGUBUvyFIN-KhIAGNZvY-dGAR6aEFMft1i0AuXu9W8aHYbO-ZCuWHNb_tP-gn9O2Iy_WWba0Vhs9FoCQTrZVquWcg-nCk0O8oPPMgAL8L4o6eXKJ3r3J1CFzXnON5VhXTh3_uO1FmtVE1aAZcaNmmtkEtWbWnU3v7xBPZzEtCMUET21OT262qW_EkHHZ6haAfbf376_a0syDhrqr0o9op4Xyi3DjfQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtPty1w9X--OQ0Fwaq-P9Ym0egaxdrZrJbGGxP1ZJgcx6i9tsldM4DeDlpUfJsOz7AGEP7PHqEYFDiLDrNjMVBFShcahRJdE2IeWQ7QigFNrIgrn8AqXkQZHeCjM4uhd6XCMcx3ZG9AJ1yEoRs4HY5WyQzdQDqy45eDPwT",
        "title": "ai.gov.pl"
      }
    ]
  },
  {
    "title": "DF40",
    "paperLink": "https://arxiv.org/abs/2406.13495",
    "description": "A highly diverse deepfake dataset constructed to address the lack of variety in existing benchmarks. It incorporates 40 distinct deepfake generation techniques, including state-of-the-art diffusion models and face-swapping methods, to evaluate detection generalization and realism.",
    "authors": [
      "Zhiyuan Yan",
      "Taiping Yao",
      "Shen Chen",
      "Yandan Zhao",
      "Xinghe Fu",
      "Junwei Zhu",
      "Donghao Luo",
      "Chengjie Wang",
      "Shouhong Ding",
      "Yunsheng Wu",
      "Li Yuan"
    ],
    "githubLink": "https://github.com/YZY-stack/DF40",
    "itemCount": "Unknown (covers 40 distinct techniques)",
    "source": "arXiv",
    "specs": "Video, Image",
    "year": "2024",
    "id": "imported-json-1769638825347-617-iznww",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCqitrKQOgC7yscIvndul9SzIvr6DryIY85tQlWvyQBZlCnbaw5_Ls6cKjWnpmUUEcWOLMGEZ6U3UHLJYQoWHVVRn1HehgLx6DAfUgY1gomFccVSJXy0YNcM-V",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIUFn-QP4iZdVcjGUBUvyFIN-KhIAGNZvY-dGAR6aEFMft1i0AuXu9W8aHYbO-ZCuWHNb_tP-gn9O2Iy_WWba0Vhs9FoCQTrZVquWcg-nCk0O8oPPMgAL8L4o6eXKJ3r3J1CFzXnON5VhXTh3_uO1FmtVE1aAZcaNmmtkEtWbWnU3v7xBPZzEtCMUET21OT262qW_EkHHZ6haAfbf376_a0syDhrqr0o9op4Xyi3DjfQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtPty1w9X--OQ0Fwaq-P9Ym0egaxdrZrJbGGxP1ZJgcx6i9tsldM4DeDlpUfJsOz7AGEP7PHqEYFDiLDrNjMVBFShcahRJdE2IeWQ7QigFNrIgrn8AqXkQZHeCjM4uhd6XCMcx3ZG9AJ1yEoRs4HY5WyQzdQDqy45eDPwT",
        "title": "ai.gov.pl"
      }
    ]
  },
  {
    "title": "ForgeryNet",
    "paperLink": "https://arxiv.org/abs/2103.05630",
    "description": "An extremely large versatile benchmark for comprehensive forgery analysis, including image and video level classification and localization. It includes a vast number of perturbation types and manipulation approaches.",
    "authors": [
      "Yinan He",
      "Bei Gan",
      "Siyu Chen",
      "Yichun Zhou",
      "Guojun Yin",
      "Luchuan Song",
      "Lu Sheng",
      "Jing Shao",
      "Ziwei Liu"
    ],
    "githubLink": "https://github.com/yinanhe/ForgeryNet",
    "itemCount": "2.9 million images, 221,247 videos",
    "source": "CVPR",
    "specs": "Image, Video",
    "year": "2021",
    "id": "imported-json-1769638825347-618-69cgp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCqitrKQOgC7yscIvndul9SzIvr6DryIY85tQlWvyQBZlCnbaw5_Ls6cKjWnpmUUEcWOLMGEZ6U3UHLJYQoWHVVRn1HehgLx6DAfUgY1gomFccVSJXy0YNcM-V",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIUFn-QP4iZdVcjGUBUvyFIN-KhIAGNZvY-dGAR6aEFMft1i0AuXu9W8aHYbO-ZCuWHNb_tP-gn9O2Iy_WWba0Vhs9FoCQTrZVquWcg-nCk0O8oPPMgAL8L4o6eXKJ3r3J1CFzXnON5VhXTh3_uO1FmtVE1aAZcaNmmtkEtWbWnU3v7xBPZzEtCMUET21OT262qW_EkHHZ6haAfbf376_a0syDhrqr0o9op4Xyi3DjfQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtPty1w9X--OQ0Fwaq-P9Ym0egaxdrZrJbGGxP1ZJgcx6i9tsldM4DeDlpUfJsOz7AGEP7PHqEYFDiLDrNjMVBFShcahRJdE2IeWQ7QigFNrIgrn8AqXkQZHeCjM4uhd6XCMcx3ZG9AJ1yEoRs4HY5WyQzdQDqy45eDPwT",
        "title": "ai.gov.pl"
      }
    ]
  },
  {
    "title": "Celeb-DF (v2)",
    "paperLink": "https://arxiv.org/abs/1909.12962",
    "description": "Designed to provide high-quality deepfake videos that are visually indistinguishable to the human eye, addressing the quality limitations of earlier datasets like FaceForensics++. It features reduced visual artifacts for better realism assessment.",
    "authors": [
      "Yuezun Li",
      "Xin Yang",
      "Pu Sun",
      "Honggang Qi",
      "Siwei Lyu"
    ],
    "githubLink": "https://github.com/yuezunli/Celeb-DF-v2",
    "itemCount": "5,639 deepfake videos, 590 real videos",
    "source": "CVPR",
    "specs": "Video (MPEG4)",
    "year": "2020",
    "id": "imported-json-1769638825347-619-gc6e4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCqitrKQOgC7yscIvndul9SzIvr6DryIY85tQlWvyQBZlCnbaw5_Ls6cKjWnpmUUEcWOLMGEZ6U3UHLJYQoWHVVRn1HehgLx6DAfUgY1gomFccVSJXy0YNcM-V",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIUFn-QP4iZdVcjGUBUvyFIN-KhIAGNZvY-dGAR6aEFMft1i0AuXu9W8aHYbO-ZCuWHNb_tP-gn9O2Iy_WWba0Vhs9FoCQTrZVquWcg-nCk0O8oPPMgAL8L4o6eXKJ3r3J1CFzXnON5VhXTh3_uO1FmtVE1aAZcaNmmtkEtWbWnU3v7xBPZzEtCMUET21OT262qW_EkHHZ6haAfbf376_a0syDhrqr0o9op4Xyi3DjfQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtPty1w9X--OQ0Fwaq-P9Ym0egaxdrZrJbGGxP1ZJgcx6i9tsldM4DeDlpUfJsOz7AGEP7PHqEYFDiLDrNjMVBFShcahRJdE2IeWQ7QigFNrIgrn8AqXkQZHeCjM4uhd6XCMcx3ZG9AJ1yEoRs4HY5WyQzdQDqy45eDPwT",
        "title": "ai.gov.pl"
      }
    ]
  },
  {
    "title": "WildDeepfake",
    "paperLink": "https://arxiv.org/abs/2101.01456",
    "description": "A dataset collected entirely from the internet to represent real-world 'wild' deepfakes. It offers diverse scenes and higher realism compared to lab-generated datasets, challenging detectors with varied quality and compression.",
    "authors": [
      "Bojia Zi",
      "Minghao Chang",
      "Jingjing Chen",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "githubLink": "https://github.com/OpenTAI/wild-deepfake",
    "itemCount": "7,314 face sequences from 707 videos",
    "source": "ACM Multimedia",
    "specs": "Video sequences",
    "year": "2020",
    "id": "imported-json-1769638825347-620-was7m",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHCqitrKQOgC7yscIvndul9SzIvr6DryIY85tQlWvyQBZlCnbaw5_Ls6cKjWnpmUUEcWOLMGEZ6U3UHLJYQoWHVVRn1HehgLx6DAfUgY1gomFccVSJXy0YNcM-V",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIUFn-QP4iZdVcjGUBUvyFIN-KhIAGNZvY-dGAR6aEFMft1i0AuXu9W8aHYbO-ZCuWHNb_tP-gn9O2Iy_WWba0Vhs9FoCQTrZVquWcg-nCk0O8oPPMgAL8L4o6eXKJ3r3J1CFzXnON5VhXTh3_uO1FmtVE1aAZcaNmmtkEtWbWnU3v7xBPZzEtCMUET21OT262qW_EkHHZ6haAfbf376_a0syDhrqr0o9op4Xyi3DjfQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtPty1w9X--OQ0Fwaq-P9Ym0egaxdrZrJbGGxP1ZJgcx6i9tsldM4DeDlpUfJsOz7AGEP7PHqEYFDiLDrNjMVBFShcahRJdE2IeWQ7QigFNrIgrn8AqXkQZHeCjM4uhd6XCMcx3ZG9AJ1yEoRs4HY5WyQzdQDqy45eDPwT",
        "title": "ai.gov.pl"
      }
    ]
  },
  {
    "title": "SafetyBench: Evaluating the Safety of Large Language Models",
    "paperLink": "https://arxiv.org/abs/2309.13796",
    "description": "A comprehensive benchmark for evaluating LLM safety comprising multiple choice questions across 7 distinct categories of safety concerns, supporting both Chinese and English.",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Linhard Wei",
      "Rui Li",
      "Yuanhui Huang",
      "et al."
    ],
    "githubLink": "https://github.com/thu-coai/SafetyBench",
    "itemCount": "11,435 questions",
    "source": "arXiv",
    "specs": "Multiple choice questions (English and Chinese)",
    "year": "2023",
    "id": "imported-json-1769638825347-621-utxoo",
    "groundingSources": []
  },
  {
    "title": "ToolHop",
    "paperLink": "https://arxiv.org/abs/2501.02506",
    "description": "A query-driven benchmark for evaluating Large Language Models in multi-hop tool use. It is designed to test understanding, reasoning, and function-calling capabilities where multiple connected tool calls are required.",
    "authors": [
      "Junjie Ye",
      "Zhengyin Du",
      "Xuesong Yao",
      "Weijian Lin",
      "Yufei Xu",
      "Zehui Chen",
      "Zaiyuan Wang",
      "Sining Zhu",
      "Zhiheng Xi",
      "Siyu Yuan",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang",
      "Jiecao Chen"
    ],
    "githubLink": "https://github.com/bytedance-research/ToolHop",
    "itemCount": "995 user queries, 3,912 associated tools",
    "source": "arXiv",
    "specs": "Text (Queries, Tools), Code/API calls",
    "year": "2025",
    "id": "imported-json-1769638825347-622-5v64w",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EUWJqTVHaciHPTPbvlvEBfDNujdjI3ThQ5NjtU1Id__l9GazV7zx-NZXDHnL1-B0Rc-AHT8IKSIWsGU9NxHevXYhNZBhdM5RLQI9T6UmC4SSV4VE0EhNZ9CB8dVrQG63qFJ7Hos=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5NfnGBGstqsrJccj2C33Myw7PK0MJDCUlcFhd8KCp1o2pk1k2PFidTNWp1Dl-xutfGbXCJALXty0E5xj6uOaPRbCYx_dZdByQo5XwMlkNNh1WocVdEDeKUM6kVwK7eGlMEIxt8A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFyF2_-icdm2DjmP6nBII_TygiM37q8EIoTbQ45zb3BVkB1lyhqsRGJmZY-ZI4__DHuoa5dE2llPuWjI0NTet1aoS4HUejsKxVAdK4fknJSkP6FqdDXPxboNg==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFsIlXzwryW6vXkdASXB6q3axhKH_Xkfcv627ZJe6ivKO4Vp4otRiqA5PP5FJNrMNgoF1lCb7xGV5Q8SCCVRN7ryr6o9c6aTS13y-FojkxmT7UgHZN96xVsShOXGkEl7W7Fq4OpxfszkmKKlOFyXx13AHzI",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFooon9wxbVlmlEDr71PpRkVhOXQlQgnYiePrdFouC6QPZza11krzrKoe7MT48dZOYUlGWqCZysZNHJZL2J3ClO1WA2jZYg31TKgkj9V20W-jgc9E6xtsuRx0AigAeKb3kV69MB",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLE1T9KNA6_IMx27Dm94tq1COGQZ3i4SHvn87XVAJAtaQ07AMGKPwno9OJjqFCyEXev-C1K7mvQ0u-hccm8uKa5ytevaX7yd7GZ2Klu8UO9A-R3LGkHWwpdw==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMKy-5a3HAWtAoQL0ju9A4fdjxePZsB0p2GfNfhtIL9j45c8AmjQNJ0cSj6z6hXDT-g1ja5awG-9poswIKbQtk3L1kDzgzpHgXt_ZxRCqByUVBx5Q2XpoOt3loyEGltp5l8L2i0ibTXabhkbDmlz4ZpCEg9NKcqrxjLTj7_9ES6A6-DPhOhGBafl5a5PWQD1NPX1x1z1331B00Not1UyMHoi8xQFfepILAWsOxo7Ov_eIDRcpzKUtVAkCTvexrmsBbq7PRyy9vNA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYuOF5rySpHOnw5xjDFvRdVJCZX34m3IlSlJl3wFS1TDKmXV8iMuOJZaK-gjd8r3NaFmIWYFUtuO-hv5RLQSZYtFQQV6JaJDQpFgsiK4Wip2ltO4J9Q55g6tcUAGGRNpBR858FdLfS",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGD95RZQXXZbF4VV8xpeq5VCWqMg6lT-abRAXQtnp9FgOukqAT-nqzDxGwpRMhyLQdUj4o_hLoiA9Axty7lpg37KjYLqOJW3NSIdKwCeM02wXK0ICHE2R3WO8MAYU2QSugfR8D2IcbKkjuCFwa2JfdAgKo7",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQhFHzevcJ-skQ-TV23VGTJV-KMhsce_ZyJUXO3P9-xLbTVluonileRTpcyyCQ6CcfggeVdK_9dnZbsdUElWKLiE2S1U10hcbHBgmXwyk3rW5cezMqGWIIJ25kCgL9tb46Kos_DrCVdleoztvcQAxq_yIAfjtPrlo7ghEoxG_tqlQkxHFAXMW1ZWsdKUI=",
        "title": "towardsdatascience.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQElXfG2nl-3c51gOSRihWs3IzGBC1pI6waNqDgSaQ5aRc45TOOM_YK1xAyHUrCFJMB8ZZIbaawlP18d5uXmQo1WLXU4LVETuC8QWv0IYjag3jIongxYrrnzwbAtcNzuS72HPGN8P8Ch0qcByhe0h0I6biJ_4m-2cN9lfYsQA0f00LcGJVl12gk=",
        "title": "manchester.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp0cEjkSk8xF3am5R_UoMKglZfNa5wikJ8Ie-t_RSUQBACDqulo6lO270axzIW34Jd1pUa020OyJ66-NJ-feOOo-gqmI1x1TuvaCo2jX3BQcNnk7p2_rHqh4j7kdkrruAn2aIzLio3qyzo8lcQLH5rdpsMoOPzkcWzJ04OTlr-ubmBC4A6aJ6Catq8KkVGB4b9kOi7uDz0C2_mpWKmPfcNkIflqe2z",
        "title": "iclr.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYV9i6G-a_nGbtj11UlffCFHtmG0VEfPlNfHmKt0bDPace2LQ4TJrOpohAFZ5LNpw6FBx-xYjfxJCrMZ07UzpYu2nAk-olzAQtFSt8fclxnTLT1OrjYD6boM_K1qkEH6yLMbY=",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "API-Bank",
    "paperLink": "https://arxiv.org/abs/2304.08244",
    "description": "A comprehensive benchmark for tool-augmented LLMs, designed to evaluate capabilities in planning, retrieving, and calling APIs. It includes a runnable evaluation system and a training set of tool-use dialogues.",
    "authors": [
      "Minghao Li",
      "Feifan Song",
      "Bowen Yu",
      "Haiyang Yu",
      "Zhoujun Li",
      "Fei Huang",
      "Yongbin Li"
    ],
    "githubLink": "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank",
    "itemCount": "73 APIs (eval), 2,138 APIs (training), 1,888 tool-use dialogues",
    "source": "arXiv",
    "specs": "Text (Dialogues, API definitions), JSON",
    "year": "2023",
    "id": "imported-json-1769638825347-623-lunby",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EUWJqTVHaciHPTPbvlvEBfDNujdjI3ThQ5NjtU1Id__l9GazV7zx-NZXDHnL1-B0Rc-AHT8IKSIWsGU9NxHevXYhNZBhdM5RLQI9T6UmC4SSV4VE0EhNZ9CB8dVrQG63qFJ7Hos=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5NfnGBGstqsrJccj2C33Myw7PK0MJDCUlcFhd8KCp1o2pk1k2PFidTNWp1Dl-xutfGbXCJALXty0E5xj6uOaPRbCYx_dZdByQo5XwMlkNNh1WocVdEDeKUM6kVwK7eGlMEIxt8A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFyF2_-icdm2DjmP6nBII_TygiM37q8EIoTbQ45zb3BVkB1lyhqsRGJmZY-ZI4__DHuoa5dE2llPuWjI0NTet1aoS4HUejsKxVAdK4fknJSkP6FqdDXPxboNg==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFsIlXzwryW6vXkdASXB6q3axhKH_Xkfcv627ZJe6ivKO4Vp4otRiqA5PP5FJNrMNgoF1lCb7xGV5Q8SCCVRN7ryr6o9c6aTS13y-FojkxmT7UgHZN96xVsShOXGkEl7W7Fq4OpxfszkmKKlOFyXx13AHzI",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFooon9wxbVlmlEDr71PpRkVhOXQlQgnYiePrdFouC6QPZza11krzrKoe7MT48dZOYUlGWqCZysZNHJZL2J3ClO1WA2jZYg31TKgkj9V20W-jgc9E6xtsuRx0AigAeKb3kV69MB",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLE1T9KNA6_IMx27Dm94tq1COGQZ3i4SHvn87XVAJAtaQ07AMGKPwno9OJjqFCyEXev-C1K7mvQ0u-hccm8uKa5ytevaX7yd7GZ2Klu8UO9A-R3LGkHWwpdw==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMKy-5a3HAWtAoQL0ju9A4fdjxePZsB0p2GfNfhtIL9j45c8AmjQNJ0cSj6z6hXDT-g1ja5awG-9poswIKbQtk3L1kDzgzpHgXt_ZxRCqByUVBx5Q2XpoOt3loyEGltp5l8L2i0ibTXabhkbDmlz4ZpCEg9NKcqrxjLTj7_9ES6A6-DPhOhGBafl5a5PWQD1NPX1x1z1331B00Not1UyMHoi8xQFfepILAWsOxo7Ov_eIDRcpzKUtVAkCTvexrmsBbq7PRyy9vNA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYuOF5rySpHOnw5xjDFvRdVJCZX34m3IlSlJl3wFS1TDKmXV8iMuOJZaK-gjd8r3NaFmIWYFUtuO-hv5RLQSZYtFQQV6JaJDQpFgsiK4Wip2ltO4J9Q55g6tcUAGGRNpBR858FdLfS",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGD95RZQXXZbF4VV8xpeq5VCWqMg6lT-abRAXQtnp9FgOukqAT-nqzDxGwpRMhyLQdUj4o_hLoiA9Axty7lpg37KjYLqOJW3NSIdKwCeM02wXK0ICHE2R3WO8MAYU2QSugfR8D2IcbKkjuCFwa2JfdAgKo7",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQhFHzevcJ-skQ-TV23VGTJV-KMhsce_ZyJUXO3P9-xLbTVluonileRTpcyyCQ6CcfggeVdK_9dnZbsdUElWKLiE2S1U10hcbHBgmXwyk3rW5cezMqGWIIJ25kCgL9tb46Kos_DrCVdleoztvcQAxq_yIAfjtPrlo7ghEoxG_tqlQkxHFAXMW1ZWsdKUI=",
        "title": "towardsdatascience.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQElXfG2nl-3c51gOSRihWs3IzGBC1pI6waNqDgSaQ5aRc45TOOM_YK1xAyHUrCFJMB8ZZIbaawlP18d5uXmQo1WLXU4LVETuC8QWv0IYjag3jIongxYrrnzwbAtcNzuS72HPGN8P8Ch0qcByhe0h0I6biJ_4m-2cN9lfYsQA0f00LcGJVl12gk=",
        "title": "manchester.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp0cEjkSk8xF3am5R_UoMKglZfNa5wikJ8Ie-t_RSUQBACDqulo6lO270axzIW34Jd1pUa020OyJ66-NJ-feOOo-gqmI1x1TuvaCo2jX3BQcNnk7p2_rHqh4j7kdkrruAn2aIzLio3qyzo8lcQLH5rdpsMoOPzkcWzJ04OTlr-ubmBC4A6aJ6Catq8KkVGB4b9kOi7uDz0C2_mpWKmPfcNkIflqe2z",
        "title": "iclr.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYV9i6G-a_nGbtj11UlffCFHtmG0VEfPlNfHmKt0bDPace2LQ4TJrOpohAFZ5LNpw6FBx-xYjfxJCrMZ07UzpYu2nAk-olzAQtFSt8fclxnTLT1OrjYD6boM_K1qkEH6yLMbY=",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "RestBench",
    "paperLink": "https://arxiv.org/abs/2306.00445",
    "description": "A high-quality benchmark for evaluating autonomous agents on real-world applications via RESTful APIs. It consists of two scenarios: TMDB (movie database) and Spotify (music player), with human-annotated instructions and gold solution paths.",
    "authors": [
      "Yifan Song",
      "Weimin Xiong",
      "Dawei Zhu",
      "Wenhao Wu",
      "Han Wang",
      "Ye Li",
      "Ming Li",
      "Suian Wang"
    ],
    "githubLink": "https://github.com/Yifan-Song793/RestGPT",
    "itemCount": "TMDB: 100 tasks, 54 tools; Spotify: 40 tools",
    "source": "arXiv",
    "specs": "Text (Instructions), RESTful API definitions",
    "year": "2023",
    "id": "imported-json-1769638825347-624-toao8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-EUWJqTVHaciHPTPbvlvEBfDNujdjI3ThQ5NjtU1Id__l9GazV7zx-NZXDHnL1-B0Rc-AHT8IKSIWsGU9NxHevXYhNZBhdM5RLQI9T6UmC4SSV4VE0EhNZ9CB8dVrQG63qFJ7Hos=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5NfnGBGstqsrJccj2C33Myw7PK0MJDCUlcFhd8KCp1o2pk1k2PFidTNWp1Dl-xutfGbXCJALXty0E5xj6uOaPRbCYx_dZdByQo5XwMlkNNh1WocVdEDeKUM6kVwK7eGlMEIxt8A==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFyF2_-icdm2DjmP6nBII_TygiM37q8EIoTbQ45zb3BVkB1lyhqsRGJmZY-ZI4__DHuoa5dE2llPuWjI0NTet1aoS4HUejsKxVAdK4fknJSkP6FqdDXPxboNg==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFsIlXzwryW6vXkdASXB6q3axhKH_Xkfcv627ZJe6ivKO4Vp4otRiqA5PP5FJNrMNgoF1lCb7xGV5Q8SCCVRN7ryr6o9c6aTS13y-FojkxmT7UgHZN96xVsShOXGkEl7W7Fq4OpxfszkmKKlOFyXx13AHzI",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFooon9wxbVlmlEDr71PpRkVhOXQlQgnYiePrdFouC6QPZza11krzrKoe7MT48dZOYUlGWqCZysZNHJZL2J3ClO1WA2jZYg31TKgkj9V20W-jgc9E6xtsuRx0AigAeKb3kV69MB",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLE1T9KNA6_IMx27Dm94tq1COGQZ3i4SHvn87XVAJAtaQ07AMGKPwno9OJjqFCyEXev-C1K7mvQ0u-hccm8uKa5ytevaX7yd7GZ2Klu8UO9A-R3LGkHWwpdw==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMKy-5a3HAWtAoQL0ju9A4fdjxePZsB0p2GfNfhtIL9j45c8AmjQNJ0cSj6z6hXDT-g1ja5awG-9poswIKbQtk3L1kDzgzpHgXt_ZxRCqByUVBx5Q2XpoOt3loyEGltp5l8L2i0ibTXabhkbDmlz4ZpCEg9NKcqrxjLTj7_9ES6A6-DPhOhGBafl5a5PWQD1NPX1x1z1331B00Not1UyMHoi8xQFfepILAWsOxo7Ov_eIDRcpzKUtVAkCTvexrmsBbq7PRyy9vNA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYuOF5rySpHOnw5xjDFvRdVJCZX34m3IlSlJl3wFS1TDKmXV8iMuOJZaK-gjd8r3NaFmIWYFUtuO-hv5RLQSZYtFQQV6JaJDQpFgsiK4Wip2ltO4J9Q55g6tcUAGGRNpBR858FdLfS",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGD95RZQXXZbF4VV8xpeq5VCWqMg6lT-abRAXQtnp9FgOukqAT-nqzDxGwpRMhyLQdUj4o_hLoiA9Axty7lpg37KjYLqOJW3NSIdKwCeM02wXK0ICHE2R3WO8MAYU2QSugfR8D2IcbKkjuCFwa2JfdAgKo7",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQhFHzevcJ-skQ-TV23VGTJV-KMhsce_ZyJUXO3P9-xLbTVluonileRTpcyyCQ6CcfggeVdK_9dnZbsdUElWKLiE2S1U10hcbHBgmXwyk3rW5cezMqGWIIJ25kCgL9tb46Kos_DrCVdleoztvcQAxq_yIAfjtPrlo7ghEoxG_tqlQkxHFAXMW1ZWsdKUI=",
        "title": "towardsdatascience.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQElXfG2nl-3c51gOSRihWs3IzGBC1pI6waNqDgSaQ5aRc45TOOM_YK1xAyHUrCFJMB8ZZIbaawlP18d5uXmQo1WLXU4LVETuC8QWv0IYjag3jIongxYrrnzwbAtcNzuS72HPGN8P8Ch0qcByhe0h0I6biJ_4m-2cN9lfYsQA0f00LcGJVl12gk=",
        "title": "manchester.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp0cEjkSk8xF3am5R_UoMKglZfNa5wikJ8Ie-t_RSUQBACDqulo6lO270axzIW34Jd1pUa020OyJ66-NJ-feOOo-gqmI1x1TuvaCo2jX3BQcNnk7p2_rHqh4j7kdkrruAn2aIzLio3qyzo8lcQLH5rdpsMoOPzkcWzJ04OTlr-ubmBC4A6aJ6Catq8KkVGB4b9kOi7uDz0C2_mpWKmPfcNkIflqe2z",
        "title": "iclr.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYV9i6G-a_nGbtj11UlffCFHtmG0VEfPlNfHmKt0bDPace2LQ4TJrOpohAFZ5LNpw6FBx-xYjfxJCrMZ07UzpYu2nAk-olzAQtFSt8fclxnTLT1OrjYD6boM_K1qkEH6yLMbY=",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "CTIBench",
    "paperLink": "https://arxiv.org/abs/2411.08272",
    "description": "A benchmark tailored for Cyber Threat Intelligence (CTI) tasks. It evaluates LLMs on knowledge, root cause mapping, vulnerability severity prediction, attack technique extraction, and threat actor attribution.",
    "authors": [
      "Md Tanvirul Alam",
      "Dipkamal Bhusal",
      "Le Nguyen",
      "Nidhi Rastogi"
    ],
    "githubLink": "https://github.com/xashru/cti-bench",
    "itemCount": "5,610 samples",
    "source": "arXiv",
    "specs": "Text. 5 tasks: MCQ, CVE-to-CWE mapping, CVSS prediction, MITRE ATT&CK extraction, Attribution.",
    "year": "2024",
    "id": "imported-json-1769638825347-625-qbrrd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGhVcefRmFgPGO3JSGjMEDNxIWFAUu70-Ffh20qfENwHsUlq-mVfhKLor15hWb_CVkbI9npy2hKMQ2_YUmny-A7pI8jqh7G5u_vbeI_t3xhIauTIT7qeQLGfC1X",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEw0X3Wi0yP-Ve4hPk3ko6D2B4OJdsYxz7LvB8g01V4T7B3dl23YknztybXUR93yo77ZQQ4jftxrHBjmHqn7q8iSbuKjOJNG8QRZGSyuMWuI6jFkN7BGcZoS48o1o_O6VNT6ZoYVWMTwIo2MrpDTafnzI1bFzNUm_Q5",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpGBE49WGtX9qyqjJgvEzx1UiB7qJojbDBJxy420UsQjPb5b_RPVKGiT7XaatQNgpyhzM8cy-_ewxuEZf1jEVHNKX1Yvfz2HT2UXJwVBmtk59LWgSHvOHI9-mN9Xnj5qJiEhiHVx7RMdtFeeaLku5MnFdVW1kdaEbsBUpPODOINnofgSxshDnaQy6fmal7H0BGsCkTVn91tN9cdQ6HQXQquoqUSzbsruY-S1aCB5wC-mAj58nca2VSdvJiKQJMJxepL54W9YYlxvVUE8zRjdIAdIRA",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHIlfiVunBNPxXV_2VV-wxTOw_64fj0m-Aoq0yNrtU4Qlsnss8dG-5CKBvN6svm3Z__0x-0Y0aFYjYigO49miKeUCUjsbm5MSv1z_-A5_16WDz0SncdbogzZw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEoIKKN3xMEhS5eepcRAY8eSx0dSiDd1McCWCxgq1MQ0tvG-tQP7wZRtcni62FuXT8B0LxoZvrP5tYAaeehK2v8lNbmPiDVf_OvqMoPSulQPZWeWqQoQAGN3HOzJgpJ",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKVgrKA17-thEFZza7s2jZ1YzCpkwgDxTWDJdl9gBOuFNlApapmpDIaJnUOeRZKVjcGHK6UXusg6L4bZxHBLqr3BFsGys-oWTWlHE2kCJnC3isfV3_VLH05F5Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfKYx-7pEZpYj2QhPsuK9rR1QO9D1Qz6WLs0grfhBlam_4RbYTaEVMvDtKf0TrRd46tgLzzn8i061sZwItaurXg7DJmKdTHjb8x6AhD7t_awt9L0I6BJ4A1mhz2dCJfWc-O6XTIxpqgpQlz1muPLqMrpevHYEy13MEHABQrTPlxrYOEwnE-CrZAZwmQR2kCioDF7OVmKoyTwJUnIRgmAPidlUcWGuS-8bj92iLMiNgBvYjqFdbHg5hxMs=",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZI6D3I7Nl58C_ShnyTMsXTY0mLKxDNLW-h3ouez0DrGuW41Qg7sEEoO8uLWyzteiETHAApY03BpjC4fFjyH6edmbhBU_67vxcavF0fQ8kiQq7fi8GMd7SxYWoqNW4uODbIRg=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH-pLqaxnkA7mXav80Sf_PAQXww9yHZu3_M8MvG-ApepAMmD94c0iYkciFZyvD-s9pWWwSiLQoKYF985LxYtMxZGn-VPG8z_GD7NVGheEGBBancsH1YmxSyWuBjACFUZoirdc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4Bit-TagJhzFLgkAbrKtHK5mMMJs7rK_doeubnCaUfToh1nIVMOH7GuRj2yCJC1FMTMGJ9BxwniE_XAD_TPhq-U2WgBTpmfGt7K0WgkpXOfLt_6OM8oLdCSSS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn-pxJtCEAFoXYmnZIaQ0c-1dD249p7h-2XFMjVNhtSApPfkErzqBNJyNSxtPbNDZVB0hnGofkwjXMeJ0WGs3YaVZe9AV0nb7EYkZTpI1l2zqTPoYippqP4YsAbQ164Me9JsrpvAFXgs-4rLq8m-2g5lJ8SRg9isJX",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFMTWyKVRJK-1UsHt9c87RwiLPG1YDAkzp6upYJSBNnabuRWuImBmHm3YqLovVdmBPokuqXe0kqGppxx8a_WMFLNh3BLutL4y48kqJecSx7pGhpJhPFWuBV4ZaihI0U",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDq1njhpSjhSvPT4Pz161MCmi5-WgWMnxGaMgEeBapgnrT6v3V14JzCF_MYSaKUcp61lcpZOu6O15k4IL7wWw-qoIubdmBWEbrb_JHyMiSbpXc_emSCKG5KoMt",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "SEvenLLM (SEvenLLM-Bench)",
    "paperLink": "https://arxiv.org/abs/2405.03446",
    "description": "A framework and benchmark for evaluating and enhancing LLMs in Cyber Threat Intelligence (CTI). It includes a bilingual instruction tuning dataset and a benchmark for incident analysis and response.",
    "authors": [
      "Hangyuan Ji",
      "Jian Yang",
      "Linzheng Chai",
      "Chaoren Wei",
      "et al."
    ],
    "githubLink": "https://github.com/CSJianYang/SEvenLLM",
    "itemCount": "1,300 benchmark samples (SEvenLLM-Bench); ~90,000 instruction samples (SEvenLLM-Instruct)",
    "source": "arXiv",
    "specs": "Text. Bilingual (English/Chinese) Q&A and instruction samples.",
    "year": "2024",
    "id": "imported-json-1769638825347-626-bcgv4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGhVcefRmFgPGO3JSGjMEDNxIWFAUu70-Ffh20qfENwHsUlq-mVfhKLor15hWb_CVkbI9npy2hKMQ2_YUmny-A7pI8jqh7G5u_vbeI_t3xhIauTIT7qeQLGfC1X",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEw0X3Wi0yP-Ve4hPk3ko6D2B4OJdsYxz7LvB8g01V4T7B3dl23YknztybXUR93yo77ZQQ4jftxrHBjmHqn7q8iSbuKjOJNG8QRZGSyuMWuI6jFkN7BGcZoS48o1o_O6VNT6ZoYVWMTwIo2MrpDTafnzI1bFzNUm_Q5",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpGBE49WGtX9qyqjJgvEzx1UiB7qJojbDBJxy420UsQjPb5b_RPVKGiT7XaatQNgpyhzM8cy-_ewxuEZf1jEVHNKX1Yvfz2HT2UXJwVBmtk59LWgSHvOHI9-mN9Xnj5qJiEhiHVx7RMdtFeeaLku5MnFdVW1kdaEbsBUpPODOINnofgSxshDnaQy6fmal7H0BGsCkTVn91tN9cdQ6HQXQquoqUSzbsruY-S1aCB5wC-mAj58nca2VSdvJiKQJMJxepL54W9YYlxvVUE8zRjdIAdIRA",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHIlfiVunBNPxXV_2VV-wxTOw_64fj0m-Aoq0yNrtU4Qlsnss8dG-5CKBvN6svm3Z__0x-0Y0aFYjYigO49miKeUCUjsbm5MSv1z_-A5_16WDz0SncdbogzZw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEoIKKN3xMEhS5eepcRAY8eSx0dSiDd1McCWCxgq1MQ0tvG-tQP7wZRtcni62FuXT8B0LxoZvrP5tYAaeehK2v8lNbmPiDVf_OvqMoPSulQPZWeWqQoQAGN3HOzJgpJ",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKVgrKA17-thEFZza7s2jZ1YzCpkwgDxTWDJdl9gBOuFNlApapmpDIaJnUOeRZKVjcGHK6UXusg6L4bZxHBLqr3BFsGys-oWTWlHE2kCJnC3isfV3_VLH05F5Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfKYx-7pEZpYj2QhPsuK9rR1QO9D1Qz6WLs0grfhBlam_4RbYTaEVMvDtKf0TrRd46tgLzzn8i061sZwItaurXg7DJmKdTHjb8x6AhD7t_awt9L0I6BJ4A1mhz2dCJfWc-O6XTIxpqgpQlz1muPLqMrpevHYEy13MEHABQrTPlxrYOEwnE-CrZAZwmQR2kCioDF7OVmKoyTwJUnIRgmAPidlUcWGuS-8bj92iLMiNgBvYjqFdbHg5hxMs=",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZI6D3I7Nl58C_ShnyTMsXTY0mLKxDNLW-h3ouez0DrGuW41Qg7sEEoO8uLWyzteiETHAApY03BpjC4fFjyH6edmbhBU_67vxcavF0fQ8kiQq7fi8GMd7SxYWoqNW4uODbIRg=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH-pLqaxnkA7mXav80Sf_PAQXww9yHZu3_M8MvG-ApepAMmD94c0iYkciFZyvD-s9pWWwSiLQoKYF985LxYtMxZGn-VPG8z_GD7NVGheEGBBancsH1YmxSyWuBjACFUZoirdc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4Bit-TagJhzFLgkAbrKtHK5mMMJs7rK_doeubnCaUfToh1nIVMOH7GuRj2yCJC1FMTMGJ9BxwniE_XAD_TPhq-U2WgBTpmfGt7K0WgkpXOfLt_6OM8oLdCSSS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn-pxJtCEAFoXYmnZIaQ0c-1dD249p7h-2XFMjVNhtSApPfkErzqBNJyNSxtPbNDZVB0hnGofkwjXMeJ0WGs3YaVZe9AV0nb7EYkZTpI1l2zqTPoYippqP4YsAbQ164Me9JsrpvAFXgs-4rLq8m-2g5lJ8SRg9isJX",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFMTWyKVRJK-1UsHt9c87RwiLPG1YDAkzp6upYJSBNnabuRWuImBmHm3YqLovVdmBPokuqXe0kqGppxx8a_WMFLNh3BLutL4y48kqJecSx7pGhpJhPFWuBV4ZaihI0U",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDq1njhpSjhSvPT4Pz161MCmi5-WgWMnxGaMgEeBapgnrT6v3V14JzCF_MYSaKUcp61lcpZOu6O15k4IL7wWw-qoIubdmBWEbrb_JHyMiSbpXc_emSCKG5KoMt",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "SecQA",
    "paperLink": "https://arxiv.org/abs/2312.15838",
    "description": "A concise question-answering dataset for evaluating LLMs in computer security. Questions are generated by GPT-4 based on a standard security textbook to assess foundational and advanced security knowledge.",
    "authors": [
      "Zefang Liu",
      "et al."
    ],
    "githubLink": "https://github.com/zefang-liu/SecQA",
    "itemCount": "Two versions (v1, v2) with multiple-choice questions (Exact count not specified in snippet)",
    "source": "arXiv",
    "specs": "Text. Multiple-choice questions (0-shot and 5-shot settings).",
    "year": "2023",
    "id": "imported-json-1769638825347-627-5w0kv",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGhVcefRmFgPGO3JSGjMEDNxIWFAUu70-Ffh20qfENwHsUlq-mVfhKLor15hWb_CVkbI9npy2hKMQ2_YUmny-A7pI8jqh7G5u_vbeI_t3xhIauTIT7qeQLGfC1X",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEw0X3Wi0yP-Ve4hPk3ko6D2B4OJdsYxz7LvB8g01V4T7B3dl23YknztybXUR93yo77ZQQ4jftxrHBjmHqn7q8iSbuKjOJNG8QRZGSyuMWuI6jFkN7BGcZoS48o1o_O6VNT6ZoYVWMTwIo2MrpDTafnzI1bFzNUm_Q5",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpGBE49WGtX9qyqjJgvEzx1UiB7qJojbDBJxy420UsQjPb5b_RPVKGiT7XaatQNgpyhzM8cy-_ewxuEZf1jEVHNKX1Yvfz2HT2UXJwVBmtk59LWgSHvOHI9-mN9Xnj5qJiEhiHVx7RMdtFeeaLku5MnFdVW1kdaEbsBUpPODOINnofgSxshDnaQy6fmal7H0BGsCkTVn91tN9cdQ6HQXQquoqUSzbsruY-S1aCB5wC-mAj58nca2VSdvJiKQJMJxepL54W9YYlxvVUE8zRjdIAdIRA",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHIlfiVunBNPxXV_2VV-wxTOw_64fj0m-Aoq0yNrtU4Qlsnss8dG-5CKBvN6svm3Z__0x-0Y0aFYjYigO49miKeUCUjsbm5MSv1z_-A5_16WDz0SncdbogzZw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEoIKKN3xMEhS5eepcRAY8eSx0dSiDd1McCWCxgq1MQ0tvG-tQP7wZRtcni62FuXT8B0LxoZvrP5tYAaeehK2v8lNbmPiDVf_OvqMoPSulQPZWeWqQoQAGN3HOzJgpJ",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKVgrKA17-thEFZza7s2jZ1YzCpkwgDxTWDJdl9gBOuFNlApapmpDIaJnUOeRZKVjcGHK6UXusg6L4bZxHBLqr3BFsGys-oWTWlHE2kCJnC3isfV3_VLH05F5Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfKYx-7pEZpYj2QhPsuK9rR1QO9D1Qz6WLs0grfhBlam_4RbYTaEVMvDtKf0TrRd46tgLzzn8i061sZwItaurXg7DJmKdTHjb8x6AhD7t_awt9L0I6BJ4A1mhz2dCJfWc-O6XTIxpqgpQlz1muPLqMrpevHYEy13MEHABQrTPlxrYOEwnE-CrZAZwmQR2kCioDF7OVmKoyTwJUnIRgmAPidlUcWGuS-8bj92iLMiNgBvYjqFdbHg5hxMs=",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZI6D3I7Nl58C_ShnyTMsXTY0mLKxDNLW-h3ouez0DrGuW41Qg7sEEoO8uLWyzteiETHAApY03BpjC4fFjyH6edmbhBU_67vxcavF0fQ8kiQq7fi8GMd7SxYWoqNW4uODbIRg=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH-pLqaxnkA7mXav80Sf_PAQXww9yHZu3_M8MvG-ApepAMmD94c0iYkciFZyvD-s9pWWwSiLQoKYF985LxYtMxZGn-VPG8z_GD7NVGheEGBBancsH1YmxSyWuBjACFUZoirdc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4Bit-TagJhzFLgkAbrKtHK5mMMJs7rK_doeubnCaUfToh1nIVMOH7GuRj2yCJC1FMTMGJ9BxwniE_XAD_TPhq-U2WgBTpmfGt7K0WgkpXOfLt_6OM8oLdCSSS",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn-pxJtCEAFoXYmnZIaQ0c-1dD249p7h-2XFMjVNhtSApPfkErzqBNJyNSxtPbNDZVB0hnGofkwjXMeJ0WGs3YaVZe9AV0nb7EYkZTpI1l2zqTPoYippqP4YsAbQ164Me9JsrpvAFXgs-4rLq8m-2g5lJ8SRg9isJX",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFMTWyKVRJK-1UsHt9c87RwiLPG1YDAkzp6upYJSBNnabuRWuImBmHm3YqLovVdmBPokuqXe0kqGppxx8a_WMFLNh3BLutL4y48kqJecSx7pGhpJhPFWuBV4ZaihI0U",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDq1njhpSjhSvPT4Pz161MCmi5-WgWMnxGaMgEeBapgnrT6v3V14JzCF_MYSaKUcp61lcpZOu6O15k4IL7wWw-qoIubdmBWEbrb_JHyMiSbpXc_emSCKG5KoMt",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "TouchStone: Evaluating Vision-Language Models by Language Models",
    "paperLink": "https://arxiv.org/abs/2308.16890",
    "description": "A visual dialogue dataset and evaluation method that uses strong LLMs (like GPT-4) as judges to assess the conversational and storytelling abilities of LVLMs, covering five major categories of abilities.",
    "authors": [
      "Shuai Bai",
      "Shusheng Yang",
      "Jinze Bai",
      "Peng Wang",
      "Xingxuan Zhang",
      "Junyang Lin",
      "Xinggang Wang",
      "Chang Zhou",
      "Jingren Zhou"
    ],
    "githubLink": "https://github.com/OFA-Sys/TouchStone",
    "itemCount": "Visual dialogue samples",
    "source": "arXiv",
    "specs": "5 ability categories, 27 subtasks, LLM-based evaluation",
    "year": "2023",
    "id": "imported-json-1769638825347-628-sdpp0",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFfT03hapn6E6ws0BlJnE8n7GdxOmxlr0vNYpbhyLwBq9sbZhZv_Wb9XbBG47XWgo1iVExKxq1NJrcmwrDG7mKSunX2RktoUMCp_BRpXIKzfSL-I6AmBv2NvfhX-Ra2g5Tv8aU-W21n0o9OSIhoWCIeQBrWzbiVxif6H1uwSTkTfoo=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtgxzDRDbvJl1ku1IqtHEgwATr7CHuz3lp17cbLbQkCNZSSa9rIU_3TKuGHPVdj1r5z-h84kna7msNLLu2rT0W4THEV3Rdh4abPfqSJh_vVrPzppjJACpVG8HeBh9s8U2Sgeb0TMN3LZAltfvFd3E-Be28rr4XJ2r6yZlfcdCNdQDUWwwsfRabTPQlDN8TdShy_uyAHTGXWCrl5bD_e0W6MieZM0dpmNtZcAS9lYA=",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFMjfrQ3LN-61BGGoqgf8tDZy6QUqLLphtIBm2IHub7AoPf4t0sMfvBgl0puXv3hKaHeDhtjLpW6j_FTFQKBPYjv-Xj1Dm3zQetrMun8_UMXLctKfuvcw5BzWPjZ-dH9a8UmLBONwXWw8hWKiUT7AmNHwLYwhmb9CeCe8mDPXdyKXL9pHmuTRjcWFDhqBWpy3156KYCM0T_bazb_--hlBGbPpfpBZ4y_qFFoZBbKqCRh88Un0rEKKWNxKSW-Cbe1adynJh4a_bFYPlhG_QjFALHUdP71PN7yNK5jZJT11Qq",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGU2u8ngv7yvk0j7FSTzBsrLbFvi2HEuC1vGRLbMEM3e7k7VcP-FV0u8bx0HbncCcYkI5FQklSzit4W901hIcPhoFyCtwnwAal-LabVo6N_TkVoMtCbx9BdOZh1lHgBxwq64jyUCd4=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEfQ4BFyvtXsYmygCM-rBY5rloi8EYOni9In2vcVZijWa6f3fckL9zuOyktpN4cGrm_MU5FGHtug1pKXERs8J-uquuJUC7n5hkeNj3xNhBljoTqa4IMc9bCjNg_lp3h",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE1kzeoEcYqlorZcF7UyGoACE1aObwKlxiWVjg_PHB2kt99opngnQ2bNB5QbhA4uPb9pMu3k6UeUq2YrWiWtCBMM1r6vHd8UuEzNcTDvFSeWWjpaKksUhWlJehDMCZTRBQ=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEAZRqOAEShjcTR7oJ1DaPptS3hy75veeICYEuN0365u87ojYJcmh6PTdz-3ixEvpS38P250b4aOOjgL0VFhcaUzNWqsHru-H-8sEc6yNZlSo2-G5rJdOGlmEOmOAQLxXkZBksQYw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpfB8OCCCqeSXCxV0WRWFHSdn67ZTJy7yIOuHlg2t0Ms3Wig0riIWWbwcRP3kfgRWE1pTNWsbFgFBMDjL5ryNO1ppOxag7pCYg3Bbvdor8c1ZQM8XZEOo5PUgje8R1sNGZxgD9YQB57Ev0uHjUO_ick-w7Kz82Vw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGk6spt994uD65V7BhLWCEvsWG6meXNmst0YEhx2kSBTy3lV1wlLXF89nYTJo3D6aHjNfgSa8JIB9YLa0oJkRIs8BhjyQGxeuBRH5kvEN25btA8efUcmBUXf6s=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFHu5QrWv6rq5as6y-ZZ7gbIXaOINifbYnLkAeHiywRrXXWHJwPIHUcw17VKXN-CvdEKNTdlZ7BV4Px6BcvGwi-ImorwnfYCIpNLH5MHszrKZueDVG1OTFHuQIvFHqPS15fhD1qDQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGhvzBDrygYrXEL0MNHsb7gul-NuaD7X9scO9bD2QVJDcx-0rM1WkoOY7p_QAneHUXiSKJqzfGpdy5kl4J8FeNU4gMYRKo_BjcP9fR0T-sEGQ7eMiQ-Q5c16mbM4RDo8sMV25t07vVQYcd0lXEgBMzFYRofHSs=",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHL-QLNhV0zyEnuOwKxvbFGitQuEWFr0nkD9Xk0k13X9JYsU8f-M1yYg1srTQJPEOCc2yG6YvpNisgEltxuif0RYnBX3-djFJpXbc_eU-WWz25cG8VRq3PkpAzO24Ofza6YuicLxrx_cAq13iquYcnDTlTX7besAVASznjtRZpiiCuBvstjAbuHeeTyeLeT5BI=",
        "title": "instaclustr.com"
      }
    ]
  },
  {
    "title": "VLABench",
    "paperLink": "https://arxiv.org/abs/2412.18194",
    "description": "A comprehensive benchmark designed to evaluate Vision-Language-Action models on language-conditioned manipulation tasks, focusing on long-horizon reasoning, world knowledge transfer, and implicit human intentions.",
    "authors": [
      "Shiduo Zhang",
      "Zhe Xu",
      "Peiju Liu",
      "Xiaopeng Yu",
      "Yuan Li",
      "Qinghui Gao",
      "Zhaoye Fei",
      "Zhangyue Yin",
      "Zuxuan Wu",
      "Yu-Gang Jiang",
      "Xipeng Qiu"
    ],
    "githubLink": "https://github.com/VLABench/VLABench",
    "itemCount": "100 task categories, 2000+ objects",
    "source": "arXiv",
    "specs": "Simulation-based, Language-Conditioned Manipulation (LCM), Multi-step reasoning tasks",
    "year": "2024",
    "id": "imported-json-1769638825347-629-7icyk",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "RoboCasa",
    "paperLink": "https://arxiv.org/abs/2406.10296",
    "description": "A large-scale simulation framework for training generalist robots, featuring realistic and diverse kitchen environments populated with assets generated by generative AI tools. It focuses on everyday tasks to scale up robot learning data.",
    "authors": [
      "Soroush Nasiriany",
      "Abhiram Maddukuri",
      "Lance Zhang",
      "Adeet Parikh",
      "Aaron Lo",
      "Abhishek Joshi",
      "Ajay Mandlekar",
      "Yuke Zhu"
    ],
    "githubLink": "https://github.com/robocasa/robocasa",
    "itemCount": "100 tasks, 2500+ 3D assets, 150+ object categories",
    "source": "arXiv",
    "specs": "MuJoCo Simulation, RGB-D observations, Language descriptions, Generative AI-augmented assets",
    "year": "2024",
    "id": "imported-json-1769638825347-630-8rrlx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "SimplerEnv",
    "paperLink": "https://arxiv.org/abs/2405.05941",
    "description": "A benchmark suite for evaluating manipulation policies in simulation (using ManiSkill2) that correlates strongly with real-world performance on Google Robot and WidowX embodiments, aiming to mitigate the Sim-to-Real gap in evaluation.",
    "authors": [
      "Xuanlin Li",
      "Kyle Hsu",
      "Jiayuan Gu",
      "Karl Pertsch",
      "Oier Mees",
      "Homer Rich Walke",
      "Chuyuan Fu",
      "Ishikaa Lunawat",
      "Isabel Sieh",
      "Sean Kirmani",
      "Sergey Levine",
      "Jiajun Wu",
      "Chelsea Finn",
      "Hao Su",
      "Quan Vuong",
      "Ted Xiao"
    ],
    "githubLink": "https://github.com/simpler-env/SimplerEnv",
    "itemCount": "Evaluation suite for multiple policies",
    "source": "arXiv",
    "specs": "Simulation (ManiSkill2 engine), Visual Matching evaluation, Real-to-Sim validation",
    "year": "2024",
    "id": "imported-json-1769638825347-631-srln4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "The Colosseum",
    "paperLink": "https://arxiv.org/abs/2402.08191",
    "description": "A simulation benchmark designed to evaluate the generalization capabilities of robotic manipulation policies against 14 axes of environmental perturbations, such as lighting, color, texture, and distractors.",
    "authors": [
      "Wilbert Pumacay",
      "Ishika Singh",
      "Jiafei Duan",
      "Ranjay Krishna",
      "Jesse Thomason",
      "Dieter Fox"
    ],
    "githubLink": "https://github.com/Robot-Colosseum/Robot-Colosseum",
    "itemCount": "20 tasks, 17,000+ task instances",
    "source": "arXiv",
    "specs": "Simulation (RLBench based), 12+ perturbation factors, RGB-D",
    "year": "2024",
    "id": "imported-json-1769638825347-632-uwv0b",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "CoVLA",
    "paperLink": "https://arxiv.org/abs/2408.10845",
    "description": "A comprehensive dataset for autonomous driving that pairs driving videos with action trajectories (CAN bus data) and detailed natural language descriptions, designed to train and evaluate VLA models for driving.",
    "authors": [
      "Hidehisa Arai",
      "Keita Miwa",
      "Kento Sasaki",
      "Yu Yamaguchi",
      "Kohei Watanabe",
      "Shunsuke Aoki",
      "Issei Yamamoto"
    ],
    "githubLink": "https://turingmotors.github.io/covla-ad/",
    "itemCount": "80+ hours of video, 10,000+ clips (Mini)",
    "source": "arXiv",
    "specs": "Video, Natural Language Captions, Action trajectories (CAN data)",
    "year": "2024",
    "id": "imported-json-1769638825347-633-wntmb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Open X-Embodiment Dataset",
    "paperLink": "https://arxiv.org/abs/2310.08864",
    "description": "A massive robotics dataset aggregating over 1 million real-world robot trajectories from 22 different robot embodiments to facilitate cross-embodiment learning and the training of generalist robot policies.",
    "authors": [
      "Open X-Embodiment Collaboration",
      "Abhishek Padalkar",
      "Acorn Pooley",
      "Ajay Mandlekar",
      "Alexencis",
      "Karl Pertsch",
      "Sergey Levine",
      "Chelsea Finn",
      "et al."
    ],
    "githubLink": "https://github.com/google-deepmind/open_x_embodiment",
    "itemCount": "1M+ trajectories, 527 skills",
    "source": "arXiv",
    "specs": "RLDS format (serialized tfrecords), RGB images, Depth, Point Clouds, Natural Language Instructions, Robot Actions (7-DOF)",
    "year": "2023",
    "id": "imported-json-1769638825347-634-n6d5j",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "LIBERO",
    "paperLink": "https://arxiv.org/abs/2306.03310",
    "description": "A benchmark for lifelong robot learning, focusing on knowledge transfer across tasks. It includes task suites designed to test declarative and procedural knowledge transfer under distribution shifts.",
    "authors": [
      "Bo Liu",
      "Yifeng Zhu",
      "Chongkai Gao",
      "Yi Ru Wang",
      "Qiang Liu",
      "Yuke Zhu",
      "Peter Stone"
    ],
    "githubLink": "https://github.com/Lifelong-Robot-Learning/LIBERO",
    "itemCount": "130 tasks across 4 suites",
    "source": "arXiv",
    "specs": "Simulation, Language-conditioned tasks, Procedural generation",
    "year": "2023",
    "id": "imported-json-1769638825347-635-vepch",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ManiSkill2",
    "paperLink": "https://arxiv.org/abs/2302.04659",
    "description": "A unified benchmark for generalizable manipulation skills, featuring a wide range of articulated objects and rigid body tasks. It serves as a foundational environment for many VLA evaluation frameworks (like SimplerEnv).",
    "authors": [
      "Jiayuan Gu",
      "Fanbo Xiang",
      "Xuanlin Li",
      "Zhan Ling",
      "Xiqiang Liu",
      "Tongzhou Mu",
      "Yihe Tang",
      "Stone Tao",
      "Xinyue Wei",
      "Yuan Yao",
      "Xiao Yuan",
      "Pengwei Xie",
      "Zhiao Huang",
      "Rui Chen",
      "Hao Su"
    ],
    "githubLink": "https://github.com/haosulab/ManiSkill",
    "itemCount": "20 task families, 2000+ object models",
    "source": "arXiv",
    "specs": "SAPIEN Simulation, RGB-D, Point Cloud, Physical Interaction",
    "year": "2023",
    "id": "imported-json-1769638825347-636-wnetu",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "CALVIN",
    "paperLink": "https://arxiv.org/abs/2112.03227",
    "description": "A benchmark for learning long-horizon language-conditioned manipulation tasks. It evaluates an agent's ability to solve complex sequences of subtasks specified by unconstrained natural language instructions.",
    "authors": [
      "Oier Mees",
      "Lukas Hermann",
      "Erick Rosete-Beas",
      "Wolfram Burgard"
    ],
    "githubLink": "https://github.com/mees/calvin",
    "itemCount": "34 hours of teleoperation data",
    "source": "arXiv",
    "specs": "Simulation (PyBullet), RGB-D (static + gripper cameras), Proprioception, Language instructions",
    "year": "2022",
    "id": "imported-json-1769638825347-637-oqqsy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmYWnodIXkPu2zRU-YLmr5fmoA49F_mTbEI2kYx5WFOhtcAG7-Kf8JhPl_yHURV1Ui57FaxVzDK_ZoJeDPJs7A7croQi7Y1oqDcIimt3Cue2oTXhg3OAHIvEVaDg24Y__c327ZNA1AY_UtPqfpNoaEcX7EDzVslBVa_tk=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGm4K58jJtLmrzwE6zpRcfJQZpu_X-BywuODEDXhw_-NKMF5WlAyRUHYjvAxKmq7ZQZhfNudlWOIl_sO001B9NLvbGQksFly1calsFmLBA7vGkEdEfhm_l89-2oM20atAacOQy2",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmHIdqfZkhstBBAs_5hXeNEND3tOm6HExDaYX0z-lR6hsxHRsPd2iF0wt8Tgsx5U2IZwjVKtIhNX8IxF2LNVLNGmddvd1YCREK9IW81dp4hqq6Lo3P7UEBVc6e-RdP",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PatientSafetyBench",
    "paperLink": "https://huggingface.co/datasets/microsoft/PatientSafetyBench",
    "description": "A benchmark designed to evaluate the safety of LLMs in the medical domain from a patient's perspective, testing for harmful, misleading, or unlicensed medical advice.",
    "authors": [
      "Microsoft"
    ],
    "githubLink": "https://huggingface.co/datasets/microsoft/PatientSafetyBench",
    "itemCount": "466 queries",
    "source": "Hugging Face",
    "specs": "Text queries, Medical domain, 5 safety policy categories",
    "year": "2025",
    "id": "imported-json-1769638825347-638-ohbr8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYlU4T1f4WXsUmwxbCcWCL1kOjlTfLRSR7F4Qvcy4M9UiwEfG86pjK71JRq6Pe_EozhmViFa9HHOsfTijFNLRYyMZazo_-4yKSVNlM2qUcr5EDJoTnDGTvsFRHtJLy9AfeySI=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHQiCZBDNa0jzlwKorMExur2a9kw6nfmq7bpmq9avIVrsiX9XuwHZRK7Xch5CzKCpgmbhC88MkmlgWgtRBzvbDVZASAYrNeiEuOAJ5s6cNmGmJgC4npBycmKl1A9Iq7D_o-FXlQtoBKGvY=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQpb1nIURpYuj0vlI4rSbf7vgr_SPevi5DahlKhqOs5orGhGsUxLmZ1Jau-oC8Y-gNJnHZVq-B4SfMsqYMEyRUOz0butkjY-oAgJ-skHs2AavrHu4PeSvn-Vl6MxJ0uR4ZrBDHBBbNUsfrUXKr0F9y9r8lGwOtHsv41sNbhXigpK2c6xSg0ZdmFRVYUywLQXxDrVoEzw-ATHIZCcjfEUu92AZurNUUV6RCnGbDRAZ1kG4GjDV8yIK0FGTh2F2qdsjaZ21aoT0=",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4LSNtD23qGAp1_nUyd3mAvG0ImEZtlO-w7Z1-T0e6ha_d57HKme0uGvuOrQ6iP06sLi8SuV-WDdcWVIUkdO7-ZsR5llRzfWegAcVd3E3hRuR6b8dsGkOiTuazI6EbE61X",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1bEOY3JNBo3CrIrytKahY80LbSEuzu5z9mCDnGjx8o4j7oHaVNWQFYjKQMBIv5PDLvL_ERW_Kaf7J8d6n0BbAOs3V7n4AW68hxNUUFdMIRjT7yuJa0vFDpoCSwuSDCe26",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETIiS7Moi-DKOzyAV1aVVsrIRmc3u5ACYTLK5_nmc2vkliSZZeT0l4t8c9LGFlyCCJMyZ27ndi97v0OFxgn5Z157EgdcfDTg34JQgi1dIPkwrZ5bV6h5S_JUytWTx7-kxGaFgIZCGok1NgBNlJpZyr_-ydbKB4MOFO",
        "title": "promptingguide.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuFfKR_YFA_mqRvalpRwKqRHqtqmIMGu-T7AwUnmqhHMOBNMIxYaXyaLLyYFoZcEfEE52sFOcUL03j6bkIf9peAxMyhH4ODf3S8m7umG90uKy1OhvvNJCGpi_x1SzxUL3gnoziKQMvrdZx",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQ7XCyCEa_9B0CafFKPBaEUyyPB7ePkHfc31rzJZ2jHUhh7uM1XD83t9sqCL6VDM6WCDOfpp2aleaL9j5sSLfGZ1rt0XRVWeucCUKTnXvJcfjab_FMO1xzGs5bpb4xRpFPc5S-9ooJtSvpTQ==",
        "title": "aclanthology.org"
      }
    ]
  },
  {
    "title": "AI Safety Benchmark v0.5",
    "paperLink": "https://arxiv.org/abs/2404.12241",
    "description": "A proof-of-concept benchmark from the MLCommons AI Safety Working Group designed to assess safety risks of chat-tuned language models across various hazard categories.",
    "authors": [
      "Bertie Vidgen",
      "Percy Liang",
      "MLCommons AI Safety Working Group",
      "et al."
    ],
    "githubLink": "https://mlcommons.org/en/groups/ai-safety/",
    "itemCount": "43,000+ test prompts (POC)",
    "source": "arXiv",
    "specs": "Text prompts, 13 hazard categories (7 tested in v0.5)",
    "year": "2024",
    "id": "imported-json-1769638825347-639-eczoz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYlU4T1f4WXsUmwxbCcWCL1kOjlTfLRSR7F4Qvcy4M9UiwEfG86pjK71JRq6Pe_EozhmViFa9HHOsfTijFNLRYyMZazo_-4yKSVNlM2qUcr5EDJoTnDGTvsFRHtJLy9AfeySI=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHQiCZBDNa0jzlwKorMExur2a9kw6nfmq7bpmq9avIVrsiX9XuwHZRK7Xch5CzKCpgmbhC88MkmlgWgtRBzvbDVZASAYrNeiEuOAJ5s6cNmGmJgC4npBycmKl1A9Iq7D_o-FXlQtoBKGvY=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQpb1nIURpYuj0vlI4rSbf7vgr_SPevi5DahlKhqOs5orGhGsUxLmZ1Jau-oC8Y-gNJnHZVq-B4SfMsqYMEyRUOz0butkjY-oAgJ-skHs2AavrHu4PeSvn-Vl6MxJ0uR4ZrBDHBBbNUsfrUXKr0F9y9r8lGwOtHsv41sNbhXigpK2c6xSg0ZdmFRVYUywLQXxDrVoEzw-ATHIZCcjfEUu92AZurNUUV6RCnGbDRAZ1kG4GjDV8yIK0FGTh2F2qdsjaZ21aoT0=",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4LSNtD23qGAp1_nUyd3mAvG0ImEZtlO-w7Z1-T0e6ha_d57HKme0uGvuOrQ6iP06sLi8SuV-WDdcWVIUkdO7-ZsR5llRzfWegAcVd3E3hRuR6b8dsGkOiTuazI6EbE61X",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1bEOY3JNBo3CrIrytKahY80LbSEuzu5z9mCDnGjx8o4j7oHaVNWQFYjKQMBIv5PDLvL_ERW_Kaf7J8d6n0BbAOs3V7n4AW68hxNUUFdMIRjT7yuJa0vFDpoCSwuSDCe26",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETIiS7Moi-DKOzyAV1aVVsrIRmc3u5ACYTLK5_nmc2vkliSZZeT0l4t8c9LGFlyCCJMyZ27ndi97v0OFxgn5Z157EgdcfDTg34JQgi1dIPkwrZ5bV6h5S_JUytWTx7-kxGaFgIZCGok1NgBNlJpZyr_-ydbKB4MOFO",
        "title": "promptingguide.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuFfKR_YFA_mqRvalpRwKqRHqtqmIMGu-T7AwUnmqhHMOBNMIxYaXyaLLyYFoZcEfEE52sFOcUL03j6bkIf9peAxMyhH4ODf3S8m7umG90uKy1OhvvNJCGpi_x1SzxUL3gnoziKQMvrdZx",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQ7XCyCEa_9B0CafFKPBaEUyyPB7ePkHfc31rzJZ2jHUhh7uM1XD83t9sqCL6VDM6WCDOfpp2aleaL9j5sSLfGZ1rt0XRVWeucCUKTnXvJcfjab_FMO1xzGs5bpb4xRpFPc5S-9ooJtSvpTQ==",
        "title": "aclanthology.org"
      }
    ]
  },
  {
    "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society",
    "paperLink": "https://arxiv.org/abs/2303.17760",
    "description": "A communicative agent framework and dataset that explores autonomous cooperation among agents using role-playing. It generates conversational data to study the behaviors and capabilities of agent societies.",
    "authors": [
      "Guohao Li",
      "Hasan Abed Al Kader Hammoud",
      "Hani Itani",
      "Dmitrii Khizbullin",
      "Bernard Ghanem"
    ],
    "githubLink": "https://github.com/camel-ai/camel",
    "itemCount": "25,000+ conversations",
    "source": "arXiv",
    "specs": "Conversational data (AI Society and Code datasets) in JSON and Tar.gz formats. Involves role-playing scenarios with instruction-following.",
    "year": "2023",
    "id": "imported-json-1769638825347-640-uudk3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGobIKxyLmRoCRC45MArH82rfgPouY8JBRi004uhTFWHOm0YTjR0SHgNGJGhLtYWvpsbCBlKeJ7NGBO-Gz3hEPdMEL141Kbmat7hEfrUrpZJh5ONFmBj7K4H8VwEciTG6W12Iwf2hsgWOnCwd37-Q==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFATT3M_0LqA3MM6PRNLHmR4dJDkuAIfiTeBjxMfxdcZJyYBFiAE7oUyuEFTNPvtO6uOM6E2hiz2cPfpfpV50cCueq36v1uAL2FUOOsFGiKyiR8X5ZmQMWbrert82Kq1g3CWm__7v4pUxPjgSD88Ztt_Vu1xcFGfslsHq_-ozpRVWuQ6fijtcUiPPWdx693l5Y05jjM_mbncVq-oJi_bJLqqeQhBAbv28TY_w==",
        "title": "illinois.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFALvpon3LtFXLgtguJd1rOKYBbpBgkEz-YbQ_AmDiE2yLTtM36KqNqi305m8R4pvjN3wgFjK_o379s7hWYHNfUz_qcgroOQQ7eGksPGv_VAWRWYrakVecN55JvPPjn",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVP-hv49opxGSf-FN4cTBvIOur5Yfe_u-qyDoyLqXqzRzRDL_TkOWQ3BspTe9OTdHxMNafR2nSnTRzVCQct8RKCv4fuLYZL1vC5m93FSUQQGHMnLA9ImFFS96AL3jM7XTUbpIYfA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_tfLItYxzvAmI4DaW0ZubpCHiL_Oivq9fHcN3KJDSjHrXKqOHC3ngiOtxOYpORYU4X8obprOy9OkMCo7abXWLN1DDnOJOEtZ9w2KYjLrGM8AQKKvC70ukows06D2yJhZh-droOavHj1Te6sgXYS_KtDDFLS0kiVAQxUL_PBxTELrStqKnqdDH7PlqkerGOzMU1JyOYtVsLX2e3U-1PFRfK2pOuNE4jFaD",
        "title": "deepai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFGKgaBbUEWHUWVi5rEiYfD1pQfKrR28zPfM5qheHjr10353T6WQX0a8cpO0l2BucbsHqUSkdPHzyye5xDvvMi-c1VvMgx_MuhrDb_4VPBTexada8RmykgqSRevV3Tucz9WG2mZaKt8Zk4cmAVf3YIAWLBlarqc258cIw2YVG2rPAcZNrZ9uAZ8M9tHs8qGa7BhNEIMrwmWEQ==",
        "title": "synteda.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4fLraw175O1P4Lh-axjB8pB46WqQrDKcDgjhG28Ii8yOAhtkuycsaubYb-XKMI5Oat0J8YZME08RkSqZ6gVha78Irs6cCprvU-CtsYaUwLVDXXCwWWfbnsDqgRdO9kQyqr43bWNXD9vhGvBnanhxwNo-4gzY=",
        "title": "anuptechtips.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHlmY5yS6AeV6FUgqh3OIIXENfiUnYLYVzKutKY3YW91U9vTWToMUAsxaYeyZ8hTCtW8WBRS8tWrHJPng-jauGvojGH5eJ6nbdXucpjQZIk7CQDirPvtI5tkRIpi1ELbEEr2aSOoWTs0QU=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiRCfjdjCBNXCWk4t5c5JME9BZsNma-eODJ9NssBathdg4-M-Qg2IV-JYfWhEYPVfyWtvc_5zaPQ7hPJHz9x1VEwKBmMR-RLVlq5_YcdYZhXBevi_0Ej1tnAzArkw=",
        "title": "netlify.app"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGS7fjC1-nwCvOzvRZsJoyfYWDuL7m-Gzhjzk9kRjvBD62W-tDb1nrK4pkHy14UF20EUE7Us-hAmiXTVCls3fS8POToKqOLoJbB1q7v7Ttsvi9i8trm4swWZgMvmKsx4O41hVk15ThF4z-jJMQ=",
        "title": "galileo.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHAEqWRD7HMxnrhlwWyWkQBssGkjZ5gl76h65t9f1inzdJAm4Vf59nVRTeYrathy4J4T6ijmFlvoqAgQnFQA4jsEtcvtVzplfBMQBgodPpYrvbSjXUsMSQhMQzyiJf6qOdmt9HGjcUp5uSsZfHo8FUkrYPsdn_Rhzu8kABmAVBo_Q3579YNETlo6fhSQGotc6yHKNQTjcqm8_r1pRHLvVdtUVfedyv8p64=",
        "title": "iclr.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyrvbk2fAJYEzo0HhjgPxnr7E4qzp-sI2Af9v5EQf12J8Vx632Tp29lozLykMyN2cpinNH-5ipo9_j62AWpHC7vnCyr8Ivnpm7V7sxg1VM4v64DFXwsdNeOlUNzGLi",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFNJMbaUFt0UMwxojhG75bumgIfsC_yLmJ60a06eG5oVx4avkBHO-twdii9eCHWd3aYt1Pvvko-9_zHFhJ90ox10ej4oKIzk8l3s93Hf7DVLNWeEwr8y2R4eRxOzlHQ",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHseONxDbeKsJ0Jy0eoXM8Ifm-R4fZLgC9c5-Ihq0c5QFfPPItiZjE7s4pmHCjCCbkP-y9M_85XnxIvX26rZl_0xKtiWCjAfWKFL4oFpARS5rLqvpvQ6ctyC8myVO4C6N6SZH6QlE_R",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
    "paperLink": "https://arxiv.org/abs/2310.11667",
    "description": "An open-ended environment to simulate and evaluate complex social interactions between artificial agents. Agents role-play in various scenarios to achieve social goals like cooperation, competition, and exchange.",
    "authors": [
      "Xuhui Zhou",
      "Hao Zhu",
      "Leena Mathur",
      "Ruohong Zhang",
      "Haofei Yu",
      "Zhengyang Qi",
      "Louis-Philippe Morency",
      "Yonatan Bisk",
      "Daniel Fried",
      "Graham Neubig",
      "Maarten Sap"
    ],
    "githubLink": "https://github.com/sotopia-lab/sotopia",
    "itemCount": "90 social scenarios",
    "source": "arXiv",
    "specs": "Interactive social scenarios involving negotiation, collaboration, and competition. Evaluates goal completion, believability, and social intelligence.",
    "year": "2023",
    "id": "imported-json-1769638825347-641-livlh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGobIKxyLmRoCRC45MArH82rfgPouY8JBRi004uhTFWHOm0YTjR0SHgNGJGhLtYWvpsbCBlKeJ7NGBO-Gz3hEPdMEL141Kbmat7hEfrUrpZJh5ONFmBj7K4H8VwEciTG6W12Iwf2hsgWOnCwd37-Q==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFATT3M_0LqA3MM6PRNLHmR4dJDkuAIfiTeBjxMfxdcZJyYBFiAE7oUyuEFTNPvtO6uOM6E2hiz2cPfpfpV50cCueq36v1uAL2FUOOsFGiKyiR8X5ZmQMWbrert82Kq1g3CWm__7v4pUxPjgSD88Ztt_Vu1xcFGfslsHq_-ozpRVWuQ6fijtcUiPPWdx693l5Y05jjM_mbncVq-oJi_bJLqqeQhBAbv28TY_w==",
        "title": "illinois.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFALvpon3LtFXLgtguJd1rOKYBbpBgkEz-YbQ_AmDiE2yLTtM36KqNqi305m8R4pvjN3wgFjK_o379s7hWYHNfUz_qcgroOQQ7eGksPGv_VAWRWYrakVecN55JvPPjn",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVP-hv49opxGSf-FN4cTBvIOur5Yfe_u-qyDoyLqXqzRzRDL_TkOWQ3BspTe9OTdHxMNafR2nSnTRzVCQct8RKCv4fuLYZL1vC5m93FSUQQGHMnLA9ImFFS96AL3jM7XTUbpIYfA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_tfLItYxzvAmI4DaW0ZubpCHiL_Oivq9fHcN3KJDSjHrXKqOHC3ngiOtxOYpORYU4X8obprOy9OkMCo7abXWLN1DDnOJOEtZ9w2KYjLrGM8AQKKvC70ukows06D2yJhZh-droOavHj1Te6sgXYS_KtDDFLS0kiVAQxUL_PBxTELrStqKnqdDH7PlqkerGOzMU1JyOYtVsLX2e3U-1PFRfK2pOuNE4jFaD",
        "title": "deepai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFGKgaBbUEWHUWVi5rEiYfD1pQfKrR28zPfM5qheHjr10353T6WQX0a8cpO0l2BucbsHqUSkdPHzyye5xDvvMi-c1VvMgx_MuhrDb_4VPBTexada8RmykgqSRevV3Tucz9WG2mZaKt8Zk4cmAVf3YIAWLBlarqc258cIw2YVG2rPAcZNrZ9uAZ8M9tHs8qGa7BhNEIMrwmWEQ==",
        "title": "synteda.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4fLraw175O1P4Lh-axjB8pB46WqQrDKcDgjhG28Ii8yOAhtkuycsaubYb-XKMI5Oat0J8YZME08RkSqZ6gVha78Irs6cCprvU-CtsYaUwLVDXXCwWWfbnsDqgRdO9kQyqr43bWNXD9vhGvBnanhxwNo-4gzY=",
        "title": "anuptechtips.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHlmY5yS6AeV6FUgqh3OIIXENfiUnYLYVzKutKY3YW91U9vTWToMUAsxaYeyZ8hTCtW8WBRS8tWrHJPng-jauGvojGH5eJ6nbdXucpjQZIk7CQDirPvtI5tkRIpi1ELbEEr2aSOoWTs0QU=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiRCfjdjCBNXCWk4t5c5JME9BZsNma-eODJ9NssBathdg4-M-Qg2IV-JYfWhEYPVfyWtvc_5zaPQ7hPJHz9x1VEwKBmMR-RLVlq5_YcdYZhXBevi_0Ej1tnAzArkw=",
        "title": "netlify.app"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGS7fjC1-nwCvOzvRZsJoyfYWDuL7m-Gzhjzk9kRjvBD62W-tDb1nrK4pkHy14UF20EUE7Us-hAmiXTVCls3fS8POToKqOLoJbB1q7v7Ttsvi9i8trm4swWZgMvmKsx4O41hVk15ThF4z-jJMQ=",
        "title": "galileo.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHAEqWRD7HMxnrhlwWyWkQBssGkjZ5gl76h65t9f1inzdJAm4Vf59nVRTeYrathy4J4T6ijmFlvoqAgQnFQA4jsEtcvtVzplfBMQBgodPpYrvbSjXUsMSQhMQzyiJf6qOdmt9HGjcUp5uSsZfHo8FUkrYPsdn_Rhzu8kABmAVBo_Q3579YNETlo6fhSQGotc6yHKNQTjcqm8_r1pRHLvVdtUVfedyv8p64=",
        "title": "iclr.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyrvbk2fAJYEzo0HhjgPxnr7E4qzp-sI2Af9v5EQf12J8Vx632Tp29lozLykMyN2cpinNH-5ipo9_j62AWpHC7vnCyr8Ivnpm7V7sxg1VM4v64DFXwsdNeOlUNzGLi",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFNJMbaUFt0UMwxojhG75bumgIfsC_yLmJ60a06eG5oVx4avkBHO-twdii9eCHWd3aYt1Pvvko-9_zHFhJ90ox10ej4oKIzk8l3s93Hf7DVLNWeEwr8y2R4eRxOzlHQ",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHseONxDbeKsJ0Jy0eoXM8Ifm-R4fZLgC9c5-Ihq0c5QFfPPItiZjE7s4pmHCjCCbkP-y9M_85XnxIvX26rZl_0xKtiWCjAfWKFL4oFpARS5rLqvpvQ6ctyC8myVO4C6N6SZH6QlE_R",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "AVeriTeC",
    "paperLink": "https://arxiv.org/abs/2305.13194",
    "description": "A dataset for real-world claim verification with evidence from the web. It uses question-answer pairs to decompose the verification process and includes textual justifications.",
    "authors": [
      "Michael Sejr Schlichtkrull",
      "Zhijiang Guo",
      "Andreas Vlachos"
    ],
    "githubLink": "https://github.com/MichSchli/AVeriTeC",
    "itemCount": "4,568 claims",
    "source": "arXiv",
    "specs": "Real-world claims, open web evidence, QA pairs, justifications",
    "year": "2023",
    "id": "imported-json-1769638825347-642-usxav",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "VitaminC",
    "paperLink": "https://arxiv.org/abs/2103.08541",
    "description": "A benchmark for robust fact verification with contrastive evidence. It contains cases where models must discern subtle factual changes in evidence that support or refute a claim.",
    "authors": [
      "Tal Schuster",
      "Adam Fisch",
      "Regina Barzilay"
    ],
    "githubLink": "https://github.com/TalSchuster/VitaminC",
    "itemCount": "450,000+ claim-evidence pairs",
    "source": "arXiv",
    "specs": "Wikipedia revisions, contrastive evidence pairs, synthetic and real revisions",
    "year": "2021",
    "id": "imported-json-1769638825347-643-rp031",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "X-Fact",
    "paperLink": "https://aclanthology.org/2021.acl-short.86/",
    "description": "A large publicly available multilingual dataset for factual verification of naturally existing real-world claims. It covers 25 languages and includes an evaluation benchmark for out-of-domain generalization.",
    "authors": [
      "Ashim Gupta",
      "Vivek Srikumar"
    ],
    "githubLink": "https://github.com/utahnlp/x-fact",
    "itemCount": "31,189 statements",
    "source": "Scholar",
    "specs": "Multilingual claims (25 languages), metadata, evidence from news stories",
    "year": "2021",
    "id": "imported-json-1769638825347-644-gtrkq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "FEVEROUS",
    "paperLink": "https://arxiv.org/abs/2106.05707",
    "description": "Fact Extraction and VERification Over Unstructured and Structured information. A dataset requiring verification against both text sentences and table cells from Wikipedia.",
    "authors": [
      "Rami Aly",
      "Zhijiang Guo",
      "Michael Sejr Schlichtkrull",
      "James Thorne",
      "Andreas Vlachos",
      "Christos Christodoulopoulos",
      "Oana Cocarascu",
      "Arpit Mittal"
    ],
    "githubLink": "https://github.com/feverousdb/feverous",
    "itemCount": "87,026 claims",
    "source": "arXiv",
    "specs": "Claims, unstructured text evidence, structured table evidence, labels",
    "year": "2021",
    "id": "imported-json-1769638825347-645-5sqzg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "FoolMeTwice",
    "paperLink": "https://arxiv.org/abs/2104.04725",
    "description": "A large dataset of challenging entailment pairs collected through a multi-player game designed to encourage adversarial examples that are hard for models to solve.",
    "authors": [
      "Julian Eisenschlos",
      "Bhuwan Dhingra",
      "Jannis Bulian",
      "Benjamin Börschinger",
      "Jordan Boyd-Graber"
    ],
    "githubLink": "https://github.com/google-research-datasets/fool-me-twice",
    "itemCount": "13,000+ claims",
    "source": "arXiv",
    "specs": "Adversarial claims, Wikipedia evidence, entailment/contradiction labels",
    "year": "2021",
    "id": "imported-json-1769638825347-646-r9ya2",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Climate-FEVER",
    "paperLink": "https://arxiv.org/abs/2012.00614",
    "description": "A dataset adapting the FEVER methodology to real-world climate change claims. It features challenging claims that relate multiple facets and disputed cases where both supporting and refuting evidence are present.",
    "authors": [
      "Thomas Diggelmann",
      "Jordan Boyd-Graber",
      "Jannis Bulian",
      "Massimiliano Ciaramita",
      "Markus Leippold"
    ],
    "githubLink": "https://github.com/climate-fever/dataset",
    "itemCount": "1,535 claims; 7,675 claim-evidence pairs",
    "source": "arXiv",
    "specs": "Real-world climate claims, Wikipedia evidence sentences, 5-annotator agreement",
    "year": "2020",
    "id": "imported-json-1769638825347-647-eriy1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PubHealth",
    "paperLink": "https://arxiv.org/abs/2010.09926",
    "description": "A comprehensive dataset for explainable automated fact-checking of public health claims. Each instance includes a veracity label and a journalist-crafted explanation.",
    "authors": [
      "Neema Kotonya",
      "Francesca Toni"
    ],
    "githubLink": "https://github.com/ImperialCollegeLondon/health_fact",
    "itemCount": "11,832 claims",
    "source": "arXiv",
    "specs": "Public health claims, explanation text, veracity labels (True, False, Unproven, Mixture)",
    "year": "2020",
    "id": "imported-json-1769638825347-648-5wy7d",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "SciFact",
    "paperLink": "https://arxiv.org/abs/2004.14974",
    "description": "A dataset for verifying scientific claims using evidence from research abstracts. It requires systems to identify relevant abstracts and rationale sentences to support or refute a claim.",
    "authors": [
      "David Wadden",
      "Shanchuan Lin",
      "Kyle Lo",
      "Lucy Lu Wang",
      "Madeleine van Zuylen",
      "Arman Cohan",
      "Hannaneh Hajishirzi"
    ],
    "githubLink": "https://github.com/allenai/scifact",
    "itemCount": "1,409 claims",
    "source": "arXiv",
    "specs": "Expert-written scientific claims, abstracts, rationales, labels (Supported, Refuted)",
    "year": "2020",
    "id": "imported-json-1769638825347-649-xojcx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ClaimBuster",
    "paperLink": "https://arxiv.org/abs/2004.14425",
    "description": "A dataset of statements extracted from U.S. general election presidential debates, annotated for 'check-worthiness' to identify factual claims that are worth fact-checking.",
    "authors": [
      "Fatma Arslan",
      "Naeemul Hassan",
      "Chengkai Li",
      "Mark Tremayne"
    ],
    "githubLink": "https://github.com/utaresearch/claimbuster-spotter",
    "itemCount": "23,533 statements",
    "source": "arXiv",
    "specs": "Debate transcripts, check-worthiness labels (non-factual, unimportant factual, check-worthy factual)",
    "year": "2020",
    "id": "imported-json-1769638825347-650-bo774",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "HoVer",
    "paperLink": "https://aclanthology.org/2020.findings-emnlp.309/",
    "description": "A dataset for many-hop fact extraction and claim verification. It challenges models to extract facts from multiple Wikipedia articles and reason over them to verify a claim.",
    "authors": [
      "Yichen Jiang",
      "Shikha Bordia",
      "Zheng Zhong",
      "Charles Dognin",
      "Maneesh Singh",
      "Mohit Bansal"
    ],
    "githubLink": "https://github.com/hover-nlp/hover",
    "itemCount": "26,000+ claims",
    "source": "Scholar",
    "specs": "Many-hop reasoning claims, Wikipedia evidence, supported/not-supported labels",
    "year": "2020",
    "id": "imported-json-1769638825347-651-70pm3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MultiFC",
    "paperLink": "https://arxiv.org/abs/1909.03242",
    "description": "A real-world multi-domain dataset for evidence-based fact checking of claims. It is collected from 26 fact-checking websites and includes rich metadata and evidence pages.",
    "authors": [
      "Isabelle Augenstein",
      "Christina Lioma",
      "Dongsheng Wang",
      "Lucas Chaves Lima",
      "Casper Hansen",
      "Christian Hansen",
      "Jakob Grue Simonsen"
    ],
    "githubLink": "https://github.com/copenlu/multifc",
    "itemCount": "34,918 claims",
    "source": "arXiv",
    "specs": "Real-world claims, evidence pages, metadata, veracity labels",
    "year": "2019",
    "id": "imported-json-1769638825347-652-4jaec",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "FEVER (Fact Extraction and VERification)",
    "paperLink": "https://arxiv.org/abs/1803.05355",
    "description": "A large-scale dataset for fact extraction and verification against textual sources. It consists of claims generated by altering sentences from Wikipedia, classified as Supported, Refuted, or NotEnoughInfo.",
    "authors": [
      "James Thorne",
      "Andreas Vlachos",
      "Christos Christodoulopoulos",
      "Arpit Mittal"
    ],
    "githubLink": "https://github.com/sheffieldnlp/fever-naacl-2018",
    "itemCount": "185,445 claims",
    "source": "arXiv",
    "specs": "Textual claims, Wikipedia evidence, 3 labels (Supported, Refuted, NotEnoughInfo)",
    "year": "2018",
    "id": "imported-json-1769638825347-653-bbv7f",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "LIAR",
    "paperLink": "https://aclanthology.org/P17-2067/",
    "description": "A benchmark dataset for fake news detection consisting of short statements labeled for truthfulness, collected from PolitiFact. It covers various contexts like political debates, TV ads, and social media.",
    "authors": [
      "William Yang Wang"
    ],
    "githubLink": "https://github.com/thiagorainmaker77/liar_dataset",
    "itemCount": "12,836 statements",
    "source": "Scholar",
    "specs": "Short statements, 6 fine-grained labels (pants-fire, false, barely-true, half-true, mostly-true, true), metadata",
    "year": "2017",
    "id": "imported-json-1769638825347-654-qt68z",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9I-2g2Wp9Cn7xKKwraYVT-w-vxI1wc8fSOoE19Vwgcwkt4QZWDkZQgUqAnMjLYo8Ez_p9tq16lwjyIw30nBkIFbi9oCJZIWNw0vDWigF_m8ZhaOJ8VclTRag7CKfwcVi3ULAuYC1UXox8IRPiZry5uZX-FPhDzF0NwGkdi2gQb2xNAqkc-W4Xo3Md7xy3BY9RQKzooJ4czFoFZYhmXxczFLE55psmW1aBhjR1_PnW",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2YUfczc1-U4ndvJOVqGeW6rGaxGd6LUON1hEvM2srJhidHoW6wfnD1u3-7zJoSohZAyiAyd5N3adxHn-DxfJFjTW6-JeTqKwrFgrMOyOr_tMeF5mGHY7Ue5op5gwc6v_BZdNmJxwB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRYh-dvY_Y8pGkP0yEvEaeUXhZuOvVWb9P-XEK_MtJMnho7rbTryyGgzybl5BzHmjTFEJxfAbsT1jyshtk3glu7_elwnsLhyE7VrEKD-XzKuL9zCVHjfnFsBNL2pXpUwu-Wyr52LHxpsC0p0U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVzIWASNSwO97ezmR8FtI1o0PhEHRwzk3ay0RVOthfvGcp72RGk8fut3SBEtIAVI3deoO6hYnHr7Ul39-gr6YUTlhzRs6Bxche9sv8QBOSw8boJNAb0YBeGdVBylxgohofGVBzXcw=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEmHtTdq2Ib5rFuNkD9ArXahnFSGxiOL62qrcYfDHifr2v1b79UwNHvaywEtH4VXGv3t-n4GXZV8ZZTHblct5Bk_vYklX7C7H6N4qjf5ZDyXWUl-UBt-tC1dM2foY5uDIoo3o=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEjPN6jhr92BI-hgcnWhbHgaM24AcBUwNUNYv23hUazJHpS8aNeBnooQXAhLaAZNZhFxJMP7zw7EGs_hTKkGpDkkV145Hyyx9LBgAw2sI-ltV-fuXY6NncLry-aINKRg_I4QDBFWg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_08l4S4ZbgfDdarkw8GeYQu4t_Vvb3p21Sv0cCzDfQwfo65_E2mqCyAsqosZPBtV0SdBnKV9WEclKkV_nMAiDAhYwNDDhJLg44mmH1tQlo1ORBrEPXvKb3xUJ",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "AACR-Bench",
    "paperLink": "https://arxiv.org/abs/2601.19494",
    "description": "A multi-lingual, repository-level context-aware code review benchmark dataset designed to evaluate the performance of Large Language Models (LLMs) in automated code review tasks. It addresses limitations in existing benchmarks by providing full cross-file context and utilizing an 'AI-assisted, Expert-verified' annotation pipeline.",
    "authors": [
      "Lei Zhang",
      "Yongda Yu",
      "Minghui Yu",
      "Xinxin Guo",
      "Zhengqi Zhuang",
      "Guoping Rong",
      "Dong Shao",
      "Haifeng Shen",
      "Hongyu Kuang",
      "Zhengfeng Li",
      "Boge Wang",
      "Guoan Zhang",
      "Bangyu Xiang",
      "Xiaobing Xu"
    ],
    "githubLink": "https://github.com/alibaba/aacr-bench",
    "itemCount": "200 Pull Requests",
    "source": "arXiv",
    "specs": "10 programming languages (Python, Java, JavaScript, TypeScript, Go, Rust, C++, C#, Ruby, PHP); Repository-level context; Cross-file reference analysis",
    "year": "2026",
    "id": "imported-json-1769638825347-655-g8cih",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEUFOMeEIGF5u6_UtFhFbkOmXiiJzA-lPO8bogRtp_lreetFQbAApjB-LO-D6Ja9MW-PhAumME1jNrOmlhK5NgYtVdkklQZF_k10TSvc2qX94BgKTfTj8q3X3EaC3EK9Mkh5y2ZVxBa",
        "title": "catalyzex.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8SMsnpyzlWkrtcJ7muC-paoNMb33PZSJAUyubQUSRHpJ1Mozxxz0ATi6LX5_epJc4o1n9UE7Ee-XclRHhEUvs_Gz7joA7c7hZdzxhGniGBIZ43CwafRBsrXrj",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXHGLzY4dlE1dxNgjBqZG8Qe56fCCIBsruP8haxn5os0Z-WtUeWvBDjXvy7movb1Ux8ATAF_6laNepl1zEHqFfRDetJXxEGh477P8tnUHuJxjonoMkGbX3Y4lzihzil2g=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEALYyV_3Fikm_GtGTK-XLmiqPLDWSX28Ki4zbNQF79piKD1bKU2azc_utoCa01_1uo9d_kWR-sxD4VuifX2sNTnYuGk-EoEAa7t6NFCRi0KO_eMysSuDSey2sQBek2lXbypQXYjGZV1NzuXoJNpmYyLuTfaw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHYMljH0S0E7NW3WO39RST-2A2NPY19jaDHsynseBI-NAYXwwd3v1YVWr4fvfqPiLM6f_3Q1qBlp3K4vq1_kUl7HQTsEMCcP6-WbCrjRcWFbr7fPataLocS2Wu8",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHsCYu4N6HgdcIQmilhTFTCWoR9LABTjHP-u42Y6jZqdJ1hcjuDoM5K-RbwqV2RkrZeN-n4iUyivTSgSOoBaF9g7mBwut-GaSfX26ELkQ0dUyfxrXl73Hk6nWLxJaAVkyQMUlyypAkARp0q8JTSk2BrQZYaOhHJSz5qfQIutF4KpaJXklXdQ4xNoWt9C8ipuGD_SclbgG8pwYm82y5NhcSR8Jm-48WcXEcID6gMgemyEf1w5srx6DzE-EtEzVrCYeZL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AgentDrive",
    "paperLink": "https://arxiv.org/abs/2601.16964",
    "description": "A massive benchmark dataset containing LLM-generated driving scenarios for training and evaluating autonomous agents. It includes a scenario generation pipeline and a multiple-choice question benchmark for reasoning.",
    "authors": [
      "M. Maferrag",
      "et al."
    ],
    "githubLink": "https://github.com/maferrag/AgentDrive",
    "itemCount": "300,000 scenarios, 100,000 MCQs",
    "source": "arXiv",
    "specs": "JSON scenario specifications, multiple-choice questions, autonomous driving domain",
    "year": "2026",
    "id": "saved-1769639007008-rh5qo",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaYiiFaTqFNFnQfOTOSy68bxZVE7x5Q_cfMSC6_tyIG2DEjT1PjKkE15JOx5qwmrt0EnAMNdRyXaJ6mNT62iplR-kq_Ldu_M_TpfIxTbfYUCsAdAvin-YMkTE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyP4UVjdR6lbCE5Ln2uo7ickWHGf5J3s6I1ioLiNSgal3Wtj8oR9AQ2E84i5kc0ehH1WFItLlZjUr3PFw_GuEJjl9fHod6ERLKthMD1w3A-xBViCoALW-8Hb81c2A=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG801l2pTStCjE5Ij8r6dVrfuuOiiln8w7b8JQlXnS8nBOp7SGrbeVfrQ_i6YUSQsPH-GpBai5GwN0vc8oaT0pgnWOnMX6c3I3CoS6q7C0QVB4HoOmKYDmG0QuRZllNhh8v28CSNtKtPyCgcpB8z-R0cj8mvY-BIeL6vSQsd5W-HpHKM7Ctmm_v",
        "title": "unl.pt"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_zAtRj8JBIQy_P-Xev_88giQjnP4-LeOKhXHOKtZxAuCVeWceFrsjFen3KgxSST1RZS-7rsdG9oKAf84-rpZwYVzXeqvoHUWXzwshfbp47WPXPvid-R6nlFKe1YUfTO8IhA==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSbAxq5l4uJWCGZcKyG4H4xaPf8YLJYBIVAMh6a7vqV2t_kLYkk_mzKlQO6tXqIDbW2ptkMFAfSqXgv97jt4wA1Fp4b7fIjBnI-hKF026bS7cSEuJxifz7BZs=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuBTZhiEtzUtq5V_n5lWz9eTxBTQUcU1Z76Eydi7fthHt2r2HzYXORSzTthadZdSwobNkQwtQKAXP5UevVn0jNdBIVJ0nlDQ9TzZaVEQ2N4UMA-8FVYisKvc6UIW3k",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF0xw9fjwxWPWppQP4l48FA6WHSblSRKl6Xx_tCpQdJnrD34u3gtbGwOLjD5tJ6TMFhDSp7wWYDODPZugZKCA_cXJ4Cc3H1yjIPNtF2bK9wrVONumdSvKSnTkw=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MultiAgentBench",
    "paperLink": "https://arxiv.org/abs/2503.01935",
    "description": "A comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. It measures task completion, collaboration, and competition using milestone-based key performance indicators.",
    "authors": [
      "Kunlun Zhu",
      "et al."
    ],
    "githubLink": "https://github.com/MultiagentBench/MARBLE",
    "itemCount": "6 scenarios",
    "source": "arXiv",
    "specs": "Interactive scenarios including Research, Minecraft, Database, Coding, Bargaining, and Werewolf; collaboration and competition metrics",
    "year": "2025",
    "id": "saved-1769639007008-3runi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaYiiFaTqFNFnQfOTOSy68bxZVE7x5Q_cfMSC6_tyIG2DEjT1PjKkE15JOx5qwmrt0EnAMNdRyXaJ6mNT62iplR-kq_Ldu_M_TpfIxTbfYUCsAdAvin-YMkTE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyP4UVjdR6lbCE5Ln2uo7ickWHGf5J3s6I1ioLiNSgal3Wtj8oR9AQ2E84i5kc0ehH1WFItLlZjUr3PFw_GuEJjl9fHod6ERLKthMD1w3A-xBViCoALW-8Hb81c2A=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG801l2pTStCjE5Ij8r6dVrfuuOiiln8w7b8JQlXnS8nBOp7SGrbeVfrQ_i6YUSQsPH-GpBai5GwN0vc8oaT0pgnWOnMX6c3I3CoS6q7C0QVB4HoOmKYDmG0QuRZllNhh8v28CSNtKtPyCgcpB8z-R0cj8mvY-BIeL6vSQsd5W-HpHKM7Ctmm_v",
        "title": "unl.pt"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_zAtRj8JBIQy_P-Xev_88giQjnP4-LeOKhXHOKtZxAuCVeWceFrsjFen3KgxSST1RZS-7rsdG9oKAf84-rpZwYVzXeqvoHUWXzwshfbp47WPXPvid-R6nlFKe1YUfTO8IhA==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSbAxq5l4uJWCGZcKyG4H4xaPf8YLJYBIVAMh6a7vqV2t_kLYkk_mzKlQO6tXqIDbW2ptkMFAfSqXgv97jt4wA1Fp4b7fIjBnI-hKF026bS7cSEuJxifz7BZs=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuBTZhiEtzUtq5V_n5lWz9eTxBTQUcU1Z76Eydi7fthHt2r2HzYXORSzTthadZdSwobNkQwtQKAXP5UevVn0jNdBIVJ0nlDQ9TzZaVEQ2N4UMA-8FVYisKvc6UIW3k",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF0xw9fjwxWPWppQP4l48FA6WHSblSRKl6Xx_tCpQdJnrD34u3gtbGwOLjD5tJ6TMFhDSp7wWYDODPZugZKCA_cXJ4Cc3H1yjIPNtF2bK9wrVONumdSvKSnTkw=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MASLegalBench",
    "paperLink": "https://arxiv.org/abs/2509.24922",
    "description": "A legal benchmark tailored for Multi-Agent Systems (MAS) using GDPR as the application scenario. It evaluates deductive legal reasoning capabilities where agents perform task decomposition and role-based collaboration.",
    "authors": [
      "Huihao Jing",
      "Wenbin Hu",
      "Hongyu Luo",
      "Jianhui Yang",
      "Wei Fan",
      "et al."
    ],
    "githubLink": "https://github.com/HKUST-KnowComp/MASLegalBench",
    "itemCount": "Legal cases/scenarios",
    "source": "arXiv",
    "specs": "Legal reasoning tasks, GDPR application scenario, multi-agent role-playing",
    "year": "2025",
    "id": "saved-1769639007008-5rp7e",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaYiiFaTqFNFnQfOTOSy68bxZVE7x5Q_cfMSC6_tyIG2DEjT1PjKkE15JOx5qwmrt0EnAMNdRyXaJ6mNT62iplR-kq_Ldu_M_TpfIxTbfYUCsAdAvin-YMkTE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyP4UVjdR6lbCE5Ln2uo7ickWHGf5J3s6I1ioLiNSgal3Wtj8oR9AQ2E84i5kc0ehH1WFItLlZjUr3PFw_GuEJjl9fHod6ERLKthMD1w3A-xBViCoALW-8Hb81c2A=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG801l2pTStCjE5Ij8r6dVrfuuOiiln8w7b8JQlXnS8nBOp7SGrbeVfrQ_i6YUSQsPH-GpBai5GwN0vc8oaT0pgnWOnMX6c3I3CoS6q7C0QVB4HoOmKYDmG0QuRZllNhh8v28CSNtKtPyCgcpB8z-R0cj8mvY-BIeL6vSQsd5W-HpHKM7Ctmm_v",
        "title": "unl.pt"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_zAtRj8JBIQy_P-Xev_88giQjnP4-LeOKhXHOKtZxAuCVeWceFrsjFen3KgxSST1RZS-7rsdG9oKAf84-rpZwYVzXeqvoHUWXzwshfbp47WPXPvid-R6nlFKe1YUfTO8IhA==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSbAxq5l4uJWCGZcKyG4H4xaPf8YLJYBIVAMh6a7vqV2t_kLYkk_mzKlQO6tXqIDbW2ptkMFAfSqXgv97jt4wA1Fp4b7fIjBnI-hKF026bS7cSEuJxifz7BZs=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuBTZhiEtzUtq5V_n5lWz9eTxBTQUcU1Z76Eydi7fthHt2r2HzYXORSzTthadZdSwobNkQwtQKAXP5UevVn0jNdBIVJ0nlDQ9TzZaVEQ2N4UMA-8FVYisKvc6UIW3k",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF0xw9fjwxWPWppQP4l48FA6WHSblSRKl6Xx_tCpQdJnrD34u3gtbGwOLjD5tJ6TMFhDSp7wWYDODPZugZKCA_cXJ4Cc3H1yjIPNtF2bK9wrVONumdSvKSnTkw=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "AgentVerse",
    "paperLink": "https://arxiv.org/abs/2308.10848",
    "description": "A framework and benchmark for facilitating multi-agent collaboration and exploring emergent behaviors. It evaluates agent groups on tasks like coding, reasoning, and social simulation (e.g., Prisoner's Dilemma).",
    "authors": [
      "Weize Chen",
      "Yusheng Su",
      "Jingwei Zuo",
      "Cheng Yang",
      "Chenfei Yuan",
      "et al."
    ],
    "githubLink": "https://github.com/OpenBMB/AgentVerse",
    "itemCount": "Various tasks",
    "source": "arXiv",
    "specs": "Multi-agent collaboration framework, text and code tasks, emergent behavior analysis",
    "year": "2023",
    "id": "saved-1769639007008-81o1f",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaYiiFaTqFNFnQfOTOSy68bxZVE7x5Q_cfMSC6_tyIG2DEjT1PjKkE15JOx5qwmrt0EnAMNdRyXaJ6mNT62iplR-kq_Ldu_M_TpfIxTbfYUCsAdAvin-YMkTE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyP4UVjdR6lbCE5Ln2uo7ickWHGf5J3s6I1ioLiNSgal3Wtj8oR9AQ2E84i5kc0ehH1WFItLlZjUr3PFw_GuEJjl9fHod6ERLKthMD1w3A-xBViCoALW-8Hb81c2A=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG801l2pTStCjE5Ij8r6dVrfuuOiiln8w7b8JQlXnS8nBOp7SGrbeVfrQ_i6YUSQsPH-GpBai5GwN0vc8oaT0pgnWOnMX6c3I3CoS6q7C0QVB4HoOmKYDmG0QuRZllNhh8v28CSNtKtPyCgcpB8z-R0cj8mvY-BIeL6vSQsd5W-HpHKM7Ctmm_v",
        "title": "unl.pt"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_zAtRj8JBIQy_P-Xev_88giQjnP4-LeOKhXHOKtZxAuCVeWceFrsjFen3KgxSST1RZS-7rsdG9oKAf84-rpZwYVzXeqvoHUWXzwshfbp47WPXPvid-R6nlFKe1YUfTO8IhA==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSbAxq5l4uJWCGZcKyG4H4xaPf8YLJYBIVAMh6a7vqV2t_kLYkk_mzKlQO6tXqIDbW2ptkMFAfSqXgv97jt4wA1Fp4b7fIjBnI-hKF026bS7cSEuJxifz7BZs=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuBTZhiEtzUtq5V_n5lWz9eTxBTQUcU1Z76Eydi7fthHt2r2HzYXORSzTthadZdSwobNkQwtQKAXP5UevVn0jNdBIVJ0nlDQ9TzZaVEQ2N4UMA-8FVYisKvc6UIW3k",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF0xw9fjwxWPWppQP4l48FA6WHSblSRKl6Xx_tCpQdJnrD34u3gtbGwOLjD5tJ6TMFhDSp7wWYDODPZugZKCA_cXJ4Cc3H1yjIPNtF2bK9wrVONumdSvKSnTkw=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "UGC-VideoCap",
    "paperLink": "https://arxiv.org/abs/2507.11336",
    "description": "A benchmark for detailed omnimodal captioning of short-form user-generated videos (e.g., TikTok). It emphasizes the balanced integration of audio and visual modalities, with annotations for audio-only, visual-only, and joint semantics.",
    "authors": [
      "Peiran Wu",
      "Yunze Liu",
      "Zhengdong Zhu",
      "Enmin Zhou",
      "Junxiao Shen"
    ],
    "githubLink": "https://huggingface.co/datasets/openinterx/UGC-VideoCap",
    "itemCount": "1,000 benchmark videos; 107k training pairs",
    "source": "Hugging Face",
    "specs": "Short-form videos; Audio, Visual, and Joint Audio-Visual captions; QA pairs",
    "year": "2025",
    "id": "saved-1769639074341-gll4e",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFt7YVvl2U7nZ-dBH0z6zJLYRHgCTRBAvBKeHks8VJ8sU0fLdtw7Mnpi5nc07u4ICpKfGdj09sQlI6EQWQv9k08qOk42tyVFAAxGX-b_MCxcbuYXZxWSueDiCl",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGAruQmis6KA3s2KKX_-f5MohIv8SlT9aOYjW15WmKL96oUQIm41O8Hs-lrVzHG_61fqh1oM0-dYsW_usHTHPsx_dMa9AJ7QBhsJEaEhE8KDWVh5TxKwrtQMOe4B0Y7B7cyrTsKGonxhBE1jfK6ctz2gET0oQIyXa5zmGc=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfgQky5sf1noyXxMUK0_KE4KLlEolbvAoSiOn5sBqPfO1Uanh7MXlLuctuoctz8g4QIpcf95wzzLlnRw-3bPgB2ZJfcl7-Xi6RI8YmP3FvzK146huNVStFB3LsvPomWL32igFZy4omCnpXoiMtZgMDH4e6-hcEdvwO8BVjK6oPF6tmSrCyQOIB8UxMNBzgJdNm5iCecc0ZCOE=",
        "title": "microsoft.com"
      }
    ]
  },
  {
    "title": "AVCaps",
    "paperLink": "https://openreview.net/forum?id=YicbFdNTTy",
    "description": "A dataset specifically designed for audio-visual captioning, providing separate textual captions for the audio, visual, and combined audio-visual contents of video clips to enable modality-specific study and evaluation.",
    "authors": [
      "Parthasaarathy Sudarsanam",
      "Irene Martín-Morató",
      "Aapo Hakala",
      "Tuomas Virtanen"
    ],
    "githubLink": "https://huggingface.co/datasets/TUT-ARG/AVCaps",
    "itemCount": "2,061 video clips (28.8 hours)",
    "source": "Hugging Face",
    "specs": "MP4 videos; Captions for Audio-only, Visual-only, and Audio-Visual modalities",
    "year": "2024",
    "id": "saved-1769639074341-s5cwr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFt7YVvl2U7nZ-dBH0z6zJLYRHgCTRBAvBKeHks8VJ8sU0fLdtw7Mnpi5nc07u4ICpKfGdj09sQlI6EQWQv9k08qOk42tyVFAAxGX-b_MCxcbuYXZxWSueDiCl",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGAruQmis6KA3s2KKX_-f5MohIv8SlT9aOYjW15WmKL96oUQIm41O8Hs-lrVzHG_61fqh1oM0-dYsW_usHTHPsx_dMa9AJ7QBhsJEaEhE8KDWVh5TxKwrtQMOe4B0Y7B7cyrTsKGonxhBE1jfK6ctz2gET0oQIyXa5zmGc=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfgQky5sf1noyXxMUK0_KE4KLlEolbvAoSiOn5sBqPfO1Uanh7MXlLuctuoctz8g4QIpcf95wzzLlnRw-3bPgB2ZJfcl7-Xi6RI8YmP3FvzK146huNVStFB3LsvPomWL32igFZy4omCnpXoiMtZgMDH4e6-hcEdvwO8BVjK6oPF6tmSrCyQOIB8UxMNBzgJdNm5iCecc0ZCOE=",
        "title": "microsoft.com"
      }
    ]
  },
  {
    "title": "VALOR-32K",
    "paperLink": "https://arxiv.org/abs/2304.08345",
    "description": "A large-scale audiovisual-language benchmark designed for pretraining and evaluating omni-perception models. It focuses on the correlation between vision, audio, and language, providing human-annotated audiovisual captions.",
    "authors": [
      "Sihan Chen",
      "Handong Li",
      "Qunbo Wang",
      "Zijia Zhao",
      "Mingzhen Sun",
      "Xinxin Zhu",
      "Jing Liu"
    ],
    "githubLink": "https://github.com/CASIA-IVA-Lab/VALOR",
    "itemCount": "32,000 videos (subset of VALOR-1M)",
    "source": "arXiv",
    "specs": "Video clips with audio; Audiovisual captions; Audio concept density metrics",
    "year": "2023",
    "id": "saved-1769639074341-tinil",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFt7YVvl2U7nZ-dBH0z6zJLYRHgCTRBAvBKeHks8VJ8sU0fLdtw7Mnpi5nc07u4ICpKfGdj09sQlI6EQWQv9k08qOk42tyVFAAxGX-b_MCxcbuYXZxWSueDiCl",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGAruQmis6KA3s2KKX_-f5MohIv8SlT9aOYjW15WmKL96oUQIm41O8Hs-lrVzHG_61fqh1oM0-dYsW_usHTHPsx_dMa9AJ7QBhsJEaEhE8KDWVh5TxKwrtQMOe4B0Y7B7cyrTsKGonxhBE1jfK6ctz2gET0oQIyXa5zmGc=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfgQky5sf1noyXxMUK0_KE4KLlEolbvAoSiOn5sBqPfO1Uanh7MXlLuctuoctz8g4QIpcf95wzzLlnRw-3bPgB2ZJfcl7-Xi6RI8YmP3FvzK146huNVStFB3LsvPomWL32igFZy4omCnpXoiMtZgMDH4e6-hcEdvwO8BVjK6oPF6tmSrCyQOIB8UxMNBzgJdNm5iCecc0ZCOE=",
        "title": "microsoft.com"
      }
    ]
  },
  {
    "title": "ActivityNet Captions",
    "paperLink": "https://arxiv.org/abs/1705.00754",
    "description": "A large-scale benchmark for dense video captioning that annotates long, untrimmed videos with temporally localized sentences. While primarily a video benchmark, it is widely used to evaluate audio-visual captioning models due to its rich event content and available audio.",
    "authors": [
      "Ranjay Krishna",
      "Kenji Hata",
      "Frederic Ren",
      "Li Fei-Fei",
      "Juan Carlos Niebles"
    ],
    "githubLink": "http://activity-net.org/challenges/2017/tasks/guest_demos.html",
    "itemCount": "20,000 videos; 100,000 captions",
    "source": "Scholar",
    "specs": "Untrimmed videos (avg 180s); Dense temporal event segments; Natural language descriptions",
    "year": "2017",
    "id": "saved-1769639074341-fn9z9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFt7YVvl2U7nZ-dBH0z6zJLYRHgCTRBAvBKeHks8VJ8sU0fLdtw7Mnpi5nc07u4ICpKfGdj09sQlI6EQWQv9k08qOk42tyVFAAxGX-b_MCxcbuYXZxWSueDiCl",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGAruQmis6KA3s2KKX_-f5MohIv8SlT9aOYjW15WmKL96oUQIm41O8Hs-lrVzHG_61fqh1oM0-dYsW_usHTHPsx_dMa9AJ7QBhsJEaEhE8KDWVh5TxKwrtQMOe4B0Y7B7cyrTsKGonxhBE1jfK6ctz2gET0oQIyXa5zmGc=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfgQky5sf1noyXxMUK0_KE4KLlEolbvAoSiOn5sBqPfO1Uanh7MXlLuctuoctz8g4QIpcf95wzzLlnRw-3bPgB2ZJfcl7-Xi6RI8YmP3FvzK146huNVStFB3LsvPomWL32igFZy4omCnpXoiMtZgMDH4e6-hcEdvwO8BVjK6oPF6tmSrCyQOIB8UxMNBzgJdNm5iCecc0ZCOE=",
        "title": "microsoft.com"
      }
    ]
  },
  {
    "title": "MSR-VTT",
    "paperLink": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Xu_MSR-VTT_A_Large_CVPR_2016_paper.html",
    "description": "A standard video captioning benchmark consisting of web video clips covering a comprehensive list of categories. It serves as a common testbed for evaluating the contribution of audio modalities in video-to-text generation.",
    "authors": [
      "Jun Xu",
      "Tao Mei",
      "Ting Yao",
      "Yong Rui"
    ],
    "githubLink": "https://github.com/xujun7024098/MSR-VTT-dataset",
    "itemCount": "10,000 video clips; 200,000 captions",
    "source": "Scholar",
    "specs": "Short video clips with audio; Sentence-level captions; 20 categories",
    "year": "2016",
    "id": "saved-1769639074341-ke58r",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFt7YVvl2U7nZ-dBH0z6zJLYRHgCTRBAvBKeHks8VJ8sU0fLdtw7Mnpi5nc07u4ICpKfGdj09sQlI6EQWQv9k08qOk42tyVFAAxGX-b_MCxcbuYXZxWSueDiCl",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGAruQmis6KA3s2KKX_-f5MohIv8SlT9aOYjW15WmKL96oUQIm41O8Hs-lrVzHG_61fqh1oM0-dYsW_usHTHPsx_dMa9AJ7QBhsJEaEhE8KDWVh5TxKwrtQMOe4B0Y7B7cyrTsKGonxhBE1jfK6ctz2gET0oQIyXa5zmGc=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGfgQky5sf1noyXxMUK0_KE4KLlEolbvAoSiOn5sBqPfO1Uanh7MXlLuctuoctz8g4QIpcf95wzzLlnRw-3bPgB2ZJfcl7-Xi6RI8YmP3FvzK146huNVStFB3LsvPomWL32igFZy4omCnpXoiMtZgMDH4e6-hcEdvwO8BVjK6oPF6tmSrCyQOIB8UxMNBzgJdNm5iCecc0ZCOE=",
        "title": "microsoft.com"
      }
    ]
  },
  {
    "title": "VC-Bench",
    "paperLink": "https://arxiv.org/abs/2601.19236",
    "description": "A pioneering benchmark explicitly designed for the 'Video Connecting' task, which involves generating smooth, temporally coherent intermediate video content between a given start and end clip. It evaluates models on video quality, start-end consistency, and transition smoothness.",
    "authors": [
      "Zhiyu Yin",
      "Zhipeng Liu",
      "Kehai Chen",
      "Lemao Liu",
      "Jin Liu",
      "Hong-Dong Li",
      "Yang Xiang",
      "Min Zhang"
    ],
    "githubLink": "https://github.com/PKU-YuanGroup/VC-Bench",
    "itemCount": "1,579 videos",
    "source": "arXiv",
    "specs": "15 main categories, 72 subcategories; high-quality videos; includes metrics for Video Quality (VQS), Consistency (SECS), and Smoothness (TSS).",
    "year": "2026",
    "id": "saved-1769639198373-pz272",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDH-3S6uGpkYTo8BP5qjBm6xuM4Gqp6CNAISisYp080pCO7YxxDVUN2_KaWxDQX_Xk5_AkWp4_GxWZWfVI2-fLSgI3KRYgf-RrsWzCTcoAVGoJPfhzuGbcJJEeOqA7dDPmXVXN",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHs1AQJGFE42fCjWcMOdaYBjOiL6YCzk53DcyTQVPe3dQqCWkvkPaqghxxVbMWfF6mXW-u8CWTgPfMlBwkTOMsMsg9EqQ5aU9SjQBdJMel6OySIVBY7enCVYPnbGUE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRjxS21zhTjdcMk0HsK67Hmz0WvLKpvALrvgOqQqn62sfm3jQiQ0NLW0JTqgj3MgsXO4kWSHwL5wlBcv2lJ_sfm9AuBNQ_DfSQB8i2mJC63XGXEFiHroD3prmSiq4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "TC-Bench (TC-Bench-I2V)",
    "paperLink": "https://arxiv.org/abs/2406.08656",
    "description": "A benchmark for evaluating temporal compositionality in video generation. The I2V component (TC-Bench-I2V) specifically assesses a model's ability to generate videos that transition between two defined states (start and end), serving as a ground truth for transition/connecting tasks.",
    "authors": [
      "Manual Selection (Curated by authors of TC-Bench)"
    ],
    "githubLink": "https://github.com/MiaoLiu-c/TC-Bench",
    "itemCount": "120 prompt-video pairs",
    "source": "arXiv",
    "specs": "120 curated pairs covering diverse attributes, actions, and objects; includes human-verified ground truth videos and CLIP-based consistency metrics.",
    "year": "2024",
    "id": "saved-1769639198373-k5jwh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDH-3S6uGpkYTo8BP5qjBm6xuM4Gqp6CNAISisYp080pCO7YxxDVUN2_KaWxDQX_Xk5_AkWp4_GxWZWfVI2-fLSgI3KRYgf-RrsWzCTcoAVGoJPfhzuGbcJJEeOqA7dDPmXVXN",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHs1AQJGFE42fCjWcMOdaYBjOiL6YCzk53DcyTQVPe3dQqCWkvkPaqghxxVbMWfF6mXW-u8CWTgPfMlBwkTOMsMsg9EqQ5aU9SjQBdJMel6OySIVBY7enCVYPnbGUE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRjxS21zhTjdcMk0HsK67Hmz0WvLKpvALrvgOqQqn62sfm3jQiQ0NLW0JTqgj3MgsXO4kWSHwL5wlBcv2lJ_sfm9AuBNQ_DfSQB8i2mJC63XGXEFiHroD3prmSiq4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "TAV Dataset (Transition-Aware Video)",
    "paperLink": "https://arxiv.org/abs/2407.15835",
    "description": "Created to enhance scene transition awareness in video generation. It consists of video clips containing scene transitions along with scene-wise descriptions, used to train and evaluate models on generating coherent transitions between distinct scenes.",
    "authors": [
      "Yingqing He",
      "Tianyu Yang",
      "Yong Zhang",
      "Ying Shan",
      "Qifeng Chen"
    ],
    "githubLink": "https://github.com/TencentARC/TAV",
    "itemCount": "Pairs of 10-second clips",
    "source": "arXiv",
    "specs": "Extracted from Panda-70M; contains clip pairs with explicit scene transitions and separate text descriptions for each scene.",
    "year": "2024",
    "id": "saved-1769639198373-s1w5k",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDH-3S6uGpkYTo8BP5qjBm6xuM4Gqp6CNAISisYp080pCO7YxxDVUN2_KaWxDQX_Xk5_AkWp4_GxWZWfVI2-fLSgI3KRYgf-RrsWzCTcoAVGoJPfhzuGbcJJEeOqA7dDPmXVXN",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHs1AQJGFE42fCjWcMOdaYBjOiL6YCzk53DcyTQVPe3dQqCWkvkPaqghxxVbMWfF6mXW-u8CWTgPfMlBwkTOMsMsg9EqQ5aU9SjQBdJMel6OySIVBY7enCVYPnbGUE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRjxS21zhTjdcMk0HsK67Hmz0WvLKpvALrvgOqQqn62sfm3jQiQ0NLW0JTqgj3MgsXO4kWSHwL5wlBcv2lJ_sfm9AuBNQ_DfSQB8i2mJC63XGXEFiHroD3prmSiq4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "I2V-Bench (Video Inbetweening)",
    "paperLink": "https://arxiv.org/abs/2307.04725",
    "description": "A comprehensive evaluation framework for image-to-video models that includes specific protocols for 'Video Inbetweening'—the task of connecting a start and end frame with generated video. It often utilizes subsets of datasets like DAVIS and UCF101 for standardized testing.",
    "authors": [
      "Emergent Mind (Compiled by community/authors)"
    ],
    "githubLink": "https://github.com/vbench/VBench",
    "itemCount": "Variable (Standardized subsets)",
    "source": "Hugging Face",
    "specs": "Includes metrics for visual consistency, motion realism, and control; specifically evaluates the ability to connect keyframes.",
    "year": "2024",
    "id": "saved-1769639198374-vmho3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDH-3S6uGpkYTo8BP5qjBm6xuM4Gqp6CNAISisYp080pCO7YxxDVUN2_KaWxDQX_Xk5_AkWp4_GxWZWfVI2-fLSgI3KRYgf-RrsWzCTcoAVGoJPfhzuGbcJJEeOqA7dDPmXVXN",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHs1AQJGFE42fCjWcMOdaYBjOiL6YCzk53DcyTQVPe3dQqCWkvkPaqghxxVbMWfF6mXW-u8CWTgPfMlBwkTOMsMsg9EqQ5aU9SjQBdJMel6OySIVBY7enCVYPnbGUE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRjxS21zhTjdcMk0HsK67Hmz0WvLKpvALrvgOqQqn62sfm3jQiQ0NLW0JTqgj3MgsXO4kWSHwL5wlBcv2lJ_sfm9AuBNQ_DfSQB8i2mJC63XGXEFiHroD3prmSiq4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MorphBench",
    "paperLink": "https://arxiv.org/abs/2312.08815",
    "description": "The first benchmark dataset designed for assessing image morphing of general objects. While focused on image-to-image morphing, it is widely used as a standard for evaluating the smoothness and quality of generated transitions (connecting) between two visual concepts.",
    "authors": [
      "Kaiwen Zhang",
      "Yifan Zhou",
      "Xudong Xu",
      "Xingang Pan",
      "Bo Dai"
    ],
    "githubLink": "https://github.com/Kevin-thu/DiffMorpher",
    "itemCount": "90 pairs",
    "source": "arXiv",
    "specs": "66 pairs for object metamorphosis (different objects) and 24 pairs for object animation (same object); diverse content and styles.",
    "year": "2023",
    "id": "saved-1769639198374-0mq99",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDH-3S6uGpkYTo8BP5qjBm6xuM4Gqp6CNAISisYp080pCO7YxxDVUN2_KaWxDQX_Xk5_AkWp4_GxWZWfVI2-fLSgI3KRYgf-RrsWzCTcoAVGoJPfhzuGbcJJEeOqA7dDPmXVXN",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHs1AQJGFE42fCjWcMOdaYBjOiL6YCzk53DcyTQVPe3dQqCWkvkPaqghxxVbMWfF6mXW-u8CWTgPfMlBwkTOMsMsg9EqQ5aU9SjQBdJMel6OySIVBY7enCVYPnbGUE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRjxS21zhTjdcMk0HsK67Hmz0WvLKpvALrvgOqQqn62sfm3jQiQ0NLW0JTqgj3MgsXO4kWSHwL5wlBcv2lJ_sfm9AuBNQ_DfSQB8i2mJC63XGXEFiHroD3prmSiq4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Video Storytelling Dataset (VideoStory / VIST)",
    "paperLink": "https://www.aclweb.org/anthology/D18-1002/",
    "description": "A dataset for the task of video storytelling, which involves generating a coherent narrative (story) that connects a sequence of video clips. It focuses on the semantic connection between clips to form a story rather than pixel-level transition generation.",
    "authors": [
      "Spandana Gella",
      "Mike Lewis",
      "Marcus Rohrbach"
    ],
    "githubLink": "https://github.com/MJ10/VideoStory",
    "itemCount": "20,000 videos",
    "source": "Semantic Scholar",
    "specs": "396 hours of video; 123k sentences; temporally aligned descriptions connecting video segments into stories.",
    "year": "2018",
    "id": "saved-1769639198374-5rxsp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDH-3S6uGpkYTo8BP5qjBm6xuM4Gqp6CNAISisYp080pCO7YxxDVUN2_KaWxDQX_Xk5_AkWp4_GxWZWfVI2-fLSgI3KRYgf-RrsWzCTcoAVGoJPfhzuGbcJJEeOqA7dDPmXVXN",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHs1AQJGFE42fCjWcMOdaYBjOiL6YCzk53DcyTQVPe3dQqCWkvkPaqghxxVbMWfF6mXW-u8CWTgPfMlBwkTOMsMsg9EqQ5aU9SjQBdJMel6OySIVBY7enCVYPnbGUE=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRjxS21zhTjdcMk0HsK67Hmz0WvLKpvALrvgOqQqn62sfm3jQiQ0NLW0JTqgj3MgsXO4kWSHwL5wlBcv2lJ_sfm9AuBNQ_DfSQB8i2mJC63XGXEFiHroD3prmSiq4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "RefCOCOg",
    "paperLink": "https://arxiv.org/abs/1511.02283",
    "description": "A referring expression dataset collected in a non-interactive setting, resulting in longer and more complex descriptions compared to RefCOCO/+. It is a key benchmark for evaluating detailed spatial and attribute understanding in 'Simple Points Prediction' models.",
    "authors": [
      "Junhua Mao",
      "Jonathan Huang",
      "Alexander Toshev",
      "Oana Camburu",
      "Alan Yuille",
      "Kevin Murphy"
    ],
    "githubLink": "https://github.com/lichengunc/refer",
    "itemCount": "85,474 expressions; 54,822 objects; 26,711 images",
    "source": "arXiv",
    "specs": "Images (COCO), Text (Complex Expressions), Masks/BBoxes",
    "year": "2016",
    "id": "saved-1769639283844-6z6ql",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEOCM8Z4AxLTaOuS_H0bG8_cIVVKZFoSCJ815ZIp70RoopQbDuA6XAEpsfxl1BKiJcMA4YQxTqildyOfyeI6fVMv5PhaskGjxX2-7B5zXMNFJHQp3eGDnkHqYu61DB6jOBmXHI7-JW_K7O-D7E=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeyAVtUNUkW2QqB0Zkhc_4t-4OHAEUWVCZRhnRytodG_4D0wwfzsHk9YwIcLMwu4gbmokC6sIyAofCrgf7IkMmm0hdCwwSklRQD5OxLBdFhGQLivHClRUi8hOjFt73fHYgExcumMK4DuYQu6znewAufVu1ON6rmQKXOy2Ly_FN0upYoaXRba8E_FM0s9bjNYtyJ1kyLcCgmMy-RTxTu5tu4WvyE_h_T7dhtRkm7mgyTNWtPGGl32yeHQwwVZyr6Q==",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVO5yangsK_5G-9Kl9qev8hobobUrW-plVAnlb1igYv1Bk6zFVAng3fnQU65x-aMnQQ8VpzfUnicFIzyeGa2kMVJZfnehpJ_Chz_SNysqCip91uJivlAgbuxeFdgUB3fOl-PMTcuTxxmcuN9Rxw_E=",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRie6TeTDgLGOLZ098Srd-KR6NACs-Y1DRE2DFCtpZuEcbMtrhMGNrEme46ksU6GdbyELGHF7Nhxyq3orlVW2gGJrnFtbQO1TFtNNH6VXCzD32grymaJ2hpYd7",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "RefCOCO",
    "paperLink": "https://aclanthology.org/D14-1086/",
    "description": "A standard benchmark dataset for Referring Expression Comprehension (REC) and Segmentation. It was collected using the ReferItGame and contains short, concise expressions including location words. Recently utilized as a primary benchmark for 'Simple Points Prediction' tasks (e.g., SimpleSeg) where models predict point trajectories for segmentation.",
    "authors": [
      "Sahar Kazemzadeh",
      "Vicente Ordonez",
      "Mark Matten",
      "Tamara Berg"
    ],
    "githubLink": "https://github.com/lichengunc/refer",
    "itemCount": "142,209 expressions; 50,000 objects; 19,994 images",
    "source": "Scholar",
    "specs": "Images (COCO), Text (Referring Expressions), Masks/BBoxes",
    "year": "2014",
    "id": "saved-1769639283844-08aaq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEOCM8Z4AxLTaOuS_H0bG8_cIVVKZFoSCJ815ZIp70RoopQbDuA6XAEpsfxl1BKiJcMA4YQxTqildyOfyeI6fVMv5PhaskGjxX2-7B5zXMNFJHQp3eGDnkHqYu61DB6jOBmXHI7-JW_K7O-D7E=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeyAVtUNUkW2QqB0Zkhc_4t-4OHAEUWVCZRhnRytodG_4D0wwfzsHk9YwIcLMwu4gbmokC6sIyAofCrgf7IkMmm0hdCwwSklRQD5OxLBdFhGQLivHClRUi8hOjFt73fHYgExcumMK4DuYQu6znewAufVu1ON6rmQKXOy2Ly_FN0upYoaXRba8E_FM0s9bjNYtyJ1kyLcCgmMy-RTxTu5tu4WvyE_h_T7dhtRkm7mgyTNWtPGGl32yeHQwwVZyr6Q==",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVO5yangsK_5G-9Kl9qev8hobobUrW-plVAnlb1igYv1Bk6zFVAng3fnQU65x-aMnQQ8VpzfUnicFIzyeGa2kMVJZfnehpJ_Chz_SNysqCip91uJivlAgbuxeFdgUB3fOl-PMTcuTxxmcuN9Rxw_E=",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRie6TeTDgLGOLZ098Srd-KR6NACs-Y1DRE2DFCtpZuEcbMtrhMGNrEme46ksU6GdbyELGHF7Nhxyq3orlVW2gGJrnFtbQO1TFtNNH6VXCzD32grymaJ2hpYd7",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "POPE",
    "paperLink": "https://aclanthology.org/2023.emnlp-main.20/",
    "description": "A polling-based object probing evaluation method designed to detect object hallucination in large vision-language models.",
    "authors": [
      "Yifan Li",
      "Yifan Du",
      "Kun Zhou",
      "Jinpeng Wang",
      "Xin Zhao",
      "Ji-Rong Wen"
    ],
    "githubLink": "https://github.com/RUCAIBox/POPE",
    "itemCount": "3,000 visual-object pairs",
    "source": "Scholar",
    "specs": "Binary Yes/No polling questions, Images",
    "year": "2023",
    "id": "saved-1769639379130-y9o8n",
    "groundingSources": []
  },
  {
    "title": "VisIT-Bench",
    "paperLink": "https://arxiv.org/abs/2308.06595",
    "description": "A benchmark for evaluating instruction-following vision-language models on real-world use cases.",
    "authors": [
      "Yonatan Bitton",
      "Hritik Bansal",
      "Jack Hessel",
      "Rulin Shao",
      "Wanrong Zhu",
      "Anas Awadalla",
      "Josh Gardner",
      "Rohan Taori",
      "Ludwig Schmidt"
    ],
    "githubLink": "https://github.com/mlfoundations/VisIT-Bench",
    "itemCount": "592 test queries",
    "source": "arXiv",
    "specs": "Open-ended instruction following, Images, Text",
    "year": "2023",
    "id": "saved-1769639379130-s8dpp",
    "groundingSources": []
  },
  {
    "title": "TextVQA",
    "paperLink": "https://arxiv.org/abs/1904.08920",
    "description": "A dataset for visual question answering that requires models to read and reason about text present in images.",
    "authors": [
      "Amanpreet Singh",
      "Vivek Natarajan",
      "Moshiur Farhadi",
      "Devi Parikh",
      "Marcus Rohrbach"
    ],
    "githubLink": "https://github.com/facebookresearch/mmf",
    "itemCount": "45,336 questions",
    "source": "arXiv",
    "specs": "Questions requiring OCR, Images, Text",
    "year": "2019",
    "id": "saved-1769639379130-w0l5c",
    "groundingSources": []
  },
  {
    "title": "GQA",
    "paperLink": "https://arxiv.org/abs/1902.09506",
    "description": "A large-scale dataset for real-world visual reasoning and compositional question answering, leveraging scene graph structures.",
    "authors": [
      "Drew A. Hudson",
      "Christopher D. Manning"
    ],
    "githubLink": "https://github.com/stanfordnlp/gqa",
    "itemCount": "22 million questions",
    "source": "arXiv",
    "specs": "Compositional questions, Scene graphs, Images",
    "year": "2019",
    "id": "saved-1769639379130-6ib0h",
    "groundingSources": []
  },
  {
    "title": "LLM-QBench",
    "paperLink": "https://arxiv.org/abs/2405.06001",
    "description": "A comprehensive benchmark designed to identify best practices for Post-Training Quantization (PTQ) of Large Language Models. It evaluates various quantization algorithms, calibration data choices, and quantization schemes to balance inference efficiency, accuracy, and calibration cost. It aims to provide a modular and fair comparison of techniques.",
    "authors": [
      "Ruihao Gong",
      "Yushi Huang",
      "Zhehao Guo",
      "Xianglong Liu",
      "Dacheng Tao",
      "et al."
    ],
    "githubLink": "https://github.com/ModelTC/LLMC",
    "itemCount": "Over 500 experiments across diverse models and datasets",
    "source": "arXiv",
    "specs": "Focuses on LLM Post-Training Quantization (PTQ); Evaluates calibration data, algorithm pipelines, and quantization configurations.",
    "year": "2024",
    "id": "saved-1769639451908-qmjpa",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzgxtCd21rqhJ2Pg0UD-DYScCQ2kWSM-PNJnnlkMVCmRHWN9YQM4g2n3hM7hwIp2gPrG7Q-6jYfIR01L3z7x0rD2-LcrTt7zPmLCKvbS1L-usDL4Mb5Cz8D4OgZMwPYlQ=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn35F96FVWbq_Xzu2k_UiuzGMUcvHOzIrSWn6k4n0YK1QYVyXJoFdQgW0Ks0Xn89TQcyMCb2kdT1yYcQ43CkGoS3GU4gYffVco4WXKlvxg3W4Z0wSXSDKz9w3hOfjSOE46PC8yGg3egkR7f2i5q7rYRcwv6VQ7-nefonQJopei6b_3IeZi_3BMM07BTCsnFBuRqYve9dUZxnmRv2O_3r3kPsyvDG9Ugo_uaZKxmoP-baEwy6GMGtMDZll5UaLWL3m2dB3WHw==",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHvKZheBN5PQZ9Vb0SW3kX5k3tvaNwlMC5lQ7E-zF02jrMbWcLbHDj4zEt4_HCX7RWTNYnE13J8BNrxEroL2V2k7VwovS8hXM_5vDyxoAICvjjEXHrUna-3MrwMDsI7kVNiitfImw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkcmjtxvOq6YqgRBc2geL4rlLGfIGAmUoAmjUtAb9M3W3tVHQOSROLECBw62x-u8UHq6D-U6spwHPpPg8huIzon1NAA3pSuA8HrWdUciBDhbay5v8J4Ji2sR1LM4JvZ1J90-OX7YSNaQoRs6QvzdTsKasSXcPlrGKRo8uIRHuc-xHL9-fprgznT3-CsmSHzUNloSGDQAZQmVgCHFhIurXNhDHlK_4Mmsm_Nd61hq4=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpM38SG-868XOyQkF_5_jEOiZVJn6pBwJ0ac-kNT7CftNiCi-pwQCam3_QAHIN-fRCD_TfDA7skQj_RBEiu1hFu95VAwPl7_bvl2nirRaziOHKccLZa41Wf5Y=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSjOtsVOVxpVWlR3RfT1-1KE-Yx7Rd0ID00nFFDwWbvA6d2ZtWeCwLbSjiYT3y-qyfBPAWqWQm2ppV4vNdwFK_ggj3kwH_MWboE4pdjVRKht3kpZRBC0g99D4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "RobustMQ",
    "paperLink": "https://arxiv.org/abs/2308.10524",
    "description": "A benchmark dedicated to evaluating the robustness of quantized models against various perturbations, including adversarial attacks, natural corruptions, and systematic noises. It aims to uncover the vulnerabilities of quantized Deep Neural Networks (DNNs) compared to their full-precision counterparts.",
    "authors": [
      "Yisong Xiao",
      "Tianyuan Zhang",
      "Shunchang Liu",
      "Haotong Qin"
    ],
    "githubLink": "https://github.com/Easezyc/RobustMQ",
    "itemCount": "Evaluates 4 architectures, 3 quantization methods, 4 bit-widths",
    "source": "arXiv",
    "specs": "Evaluation against adversarial attacks, natural corruptions (impulse noise, glass blur, etc.), and systematic noises.",
    "year": "2023",
    "id": "saved-1769639451908-k8xkc",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzgxtCd21rqhJ2Pg0UD-DYScCQ2kWSM-PNJnnlkMVCmRHWN9YQM4g2n3hM7hwIp2gPrG7Q-6jYfIR01L3z7x0rD2-LcrTt7zPmLCKvbS1L-usDL4Mb5Cz8D4OgZMwPYlQ=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn35F96FVWbq_Xzu2k_UiuzGMUcvHOzIrSWn6k4n0YK1QYVyXJoFdQgW0Ks0Xn89TQcyMCb2kdT1yYcQ43CkGoS3GU4gYffVco4WXKlvxg3W4Z0wSXSDKz9w3hOfjSOE46PC8yGg3egkR7f2i5q7rYRcwv6VQ7-nefonQJopei6b_3IeZi_3BMM07BTCsnFBuRqYve9dUZxnmRv2O_3r3kPsyvDG9Ugo_uaZKxmoP-baEwy6GMGtMDZll5UaLWL3m2dB3WHw==",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHvKZheBN5PQZ9Vb0SW3kX5k3tvaNwlMC5lQ7E-zF02jrMbWcLbHDj4zEt4_HCX7RWTNYnE13J8BNrxEroL2V2k7VwovS8hXM_5vDyxoAICvjjEXHrUna-3MrwMDsI7kVNiitfImw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkcmjtxvOq6YqgRBc2geL4rlLGfIGAmUoAmjUtAb9M3W3tVHQOSROLECBw62x-u8UHq6D-U6spwHPpPg8huIzon1NAA3pSuA8HrWdUciBDhbay5v8J4Ji2sR1LM4JvZ1J90-OX7YSNaQoRs6QvzdTsKasSXcPlrGKRo8uIRHuc-xHL9-fprgznT3-CsmSHzUNloSGDQAZQmVgCHFhIurXNhDHlK_4Mmsm_Nd61hq4=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpM38SG-868XOyQkF_5_jEOiZVJn6pBwJ0ac-kNT7CftNiCi-pwQCam3_QAHIN-fRCD_TfDA7skQj_RBEiu1hFu95VAwPl7_bvl2nirRaziOHKccLZa41Wf5Y=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSjOtsVOVxpVWlR3RfT1-1KE-Yx7Rd0ID00nFFDwWbvA6d2ZtWeCwLbSjiYT3y-qyfBPAWqWQm2ppV4vNdwFK_ggj3kwH_MWboE4pdjVRKht3kpZRBC0g99D4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MQBench",
    "paperLink": "https://arxiv.org/abs/2111.03759",
    "description": "A benchmark focused on the reproducibility and deployability of model quantization algorithms. It acts as a bridge between academic quantization algorithms and real-world hardware deployments, evaluating performance across varying hardware platforms including CPUs, GPUs, ASICs, and DSPs under a unified training pipeline.",
    "authors": [
      "Yuhang Li",
      "Mingzhu Shen",
      "Jian Ma",
      "Yan Ren",
      "Mingxin Zhao",
      "Qi Zhang",
      "Ruihao Gong",
      "Fengwei Yu",
      "Junjie Yan"
    ],
    "githubLink": "https://github.com/ModelTC/MQBench",
    "itemCount": "Evaluates multiple state-of-the-art algorithms on 4+ hardware platforms",
    "source": "arXiv",
    "specs": "Unified training pipeline for quantization; Hardware-aware evaluation (CPU, GPU, ASIC, DSP); Focuses on deployability and reproducibility.",
    "year": "2021",
    "id": "saved-1769639451908-zr32b",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzgxtCd21rqhJ2Pg0UD-DYScCQ2kWSM-PNJnnlkMVCmRHWN9YQM4g2n3hM7hwIp2gPrG7Q-6jYfIR01L3z7x0rD2-LcrTt7zPmLCKvbS1L-usDL4Mb5Cz8D4OgZMwPYlQ=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn35F96FVWbq_Xzu2k_UiuzGMUcvHOzIrSWn6k4n0YK1QYVyXJoFdQgW0Ks0Xn89TQcyMCb2kdT1yYcQ43CkGoS3GU4gYffVco4WXKlvxg3W4Z0wSXSDKz9w3hOfjSOE46PC8yGg3egkR7f2i5q7rYRcwv6VQ7-nefonQJopei6b_3IeZi_3BMM07BTCsnFBuRqYve9dUZxnmRv2O_3r3kPsyvDG9Ugo_uaZKxmoP-baEwy6GMGtMDZll5UaLWL3m2dB3WHw==",
        "title": "medium.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHvKZheBN5PQZ9Vb0SW3kX5k3tvaNwlMC5lQ7E-zF02jrMbWcLbHDj4zEt4_HCX7RWTNYnE13J8BNrxEroL2V2k7VwovS8hXM_5vDyxoAICvjjEXHrUna-3MrwMDsI7kVNiitfImw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkcmjtxvOq6YqgRBc2geL4rlLGfIGAmUoAmjUtAb9M3W3tVHQOSROLECBw62x-u8UHq6D-U6spwHPpPg8huIzon1NAA3pSuA8HrWdUciBDhbay5v8J4Ji2sR1LM4JvZ1J90-OX7YSNaQoRs6QvzdTsKasSXcPlrGKRo8uIRHuc-xHL9-fprgznT3-CsmSHzUNloSGDQAZQmVgCHFhIurXNhDHlK_4Mmsm_Nd61hq4=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFpM38SG-868XOyQkF_5_jEOiZVJn6pBwJ0ac-kNT7CftNiCi-pwQCam3_QAHIN-fRCD_TfDA7skQj_RBEiu1hFu95VAwPl7_bvl2nirRaziOHKccLZa41Wf5Y=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSjOtsVOVxpVWlR3RfT1-1KE-Yx7Rd0ID00nFFDwWbvA6d2ZtWeCwLbSjiYT3y-qyfBPAWqWQm2ppV4vNdwFK_ggj3kwH_MWboE4pdjVRKht3kpZRBC0g99D4=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "LUMA",
    "paperLink": "https://arxiv.org/abs/2406.09864",
    "description": "A benchmark dataset designed for learning from uncertain and multimodal data. It extends CIFAR-10/100 with audio and text modalities to enable controlled injection of uncertainty types and degrees.",
    "authors": [
      "Grigor Bezirganyan",
      "Sana Sellami",
      "Laure Berti-Équille",
      "Sébastien Fournier"
    ],
    "githubLink": "https://huggingface.co/datasets/LUMA-Booster/LUMA",
    "itemCount": "50 classes (Extends CIFAR size)",
    "source": "Hugging Face",
    "specs": "Multimodal: Image, Audio, Text",
    "year": "2024",
    "id": "saved-1769639489520-zm0hn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHme1xlRMo2uc9_o7drwMvCk6eSEptPiKc1M9Nu4TCd26aC7Jhhj6VnglQCPRHQy248ElPqk2bDuuF2gEKfliboY-NhFz3cOPvjZrVlK5wGAUHbdt8=",
        "title": "shifts.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEChhHkmkdo0SAomiz6uek34xJ3piT_DdChJlwTxEC32Qt4F9irx6F2lf396NfbW_nqI1AckErwN6gH3qT8P5h5SQfOgPaBafmjtepeDTHt3_mNJ2Q5EOZ2lro38lPz",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFi-hA4KoB7BZkxL5JjEzKih8e7aO98mzxkGJ8r3NcRxOUObrgSH2EMg9zv9iV5kS4_eK_bBO8zHQuFA5YmSk9pBOdt-XYPM8ZIajVw7c96lcuEleqrdeYc4QaMgzhG5L6QXfQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwslPVha0hFf7zc_7VSESutr-oVXJKE2CZfr4r_GP5y6EZipxInuZCKOp98to5PfeyrpBcvrpPBlz_8ugqbOrhEuRn5IqGenzq3vJitu8keMv1_HiBqLr8-I18UaNICJNwU-V5N67vLcXTZfrfnjWR4UgloLPf8kYyddtENwKcbKAT7jm9zUnWZRKRYxGo",
        "title": "ucl.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFYHeF7uII4CvAKQeNHlwcVBZ-_DnWXVHMNQ2x5UMjW5G1ZvYSZ96HEh9ONX2bKNrXRDmAAmlPqFLHQogeGWap1DSmaRBh_-4lKa8_qhjeMHmk5j3TehAd8B4XN_w1DEBhir0QmFBPVWLKQbEU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDgb22NmevxPVBSwvQam6WbHaSgS2iQLtf-Bfu4Pk0A5pCXieb0Xyxcj7MYJQjzHFQ0hZfapQQ9SMrMd0axuOSEHN3rT4v2yi_oh3eS9qvTNfWVmvXwqN0LH8b",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcsTCVcSg5I80yR4zTeBM1fouRPEAmopJ7f02uuji0cZRcMYceMBDyaMuMKSMSgnuiaXXPO10HqfUgNV7E8VD3rXfCGNk-O0ajxdEElEnYR6Cs1QmqXEv3saKy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgfZwF_tHty__oOcibpEOEjjT9BRLbBk6iSzjK_FUPe0TVQMsfMPnMz_McB0gaXY-maHYaGNwhOy3IG-mMhrF-s_JwIb0NVhlKCBg_lgoKxgNTGgOx5MUq3hDg9Mz4IoAYrGxud3SWAAmK1T7GQk1iZ15JKCZqfVlI29MWY2yjAuB0S6LkKc41F-s1dDPuq4K-PwjwTwsmfrQyCUm_FyDSFDFVuJkgEo6jkG75H-9KsyM8it84A-A7",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMV5m0Q7A-x6DVO4LFvvebkDZP8ga2RvfWi4C_cRG5nRStBPuD6io28wmqTp7yRZCQdBmHCMJOFXraxNEjZTcrcOIpKKYcsWa2-zRkBtCRh5m1SYLbh411bGFMlLHMecxpoQ==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFBFJ6qQl75a2Bx6hacBGhzBiebb3UZaTiPdLWFIEHtOU9W1tQBCzy1r4SO8VeRd7fTnm0EGEKsRSmRdoljmk0VKLoyZPYGOIzEqoMvppM4-3WS6pEexvOok46LJ1o0jyGoK9E=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "LLM-Uncertainty-Bench",
    "paperLink": "https://arxiv.org/abs/2401.12794",
    "description": "A benchmarking approach for Large Language Models that integrates uncertainty quantification. It evaluates LLMs across representative NLP tasks like Question Answering and Summarization with a focus on prediction certainty.",
    "authors": [
      "Diao Zhu",
      "Yiwen Guo",
      "Hao Wang",
      "et al."
    ],
    "githubLink": "https://github.com/smartyfh/LLM-Uncertainty-Bench",
    "itemCount": "10,000 instances per task (5 tasks)",
    "source": "arXiv",
    "specs": "Text (Question Answering, Summarization, etc.)",
    "year": "2024",
    "id": "saved-1769639489520-85ydq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHme1xlRMo2uc9_o7drwMvCk6eSEptPiKc1M9Nu4TCd26aC7Jhhj6VnglQCPRHQy248ElPqk2bDuuF2gEKfliboY-NhFz3cOPvjZrVlK5wGAUHbdt8=",
        "title": "shifts.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEChhHkmkdo0SAomiz6uek34xJ3piT_DdChJlwTxEC32Qt4F9irx6F2lf396NfbW_nqI1AckErwN6gH3qT8P5h5SQfOgPaBafmjtepeDTHt3_mNJ2Q5EOZ2lro38lPz",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFi-hA4KoB7BZkxL5JjEzKih8e7aO98mzxkGJ8r3NcRxOUObrgSH2EMg9zv9iV5kS4_eK_bBO8zHQuFA5YmSk9pBOdt-XYPM8ZIajVw7c96lcuEleqrdeYc4QaMgzhG5L6QXfQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwslPVha0hFf7zc_7VSESutr-oVXJKE2CZfr4r_GP5y6EZipxInuZCKOp98to5PfeyrpBcvrpPBlz_8ugqbOrhEuRn5IqGenzq3vJitu8keMv1_HiBqLr8-I18UaNICJNwU-V5N67vLcXTZfrfnjWR4UgloLPf8kYyddtENwKcbKAT7jm9zUnWZRKRYxGo",
        "title": "ucl.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFYHeF7uII4CvAKQeNHlwcVBZ-_DnWXVHMNQ2x5UMjW5G1ZvYSZ96HEh9ONX2bKNrXRDmAAmlPqFLHQogeGWap1DSmaRBh_-4lKa8_qhjeMHmk5j3TehAd8B4XN_w1DEBhir0QmFBPVWLKQbEU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDgb22NmevxPVBSwvQam6WbHaSgS2iQLtf-Bfu4Pk0A5pCXieb0Xyxcj7MYJQjzHFQ0hZfapQQ9SMrMd0axuOSEHN3rT4v2yi_oh3eS9qvTNfWVmvXwqN0LH8b",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcsTCVcSg5I80yR4zTeBM1fouRPEAmopJ7f02uuji0cZRcMYceMBDyaMuMKSMSgnuiaXXPO10HqfUgNV7E8VD3rXfCGNk-O0ajxdEElEnYR6Cs1QmqXEv3saKy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgfZwF_tHty__oOcibpEOEjjT9BRLbBk6iSzjK_FUPe0TVQMsfMPnMz_McB0gaXY-maHYaGNwhOy3IG-mMhrF-s_JwIb0NVhlKCBg_lgoKxgNTGgOx5MUq3hDg9Mz4IoAYrGxud3SWAAmK1T7GQk1iZ15JKCZqfVlI29MWY2yjAuB0S6LkKc41F-s1dDPuq4K-PwjwTwsmfrQyCUm_FyDSFDFVuJkgEo6jkG75H-9KsyM8it84A-A7",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMV5m0Q7A-x6DVO4LFvvebkDZP8ga2RvfWi4C_cRG5nRStBPuD6io28wmqTp7yRZCQdBmHCMJOFXraxNEjZTcrcOIpKKYcsWa2-zRkBtCRh5m1SYLbh411bGFMlLHMecxpoQ==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFBFJ6qQl75a2Bx6hacBGhzBiebb3UZaTiPdLWFIEHtOU9W1tQBCzy1r4SO8VeRd7fTnm0EGEKsRSmRdoljmk0VKLoyZPYGOIzEqoMvppM4-3WS6pEexvOok46LJ1o0jyGoK9E=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "EO Uncertainty Benchmarks",
    "paperLink": "https://arxiv.org/abs/2412.06451",
    "description": "Three novel Earth Observation datasets specifically designed for benchmarking uncertainty quantification in machine learning. It addresses regression, image segmentation, and scene classification with reference uncertainty.",
    "authors": [
      "Author list not fully indexed yet (arXiv:2412.06451)"
    ],
    "githubLink": "N/A",
    "itemCount": "3 distinct datasets",
    "source": "arXiv",
    "specs": "Satellite/Aerial Imagery (Regression, Segmentation, Classification)",
    "year": "2024",
    "id": "saved-1769639489520-ft2zz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHme1xlRMo2uc9_o7drwMvCk6eSEptPiKc1M9Nu4TCd26aC7Jhhj6VnglQCPRHQy248ElPqk2bDuuF2gEKfliboY-NhFz3cOPvjZrVlK5wGAUHbdt8=",
        "title": "shifts.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEChhHkmkdo0SAomiz6uek34xJ3piT_DdChJlwTxEC32Qt4F9irx6F2lf396NfbW_nqI1AckErwN6gH3qT8P5h5SQfOgPaBafmjtepeDTHt3_mNJ2Q5EOZ2lro38lPz",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFi-hA4KoB7BZkxL5JjEzKih8e7aO98mzxkGJ8r3NcRxOUObrgSH2EMg9zv9iV5kS4_eK_bBO8zHQuFA5YmSk9pBOdt-XYPM8ZIajVw7c96lcuEleqrdeYc4QaMgzhG5L6QXfQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwslPVha0hFf7zc_7VSESutr-oVXJKE2CZfr4r_GP5y6EZipxInuZCKOp98to5PfeyrpBcvrpPBlz_8ugqbOrhEuRn5IqGenzq3vJitu8keMv1_HiBqLr8-I18UaNICJNwU-V5N67vLcXTZfrfnjWR4UgloLPf8kYyddtENwKcbKAT7jm9zUnWZRKRYxGo",
        "title": "ucl.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFYHeF7uII4CvAKQeNHlwcVBZ-_DnWXVHMNQ2x5UMjW5G1ZvYSZ96HEh9ONX2bKNrXRDmAAmlPqFLHQogeGWap1DSmaRBh_-4lKa8_qhjeMHmk5j3TehAd8B4XN_w1DEBhir0QmFBPVWLKQbEU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDgb22NmevxPVBSwvQam6WbHaSgS2iQLtf-Bfu4Pk0A5pCXieb0Xyxcj7MYJQjzHFQ0hZfapQQ9SMrMd0axuOSEHN3rT4v2yi_oh3eS9qvTNfWVmvXwqN0LH8b",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcsTCVcSg5I80yR4zTeBM1fouRPEAmopJ7f02uuji0cZRcMYceMBDyaMuMKSMSgnuiaXXPO10HqfUgNV7E8VD3rXfCGNk-O0ajxdEElEnYR6Cs1QmqXEv3saKy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgfZwF_tHty__oOcibpEOEjjT9BRLbBk6iSzjK_FUPe0TVQMsfMPnMz_McB0gaXY-maHYaGNwhOy3IG-mMhrF-s_JwIb0NVhlKCBg_lgoKxgNTGgOx5MUq3hDg9Mz4IoAYrGxud3SWAAmK1T7GQk1iZ15JKCZqfVlI29MWY2yjAuB0S6LkKc41F-s1dDPuq4K-PwjwTwsmfrQyCUm_FyDSFDFVuJkgEo6jkG75H-9KsyM8it84A-A7",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMV5m0Q7A-x6DVO4LFvvebkDZP8ga2RvfWi4C_cRG5nRStBPuD6io28wmqTp7yRZCQdBmHCMJOFXraxNEjZTcrcOIpKKYcsWa2-zRkBtCRh5m1SYLbh411bGFMlLHMecxpoQ==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFBFJ6qQl75a2Bx6hacBGhzBiebb3UZaTiPdLWFIEHtOU9W1tQBCzy1r4SO8VeRd7fTnm0EGEKsRSmRdoljmk0VKLoyZPYGOIzEqoMvppM4-3WS6pEexvOok46LJ1o0jyGoK9E=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "MUBen",
    "paperLink": "https://arxiv.org/abs/2306.10060",
    "description": "A benchmark aiming to investigate the performance of uncertainty quantification (UQ) methods built upon backbone molecular representation models. It evaluates combinations of backbone models and UQ methods on molecular properties.",
    "authors": [
      "Yinghao Li",
      "Lingkai Kong",
      "Yuanqi Du",
      "Yue Yu",
      "Yuchen Zhuang",
      "Chao Zhang"
    ],
    "githubLink": "https://github.com/VirtuosoResearch/MUBen",
    "itemCount": "14 datasets",
    "source": "arXiv",
    "specs": "Molecular graphs, SMILES strings (Classification and Regression tasks)",
    "year": "2023",
    "id": "saved-1769639489520-4ki5j",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHme1xlRMo2uc9_o7drwMvCk6eSEptPiKc1M9Nu4TCd26aC7Jhhj6VnglQCPRHQy248ElPqk2bDuuF2gEKfliboY-NhFz3cOPvjZrVlK5wGAUHbdt8=",
        "title": "shifts.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEChhHkmkdo0SAomiz6uek34xJ3piT_DdChJlwTxEC32Qt4F9irx6F2lf396NfbW_nqI1AckErwN6gH3qT8P5h5SQfOgPaBafmjtepeDTHt3_mNJ2Q5EOZ2lro38lPz",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFi-hA4KoB7BZkxL5JjEzKih8e7aO98mzxkGJ8r3NcRxOUObrgSH2EMg9zv9iV5kS4_eK_bBO8zHQuFA5YmSk9pBOdt-XYPM8ZIajVw7c96lcuEleqrdeYc4QaMgzhG5L6QXfQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwslPVha0hFf7zc_7VSESutr-oVXJKE2CZfr4r_GP5y6EZipxInuZCKOp98to5PfeyrpBcvrpPBlz_8ugqbOrhEuRn5IqGenzq3vJitu8keMv1_HiBqLr8-I18UaNICJNwU-V5N67vLcXTZfrfnjWR4UgloLPf8kYyddtENwKcbKAT7jm9zUnWZRKRYxGo",
        "title": "ucl.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFYHeF7uII4CvAKQeNHlwcVBZ-_DnWXVHMNQ2x5UMjW5G1ZvYSZ96HEh9ONX2bKNrXRDmAAmlPqFLHQogeGWap1DSmaRBh_-4lKa8_qhjeMHmk5j3TehAd8B4XN_w1DEBhir0QmFBPVWLKQbEU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDgb22NmevxPVBSwvQam6WbHaSgS2iQLtf-Bfu4Pk0A5pCXieb0Xyxcj7MYJQjzHFQ0hZfapQQ9SMrMd0axuOSEHN3rT4v2yi_oh3eS9qvTNfWVmvXwqN0LH8b",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcsTCVcSg5I80yR4zTeBM1fouRPEAmopJ7f02uuji0cZRcMYceMBDyaMuMKSMSgnuiaXXPO10HqfUgNV7E8VD3rXfCGNk-O0ajxdEElEnYR6Cs1QmqXEv3saKy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgfZwF_tHty__oOcibpEOEjjT9BRLbBk6iSzjK_FUPe0TVQMsfMPnMz_McB0gaXY-maHYaGNwhOy3IG-mMhrF-s_JwIb0NVhlKCBg_lgoKxgNTGgOx5MUq3hDg9Mz4IoAYrGxud3SWAAmK1T7GQk1iZ15JKCZqfVlI29MWY2yjAuB0S6LkKc41F-s1dDPuq4K-PwjwTwsmfrQyCUm_FyDSFDFVuJkgEo6jkG75H-9KsyM8it84A-A7",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMV5m0Q7A-x6DVO4LFvvebkDZP8ga2RvfWi4C_cRG5nRStBPuD6io28wmqTp7yRZCQdBmHCMJOFXraxNEjZTcrcOIpKKYcsWa2-zRkBtCRh5m1SYLbh411bGFMlLHMecxpoQ==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFBFJ6qQl75a2Bx6hacBGhzBiebb3UZaTiPdLWFIEHtOU9W1tQBCzy1r4SO8VeRd7fTnm0EGEKsRSmRdoljmk0VKLoyZPYGOIzEqoMvppM4-3WS6pEexvOok46LJ1o0jyGoK9E=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Shifts Dataset",
    "paperLink": "https://arxiv.org/abs/2107.07455",
    "description": "A dataset of real distributional shifts across multiple large-scale tasks to benchmark robustness and uncertainty quantification. It includes modalities such as weather prediction, machine translation, and vehicle motion prediction.",
    "authors": [
      "Andrey Malinin",
      "Neil Band",
      "Yarin Gal",
      "Mark Gales",
      "Alexander Ganshin",
      "et al."
    ],
    "githubLink": "https://github.com/Shifts-Project/shifts",
    "itemCount": "Multiple datasets (e.g., 600k+ vehicle scenes, large-scale weather data)",
    "source": "arXiv",
    "specs": "Tabular (Weather), Text (Translation), Audio/Features (Vehicle Motion), 3D MRI (Lesion Segmentation)",
    "year": "2021",
    "id": "saved-1769639489521-7ifql",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHme1xlRMo2uc9_o7drwMvCk6eSEptPiKc1M9Nu4TCd26aC7Jhhj6VnglQCPRHQy248ElPqk2bDuuF2gEKfliboY-NhFz3cOPvjZrVlK5wGAUHbdt8=",
        "title": "shifts.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEChhHkmkdo0SAomiz6uek34xJ3piT_DdChJlwTxEC32Qt4F9irx6F2lf396NfbW_nqI1AckErwN6gH3qT8P5h5SQfOgPaBafmjtepeDTHt3_mNJ2Q5EOZ2lro38lPz",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFi-hA4KoB7BZkxL5JjEzKih8e7aO98mzxkGJ8r3NcRxOUObrgSH2EMg9zv9iV5kS4_eK_bBO8zHQuFA5YmSk9pBOdt-XYPM8ZIajVw7c96lcuEleqrdeYc4QaMgzhG5L6QXfQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwslPVha0hFf7zc_7VSESutr-oVXJKE2CZfr4r_GP5y6EZipxInuZCKOp98to5PfeyrpBcvrpPBlz_8ugqbOrhEuRn5IqGenzq3vJitu8keMv1_HiBqLr8-I18UaNICJNwU-V5N67vLcXTZfrfnjWR4UgloLPf8kYyddtENwKcbKAT7jm9zUnWZRKRYxGo",
        "title": "ucl.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFYHeF7uII4CvAKQeNHlwcVBZ-_DnWXVHMNQ2x5UMjW5G1ZvYSZ96HEh9ONX2bKNrXRDmAAmlPqFLHQogeGWap1DSmaRBh_-4lKa8_qhjeMHmk5j3TehAd8B4XN_w1DEBhir0QmFBPVWLKQbEU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDgb22NmevxPVBSwvQam6WbHaSgS2iQLtf-Bfu4Pk0A5pCXieb0Xyxcj7MYJQjzHFQ0hZfapQQ9SMrMd0axuOSEHN3rT4v2yi_oh3eS9qvTNfWVmvXwqN0LH8b",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcsTCVcSg5I80yR4zTeBM1fouRPEAmopJ7f02uuji0cZRcMYceMBDyaMuMKSMSgnuiaXXPO10HqfUgNV7E8VD3rXfCGNk-O0ajxdEElEnYR6Cs1QmqXEv3saKy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgfZwF_tHty__oOcibpEOEjjT9BRLbBk6iSzjK_FUPe0TVQMsfMPnMz_McB0gaXY-maHYaGNwhOy3IG-mMhrF-s_JwIb0NVhlKCBg_lgoKxgNTGgOx5MUq3hDg9Mz4IoAYrGxud3SWAAmK1T7GQk1iZ15JKCZqfVlI29MWY2yjAuB0S6LkKc41F-s1dDPuq4K-PwjwTwsmfrQyCUm_FyDSFDFVuJkgEo6jkG75H-9KsyM8it84A-A7",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMV5m0Q7A-x6DVO4LFvvebkDZP8ga2RvfWi4C_cRG5nRStBPuD6io28wmqTp7yRZCQdBmHCMJOFXraxNEjZTcrcOIpKKYcsWa2-zRkBtCRh5m1SYLbh411bGFMlLHMecxpoQ==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFBFJ6qQl75a2Bx6hacBGhzBiebb3UZaTiPdLWFIEHtOU9W1tQBCzy1r4SO8VeRd7fTnm0EGEKsRSmRdoljmk0VKLoyZPYGOIzEqoMvppM4-3WS6pEexvOok46LJ1o0jyGoK9E=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Uncertainty Baselines",
    "paperLink": "https://arxiv.org/abs/2106.04015",
    "description": "A collection of high-quality implementations of standard and state-of-the-art deep learning methods for uncertainty and robustness. It provides a shared template for benchmarking across standard tasks like ImageNet and CIFAR.",
    "authors": [
      "Zachary Nado",
      "Neil Band",
      "Mark Collier",
      "Josip Djolonga",
      "Michael Dusenberry",
      "et al."
    ],
    "githubLink": "https://github.com/google/uncertainty-baselines",
    "itemCount": "9 tasks (spanning 19 methods)",
    "source": "arXiv",
    "specs": "Images (ImageNet, CIFAR), Text (Glue), various standard formats",
    "year": "2021",
    "id": "saved-1769639489521-ov6am",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHme1xlRMo2uc9_o7drwMvCk6eSEptPiKc1M9Nu4TCd26aC7Jhhj6VnglQCPRHQy248ElPqk2bDuuF2gEKfliboY-NhFz3cOPvjZrVlK5wGAUHbdt8=",
        "title": "shifts.ai"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEChhHkmkdo0SAomiz6uek34xJ3piT_DdChJlwTxEC32Qt4F9irx6F2lf396NfbW_nqI1AckErwN6gH3qT8P5h5SQfOgPaBafmjtepeDTHt3_mNJ2Q5EOZ2lro38lPz",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFi-hA4KoB7BZkxL5JjEzKih8e7aO98mzxkGJ8r3NcRxOUObrgSH2EMg9zv9iV5kS4_eK_bBO8zHQuFA5YmSk9pBOdt-XYPM8ZIajVw7c96lcuEleqrdeYc4QaMgzhG5L6QXfQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwslPVha0hFf7zc_7VSESutr-oVXJKE2CZfr4r_GP5y6EZipxInuZCKOp98to5PfeyrpBcvrpPBlz_8ugqbOrhEuRn5IqGenzq3vJitu8keMv1_HiBqLr8-I18UaNICJNwU-V5N67vLcXTZfrfnjWR4UgloLPf8kYyddtENwKcbKAT7jm9zUnWZRKRYxGo",
        "title": "ucl.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFYHeF7uII4CvAKQeNHlwcVBZ-_DnWXVHMNQ2x5UMjW5G1ZvYSZ96HEh9ONX2bKNrXRDmAAmlPqFLHQogeGWap1DSmaRBh_-4lKa8_qhjeMHmk5j3TehAd8B4XN_w1DEBhir0QmFBPVWLKQbEU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDgb22NmevxPVBSwvQam6WbHaSgS2iQLtf-Bfu4Pk0A5pCXieb0Xyxcj7MYJQjzHFQ0hZfapQQ9SMrMd0axuOSEHN3rT4v2yi_oh3eS9qvTNfWVmvXwqN0LH8b",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcsTCVcSg5I80yR4zTeBM1fouRPEAmopJ7f02uuji0cZRcMYceMBDyaMuMKSMSgnuiaXXPO10HqfUgNV7E8VD3rXfCGNk-O0ajxdEElEnYR6Cs1QmqXEv3saKy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgfZwF_tHty__oOcibpEOEjjT9BRLbBk6iSzjK_FUPe0TVQMsfMPnMz_McB0gaXY-maHYaGNwhOy3IG-mMhrF-s_JwIb0NVhlKCBg_lgoKxgNTGgOx5MUq3hDg9Mz4IoAYrGxud3SWAAmK1T7GQk1iZ15JKCZqfVlI29MWY2yjAuB0S6LkKc41F-s1dDPuq4K-PwjwTwsmfrQyCUm_FyDSFDFVuJkgEo6jkG75H-9KsyM8it84A-A7",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMV5m0Q7A-x6DVO4LFvvebkDZP8ga2RvfWi4C_cRG5nRStBPuD6io28wmqTp7yRZCQdBmHCMJOFXraxNEjZTcrcOIpKKYcsWa2-zRkBtCRh5m1SYLbh411bGFMlLHMecxpoQ==",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFBFJ6qQl75a2Bx6hacBGhzBiebb3UZaTiPdLWFIEHtOU9W1tQBCzy1r4SO8VeRd7fTnm0EGEKsRSmRdoljmk0VKLoyZPYGOIzEqoMvppM4-3WS6pEexvOok46LJ1o0jyGoK9E=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "CoReTab",
    "paperLink": "https://arxiv.org/abs/2601.19193",
    "description": "A code-driven reasoning framework and dataset that couples multi-step reasoning with executable Python code. It addresses the lack of explicit reasoning supervision in existing multimodal table understanding datasets.",
    "authors": [
      "Van-Quang Nguyen",
      "Takayuki Okatani"
    ],
    "githubLink": "https://arxiv.org/abs/2601.19193",
    "itemCount": "115,000 verified samples",
    "source": "arXiv",
    "specs": "Table images, natural language questions, step-by-step reasoning traces, executable Python code.",
    "year": "2026",
    "id": "saved-1769639573348-kt89d",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHBCQdHHHr9wL4TjxLIZ1CmfCdzCDxZsmJcoKYyKWJ_SXwgKMODnLxQgeGL6dNgzGUtVTdPwFXSeNjkSc4IEpHEeiv6T7aUTNHvj-OdqNnPekSaWj3sst6o_uFyQ4s",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgLgudSlaJvxSAQD90bviftQjJAFfjuxEjJUOtjXUiALEUOBXi1drUXNo68VBAcMIM870BpoYVg2TQy1es1iRMHLAuN-CQBNnzDOaj0bjvf4eD9cQJ5f4Q2rgA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxUo4Ogdp_p9XW8tNHJvv9K0Use_smRsO-Jsg7afSJpZW6nGGLMrCelKkxBrYweeDWrMut024DwaS3oAptNbLz7qij94uuCXJNGm6dGDL5Lcem4wkVktm9CE4=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRSvzB056DrRavaogRZ8-sT7RYG-Ua_-L2XvBNXn7mCwD8HYqW-IBe0B8MI8TAIVJd6xgY8FX59sZS453lT1MdY07oYlaPoW0ejz_W899yNXUipvhiDsMYoI49IenP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtJdgN4EKohmZRZSkvOMXht3okA32Ptad09qaaynntfItagUrvMZWOBRZpiFaezkaVGysO9cWE2aOKT8gjITOwtnGxFQgaZU9O8ib9QGWQZ2wYHohyzeUUnkB_kAw_zMQ5UxKQn8Ennq1hzJviGjtmzso=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "SynTab",
    "paperLink": "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_SynTab-LLaVA_Enhancing_Multimodal_Table_Understanding_with_Decoupled_Synthesis_CVPR_2025_paper.html",
    "description": "A large-scale synthetic dataset for multimodal table understanding, created using a decoupled synthesis framework that separates table image rendering from Q&A generation. It aims to overcome data scarcity and reduce hallucinations in multimodal models.",
    "authors": [
      "Bangbang Zhou",
      "Zuan Gao",
      "Zixiao Wang",
      "Boqiang Zhang",
      "Yuxin Wang",
      "Zhineng Chen",
      "Hongtao Xie"
    ],
    "githubLink": "https://github.com/bang123-box/SynTab-LLaVA",
    "itemCount": "636,000 images; 1.8 million samples",
    "source": "Scholar",
    "specs": "Synthetic table images (up to 1536x1536 resolution), diverse visual styles, and generated Q&A pairs.",
    "year": "2025",
    "id": "saved-1769639573348-ssaak",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHBCQdHHHr9wL4TjxLIZ1CmfCdzCDxZsmJcoKYyKWJ_SXwgKMODnLxQgeGL6dNgzGUtVTdPwFXSeNjkSc4IEpHEeiv6T7aUTNHvj-OdqNnPekSaWj3sst6o_uFyQ4s",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgLgudSlaJvxSAQD90bviftQjJAFfjuxEjJUOtjXUiALEUOBXi1drUXNo68VBAcMIM870BpoYVg2TQy1es1iRMHLAuN-CQBNnzDOaj0bjvf4eD9cQJ5f4Q2rgA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxUo4Ogdp_p9XW8tNHJvv9K0Use_smRsO-Jsg7afSJpZW6nGGLMrCelKkxBrYweeDWrMut024DwaS3oAptNbLz7qij94uuCXJNGm6dGDL5Lcem4wkVktm9CE4=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRSvzB056DrRavaogRZ8-sT7RYG-Ua_-L2XvBNXn7mCwD8HYqW-IBe0B8MI8TAIVJd6xgY8FX59sZS453lT1MdY07oYlaPoW0ejz_W899yNXUipvhiDsMYoI49IenP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtJdgN4EKohmZRZSkvOMXht3okA32Ptad09qaaynntfItagUrvMZWOBRZpiFaezkaVGysO9cWE2aOKT8gjITOwtnGxFQgaZU9O8ib9QGWQZ2wYHohyzeUUnkB_kAw_zMQ5UxKQn8Ennq1hzJviGjtmzso=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "MTabVQA",
    "paperLink": "https://aclanthology.org/2025.findings-emnlp.1083/",
    "description": "A benchmark specifically designed for multi-tabular visual question answering, requiring models to perform multi-hop reasoning across multiple visually rendered table images. It includes a benchmark split and a large-scale instruction-tuning dataset.",
    "authors": [
      "Anshul Singh",
      "Chris Biemann",
      "Jan Strich"
    ],
    "githubLink": "https://anshulsc.github.io/MTabVQA/",
    "itemCount": "3,745 evaluation QA pairs; 15,853 instruction-tuning samples",
    "source": "Hugging Face",
    "specs": "Visual table images, complex multi-hop reasoning questions, multiple tables per query.",
    "year": "2025",
    "id": "saved-1769639573348-a7xof",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHBCQdHHHr9wL4TjxLIZ1CmfCdzCDxZsmJcoKYyKWJ_SXwgKMODnLxQgeGL6dNgzGUtVTdPwFXSeNjkSc4IEpHEeiv6T7aUTNHvj-OdqNnPekSaWj3sst6o_uFyQ4s",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgLgudSlaJvxSAQD90bviftQjJAFfjuxEjJUOtjXUiALEUOBXi1drUXNo68VBAcMIM870BpoYVg2TQy1es1iRMHLAuN-CQBNnzDOaj0bjvf4eD9cQJ5f4Q2rgA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxUo4Ogdp_p9XW8tNHJvv9K0Use_smRsO-Jsg7afSJpZW6nGGLMrCelKkxBrYweeDWrMut024DwaS3oAptNbLz7qij94uuCXJNGm6dGDL5Lcem4wkVktm9CE4=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRSvzB056DrRavaogRZ8-sT7RYG-Ua_-L2XvBNXn7mCwD8HYqW-IBe0B8MI8TAIVJd6xgY8FX59sZS453lT1MdY07oYlaPoW0ejz_W899yNXUipvhiDsMYoI49IenP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtJdgN4EKohmZRZSkvOMXht3okA32Ptad09qaaynntfItagUrvMZWOBRZpiFaezkaVGysO9cWE2aOKT8gjITOwtnGxFQgaZU9O8ib9QGWQZ2wYHohyzeUUnkB_kAw_zMQ5UxKQn8Ennq1hzJviGjtmzso=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "TableEval",
    "paperLink": "https://arxiv.org/abs/2506.03949",
    "description": "A real-world benchmark for evaluating Large Language Models (LLMs) on complex, multilingual, and multi-structured table question answering. It features tables from diverse domains like finance and academia, supporting Simplified Chinese, Traditional Chinese, and English.",
    "authors": [
      "Junnan Zhu",
      "Jingyi Wang",
      "Bohan Yu",
      "Xiaoyu Wu",
      "Junbo Li",
      "Lei Wang",
      "Nan Xu"
    ],
    "githubLink": "https://github.com/wenge-research/TableEval",
    "itemCount": "2,325 QA pairs; 617 Excel spreadsheets",
    "source": "Hugging Face",
    "specs": "Multilingual (English, Chinese), diverse structures (hierarchical, nested), multiple file formats (images, HTML, LaTeX).",
    "year": "2025",
    "id": "saved-1769639573349-ufi67",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHBCQdHHHr9wL4TjxLIZ1CmfCdzCDxZsmJcoKYyKWJ_SXwgKMODnLxQgeGL6dNgzGUtVTdPwFXSeNjkSc4IEpHEeiv6T7aUTNHvj-OdqNnPekSaWj3sst6o_uFyQ4s",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgLgudSlaJvxSAQD90bviftQjJAFfjuxEjJUOtjXUiALEUOBXi1drUXNo68VBAcMIM870BpoYVg2TQy1es1iRMHLAuN-CQBNnzDOaj0bjvf4eD9cQJ5f4Q2rgA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxUo4Ogdp_p9XW8tNHJvv9K0Use_smRsO-Jsg7afSJpZW6nGGLMrCelKkxBrYweeDWrMut024DwaS3oAptNbLz7qij94uuCXJNGm6dGDL5Lcem4wkVktm9CE4=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRSvzB056DrRavaogRZ8-sT7RYG-Ua_-L2XvBNXn7mCwD8HYqW-IBe0B8MI8TAIVJd6xgY8FX59sZS453lT1MdY07oYlaPoW0ejz_W899yNXUipvhiDsMYoI49IenP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtJdgN4EKohmZRZSkvOMXht3okA32Ptad09qaaynntfItagUrvMZWOBRZpiFaezkaVGysO9cWE2aOKT8gjITOwtnGxFQgaZU9O8ib9QGWQZ2wYHohyzeUUnkB_kAw_zMQ5UxKQn8Ennq1hzJviGjtmzso=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "MMTab",
    "paperLink": "https://arxiv.org/abs/2406.08100",
    "description": "A large-scale multimodal table understanding dataset covering diverse table images, instructions, and tasks. It is designed to enable AI systems to directly understand and reason about tables in their visual form, supporting tasks like table recognition, question answering, and fact verification.",
    "authors": [
      "Mingyu Zheng",
      "Xinwei Feng",
      "Qingyi Si",
      "Qiaoqiao She",
      "Zheng Lin",
      "Wenbin Jiang",
      "Weiping Wang"
    ],
    "githubLink": "https://github.com/SpursGoZmy/Table-LLaVA",
    "itemCount": "431,000+ samples (150K pre-training, 232K instruct, 49K eval)",
    "source": "arXiv",
    "specs": "Unified format <table image, input request, output response>; Covers 14 table-based tasks including recognition, QA, and fact verification.",
    "year": "2024",
    "id": "saved-1769639573349-uv43t",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHBCQdHHHr9wL4TjxLIZ1CmfCdzCDxZsmJcoKYyKWJ_SXwgKMODnLxQgeGL6dNgzGUtVTdPwFXSeNjkSc4IEpHEeiv6T7aUTNHvj-OdqNnPekSaWj3sst6o_uFyQ4s",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgLgudSlaJvxSAQD90bviftQjJAFfjuxEjJUOtjXUiALEUOBXi1drUXNo68VBAcMIM870BpoYVg2TQy1es1iRMHLAuN-CQBNnzDOaj0bjvf4eD9cQJ5f4Q2rgA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxUo4Ogdp_p9XW8tNHJvv9K0Use_smRsO-Jsg7afSJpZW6nGGLMrCelKkxBrYweeDWrMut024DwaS3oAptNbLz7qij94uuCXJNGm6dGDL5Lcem4wkVktm9CE4=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRSvzB056DrRavaogRZ8-sT7RYG-Ua_-L2XvBNXn7mCwD8HYqW-IBe0B8MI8TAIVJd6xgY8FX59sZS453lT1MdY07oYlaPoW0ejz_W899yNXUipvhiDsMYoI49IenP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtJdgN4EKohmZRZSkvOMXht3okA32Ptad09qaaynntfItagUrvMZWOBRZpiFaezkaVGysO9cWE2aOKT8gjITOwtnGxFQgaZU9O8ib9QGWQZ2wYHohyzeUUnkB_kAw_zMQ5UxKQn8Ennq1hzJviGjtmzso=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "CodeReviewQA",
    "paperLink": "https://arxiv.org/abs/2503.16167",
    "description": "A novel evaluation benchmark for assessing Large Language Models' comprehension of code reviews. It decomposes code refinement into three reasoning steps: change type recognition, change localization, and solution identification, formatted as multiple-choice questions.",
    "authors": [
      "Hong Yi Lin",
      "et al."
    ],
    "githubLink": "https://huggingface.co/datasets/Tomo-Melb/CodeReviewQA",
    "itemCount": "900 manually curated examples",
    "source": "Hugging Face",
    "specs": "Multilingual (9 languages); MCQA format; High-quality manual curation",
    "year": "2025",
    "id": "saved-1769639630181-u5pkd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_reb8cWKd-nAmjuv0KOzKG27lLkuT_LfjJ0GiQhP7kKmdvUVeOU7AEYWoTtcyaqCg9fp7ZCmGRFpNPPOLQBrXzaXcXZug9M4FgGIFUSM3t1O7bhuYlKL5KO2C3yaCA4Msa5t0Hu3Ng0Ng_hB_eQ5i3WpWPRI=",
        "title": "rero.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMPXNUBKB2HxjDFOn4bFk1eXerX5wN322JVnSeAP52v1S2GhX2NcQfkxAYDNyVxMDW6fea80e54RpV-5n39U-j0WQkZyE7QDdgq5VKwbglLCdyBL9UHbsZ5heEmCSQ9ivGvmKwMLjheErCRjzRX-luIp4eTbC1LH_pVJKd0fNaZPOq6lsoDMfJ-FIyTURyA4Dop9-IHQgaKQ4GkGnLkiSJ5iF5wAw2H60=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvX20D19DyJGAtFcm8psBflmMbagSr8GsUScRAaEcu-o0nCfnsItaIldlz0cot16MVkjwXbv81x-cbgRuCdgI2NAy7RLlKjRX0NalZ4q5xY6fRTiroCc90I_zTPDHh9LV_RtsJ",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF9QmlKLqlV7roz9WYbR1kCNv2ScBQ8z7POeGHT5VM9YeTYqJvJy07c0wEnuSVREblpsdKqTJOZwzgDV-NqtZLck7k2mp4EDzcXaMQQYvERGSNRFsGv2s1zNa0=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "CodeReviewer",
    "paperLink": "https://arxiv.org/abs/2203.09095",
    "description": "A large-scale benchmark and pre-training dataset designed for automating code review activities. It supports three primary tasks: code change quality estimation, review comment generation, and code refinement. The dataset is derived from real-world open-source projects.",
    "authors": [
      "Zhiyu Li",
      "Shuai Lu",
      "Daya Guo",
      "Nan Duan",
      "et al."
    ],
    "githubLink": "https://github.com/microsoft/CodeReviewer",
    "itemCount": "Pre-training: ~11M review comments; Benchmark: ~20k-200k samples per task",
    "source": "arXiv",
    "specs": "Multilingual (9 languages including Java, C++, Python); Input: Code Diffs + Comments",
    "year": "2022",
    "id": "saved-1769639630182-epheu",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_reb8cWKd-nAmjuv0KOzKG27lLkuT_LfjJ0GiQhP7kKmdvUVeOU7AEYWoTtcyaqCg9fp7ZCmGRFpNPPOLQBrXzaXcXZug9M4FgGIFUSM3t1O7bhuYlKL5KO2C3yaCA4Msa5t0Hu3Ng0Ng_hB_eQ5i3WpWPRI=",
        "title": "rero.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMPXNUBKB2HxjDFOn4bFk1eXerX5wN322JVnSeAP52v1S2GhX2NcQfkxAYDNyVxMDW6fea80e54RpV-5n39U-j0WQkZyE7QDdgq5VKwbglLCdyBL9UHbsZ5heEmCSQ9ivGvmKwMLjheErCRjzRX-luIp4eTbC1LH_pVJKd0fNaZPOq6lsoDMfJ-FIyTURyA4Dop9-IHQgaKQ4GkGnLkiSJ5iF5wAw2H60=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvX20D19DyJGAtFcm8psBflmMbagSr8GsUScRAaEcu-o0nCfnsItaIldlz0cot16MVkjwXbv81x-cbgRuCdgI2NAy7RLlKjRX0NalZ4q5xY6fRTiroCc90I_zTPDHh9LV_RtsJ",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF9QmlKLqlV7roz9WYbR1kCNv2ScBQ8z7POeGHT5VM9YeTYqJvJy07c0wEnuSVREblpsdKqTJOZwzgDV-NqtZLck7k2mp4EDzcXaMQQYvERGSNRFsGv2s1zNa0=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Automating Code Review Activities Dataset (Tufano et al.)",
    "paperLink": "https://arxiv.org/abs/2101.02518",
    "description": "A dataset created to investigate the possibility of using Deep Learning to automate code review tasks. It contains triples of (submitted code, review comment, revised code) extracted from real code reviews in open-source Java projects.",
    "authors": [
      "Rosalia Tufano",
      "Luca Pascarella",
      "Michele Tufano",
      "Gabriele Bavota",
      "et al."
    ],
    "githubLink": "https://github.com/RosaliaTufano/code_review",
    "itemCount": "~130,000 - 178,000 samples (depending on filtering)",
    "source": "arXiv",
    "specs": "Java; Text/Code; Triples: Source Method, Target Method, Review Comment",
    "year": "2021",
    "id": "saved-1769639630182-h5dke",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_reb8cWKd-nAmjuv0KOzKG27lLkuT_LfjJ0GiQhP7kKmdvUVeOU7AEYWoTtcyaqCg9fp7ZCmGRFpNPPOLQBrXzaXcXZug9M4FgGIFUSM3t1O7bhuYlKL5KO2C3yaCA4Msa5t0Hu3Ng0Ng_hB_eQ5i3WpWPRI=",
        "title": "rero.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMPXNUBKB2HxjDFOn4bFk1eXerX5wN322JVnSeAP52v1S2GhX2NcQfkxAYDNyVxMDW6fea80e54RpV-5n39U-j0WQkZyE7QDdgq5VKwbglLCdyBL9UHbsZ5heEmCSQ9ivGvmKwMLjheErCRjzRX-luIp4eTbC1LH_pVJKd0fNaZPOq6lsoDMfJ-FIyTURyA4Dop9-IHQgaKQ4GkGnLkiSJ5iF5wAw2H60=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvX20D19DyJGAtFcm8psBflmMbagSr8GsUScRAaEcu-o0nCfnsItaIldlz0cot16MVkjwXbv81x-cbgRuCdgI2NAy7RLlKjRX0NalZ4q5xY6fRTiroCc90I_zTPDHh9LV_RtsJ",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF9QmlKLqlV7roz9WYbR1kCNv2ScBQ8z7POeGHT5VM9YeTYqJvJy07c0wEnuSVREblpsdKqTJOZwzgDV-NqtZLck7k2mp4EDzcXaMQQYvERGSNRFsGv2s1zNa0=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Review4Repair",
    "paperLink": "https://arxiv.org/abs/2010.01544",
    "description": "A benchmark dataset for Code Review Aided Automatic Program Repair. It leverages code review comments to guide the repair process of buggy code, containing code changes and their associated review comments.",
    "authors": [
      "Syed Fatiul Huq",
      "Masum Hassan",
      "Toufique Ahmed",
      "Anita Sarma",
      "et al."
    ],
    "githubLink": "https://github.com/Review4Repair/Review4Repair",
    "itemCount": "55,060 training samples, 2,961 test samples",
    "source": "arXiv",
    "specs": "Java; Input: Buggy Code + Review Comment",
    "year": "2020",
    "id": "saved-1769639630182-jboal",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_reb8cWKd-nAmjuv0KOzKG27lLkuT_LfjJ0GiQhP7kKmdvUVeOU7AEYWoTtcyaqCg9fp7ZCmGRFpNPPOLQBrXzaXcXZug9M4FgGIFUSM3t1O7bhuYlKL5KO2C3yaCA4Msa5t0Hu3Ng0Ng_hB_eQ5i3WpWPRI=",
        "title": "rero.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMPXNUBKB2HxjDFOn4bFk1eXerX5wN322JVnSeAP52v1S2GhX2NcQfkxAYDNyVxMDW6fea80e54RpV-5n39U-j0WQkZyE7QDdgq5VKwbglLCdyBL9UHbsZ5heEmCSQ9ivGvmKwMLjheErCRjzRX-luIp4eTbC1LH_pVJKd0fNaZPOq6lsoDMfJ-FIyTURyA4Dop9-IHQgaKQ4GkGnLkiSJ5iF5wAw2H60=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvX20D19DyJGAtFcm8psBflmMbagSr8GsUScRAaEcu-o0nCfnsItaIldlz0cot16MVkjwXbv81x-cbgRuCdgI2NAy7RLlKjRX0NalZ4q5xY6fRTiroCc90I_zTPDHh9LV_RtsJ",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF9QmlKLqlV7roz9WYbR1kCNv2ScBQ8z7POeGHT5VM9YeTYqJvJy07c0wEnuSVREblpsdKqTJOZwzgDV-NqtZLck7k2mp4EDzcXaMQQYvERGSNRFsGv2s1zNa0=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PrimeVul",
    "paperLink": "https://arxiv.org/abs/2403.18624",
    "description": "A dataset designed to rigorously evaluate code language models for vulnerability detection, addressing issues of data leakage and label accuracy in earlier benchmarks.",
    "authors": [
      "Yangruibo Ding",
      "Yanjun Fu",
      "Omniyyah Ibrahim",
      "Chawin Sitawarin",
      "Xinyun Chen",
      "Basel Alomair",
      "David Wagner",
      "Baishakhi Ray",
      "Yizheng Chen"
    ],
    "githubLink": "https://github.com/DLVulDet/PrimeVul",
    "itemCount": "~7,000 vulnerable / ~229,000 benign functions",
    "source": "arXiv",
    "specs": "C/C++, Function-level, Chronological Split",
    "year": "2024",
    "id": "saved-1769639697764-7bdk4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-NDWF6oK8_urRWrQzhaLfJw3Tvrl9ONrVKLmRyqwKyVIL5hFurHvO2yACNpm1czczgoWb_HiqZnqhFl47eif29vop1D1cTp9AnqxZbFczhRgnuJTEmFwaND_TLf07b7yBLA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEh73YXgeDxHnMfwlidxUdKTsiKNQ1FF0RBWU4mNuHh6rpHbkjywoORi1rGlsi1BvemKJwvr9qioXf1DdBGQUf6mYGXyhyroPpB7k_thEeqWJ5_v5WMo3SIYEx8o8N4DQBf724R5w==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "SVEN",
    "paperLink": "https://arxiv.org/abs/2302.05319",
    "description": "A dataset and framework for security hardening and adversarial testing of Large Language Models. It focuses on controlling models to generate secure or unsafe code.",
    "authors": [
      "Jingxuan He",
      "Martin Vechev"
    ],
    "githubLink": "https://github.com/eth-sri/sven",
    "itemCount": "~4,000 training pairs, ~803 evaluation functions",
    "source": "arXiv",
    "specs": "C/C++, Python, Prompt/Prefix Tuning",
    "year": "2023",
    "id": "saved-1769639697764-b0pxv",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-NDWF6oK8_urRWrQzhaLfJw3Tvrl9ONrVKLmRyqwKyVIL5hFurHvO2yACNpm1czczgoWb_HiqZnqhFl47eif29vop1D1cTp9AnqxZbFczhRgnuJTEmFwaND_TLf07b7yBLA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEh73YXgeDxHnMfwlidxUdKTsiKNQ1FF0RBWU4mNuHh6rpHbkjywoORi1rGlsi1BvemKJwvr9qioXf1DdBGQUf6mYGXyhyroPpB7k_thEeqWJ5_v5WMo3SIYEx8o8N4DQBf724R5w==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "CVEfixes",
    "paperLink": "https://arxiv.org/abs/2107.08760",
    "description": "An automatically collected and curated dataset of vulnerabilities and their fixes from open-source software, organized as a relational database of commits.",
    "authors": [
      "Guru Prasad Bhandari",
      "Amara Naseer",
      "Leon Moonen"
    ],
    "githubLink": "https://github.com/secureIT-project/CVEfixes",
    "itemCount": "~5,365 CVEs / ~5,495 commits (initial release)",
    "source": "arXiv",
    "specs": "Multi-language (C/C++, Java, etc.), Commit-level",
    "year": "2021",
    "id": "saved-1769639697764-lw04t",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-NDWF6oK8_urRWrQzhaLfJw3Tvrl9ONrVKLmRyqwKyVIL5hFurHvO2yACNpm1czczgoWb_HiqZnqhFl47eif29vop1D1cTp9AnqxZbFczhRgnuJTEmFwaND_TLf07b7yBLA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEh73YXgeDxHnMfwlidxUdKTsiKNQ1FF0RBWU4mNuHh6rpHbkjywoORi1rGlsi1BvemKJwvr9qioXf1DdBGQUf6mYGXyhyroPpB7k_thEeqWJ5_v5WMo3SIYEx8o8N4DQBf724R5w==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Big-Vul",
    "paperLink": "https://arxiv.org/abs/2005.06520",
    "description": "A large-scale C/C++ code vulnerability dataset collected from open-source GitHub projects. It contains code changes and CVE summaries, widely used for deep learning-based vulnerability detection.",
    "authors": [
      "Jiahao Fan",
      "Yi Li",
      "Shaohua Wang",
      "Tien N. Nguyen"
    ],
    "githubLink": "https://github.com/ZeoVan/MSR_20_Code_vulnerability_CSV_Dataset",
    "itemCount": "~190,000 functions (~3,754 vulnerable)",
    "source": "arXiv",
    "specs": "C/C++, Function-level, Commit-based",
    "year": "2020",
    "id": "saved-1769639697765-sltzy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-NDWF6oK8_urRWrQzhaLfJw3Tvrl9ONrVKLmRyqwKyVIL5hFurHvO2yACNpm1czczgoWb_HiqZnqhFl47eif29vop1D1cTp9AnqxZbFczhRgnuJTEmFwaND_TLf07b7yBLA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEh73YXgeDxHnMfwlidxUdKTsiKNQ1FF0RBWU4mNuHh6rpHbkjywoORi1rGlsi1BvemKJwvr9qioXf1DdBGQUf6mYGXyhyroPpB7k_thEeqWJ5_v5WMo3SIYEx8o8N4DQBf724R5w==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Draper VDISC",
    "paperLink": "https://arxiv.org/abs/1807.04320",
    "description": "A massive dataset of over a million function-level examples mined from open-source code, labeled using static analysis tools (Clang, Cppcheck, Flawfinder).",
    "authors": [
      "Rebecca L. Russell",
      "Louis Y. Kim",
      "Lei H. Hamilton",
      "Tomo Lazovich",
      "Jacob A. Harer",
      "Onur Ozdemir",
      "Paul M. Ellingwood",
      "Marc W. McConley"
    ],
    "githubLink": "https://osf.io/d45bw/",
    "itemCount": "1.27 million functions",
    "source": "arXiv",
    "specs": "C/C++, Function-level, Static Analysis Labels",
    "year": "2018",
    "id": "saved-1769639697765-qxefs",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-NDWF6oK8_urRWrQzhaLfJw3Tvrl9ONrVKLmRyqwKyVIL5hFurHvO2yACNpm1czczgoWb_HiqZnqhFl47eif29vop1D1cTp9AnqxZbFczhRgnuJTEmFwaND_TLf07b7yBLA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEh73YXgeDxHnMfwlidxUdKTsiKNQ1FF0RBWU4mNuHh6rpHbkjywoORi1rGlsi1BvemKJwvr9qioXf1DdBGQUf6mYGXyhyroPpB7k_thEeqWJ5_v5WMo3SIYEx8o8N4DQBf724R5w==",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Aegis AI Content Safety Dataset 2.0",
    "paperLink": "https://arxiv.org/abs/2501.09004",
    "description": "A diverse AI safety dataset and risks taxonomy for aligning LLM guardrails, focusing on content safety risks and human-LLM interactions.",
    "authors": [
      "Shaona Ghosh",
      "Prasoon Varshney",
      "Makesh Narsimhan Sreedhar",
      "Aishwarya Padmakumar",
      "Traian Rebedea",
      "Jibin Rajan Varghese",
      "Christopher Parisien"
    ],
    "githubLink": "https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0",
    "itemCount": "34,248 samples",
    "source": "arXiv",
    "specs": "12 hazard categories, 9 fine-grained subcategories, human-LLM interactions",
    "year": "2025",
    "id": "saved-1769639783326-94gqr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRxnaEo5eHeil3J_CofVY7h6EMS57Xrw_G3uZm1QlpQg_U1Vfnse3TW0ZsmCHu_YsOjnz8AdWBSG-b6beRYltnSKhxyRWROjvei0vSuLreoG6gDMYoNSgNh3LFnlrXvluMJDSeCvT0Q_rUUD8E0CyAAKoH",
        "title": "berkeley.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXLodK8eaUFlpXcnFS6XjkW8o7bReMo0HOw3L0cs43EB-urQfb1afx71sldOmZPuLUPXSp5-TVOTzaEyrqBg5ktJyXA56z4pWQo5tfRcZeHwbqQdIHHkUJjrTa",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEttDVq8-qCyf0O_se_JzOzX9CYP4PTaGTe3t6NMRGEpdGcgXDpA3TgJMvR35uOpkhMcgQthCGtOmDcop_33JdZ0_-mVb876Y1Ufd5NTXCJtP-W-H2tqemQ",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFTYrF1hBpGZmen8STFsr2fPX0s1bFON0dSrVjNoq6G1BbGj-lgC8D0LtlFuUhSHwQIFhSF8PWMlyYNN4Z-tDi9Y3-MsliOKEdN0E7tJf9A4oz8igdJMUOaPcRu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmPEK19wOxky0fNWFpuZUENOkWmg_hptkTLKdqF-MWlWRbS_vI7DCgb2dfpIu9LRQDaM-nC4v6LH-m_uy19tzDkJWPMUr3r4GA7KVAPNsB976DcJX0wMLsxewGCqD-7KxTLOOuWyKu1euaYyuLLEtlg7gQ-cyGwsSdPg-6Mw5oXCmC4mc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWwuG8pJgbIPboIxAcAgw7tlpbVgANb3wL6fwIr0GXiCjhSNnSos8uMEKv_pF9wY0UWc9d2SlkPDUjGSycHP6-0lnVwdQVDTATROXMx3uka2Th14qz3-n4k5HXmWXA84ULmFozq2MyH37yYffRenYh7gPCYQCpCVFfFm7Y5w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFLQD__Ld9_2JUPvDRE2dv65HcSbspuHSYGITkcvlBWtJ7EgAPiiuHY614icR3-TXFzK4UQpnDASBn92pFC5p9HFyPBCgHrWg5TZ0bLf-HstFvsSgoSWsYYhnlL9gYO7Za5L8A2M-mAKRGUOvJVuglilZ8usDDQ6w9a5wJtNolnwF7OCcHB2BEiV5oCkt4WQKf3WAFyXQuzw8DVhNOPsma1-kpABGH-SMJRPqnktAcX9npgSSmMuT1SIzM=",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Agent-SafetyBench",
    "paperLink": "https://arxiv.org/abs/2412.14470",
    "description": "A comprehensive benchmark designed to evaluate the safety of LLM agents. It encompasses interaction environments and test cases to evaluate safety risks and common failure modes in agentic interactions.",
    "authors": [
      "Zhexin Zhang",
      "Peiyo Liu",
      "Shangyu Tan",
      "Wensheng Zhang",
      "Yuanzhi Li",
      "Lei Zhang",
      "Yue Zhang"
    ],
    "githubLink": "https://github.com/Thu-CoAI/Agent-SafetyBench",
    "itemCount": "2,000 test cases",
    "source": "arXiv",
    "specs": "349 interaction environments, 8 categories of safety risks, 10 failure modes",
    "year": "2024",
    "id": "saved-1769639783326-infqr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRxnaEo5eHeil3J_CofVY7h6EMS57Xrw_G3uZm1QlpQg_U1Vfnse3TW0ZsmCHu_YsOjnz8AdWBSG-b6beRYltnSKhxyRWROjvei0vSuLreoG6gDMYoNSgNh3LFnlrXvluMJDSeCvT0Q_rUUD8E0CyAAKoH",
        "title": "berkeley.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXLodK8eaUFlpXcnFS6XjkW8o7bReMo0HOw3L0cs43EB-urQfb1afx71sldOmZPuLUPXSp5-TVOTzaEyrqBg5ktJyXA56z4pWQo5tfRcZeHwbqQdIHHkUJjrTa",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEttDVq8-qCyf0O_se_JzOzX9CYP4PTaGTe3t6NMRGEpdGcgXDpA3TgJMvR35uOpkhMcgQthCGtOmDcop_33JdZ0_-mVb876Y1Ufd5NTXCJtP-W-H2tqemQ",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFTYrF1hBpGZmen8STFsr2fPX0s1bFON0dSrVjNoq6G1BbGj-lgC8D0LtlFuUhSHwQIFhSF8PWMlyYNN4Z-tDi9Y3-MsliOKEdN0E7tJf9A4oz8igdJMUOaPcRu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmPEK19wOxky0fNWFpuZUENOkWmg_hptkTLKdqF-MWlWRbS_vI7DCgb2dfpIu9LRQDaM-nC4v6LH-m_uy19tzDkJWPMUr3r4GA7KVAPNsB976DcJX0wMLsxewGCqD-7KxTLOOuWyKu1euaYyuLLEtlg7gQ-cyGwsSdPg-6Mw5oXCmC4mc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWwuG8pJgbIPboIxAcAgw7tlpbVgANb3wL6fwIr0GXiCjhSNnSos8uMEKv_pF9wY0UWc9d2SlkPDUjGSycHP6-0lnVwdQVDTATROXMx3uka2Th14qz3-n4k5HXmWXA84ULmFozq2MyH37yYffRenYh7gPCYQCpCVFfFm7Y5w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFLQD__Ld9_2JUPvDRE2dv65HcSbspuHSYGITkcvlBWtJ7EgAPiiuHY614icR3-TXFzK4UQpnDASBn92pFC5p9HFyPBCgHrWg5TZ0bLf-HstFvsSgoSWsYYhnlL9gYO7Za5L8A2M-mAKRGUOvJVuglilZ8usDDQ6w9a5wJtNolnwF7OCcHB2BEiV5oCkt4WQKf3WAFyXQuzw8DVhNOPsma1-kpABGH-SMJRPqnktAcX9npgSSmMuT1SIzM=",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "SG-Bench",
    "paperLink": "https://arxiv.org/abs/2410.21965",
    "description": "A benchmark to evaluate the generalization of LLM safety across diverse tasks (generation vs. discrimination) and prompt types (system prompts, few-shot, CoT).",
    "authors": [
      "Yutao Mou",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "githubLink": "https://github.com/MurrayTom/SG-Bench",
    "itemCount": "N/A",
    "source": "arXiv",
    "specs": "4 evaluation subsets, 7 extended test sets, generative and discriminative tasks",
    "year": "2024",
    "id": "saved-1769639783326-0hpgu",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRxnaEo5eHeil3J_CofVY7h6EMS57Xrw_G3uZm1QlpQg_U1Vfnse3TW0ZsmCHu_YsOjnz8AdWBSG-b6beRYltnSKhxyRWROjvei0vSuLreoG6gDMYoNSgNh3LFnlrXvluMJDSeCvT0Q_rUUD8E0CyAAKoH",
        "title": "berkeley.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXLodK8eaUFlpXcnFS6XjkW8o7bReMo0HOw3L0cs43EB-urQfb1afx71sldOmZPuLUPXSp5-TVOTzaEyrqBg5ktJyXA56z4pWQo5tfRcZeHwbqQdIHHkUJjrTa",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEttDVq8-qCyf0O_se_JzOzX9CYP4PTaGTe3t6NMRGEpdGcgXDpA3TgJMvR35uOpkhMcgQthCGtOmDcop_33JdZ0_-mVb876Y1Ufd5NTXCJtP-W-H2tqemQ",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFTYrF1hBpGZmen8STFsr2fPX0s1bFON0dSrVjNoq6G1BbGj-lgC8D0LtlFuUhSHwQIFhSF8PWMlyYNN4Z-tDi9Y3-MsliOKEdN0E7tJf9A4oz8igdJMUOaPcRu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmPEK19wOxky0fNWFpuZUENOkWmg_hptkTLKdqF-MWlWRbS_vI7DCgb2dfpIu9LRQDaM-nC4v6LH-m_uy19tzDkJWPMUr3r4GA7KVAPNsB976DcJX0wMLsxewGCqD-7KxTLOOuWyKu1euaYyuLLEtlg7gQ-cyGwsSdPg-6Mw5oXCmC4mc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWwuG8pJgbIPboIxAcAgw7tlpbVgANb3wL6fwIr0GXiCjhSNnSos8uMEKv_pF9wY0UWc9d2SlkPDUjGSycHP6-0lnVwdQVDTATROXMx3uka2Th14qz3-n4k5HXmWXA84ULmFozq2MyH37yYffRenYh7gPCYQCpCVFfFm7Y5w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFLQD__Ld9_2JUPvDRE2dv65HcSbspuHSYGITkcvlBWtJ7EgAPiiuHY614icR3-TXFzK4UQpnDASBn92pFC5p9HFyPBCgHrWg5TZ0bLf-HstFvsSgoSWsYYhnlL9gYO7Za5L8A2M-mAKRGUOvJVuglilZ8usDDQ6w9a5wJtNolnwF7OCcHB2BEiV5oCkt4WQKf3WAFyXQuzw8DVhNOPsma1-kpABGH-SMJRPqnktAcX9npgSSmMuT1SIzM=",
        "title": "openreview.net"
      }
    ]
  },
  {
    "title": "Tricky²",
    "paperLink": "https://arxiv.org/abs/2601.18949",
    "description": "A hybrid benchmark dataset designed to evaluate how human and LLM-generated errors coexist, interact, and compound in software development. It augments the TrickyBugs corpus of human-written defects with errors injected by models like GPT-5 and OpenAI-oss-20b.",
    "authors": [
      "Cole Granger",
      "Dipin Khati",
      "Daniel Rodriguez-Cardenas",
      "Denys Poshyvanyk"
    ],
    "githubLink": "https://github.com/CESEL/Tricky2",
    "itemCount": "Unknown",
    "source": "arXiv",
    "specs": "Code (C++, Python, Java); Human-only, LLM-only, and Human+LLM splits",
    "year": "2026",
    "id": "saved-1769639848580-5phsi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFu7ifVXp_7ePHLGTJ0JQxHA0nHdsuxvTT6ZxF0XOrWnQnG7JAnk5NTB2R3aR2XmekYJP7D-zWjpuohmuHI4jTbooewLbDlJ64324LR84gL6VolcsxTjut8qFPRyO0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtFD3Aog8cwQxxQQGWCFLc44Xe5ecs8v9UsT9t1XToniJRU4hIGFzy7UBfhtPyBIrHV20eWbxzUlJ0RKfVjy7z76iK3FpZRt1yRL3D9hOiBQgjKcNIUGEiTRk=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGWdKwnNiIrzr3rY6u3wL5sjO6MdERmpgqyuBuj9OZb1Wf8_9xsPtltOaQl0Uv3hHdgEen4QN4ekhoNPTPjtVztYKLwe_ArfOltjE6Os76ixhIqrWmeEf2_-ohdSzH52ZB6imOiAEU_t8E27Ohmtj1Yp0J9q2yxiw7fvomQ0Hz-zYz0yL5bS3xXLH_w0GNPzwGGFOwuvDy3TfHx5Y9sGDlhwnuRQ7fUg148EbCxvil2ZWyci6nFnA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWggg2nv5dWWwhU6fGb0n67ajbzJHdHG9V9e32u7Bikax-xfXr1Yb7gtCpGX7AH5W1nOxCYU8D64A5L5mFxehm2CK3RDEPVAARSi2gA9Z-8a9rm0rsLnq61THF6pE0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8h7lPmA3HM-uBTZbLsr1d4j_5YJVHBQDPtIYVzkE_vytnPTc5Qy2xEOOBoqmryTnljiupEGslpO-DkYSWizkMxXeV8_MgxQIqZf9V1zHyGKu5WqkbV5myHXjFnP4g",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFrEfQWuwzfsXXRyW84JrG3sVwEg-Rqtyqq9IA7SC7H1VcnfRYGz-SHYUDkaAMxasrZmyMDnEHrKuJgS3273_NFWHkwaoxh1DPakAzG4dtxoSiwoEoAZ2n_zdwWf-l79WWL65x4Uw1J2Fe",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO3LHldQMpKWrhu1T8GPOezy_kk4Gkr7GR_xSgIHsHJ4n9xwTJ_7DjUHH0lKM8aoPCluasf_GKnWSiFlOQiNYOMMznTOBMHbD5W-T7yBbGsuzinoX90Hi3nkWNV5N7dtss2Sv11WZXyv3ryv4Hhes=",
        "title": "aclanthology.org"
      }
    ]
  },
  {
    "title": "RIFTS",
    "paperLink": "https://arxiv.org/abs/2503.13975",
    "description": "A benchmark focusing on 'grounding' failures in human-LLM interactions—situations where LLMs fail to establish mutual understanding (e.g., by not asking for clarification). Derived from datasets like WildChat, MultiWOZ, and Bing Chat.",
    "authors": [
      "Omar Shaikh",
      "Hussein Mozannar",
      "Gagan Bansal",
      "Adam Fourney",
      "Eric Horvitz"
    ],
    "githubLink": "https://github.com/microsoft/rifts",
    "itemCount": "1,740 tasks",
    "source": "arXiv",
    "specs": "Text; Interaction logs, Grounding acts annotations",
    "year": "2025",
    "id": "saved-1769639848580-m5k6c",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFu7ifVXp_7ePHLGTJ0JQxHA0nHdsuxvTT6ZxF0XOrWnQnG7JAnk5NTB2R3aR2XmekYJP7D-zWjpuohmuHI4jTbooewLbDlJ64324LR84gL6VolcsxTjut8qFPRyO0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtFD3Aog8cwQxxQQGWCFLc44Xe5ecs8v9UsT9t1XToniJRU4hIGFzy7UBfhtPyBIrHV20eWbxzUlJ0RKfVjy7z76iK3FpZRt1yRL3D9hOiBQgjKcNIUGEiTRk=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGWdKwnNiIrzr3rY6u3wL5sjO6MdERmpgqyuBuj9OZb1Wf8_9xsPtltOaQl0Uv3hHdgEen4QN4ekhoNPTPjtVztYKLwe_ArfOltjE6Os76ixhIqrWmeEf2_-ohdSzH52ZB6imOiAEU_t8E27Ohmtj1Yp0J9q2yxiw7fvomQ0Hz-zYz0yL5bS3xXLH_w0GNPzwGGFOwuvDy3TfHx5Y9sGDlhwnuRQ7fUg148EbCxvil2ZWyci6nFnA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWggg2nv5dWWwhU6fGb0n67ajbzJHdHG9V9e32u7Bikax-xfXr1Yb7gtCpGX7AH5W1nOxCYU8D64A5L5mFxehm2CK3RDEPVAARSi2gA9Z-8a9rm0rsLnq61THF6pE0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8h7lPmA3HM-uBTZbLsr1d4j_5YJVHBQDPtIYVzkE_vytnPTc5Qy2xEOOBoqmryTnljiupEGslpO-DkYSWizkMxXeV8_MgxQIqZf9V1zHyGKu5WqkbV5myHXjFnP4g",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFrEfQWuwzfsXXRyW84JrG3sVwEg-Rqtyqq9IA7SC7H1VcnfRYGz-SHYUDkaAMxasrZmyMDnEHrKuJgS3273_NFWHkwaoxh1DPakAzG4dtxoSiwoEoAZ2n_zdwWf-l79WWL65x4Uw1J2Fe",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO3LHldQMpKWrhu1T8GPOezy_kk4Gkr7GR_xSgIHsHJ4n9xwTJ_7DjUHH0lKM8aoPCluasf_GKnWSiFlOQiNYOMMznTOBMHbD5W-T7yBbGsuzinoX90Hi3nkWNV5N7dtss2Sv11WZXyv3ryv4Hhes=",
        "title": "aclanthology.org"
      }
    ]
  },
  {
    "title": "ReaLMistake",
    "paperLink": "https://arxiv.org/abs/2404.03061",
    "description": "A benchmark for evaluating error detection in LLM responses. It consists of objective, realistic, and diverse errors made by LLMs (GPT-4, Llama 2) and annotated by experts across reasoning, instruction-following, and fact-verification tasks.",
    "authors": [
      "Ryo Kamoi",
      "Tanya Goyal",
      "Juan Diego Rodriguez",
      "Greg Durrett"
    ],
    "githubLink": "https://github.com/psunlpgroup/ReaLMistake",
    "itemCount": "Three tasks (Math, Fact Verification, Answerability)",
    "source": "arXiv",
    "specs": "Text; Binary error labels, Error categories, Human explanations",
    "year": "2024",
    "id": "saved-1769639848580-2fby6",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFu7ifVXp_7ePHLGTJ0JQxHA0nHdsuxvTT6ZxF0XOrWnQnG7JAnk5NTB2R3aR2XmekYJP7D-zWjpuohmuHI4jTbooewLbDlJ64324LR84gL6VolcsxTjut8qFPRyO0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtFD3Aog8cwQxxQQGWCFLc44Xe5ecs8v9UsT9t1XToniJRU4hIGFzy7UBfhtPyBIrHV20eWbxzUlJ0RKfVjy7z76iK3FpZRt1yRL3D9hOiBQgjKcNIUGEiTRk=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGWdKwnNiIrzr3rY6u3wL5sjO6MdERmpgqyuBuj9OZb1Wf8_9xsPtltOaQl0Uv3hHdgEen4QN4ekhoNPTPjtVztYKLwe_ArfOltjE6Os76ixhIqrWmeEf2_-ohdSzH52ZB6imOiAEU_t8E27Ohmtj1Yp0J9q2yxiw7fvomQ0Hz-zYz0yL5bS3xXLH_w0GNPzwGGFOwuvDy3TfHx5Y9sGDlhwnuRQ7fUg148EbCxvil2ZWyci6nFnA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWggg2nv5dWWwhU6fGb0n67ajbzJHdHG9V9e32u7Bikax-xfXr1Yb7gtCpGX7AH5W1nOxCYU8D64A5L5mFxehm2CK3RDEPVAARSi2gA9Z-8a9rm0rsLnq61THF6pE0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8h7lPmA3HM-uBTZbLsr1d4j_5YJVHBQDPtIYVzkE_vytnPTc5Qy2xEOOBoqmryTnljiupEGslpO-DkYSWizkMxXeV8_MgxQIqZf9V1zHyGKu5WqkbV5myHXjFnP4g",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFrEfQWuwzfsXXRyW84JrG3sVwEg-Rqtyqq9IA7SC7H1VcnfRYGz-SHYUDkaAMxasrZmyMDnEHrKuJgS3273_NFWHkwaoxh1DPakAzG4dtxoSiwoEoAZ2n_zdwWf-l79WWL65x4Uw1J2Fe",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO3LHldQMpKWrhu1T8GPOezy_kk4Gkr7GR_xSgIHsHJ4n9xwTJ_7DjUHH0lKM8aoPCluasf_GKnWSiFlOQiNYOMMznTOBMHbD5W-T7yBbGsuzinoX90Hi3nkWNV5N7dtss2Sv11WZXyv3ryv4Hhes=",
        "title": "aclanthology.org"
      }
    ]
  },
  {
    "title": "HalluRAG",
    "paperLink": "https://arxiv.org/abs/2412.16000",
    "description": "A dataset designed to detect closed-domain hallucinations in Retrieval-Augmented Generation (RAG) applications by analyzing the internal states (activations) of LLMs.",
    "authors": [
      "Fabian Ridder",
      "Malte Schilling"
    ],
    "githubLink": "https://github.com/F4biian/HalluRAG",
    "itemCount": "19,731 sentences",
    "source": "arXiv",
    "specs": "Text, Model Activations; Hallucination labels",
    "year": "2024",
    "id": "saved-1769639848580-vg6ce",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFu7ifVXp_7ePHLGTJ0JQxHA0nHdsuxvTT6ZxF0XOrWnQnG7JAnk5NTB2R3aR2XmekYJP7D-zWjpuohmuHI4jTbooewLbDlJ64324LR84gL6VolcsxTjut8qFPRyO0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtFD3Aog8cwQxxQQGWCFLc44Xe5ecs8v9UsT9t1XToniJRU4hIGFzy7UBfhtPyBIrHV20eWbxzUlJ0RKfVjy7z76iK3FpZRt1yRL3D9hOiBQgjKcNIUGEiTRk=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGWdKwnNiIrzr3rY6u3wL5sjO6MdERmpgqyuBuj9OZb1Wf8_9xsPtltOaQl0Uv3hHdgEen4QN4ekhoNPTPjtVztYKLwe_ArfOltjE6Os76ixhIqrWmeEf2_-ohdSzH52ZB6imOiAEU_t8E27Ohmtj1Yp0J9q2yxiw7fvomQ0Hz-zYz0yL5bS3xXLH_w0GNPzwGGFOwuvDy3TfHx5Y9sGDlhwnuRQ7fUg148EbCxvil2ZWyci6nFnA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWggg2nv5dWWwhU6fGb0n67ajbzJHdHG9V9e32u7Bikax-xfXr1Yb7gtCpGX7AH5W1nOxCYU8D64A5L5mFxehm2CK3RDEPVAARSi2gA9Z-8a9rm0rsLnq61THF6pE0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8h7lPmA3HM-uBTZbLsr1d4j_5YJVHBQDPtIYVzkE_vytnPTc5Qy2xEOOBoqmryTnljiupEGslpO-DkYSWizkMxXeV8_MgxQIqZf9V1zHyGKu5WqkbV5myHXjFnP4g",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFrEfQWuwzfsXXRyW84JrG3sVwEg-Rqtyqq9IA7SC7H1VcnfRYGz-SHYUDkaAMxasrZmyMDnEHrKuJgS3273_NFWHkwaoxh1DPakAzG4dtxoSiwoEoAZ2n_zdwWf-l79WWL65x4Uw1J2Fe",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO3LHldQMpKWrhu1T8GPOezy_kk4Gkr7GR_xSgIHsHJ4n9xwTJ_7DjUHH0lKM8aoPCluasf_GKnWSiFlOQiNYOMMznTOBMHbD5W-T7yBbGsuzinoX90Hi3nkWNV5N7dtss2Sv11WZXyv3ryv4Hhes=",
        "title": "aclanthology.org"
      }
    ]
  },
  {
    "title": "FAVA-Bench",
    "paperLink": "https://arxiv.org/abs/2401.06855",
    "description": "A fine-grained hallucination detection benchmark with a taxonomy of hallucination types. It includes synthetic and human-annotated datasets for identifying and correcting hallucinations in LLM-generated text.",
    "authors": [
      "Abhiraj Mishra",
      "Widhya Sary",
      "Kavya Srinet",
      "Yejin Choi",
      "Parminder Bhatia"
    ],
    "githubLink": "https://huggingface.co/datasets/fava-bench",
    "itemCount": "Unknown",
    "source": "arXiv",
    "specs": "Text; Fine-grained hallucination taxonomy, Span-level annotations",
    "year": "2024",
    "id": "saved-1769639848580-ds0jk",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFu7ifVXp_7ePHLGTJ0JQxHA0nHdsuxvTT6ZxF0XOrWnQnG7JAnk5NTB2R3aR2XmekYJP7D-zWjpuohmuHI4jTbooewLbDlJ64324LR84gL6VolcsxTjut8qFPRyO0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtFD3Aog8cwQxxQQGWCFLc44Xe5ecs8v9UsT9t1XToniJRU4hIGFzy7UBfhtPyBIrHV20eWbxzUlJ0RKfVjy7z76iK3FpZRt1yRL3D9hOiBQgjKcNIUGEiTRk=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGWdKwnNiIrzr3rY6u3wL5sjO6MdERmpgqyuBuj9OZb1Wf8_9xsPtltOaQl0Uv3hHdgEen4QN4ekhoNPTPjtVztYKLwe_ArfOltjE6Os76ixhIqrWmeEf2_-ohdSzH52ZB6imOiAEU_t8E27Ohmtj1Yp0J9q2yxiw7fvomQ0Hz-zYz0yL5bS3xXLH_w0GNPzwGGFOwuvDy3TfHx5Y9sGDlhwnuRQ7fUg148EbCxvil2ZWyci6nFnA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWggg2nv5dWWwhU6fGb0n67ajbzJHdHG9V9e32u7Bikax-xfXr1Yb7gtCpGX7AH5W1nOxCYU8D64A5L5mFxehm2CK3RDEPVAARSi2gA9Z-8a9rm0rsLnq61THF6pE0",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8h7lPmA3HM-uBTZbLsr1d4j_5YJVHBQDPtIYVzkE_vytnPTc5Qy2xEOOBoqmryTnljiupEGslpO-DkYSWizkMxXeV8_MgxQIqZf9V1zHyGKu5WqkbV5myHXjFnP4g",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFrEfQWuwzfsXXRyW84JrG3sVwEg-Rqtyqq9IA7SC7H1VcnfRYGz-SHYUDkaAMxasrZmyMDnEHrKuJgS3273_NFWHkwaoxh1DPakAzG4dtxoSiwoEoAZ2n_zdwWf-l79WWL65x4Uw1J2Fe",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO3LHldQMpKWrhu1T8GPOezy_kk4Gkr7GR_xSgIHsHJ4n9xwTJ_7DjUHH0lKM8aoPCluasf_GKnWSiFlOQiNYOMMznTOBMHbD5W-T7yBbGsuzinoX90Hi3nkWNV5N7dtss2Sv11WZXyv3ryv4Hhes=",
        "title": "aclanthology.org"
      }
    ]
  },
  {
    "title": "HealthSLM-Bench",
    "paperLink": "https://arxiv.org/abs/2509.07260",
    "description": "A benchmark designed to systematically evaluate Small Language Models (SLMs) on mobile and wearable healthcare monitoring tasks. It covers zero-shot, few-shot, and instruction-tuned settings for health prediction tasks using sensor data.",
    "authors": [
      "HealthSLM Team"
    ],
    "githubLink": "https://github.com/paperscope/AIConf",
    "itemCount": "Aggregates 4 public wearable datasets",
    "source": "arXiv",
    "specs": "Text/Sensor Data; 8 health prediction tasks; Classification and Regression",
    "year": "2025",
    "id": "saved-1769639903521-sf5og",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuSiWpye7gOniBdf6747P9K9QpTGbPc-dZSbOF4Qzh3bor6sBfXkASzN-7x2Ii9A8PuSJ5S_EOAVeMzLPPqBXqUAwA_KHaE2yUm83sBtem2JX0DzWu_DHO3y0I",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdkpxx4QwdR7tFNKd-ii2YRBbDL_R1YLe-kGFH12mECfd5P8BpG0dyjftQGhTZXYk2ml_Q0CVXGOAEQgNMJPCWW2ngWNERngpcIuiaEQ2BEI1L0hni3S_keikKpUdF5jUnNjfiP3Y2-NtNzcOIPr4ekA==",
        "title": "ijirt.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEE8swwK-65-emVMoo7hq9EcrdzxHielRTL5gIOIUMTNOiSGrl6aGT2bvx_-zx4sBbCovFD8bX1wa4PJnsi2dhmBimWYVhvkAvP2KVQLFH30VSDqx633L-zWsAfK66YuDFFhOnMJg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVZPfYWP4aZriyOmthYDwOcCJ-XppBnZPS_aF4PFDI3LEAQnB43UD1VDGYLtXJvYkeXuY7RNldUOzLGyDXEOXxj6W9-n1JjIdZOFsjpCmw1wp8GQrmzZfepilzj00P",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-VvhdB6J63RVwwnLaWFrPmyN4-CvzQF5cr_cceRD1nQJ58ZJBqKEaTNpP-aRaVMMA0FbpkpKRSWEBm0fetDlvjxlho-EptwAEVr6MX0nQ_DG5NRAEvSoGBiv_qCXrPJBkVI5SOtzoS_DeCPdtRxpc",
        "title": "modelscope.cn"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlYBi869LhYs0tiA31dhKmohrDjOV4FG2KstwAVpX9_BfZc93m0z_uABvZhyonUyOGYhrBkYg2zPwbc3btaaXM3NnerohdlV3mCxYgiXBlx1ef0v9d-q0CzUF5iynq",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaLxILyjeRFxM84kaGrY3dmiGPdlAyokCZj6opCtgy5adbD2D_Q777K69ky9GDd8iQV_WVsoC6xac7KFVenUa9sdVKOTQRJGLB7PDg_J1kXgXr5FXACC3aUZE4kjn6_-DU1aaHHH4-XLyAbwtTxOYhLt8v8iwJxmrdtX7Wv2Ya2_nsow==",
        "title": "confident-ai.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9qutAbeP7RXvkjps1wgCKwgOI_QzRHTfWiaDMtl-nDNtLTAZdOxVnKhfC3YshynGjL-I5K4APjkFojxSDB85V55UX-WWZjGKOeWwFBs0dKoMlnHjaV28rw3eM3tXUYz79DvzEbFMNDLH1xOn_12fXZjmOemUS8_6NixeeMHFbFilr7oH5Adq_kG-Y215pKpguMKy0iXSnGGfWoRcULB4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8woyWBNQqSobFomOjA3ZUvDE3TcbwqQODNJQMqmgYZQlaBmjCeUELRvQer2NbxEdIMzgl7Afu0YwR0iwk6IQySsSsoOwmCt7tPMwTZle_8aNy8lNF_A8SaM4hYP3GF2LrXqTsl8i_uZ3UuN36Lpk4kI9_vwB_DgBwQ0hLmxexGZz_yRzT1UyRzZeqmjFtQAC3zLv6zEICeO_k9U7b1AKPWMLjUb1iAHoehtAGEt2MM5mKY9WkF7Lf3uBistpOGvZRjfJbpY3A6PF_UMX6oZXaF0F0J68=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "BeDGED (GenAI on the Edge)",
    "paperLink": "https://ieeexplore.ieee.org/document/10347231",
    "description": "A comprehensive benchmarking dataset designed to evaluate the performance of LLMs deployed on edge devices (like Raspberry Pi). It captures metrics such as throughput, inference latency, memory utilization, and energy consumption.",
    "authors": [
      "IEEE Communications Society"
    ],
    "githubLink": "N/A",
    "itemCount": "Performance metrics across varying model sizes",
    "source": "IEEE Xplore",
    "specs": "Structured Data (CSV); Performance Metrics (Latency, Memory, Energy)",
    "year": "2025",
    "id": "saved-1769639903521-zlh1u",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuSiWpye7gOniBdf6747P9K9QpTGbPc-dZSbOF4Qzh3bor6sBfXkASzN-7x2Ii9A8PuSJ5S_EOAVeMzLPPqBXqUAwA_KHaE2yUm83sBtem2JX0DzWu_DHO3y0I",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdkpxx4QwdR7tFNKd-ii2YRBbDL_R1YLe-kGFH12mECfd5P8BpG0dyjftQGhTZXYk2ml_Q0CVXGOAEQgNMJPCWW2ngWNERngpcIuiaEQ2BEI1L0hni3S_keikKpUdF5jUnNjfiP3Y2-NtNzcOIPr4ekA==",
        "title": "ijirt.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEE8swwK-65-emVMoo7hq9EcrdzxHielRTL5gIOIUMTNOiSGrl6aGT2bvx_-zx4sBbCovFD8bX1wa4PJnsi2dhmBimWYVhvkAvP2KVQLFH30VSDqx633L-zWsAfK66YuDFFhOnMJg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVZPfYWP4aZriyOmthYDwOcCJ-XppBnZPS_aF4PFDI3LEAQnB43UD1VDGYLtXJvYkeXuY7RNldUOzLGyDXEOXxj6W9-n1JjIdZOFsjpCmw1wp8GQrmzZfepilzj00P",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-VvhdB6J63RVwwnLaWFrPmyN4-CvzQF5cr_cceRD1nQJ58ZJBqKEaTNpP-aRaVMMA0FbpkpKRSWEBm0fetDlvjxlho-EptwAEVr6MX0nQ_DG5NRAEvSoGBiv_qCXrPJBkVI5SOtzoS_DeCPdtRxpc",
        "title": "modelscope.cn"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlYBi869LhYs0tiA31dhKmohrDjOV4FG2KstwAVpX9_BfZc93m0z_uABvZhyonUyOGYhrBkYg2zPwbc3btaaXM3NnerohdlV3mCxYgiXBlx1ef0v9d-q0CzUF5iynq",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaLxILyjeRFxM84kaGrY3dmiGPdlAyokCZj6opCtgy5adbD2D_Q777K69ky9GDd8iQV_WVsoC6xac7KFVenUa9sdVKOTQRJGLB7PDg_J1kXgXr5FXACC3aUZE4kjn6_-DU1aaHHH4-XLyAbwtTxOYhLt8v8iwJxmrdtX7Wv2Ya2_nsow==",
        "title": "confident-ai.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9qutAbeP7RXvkjps1wgCKwgOI_QzRHTfWiaDMtl-nDNtLTAZdOxVnKhfC3YshynGjL-I5K4APjkFojxSDB85V55UX-WWZjGKOeWwFBs0dKoMlnHjaV28rw3eM3tXUYz79DvzEbFMNDLH1xOn_12fXZjmOemUS8_6NixeeMHFbFilr7oH5Adq_kG-Y215pKpguMKy0iXSnGGfWoRcULB4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8woyWBNQqSobFomOjA3ZUvDE3TcbwqQODNJQMqmgYZQlaBmjCeUELRvQer2NbxEdIMzgl7Afu0YwR0iwk6IQySsSsoOwmCt7tPMwTZle_8aNy8lNF_A8SaM4hYP3GF2LrXqTsl8i_uZ3UuN36Lpk4kI9_vwB_DgBwQ0hLmxexGZz_yRzT1UyRzZeqmjFtQAC3zLv6zEICeO_k9U7b1AKPWMLjUb1iAHoehtAGEt2MM5mKY9WkF7Lf3uBistpOGvZRjfJbpY3A6PF_UMX6oZXaF0F0J68=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "tinyBenchmarks",
    "paperLink": "https://arxiv.org/abs/2402.14992",
    "description": "A set of curated, smaller versions of popular benchmarks (like MMLU, HELM, AlpacaEval) specifically designed to efficiently evaluate Large Language Models with significantly fewer examples (e.g., 100 vs 14,000), making frequent evaluation of small or resource-constrained models feasible.",
    "authors": [
      "Felipe Maia Polo",
      "Lucas Weber",
      "Leshem Choshen",
      "Yuekai Sun",
      "Gongjun Xu",
      "Mikhail Yurochkin"
    ],
    "githubLink": "https://github.com/felipemaiapolo/tinyBenchmarks",
    "itemCount": "100 examples per scenario (e.g., tinyMMLU, tinyHELM)",
    "source": "arXiv",
    "specs": "Text; Question Answering; Multiple Choice; Efficient Evaluation Subsets",
    "year": "2024",
    "id": "saved-1769639903521-moddm",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuSiWpye7gOniBdf6747P9K9QpTGbPc-dZSbOF4Qzh3bor6sBfXkASzN-7x2Ii9A8PuSJ5S_EOAVeMzLPPqBXqUAwA_KHaE2yUm83sBtem2JX0DzWu_DHO3y0I",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdkpxx4QwdR7tFNKd-ii2YRBbDL_R1YLe-kGFH12mECfd5P8BpG0dyjftQGhTZXYk2ml_Q0CVXGOAEQgNMJPCWW2ngWNERngpcIuiaEQ2BEI1L0hni3S_keikKpUdF5jUnNjfiP3Y2-NtNzcOIPr4ekA==",
        "title": "ijirt.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEE8swwK-65-emVMoo7hq9EcrdzxHielRTL5gIOIUMTNOiSGrl6aGT2bvx_-zx4sBbCovFD8bX1wa4PJnsi2dhmBimWYVhvkAvP2KVQLFH30VSDqx633L-zWsAfK66YuDFFhOnMJg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVZPfYWP4aZriyOmthYDwOcCJ-XppBnZPS_aF4PFDI3LEAQnB43UD1VDGYLtXJvYkeXuY7RNldUOzLGyDXEOXxj6W9-n1JjIdZOFsjpCmw1wp8GQrmzZfepilzj00P",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-VvhdB6J63RVwwnLaWFrPmyN4-CvzQF5cr_cceRD1nQJ58ZJBqKEaTNpP-aRaVMMA0FbpkpKRSWEBm0fetDlvjxlho-EptwAEVr6MX0nQ_DG5NRAEvSoGBiv_qCXrPJBkVI5SOtzoS_DeCPdtRxpc",
        "title": "modelscope.cn"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlYBi869LhYs0tiA31dhKmohrDjOV4FG2KstwAVpX9_BfZc93m0z_uABvZhyonUyOGYhrBkYg2zPwbc3btaaXM3NnerohdlV3mCxYgiXBlx1ef0v9d-q0CzUF5iynq",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaLxILyjeRFxM84kaGrY3dmiGPdlAyokCZj6opCtgy5adbD2D_Q777K69ky9GDd8iQV_WVsoC6xac7KFVenUa9sdVKOTQRJGLB7PDg_J1kXgXr5FXACC3aUZE4kjn6_-DU1aaHHH4-XLyAbwtTxOYhLt8v8iwJxmrdtX7Wv2Ya2_nsow==",
        "title": "confident-ai.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9qutAbeP7RXvkjps1wgCKwgOI_QzRHTfWiaDMtl-nDNtLTAZdOxVnKhfC3YshynGjL-I5K4APjkFojxSDB85V55UX-WWZjGKOeWwFBs0dKoMlnHjaV28rw3eM3tXUYz79DvzEbFMNDLH1xOn_12fXZjmOemUS8_6NixeeMHFbFilr7oH5Adq_kG-Y215pKpguMKy0iXSnGGfWoRcULB4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8woyWBNQqSobFomOjA3ZUvDE3TcbwqQODNJQMqmgYZQlaBmjCeUELRvQer2NbxEdIMzgl7Afu0YwR0iwk6IQySsSsoOwmCt7tPMwTZle_8aNy8lNF_A8SaM4hYP3GF2LrXqTsl8i_uZ3UuN36Lpk4kI9_vwB_DgBwQ0hLmxexGZz_yRzT1UyRzZeqmjFtQAC3zLv6zEICeO_k9U7b1AKPWMLjUb1iAHoehtAGEt2MM5mKY9WkF7Lf3uBistpOGvZRjfJbpY3A6PF_UMX6oZXaF0F0J68=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "MobileAIBench",
    "paperLink": "https://arxiv.org/abs/2406.10290",
    "description": "A comprehensive framework and benchmark for evaluating LLMs and LMMs (Large Multimodal Models) specifically for on-device use cases. It assesses model viability on mobile hardware, focusing on quantization effects, latency, and accuracy.",
    "authors": [
      "Salesforce AI Research",
      "Paweł Kapica"
    ],
    "githubLink": "https://github.com/SalesforceAIResearch/MobileAIBench",
    "itemCount": "Multiple tasks across NLP and Vision-Language domains",
    "source": "arXiv",
    "specs": "Text, Image; Quantized model evaluation (Int4, Int8, FP16)",
    "year": "2024",
    "id": "saved-1769639903521-pyinl",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuSiWpye7gOniBdf6747P9K9QpTGbPc-dZSbOF4Qzh3bor6sBfXkASzN-7x2Ii9A8PuSJ5S_EOAVeMzLPPqBXqUAwA_KHaE2yUm83sBtem2JX0DzWu_DHO3y0I",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdkpxx4QwdR7tFNKd-ii2YRBbDL_R1YLe-kGFH12mECfd5P8BpG0dyjftQGhTZXYk2ml_Q0CVXGOAEQgNMJPCWW2ngWNERngpcIuiaEQ2BEI1L0hni3S_keikKpUdF5jUnNjfiP3Y2-NtNzcOIPr4ekA==",
        "title": "ijirt.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEE8swwK-65-emVMoo7hq9EcrdzxHielRTL5gIOIUMTNOiSGrl6aGT2bvx_-zx4sBbCovFD8bX1wa4PJnsi2dhmBimWYVhvkAvP2KVQLFH30VSDqx633L-zWsAfK66YuDFFhOnMJg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVZPfYWP4aZriyOmthYDwOcCJ-XppBnZPS_aF4PFDI3LEAQnB43UD1VDGYLtXJvYkeXuY7RNldUOzLGyDXEOXxj6W9-n1JjIdZOFsjpCmw1wp8GQrmzZfepilzj00P",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-VvhdB6J63RVwwnLaWFrPmyN4-CvzQF5cr_cceRD1nQJ58ZJBqKEaTNpP-aRaVMMA0FbpkpKRSWEBm0fetDlvjxlho-EptwAEVr6MX0nQ_DG5NRAEvSoGBiv_qCXrPJBkVI5SOtzoS_DeCPdtRxpc",
        "title": "modelscope.cn"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlYBi869LhYs0tiA31dhKmohrDjOV4FG2KstwAVpX9_BfZc93m0z_uABvZhyonUyOGYhrBkYg2zPwbc3btaaXM3NnerohdlV3mCxYgiXBlx1ef0v9d-q0CzUF5iynq",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaLxILyjeRFxM84kaGrY3dmiGPdlAyokCZj6opCtgy5adbD2D_Q777K69ky9GDd8iQV_WVsoC6xac7KFVenUa9sdVKOTQRJGLB7PDg_J1kXgXr5FXACC3aUZE4kjn6_-DU1aaHHH4-XLyAbwtTxOYhLt8v8iwJxmrdtX7Wv2Ya2_nsow==",
        "title": "confident-ai.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9qutAbeP7RXvkjps1wgCKwgOI_QzRHTfWiaDMtl-nDNtLTAZdOxVnKhfC3YshynGjL-I5K4APjkFojxSDB85V55UX-WWZjGKOeWwFBs0dKoMlnHjaV28rw3eM3tXUYz79DvzEbFMNDLH1xOn_12fXZjmOemUS8_6NixeeMHFbFilr7oH5Adq_kG-Y215pKpguMKy0iXSnGGfWoRcULB4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8woyWBNQqSobFomOjA3ZUvDE3TcbwqQODNJQMqmgYZQlaBmjCeUELRvQer2NbxEdIMzgl7Afu0YwR0iwk6IQySsSsoOwmCt7tPMwTZle_8aNy8lNF_A8SaM4hYP3GF2LrXqTsl8i_uZ3UuN36Lpk4kI9_vwB_DgBwQ0hLmxexGZz_yRzT1UyRzZeqmjFtQAC3zLv6zEICeO_k9U7b1AKPWMLjUb1iAHoehtAGEt2MM5mKY9WkF7Lf3uBistpOGvZRjfJbpY3A6PF_UMX6oZXaF0F0J68=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "SmolTalk",
    "paperLink": "https://huggingface.co/datasets/HuggingFaceTB/smoltalk",
    "description": "A synthetic dataset designed for the supervised fine-tuning (SFT) of the SmolLM2 family of small language models. It combines new synthetic data with curated public datasets to improve instruction following, rewriting, and reasoning in compact models.",
    "authors": [
      "Hugging Face",
      "Loubna Ben Allal",
      "Anton Lozhkov",
      "Elie Bakouch"
    ],
    "githubLink": "https://github.com/huggingface/smollm/tree/main/text/data/smoltalk",
    "itemCount": "1,000,000 samples",
    "source": "Hugging Face",
    "specs": "Text; Instruction Tuning; Subsets: Smol-Magpie-Ultra, Smol-constraints, Smol-rewrite",
    "year": "2024",
    "id": "saved-1769639903522-gh21d",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuSiWpye7gOniBdf6747P9K9QpTGbPc-dZSbOF4Qzh3bor6sBfXkASzN-7x2Ii9A8PuSJ5S_EOAVeMzLPPqBXqUAwA_KHaE2yUm83sBtem2JX0DzWu_DHO3y0I",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdkpxx4QwdR7tFNKd-ii2YRBbDL_R1YLe-kGFH12mECfd5P8BpG0dyjftQGhTZXYk2ml_Q0CVXGOAEQgNMJPCWW2ngWNERngpcIuiaEQ2BEI1L0hni3S_keikKpUdF5jUnNjfiP3Y2-NtNzcOIPr4ekA==",
        "title": "ijirt.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEE8swwK-65-emVMoo7hq9EcrdzxHielRTL5gIOIUMTNOiSGrl6aGT2bvx_-zx4sBbCovFD8bX1wa4PJnsi2dhmBimWYVhvkAvP2KVQLFH30VSDqx633L-zWsAfK66YuDFFhOnMJg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVZPfYWP4aZriyOmthYDwOcCJ-XppBnZPS_aF4PFDI3LEAQnB43UD1VDGYLtXJvYkeXuY7RNldUOzLGyDXEOXxj6W9-n1JjIdZOFsjpCmw1wp8GQrmzZfepilzj00P",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-VvhdB6J63RVwwnLaWFrPmyN4-CvzQF5cr_cceRD1nQJ58ZJBqKEaTNpP-aRaVMMA0FbpkpKRSWEBm0fetDlvjxlho-EptwAEVr6MX0nQ_DG5NRAEvSoGBiv_qCXrPJBkVI5SOtzoS_DeCPdtRxpc",
        "title": "modelscope.cn"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlYBi869LhYs0tiA31dhKmohrDjOV4FG2KstwAVpX9_BfZc93m0z_uABvZhyonUyOGYhrBkYg2zPwbc3btaaXM3NnerohdlV3mCxYgiXBlx1ef0v9d-q0CzUF5iynq",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaLxILyjeRFxM84kaGrY3dmiGPdlAyokCZj6opCtgy5adbD2D_Q777K69ky9GDd8iQV_WVsoC6xac7KFVenUa9sdVKOTQRJGLB7PDg_J1kXgXr5FXACC3aUZE4kjn6_-DU1aaHHH4-XLyAbwtTxOYhLt8v8iwJxmrdtX7Wv2Ya2_nsow==",
        "title": "confident-ai.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9qutAbeP7RXvkjps1wgCKwgOI_QzRHTfWiaDMtl-nDNtLTAZdOxVnKhfC3YshynGjL-I5K4APjkFojxSDB85V55UX-WWZjGKOeWwFBs0dKoMlnHjaV28rw3eM3tXUYz79DvzEbFMNDLH1xOn_12fXZjmOemUS8_6NixeeMHFbFilr7oH5Adq_kG-Y215pKpguMKy0iXSnGGfWoRcULB4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8woyWBNQqSobFomOjA3ZUvDE3TcbwqQODNJQMqmgYZQlaBmjCeUELRvQer2NbxEdIMzgl7Afu0YwR0iwk6IQySsSsoOwmCt7tPMwTZle_8aNy8lNF_A8SaM4hYP3GF2LrXqTsl8i_uZ3UuN36Lpk4kI9_vwB_DgBwQ0hLmxexGZz_yRzT1UyRzZeqmjFtQAC3zLv6zEICeO_k9U7b1AKPWMLjUb1iAHoehtAGEt2MM5mKY9WkF7Lf3uBistpOGvZRjfJbpY3A6PF_UMX6oZXaF0F0J68=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "Open-LLM-Leaderboard (Small-Scale Regime)",
    "paperLink": "https://arxiv.org/abs/2406.08436",
    "description": "A specific evaluation regime within the broader Open-LLM-Leaderboard framework targeting models smaller than 3B parameters. It evaluates capabilities across MMLU, ARC, Winogrande, and other standard benchmarks to track SLM progress.",
    "authors": [
      "VILA Lab",
      "Lianmin Zheng",
      "Wei-Lin Chiang",
      "Ying Sheng"
    ],
    "githubLink": "https://github.com/VILA-Lab/Open-LLM-Leaderboard",
    "itemCount": "Suite of 6+ major datasets (MMLU, ARC, etc.)",
    "source": "arXiv",
    "specs": "Text; Multiple Choice; Open-ended QA; Models < 3B parameters",
    "year": "2024",
    "id": "saved-1769639903522-0g404",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuSiWpye7gOniBdf6747P9K9QpTGbPc-dZSbOF4Qzh3bor6sBfXkASzN-7x2Ii9A8PuSJ5S_EOAVeMzLPPqBXqUAwA_KHaE2yUm83sBtem2JX0DzWu_DHO3y0I",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdkpxx4QwdR7tFNKd-ii2YRBbDL_R1YLe-kGFH12mECfd5P8BpG0dyjftQGhTZXYk2ml_Q0CVXGOAEQgNMJPCWW2ngWNERngpcIuiaEQ2BEI1L0hni3S_keikKpUdF5jUnNjfiP3Y2-NtNzcOIPr4ekA==",
        "title": "ijirt.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEE8swwK-65-emVMoo7hq9EcrdzxHielRTL5gIOIUMTNOiSGrl6aGT2bvx_-zx4sBbCovFD8bX1wa4PJnsi2dhmBimWYVhvkAvP2KVQLFH30VSDqx633L-zWsAfK66YuDFFhOnMJg==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVZPfYWP4aZriyOmthYDwOcCJ-XppBnZPS_aF4PFDI3LEAQnB43UD1VDGYLtXJvYkeXuY7RNldUOzLGyDXEOXxj6W9-n1JjIdZOFsjpCmw1wp8GQrmzZfepilzj00P",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-VvhdB6J63RVwwnLaWFrPmyN4-CvzQF5cr_cceRD1nQJ58ZJBqKEaTNpP-aRaVMMA0FbpkpKRSWEBm0fetDlvjxlho-EptwAEVr6MX0nQ_DG5NRAEvSoGBiv_qCXrPJBkVI5SOtzoS_DeCPdtRxpc",
        "title": "modelscope.cn"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlYBi869LhYs0tiA31dhKmohrDjOV4FG2KstwAVpX9_BfZc93m0z_uABvZhyonUyOGYhrBkYg2zPwbc3btaaXM3NnerohdlV3mCxYgiXBlx1ef0v9d-q0CzUF5iynq",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaLxILyjeRFxM84kaGrY3dmiGPdlAyokCZj6opCtgy5adbD2D_Q777K69ky9GDd8iQV_WVsoC6xac7KFVenUa9sdVKOTQRJGLB7PDg_J1kXgXr5FXACC3aUZE4kjn6_-DU1aaHHH4-XLyAbwtTxOYhLt8v8iwJxmrdtX7Wv2Ya2_nsow==",
        "title": "confident-ai.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9qutAbeP7RXvkjps1wgCKwgOI_QzRHTfWiaDMtl-nDNtLTAZdOxVnKhfC3YshynGjL-I5K4APjkFojxSDB85V55UX-WWZjGKOeWwFBs0dKoMlnHjaV28rw3eM3tXUYz79DvzEbFMNDLH1xOn_12fXZjmOemUS8_6NixeeMHFbFilr7oH5Adq_kG-Y215pKpguMKy0iXSnGGfWoRcULB4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8woyWBNQqSobFomOjA3ZUvDE3TcbwqQODNJQMqmgYZQlaBmjCeUELRvQer2NbxEdIMzgl7Afu0YwR0iwk6IQySsSsoOwmCt7tPMwTZle_8aNy8lNF_A8SaM4hYP3GF2LrXqTsl8i_uZ3UuN36Lpk4kI9_vwB_DgBwQ0hLmxexGZz_yRzT1UyRzZeqmjFtQAC3zLv6zEICeO_k9U7b1AKPWMLjUb1iAHoehtAGEt2MM5mKY9WkF7Lf3uBistpOGvZRjfJbpY3A6PF_UMX6oZXaF0F0J68=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "PEER",
    "paperLink": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/02316a39423b40097120a2f7c9509424-Abstract-Databases.html",
    "description": "A comprehensive and multi-task benchmark for protein sequence understanding. It covers diverse tasks including protein function prediction, localization, structure prediction, and protein-protein interaction, designed to evaluate deep learning methods.",
    "authors": [
      "Xu, M.",
      "et al."
    ],
    "githubLink": "https://github.com/DeepGraphLearning/PEER_Benchmark",
    "itemCount": "17 tasks",
    "source": "Scholar",
    "specs": "Protein sequences",
    "year": "2022",
    "id": "saved-1769639987508-eht3y",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFT-FuONcFl3mjnSJfHPka7r-u-dM_eI5O9GII6kpFhnaZRjZgOSP4AeTNU_GFkwwxVE9F9j81rP2lcIYs99RAoPsBL6e0BArfsOyEP90OZIQdGLHkWTXmh4DcwdEZC9wRjBSOdPqZ1cj1BgUVN_jQlMF4gKqIR",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEb6rE5GOZayz2KH_1I4XtxTEXbtXv7AW66DgBrc_5IZRHqaL7SfRIZoiTdQai-DqvAkGljm1cFczJ-Nlcel3YINE01mYH7A6Hek_UZKfS0UzsIdjjD6tj3mVVXCo4WateVHup-xVRv-zXhDA22jC0o6nWPdFJ5u28Rc5CEaCwmfcDZDBMVDyWDB23-41NW8ly23mD67ie4djYL2gxY7zYHUv7n7TGDsTH_VvDMkMU3lLSw7U7HjUosMpLsQrkekQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMCz7Sr65L55Yw8HALgwNph3ddv0cFnVctjWrvrkWUne4VETrY_OPoR70MvixrupgLWeqUYNKG9UFJHgTJ6oVPimvyzYSVvvVqd1qoUN7vBYyfihxdKXV0YP142-nKkP_RlPxh_qbcD8T-dZLylAzC2rnXidqO8mgckgF-MmKUxNQmgkvElPdCSnYsaJzSv53lcI-NFrauMQFCQhzsPKdkV0uAErTgAKR4jbh8DGGBZ7wM4w==",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "GenomicBenchmarks",
    "paperLink": "https://www.biorxiv.org/content/10.1101/2022.06.10.495262v1",
    "description": "A collection of curated and easily accessible datasets for genomic sequence classification, derived from publicly available databases and existing literature. It aims to standardize benchmarks for machine learning in genomics.",
    "authors": [
      "Grešová, K.",
      "et al."
    ],
    "githubLink": "https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks",
    "itemCount": "8 datasets",
    "source": "arXiv",
    "specs": "Genomic sequences (DNA), classification labels",
    "year": "2022",
    "id": "saved-1769639987508-y5z8m",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFT-FuONcFl3mjnSJfHPka7r-u-dM_eI5O9GII6kpFhnaZRjZgOSP4AeTNU_GFkwwxVE9F9j81rP2lcIYs99RAoPsBL6e0BArfsOyEP90OZIQdGLHkWTXmh4DcwdEZC9wRjBSOdPqZ1cj1BgUVN_jQlMF4gKqIR",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEb6rE5GOZayz2KH_1I4XtxTEXbtXv7AW66DgBrc_5IZRHqaL7SfRIZoiTdQai-DqvAkGljm1cFczJ-Nlcel3YINE01mYH7A6Hek_UZKfS0UzsIdjjD6tj3mVVXCo4WateVHup-xVRv-zXhDA22jC0o6nWPdFJ5u28Rc5CEaCwmfcDZDBMVDyWDB23-41NW8ly23mD67ie4djYL2gxY7zYHUv7n7TGDsTH_VvDMkMU3lLSw7U7HjUosMpLsQrkekQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMCz7Sr65L55Yw8HALgwNph3ddv0cFnVctjWrvrkWUne4VETrY_OPoR70MvixrupgLWeqUYNKG9UFJHgTJ6oVPimvyzYSVvvVqd1qoUN7vBYyfihxdKXV0YP142-nKkP_RlPxh_qbcD8T-dZLylAzC2rnXidqO8mgckgF-MmKUxNQmgkvElPdCSnYsaJzSv53lcI-NFrauMQFCQhzsPKdkV0uAErTgAKR4jbh8DGGBZ7wM4w==",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "DeepLoc 2.0 Dataset",
    "paperLink": "https://academic.oup.com/nar/article/50/W1/W228/6582163",
    "description": "A benchmark dataset used for training and testing the DeepLoc 2.0 model, focusing on multi-label subcellular localization prediction of eukaryotic proteins. It includes experimentally verified sorting signals and covers multiple compartments.",
    "authors": [
      "Thumuluri, V.",
      "Almagro Armenteros, J.J.",
      "Johansen, A.R.",
      "Nielsen, H.",
      "Winther, O."
    ],
    "githubLink": "https://services.healthtech.dtu.dk/services/DeepLoc-2.0/",
    "itemCount": "~15,000+ proteins",
    "source": "Scholar",
    "specs": "Protein sequences (FASTA), multi-label localization targets",
    "year": "2022",
    "id": "saved-1769639987508-4jzcn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFT-FuONcFl3mjnSJfHPka7r-u-dM_eI5O9GII6kpFhnaZRjZgOSP4AeTNU_GFkwwxVE9F9j81rP2lcIYs99RAoPsBL6e0BArfsOyEP90OZIQdGLHkWTXmh4DcwdEZC9wRjBSOdPqZ1cj1BgUVN_jQlMF4gKqIR",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEb6rE5GOZayz2KH_1I4XtxTEXbtXv7AW66DgBrc_5IZRHqaL7SfRIZoiTdQai-DqvAkGljm1cFczJ-Nlcel3YINE01mYH7A6Hek_UZKfS0UzsIdjjD6tj3mVVXCo4WateVHup-xVRv-zXhDA22jC0o6nWPdFJ5u28Rc5CEaCwmfcDZDBMVDyWDB23-41NW8ly23mD67ie4djYL2gxY7zYHUv7n7TGDsTH_VvDMkMU3lLSw7U7HjUosMpLsQrkekQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMCz7Sr65L55Yw8HALgwNph3ddv0cFnVctjWrvrkWUne4VETrY_OPoR70MvixrupgLWeqUYNKG9UFJHgTJ6oVPimvyzYSVvvVqd1qoUN7vBYyfihxdKXV0YP142-nKkP_RlPxh_qbcD8T-dZLylAzC2rnXidqO8mgckgF-MmKUxNQmgkvElPdCSnYsaJzSv53lcI-NFrauMQFCQhzsPKdkV0uAErTgAKR4jbh8DGGBZ7wM4w==",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "TAPE",
    "paperLink": "https://proceedings.neurips.cc/paper/2019/hash/37f0e884fbad9667e38940169d0a3c95-Abstract.html",
    "description": "Tasks Assessing Protein Embeddings (TAPE) is a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology, including structure prediction, remote homology detection, and protein engineering.",
    "authors": [
      "Rao, R.",
      "et al."
    ],
    "githubLink": "https://github.com/songlab-cal/tape",
    "itemCount": "5 tasks",
    "source": "Scholar",
    "specs": "Protein sequences, varied task labels",
    "year": "2019",
    "id": "saved-1769639987508-3whhi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFT-FuONcFl3mjnSJfHPka7r-u-dM_eI5O9GII6kpFhnaZRjZgOSP4AeTNU_GFkwwxVE9F9j81rP2lcIYs99RAoPsBL6e0BArfsOyEP90OZIQdGLHkWTXmh4DcwdEZC9wRjBSOdPqZ1cj1BgUVN_jQlMF4gKqIR",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEb6rE5GOZayz2KH_1I4XtxTEXbtXv7AW66DgBrc_5IZRHqaL7SfRIZoiTdQai-DqvAkGljm1cFczJ-Nlcel3YINE01mYH7A6Hek_UZKfS0UzsIdjjD6tj3mVVXCo4WateVHup-xVRv-zXhDA22jC0o6nWPdFJ5u28Rc5CEaCwmfcDZDBMVDyWDB23-41NW8ly23mD67ie4djYL2gxY7zYHUv7n7TGDsTH_VvDMkMU3lLSw7U7HjUosMpLsQrkekQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMCz7Sr65L55Yw8HALgwNph3ddv0cFnVctjWrvrkWUne4VETrY_OPoR70MvixrupgLWeqUYNKG9UFJHgTJ6oVPimvyzYSVvvVqd1qoUN7vBYyfihxdKXV0YP142-nKkP_RlPxh_qbcD8T-dZLylAzC2rnXidqO8mgckgF-MmKUxNQmgkvElPdCSnYsaJzSv53lcI-NFrauMQFCQhzsPKdkV0uAErTgAKR4jbh8DGGBZ7wM4w==",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "IndicGenBench",
    "paperLink": "https://arxiv.org/abs/2404.16816",
    "description": "A benchmark for evaluating LLMs on user-facing generation tasks like summarization, translation, and QA across 29 Indic languages.",
    "authors": [
      "Harman Singh",
      "Nitish Gupta",
      "Dinesh Tewari",
      "Partha Talukdar"
    ],
    "githubLink": "https://github.com/google-research-datasets/indic-gen-bench",
    "itemCount": "29 languages",
    "source": "arXiv",
    "specs": "Text; Tasks: Generation (Summarization, MT, QA)",
    "year": "2024",
    "id": "saved-1769640091832-0c8pc",
    "groundingSources": []
  },
  {
    "title": "XGLUE",
    "paperLink": "https://arxiv.org/abs/2004.04720",
    "description": "A benchmark dataset to evaluate the performance of cross-lingual pre-trained models with respect to cross-lingual natural language understanding and generation, covering 11 tasks and 19 languages.",
    "authors": [
      "Yaobo Liang",
      "Nan Duan",
      "Yeyun Gong",
      "Ning Wu",
      "Fenfei Guo",
      "Weizhen Qi",
      "Ming Gong",
      "Linjun Shou",
      "Daxin Jiang",
      "Guihong Cao",
      "Xiaodong Fan",
      "Ruofei Zhang",
      "Rahul Agrawal",
      "Edward Cui",
      "Sining Wei",
      "Taroon Bharti",
      "Ying Qiao",
      "Jiun-Hung Chen",
      "Winnie Wu",
      "Shuguang Liu",
      "Fan Yang",
      "Daniel Campos",
      "Rangan Majumder",
      "Ming Zhou"
    ],
    "githubLink": "https://github.com/microsoft/XGLUE",
    "itemCount": "19 languages, 11 tasks",
    "source": "arXiv",
    "specs": "Text; Tasks: NLU and NLG",
    "year": "2020",
    "id": "saved-1769640091832-9ksdg",
    "groundingSources": []
  },
  {
    "title": "BugsInPy",
    "paperLink": "https://dl.acm.org/doi/10.1145/3368089.3417051",
    "description": "A dataset of existing bugs in Python programs inspired by Defects4J, designed to enable controlled testing and debugging studies for Python.",
    "authors": [
      "Ratnadira Widyasari",
      "Sheng Qin Sim",
      "Camellia Lok",
      "Haodi Qi",
      "Jack Phan",
      "Qijin Tay",
      "Constance Tan",
      "Fiona Wee",
      "Jodie Ethelda Tan",
      "Yuheng Yieh",
      "Brian Goh",
      "Ferdian Thung",
      "Hong Jin Kang",
      "Thong Hoang",
      "David Lo",
      "Eng Lieh Ouh"
    ],
    "githubLink": "https://github.com/soarsmu/BugsInPy",
    "itemCount": "493 bugs",
    "source": "Scholar",
    "specs": "Python source code, real-world bugs from 17 projects",
    "year": "2020",
    "id": "saved-1769640153774-jk2px",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKE_vQWL5zIi5PBYMz4xRpi3ToyCuU7oz8gebgj_ZQv7JVTmIZ89ToYwIwkJCqnaGiXHXfyr63CxZRUqzGOp4usUM0BQDVNd4KveamWVW34Fw4AMcmymHDSwkfgm2dBsQWHIckNs8=",
        "title": "youtube.com"
      }
    ]
  },
  {
    "title": "BigVul",
    "paperLink": "https://dl.acm.org/doi/10.1145/3379597.3387501",
    "description": "A large-scale C/C++ code vulnerability dataset containing code changes and CVE summaries, mined from open-source GitHub projects.",
    "authors": [
      "Jiahao Fan",
      "Yi Li",
      "Shaohua Wang",
      "Tien N. Nguyen"
    ],
    "githubLink": "https://github.com/ZeoVan/MSR_20_Code_vulnerability_CSV_Dataset",
    "itemCount": "3,754 vulnerabilities / 4,432 commits",
    "source": "arXiv",
    "specs": "C/C++, CSV format, CVE metadata",
    "year": "2020",
    "id": "saved-1769640153775-fok8y",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKE_vQWL5zIi5PBYMz4xRpi3ToyCuU7oz8gebgj_ZQv7JVTmIZ89ToYwIwkJCqnaGiXHXfyr63CxZRUqzGOp4usUM0BQDVNd4KveamWVW34Fw4AMcmymHDSwkfgm2dBsQWHIckNs8=",
        "title": "youtube.com"
      }
    ]
  },
  {
    "title": "Devign",
    "paperLink": "https://papers.nips.cc/paper/2019/hash/49265d2447bc3bb99505d314d55d3b91-Abstract.html",
    "description": "A dataset created for a graph neural network model of the same name, containing manually labeled functions as vulnerable or non-vulnerable from popular C libraries (Linux, FFmpeg, Qemu, Wireshark).",
    "authors": [
      "Yaqin Zhou",
      "Shangqing Liu",
      "Jingkai Siow",
      "Xiaoning Du",
      "Yang Liu"
    ],
    "githubLink": "https://github.com/epicosy/devign",
    "itemCount": "27,318 functions",
    "source": "Scholar",
    "specs": "C source code, Graph-based representations",
    "year": "2019",
    "id": "saved-1769640153775-uirfj",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKE_vQWL5zIi5PBYMz4xRpi3ToyCuU7oz8gebgj_ZQv7JVTmIZ89ToYwIwkJCqnaGiXHXfyr63CxZRUqzGOp4usUM0BQDVNd4KveamWVW34Fw4AMcmymHDSwkfgm2dBsQWHIckNs8=",
        "title": "youtube.com"
      }
    ]
  },
  {
    "title": "Draper VDISC",
    "paperLink": "https://osf.io/d45bw/",
    "description": "A massive dataset of over a million function-level C/C++ examples labeled by static analysis tools for potential vulnerabilities, used for training deep learning models.",
    "authors": [
      "Louis Y. Kim",
      "Rebecca L. Russell"
    ],
    "githubLink": "https://osf.io/d45bw/",
    "itemCount": "1.27 million functions",
    "source": "Scholar",
    "specs": "C/C++, static analysis labels (CWEs)",
    "year": "2018",
    "id": "saved-1769640153775-1ti3h",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKE_vQWL5zIi5PBYMz4xRpi3ToyCuU7oz8gebgj_ZQv7JVTmIZ89ToYwIwkJCqnaGiXHXfyr63CxZRUqzGOp4usUM0BQDVNd4KveamWVW34Fw4AMcmymHDSwkfgm2dBsQWHIckNs8=",
        "title": "youtube.com"
      }
    ]
  },
  {
    "title": "Defects4J",
    "paperLink": "https://dl.acm.org/doi/10.1145/2610384.2610404",
    "description": "A database of real faults from open source Java projects. It contains reproducible bugs, each with a test suite that triggers the bug, facilitating controlled experiments in software testing and debugging.",
    "authors": [
      "René Just",
      "D. Jalali",
      "M. D. Ernst"
    ],
    "githubLink": "https://github.com/rjust/defects4j",
    "itemCount": "835 bugs (v2.0)",
    "source": "Scholar",
    "specs": "Java source code, buggy and fixed versions, test suites",
    "year": "2014",
    "id": "saved-1769640153775-mi2u7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKE_vQWL5zIi5PBYMz4xRpi3ToyCuU7oz8gebgj_ZQv7JVTmIZ89ToYwIwkJCqnaGiXHXfyr63CxZRUqzGOp4usUM0BQDVNd4KveamWVW34Fw4AMcmymHDSwkfgm2dBsQWHIckNs8=",
        "title": "youtube.com"
      }
    ]
  },
  {
    "title": "Juliet Test Suite",
    "paperLink": "https://samate.nist.gov/SARD/testsuite.php",
    "description": "A comprehensive test suite of synthetic C/C++ and Java programs with documented weaknesses, created by the NSA Center for Assured Software and hosted by NIST.",
    "authors": [
      "NIST",
      "NSA"
    ],
    "githubLink": "https://github.com/arichardson/juliet-test-suite-c",
    "itemCount": "64,099 test cases (C/C++ v1.3)",
    "source": "Scholar",
    "specs": "C/C++, Java, synthetic test cases, CWE mapped",
    "year": "2010",
    "id": "saved-1769640153775-aeggj",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKE_vQWL5zIi5PBYMz4xRpi3ToyCuU7oz8gebgj_ZQv7JVTmIZ89ToYwIwkJCqnaGiXHXfyr63CxZRUqzGOp4usUM0BQDVNd4KveamWVW34Fw4AMcmymHDSwkfgm2dBsQWHIckNs8=",
        "title": "youtube.com"
      }
    ]
  },
  {
    "title": "fev-bench",
    "paperLink": "https://arxiv.org/abs/2509.26468",
    "description": "A realistic benchmark for time series forecasting that includes tasks with covariates and employs principled aggregation methods (win rates, skill scores) for robust model evaluation.",
    "authors": [
      "Oleksandr Shchur",
      "Abdul Fatir Ansari",
      "Caner Turkmen",
      "Lorenzo Stella",
      "Nick Erickson",
      "Pablo Guerron-Quintana",
      "Michael Bohlke-Schneider",
      "Yuyang Wang"
    ],
    "githubLink": "https://huggingface.co/autogluon/fev-bench",
    "itemCount": "100 forecasting tasks",
    "source": "Hugging Face",
    "specs": "Diverse domains, includes covariates (46 tasks)",
    "year": "2025",
    "id": "saved-1769640230836-k1nyy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "GIFT-Eval",
    "paperLink": "https://arxiv.org/abs/2410.10393",
    "description": "A benchmark designed to evaluate zero-shot forecasting capabilities of foundation models. It features a diverse collection of datasets spanning multiple domains and frequencies to prevent test data leakage.",
    "authors": [
      "Taha Aksu",
      "Gerald Woo",
      "Juncheng Liu",
      "Xu Liu",
      "Chenghao Liu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Doyen Sahoo"
    ],
    "githubLink": "https://github.com/SalesforceAIResearch/gift-eval",
    "itemCount": "28 datasets, 144,000 series",
    "source": "Hugging Face",
    "specs": "Multivariate/Univariate, varied frequencies, zero-shot focus",
    "year": "2024",
    "id": "saved-1769640230836-92dnz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "LargeST (Large-Scale Traffic Forecasting)",
    "paperLink": "https://openreview.net/forum?id=GoYKy28nSg",
    "description": "A comprehensive traffic forecasting benchmark derived from the Caltrans PeMS system, designed to address the limitations of smaller traffic datasets by covering a larger scale and longer duration.",
    "authors": [
      "Xu Liu",
      "Yuxuan Liang",
      "Chao Huang",
      "Yu Zheng",
      "Bryan Hooi",
      "Roger Zimmermann"
    ],
    "githubLink": "https://github.com/liuxu77/LargeST",
    "itemCount": "8,600 sensors, 5 years data",
    "source": "Scholar",
    "specs": "Graph-based, multivariate, 5-min intervals",
    "year": "2023",
    "id": "saved-1769640230836-t1hi7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "Monash Time Series Forecasting Archive",
    "paperLink": "https://arxiv.org/abs/2105.06643",
    "description": "A comprehensive archive containing diverse time series datasets from varied domains (finance, traffic, weather, etc.) to facilitate the evaluation of global forecasting models. It unifies data into a standard .tsf format.",
    "authors": [
      "Rakshitha Godahewa",
      "Christoph Bergmeir",
      "Geoffrey I. Webb",
      "Rob J. Hyndman",
      "Pablo Montero-Manso"
    ],
    "githubLink": "https://github.com/rakshitha123/TSForecasting",
    "itemCount": "30 datasets (58 variations)",
    "source": "arXiv",
    "specs": ".tsf format, varied frequencies and lengths, univariate",
    "year": "2021",
    "id": "saved-1769640230836-ljhwd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "ETT (Electricity Transformer Temperature)",
    "paperLink": "https://arxiv.org/abs/2012.07436",
    "description": "A widely used benchmark for Long Sequence Time-Series Forecasting (LSTF). It consists of data collected from electricity transformers, including oil temperature and power load features.",
    "authors": [
      "Haoyi Zhou",
      "Shanghang Zhang",
      "Jieqi Peng",
      "Shuai Zhang",
      "Jianxin Li",
      "Hui Xiong",
      "Wancai Zhang"
    ],
    "githubLink": "https://github.com/zhouhaoyi/ETDataset",
    "itemCount": "4 datasets (~70k points each)",
    "source": "arXiv",
    "specs": "Multivariate, hourly/15-min frequency",
    "year": "2021",
    "id": "saved-1769640230836-28nir",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "M5 Forecasting Competition",
    "paperLink": "https://doi.org/10.1016/j.ijforecast.2021.11.013",
    "description": "A benchmark focused on hierarchical forecasting using retail sales data from Walmart. It includes explanatory variables such as price, promotions, and calendar events.",
    "authors": [
      "Spyros Makridakis",
      "Evangelos Spiliotis",
      "Vassilios Assimakopoulos"
    ],
    "githubLink": "https://github.com/Mcompetitions/M5-methods",
    "itemCount": "42,840 hierarchical series",
    "source": "Scholar",
    "specs": "Hierarchical, multivariate, daily frequency",
    "year": "2020",
    "id": "saved-1769640230836-xh5nw",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "M4 Forecasting Competition",
    "paperLink": "https://doi.org/10.1016/j.ijforecast.2019.04.014",
    "description": "A large-scale competition dataset designed to evaluate forecasting methods. It includes a wide range of domains and frequencies, emphasizing both point forecasts and prediction intervals.",
    "authors": [
      "Spyros Makridakis",
      "Evangelos Spiliotis",
      "Vassilios Assimakopoulos"
    ],
    "githubLink": "https://github.com/Mcompetitions/M4-methods",
    "itemCount": "100,000 time series",
    "source": "Scholar",
    "specs": "Univariate, Yearly/Quarterly/Monthly/Weekly/Daily/Hourly",
    "year": "2018",
    "id": "saved-1769640230836-odg79",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "Electricity Load Diagrams 2011-2014",
    "paperLink": "https://archive.ics.uci.edu/dataset/321/electricity+load+diagrams+20112014",
    "description": "A classic benchmark dataset containing electricity consumption data from 370 clients. It is frequently used to evaluate multivariate time series forecasting models.",
    "authors": [
      "Artur Trindade"
    ],
    "githubLink": "https://github.com/laiguokun/multivariate-time-series-data",
    "itemCount": "370 clients (321 used in benchmarks)",
    "source": "UCI (often cited via arXiv)",
    "specs": "Multivariate, 15-min intervals (often aggregated to hourly)",
    "year": "2015",
    "id": "saved-1769640230837-f80q3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzyjPOElYHHBtqCqrkKVV3es6zi5oAp-8sDLS5PLaa4zlpBDtvXxGmj16MS-fW9eDjt5I0mf_YrP8Nh7AWeS8G-MtdRShHJyy2m6bsFrHf67gzPb5hABnvNU4Yk_8Di2e8K3vs",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE--3QhiNlhNelV2Bmlf7cdS6Dcfdr8BMlFt3LSJY8A0SZk1u0trzfnPrfVNcTEBinyZS1hihmvcSqGch2FR2L3fmHLY5Mn604D8s98FBr1t9IpZsilPzVb3GTN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiTG5R1oDC-4ROnI2JlNyZd2quvw3juUrpjEfFdzqBPlM5_j7a4kLMwW2b9zTLdFjltxXuPJCccHInoKnZ6qzij_L6UIyIP918Qq1VEubihRRNLyogp4v5XVDjO-d-0L_x7QHxkwpCIaQ2n1U=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfF9zzLm-en8skw0W_raky_8-bsOWFelv_TCTZcUGB7R-pEuW12Vt-GFeqxscKjFwNUbGPDc39-5f12yK7eZNtM8DYEddLv_ALAAv32j3saqp63dJWUs0YGX2PYwcfUew92lwiM1XCua8fX2PMu4TLWRuhryVvbe2KwCJbjbWvlc8wd9A-DpSVH2p6fs0uMT4XqzgVR9ZMGMF7aLXFBFmHEne5IZC1qHOQcFF0AW6UobEei7y1T9zr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKVuPcCX1NOjLPoGkc0aBXvb-9BrseR8OQ7iy84gX92mc8lUbCwCihg-uO79Us2u7Vy32GJjqL3Co0jlEneCCoiUi_KYhrdUvbYupxHvpqsgihpkDqjd4eJoH3MqgfcnJ9cUw=",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "IMed-361M",
    "paperLink": "https://arxiv.org/abs/2411.07885",
    "description": "A large-scale benchmark for interactive medical image segmentation. It features a massive collection of medical images with dense, interactive masks generated and verified using foundation models to simulate human-in-the-loop annotation workflows.",
    "authors": [
      "Yuanfeng Ji",
      "Zhuping Xu",
      "Jiawei Liu",
      "Lequan Yu",
      "et al."
    ],
    "githubLink": "https://github.com/uni-medical/IMIS-Bench",
    "itemCount": "6.4 million images, 361 million masks",
    "source": "arXiv, Hugging Face",
    "specs": "Image (Medical), Segmentation Masks",
    "year": "2024",
    "id": "saved-1769640302487-8tjmd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQhXjwxWK3AA6h6a8wou13dGtZovx1xxLyC1KTu98F9U1_N0YCBQiTJO1ghcZuP66ySRFohcqrJQPi5JNqAn7cn3hye7-YjJ_oCuKIgDLkh7SIl-IptKmUa-D-k7XGfl105XooJKPTV64VktZ3AA5VM6V5Epju-haymGEzZRf9MvsNUYWJ2hbXGWBCkfiCp3w3-twghOTOs6VuE4kYyWzB_A==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvjPO0a7sUGmnuWr7gV0qzMf14Ye0lRg0NkoMZakR-8eSpUb8DAXT1UsQOZ5x8fIFs9ieNw4kpYgN_-H-mRi3WqgbkODv1k96BHhomDOixLK1Y5OA-0YNlwYkdADaHVqp_GVAdCodYEbI=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFH2fyAvjb8MCACrYhajnNXQQtYdyItXF_IOHqx0yx_W3MeO6Gp1OdZAPoDAm2HvDbF3unl1QadzpRzsG78brLXU7aoNO9b5jCEiNj2Bv0DL3xSQUiR4fw7hYbGY8GFfP754v8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFff18HKfsRaaFJQlWwLAmo2huI5NZsvNa4lTSlGUBB4oCkY_v55CQS8xWwLFFpS31OvtrFNT3QqEBtf8rG0FJau6MpAKSmqPUQPG_B4_w4-NxLisNztHC2-yhi5Dit9re2mq0UW_8i5caWHSw14zzE7ogxWOTzcr7cS5nH4nmB7iFo2hANeY19gJWQnyhdtwK8Q_nl5ZDRKE5nEJx2UdR4eMjX6O61R2YIVK9nJf6XBjQlPJgWualo5-rw3QEnpPNRhiw=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEdzJAkpCmb8CSqUyz4n_ug7nfUxqo2UuvmecXA3KZQrotZ-nXADzOkwGLJNxSGxnqcjoo_KsqKSwAAZfspbky0e-WupjuEjnTXUtAy5rIBwq-HfG8R5Vlql9tK505pvBUW_AtL7xl4LaStpovK87ZMxuLsfyQ",
        "title": "oup.com"
      }
    ]
  },
  {
    "title": "CDALBench",
    "paperLink": "https://arxiv.org/abs/2408.00426",
    "description": "A cross-domain benchmark for Active Learning that includes tasks in computer vision, natural language processing, and tabular learning. It is designed to evaluate AL methods robustly across different modalities and settings.",
    "authors": [
      "Thorben Werner",
      "Johannes Burchert",
      "Maximilian Stubbemann",
      "Lars Schmidt-Thieme"
    ],
    "githubLink": "https://github.com/ariapoy/active-learning-benchmark",
    "itemCount": "Multiple datasets across domains",
    "source": "arXiv",
    "specs": "Multimodal (Image, Text, Tabular)",
    "year": "2024",
    "id": "saved-1769640302487-5p2f8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQhXjwxWK3AA6h6a8wou13dGtZovx1xxLyC1KTu98F9U1_N0YCBQiTJO1ghcZuP66ySRFohcqrJQPi5JNqAn7cn3hye7-YjJ_oCuKIgDLkh7SIl-IptKmUa-D-k7XGfl105XooJKPTV64VktZ3AA5VM6V5Epju-haymGEzZRf9MvsNUYWJ2hbXGWBCkfiCp3w3-twghOTOs6VuE4kYyWzB_A==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvjPO0a7sUGmnuWr7gV0qzMf14Ye0lRg0NkoMZakR-8eSpUb8DAXT1UsQOZ5x8fIFs9ieNw4kpYgN_-H-mRi3WqgbkODv1k96BHhomDOixLK1Y5OA-0YNlwYkdADaHVqp_GVAdCodYEbI=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFH2fyAvjb8MCACrYhajnNXQQtYdyItXF_IOHqx0yx_W3MeO6Gp1OdZAPoDAm2HvDbF3unl1QadzpRzsG78brLXU7aoNO9b5jCEiNj2Bv0DL3xSQUiR4fw7hYbGY8GFfP754v8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFff18HKfsRaaFJQlWwLAmo2huI5NZsvNa4lTSlGUBB4oCkY_v55CQS8xWwLFFpS31OvtrFNT3QqEBtf8rG0FJau6MpAKSmqPUQPG_B4_w4-NxLisNztHC2-yhi5Dit9re2mq0UW_8i5caWHSw14zzE7ogxWOTzcr7cS5nH4nmB7iFo2hANeY19gJWQnyhdtwK8Q_nl5ZDRKE5nEJx2UdR4eMjX6O61R2YIVK9nJf6XBjQlPJgWualo5-rw3QEnpPNRhiw=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEdzJAkpCmb8CSqUyz4n_ug7nfUxqo2UuvmecXA3KZQrotZ-nXADzOkwGLJNxSGxnqcjoo_KsqKSwAAZfspbky0e-WupjuEjnTXUtAy5rIBwq-HfG8R5Vlql9tK505pvBUW_AtL7xl4LaStpovK87ZMxuLsfyQ",
        "title": "oup.com"
      }
    ]
  },
  {
    "title": "DynaSent: A Dynamic Benchmark for Sentiment Analysis",
    "paperLink": "https://arxiv.org/abs/2012.15349",
    "description": "A ternary (positive/negative/neutral) sentiment analysis dataset created using the Dynabench platform. It combines naturally occurring sentences with examples generated by human annotators specifically designed to fool state-of-the-art models, creating a 'dynamic' benchmark that evolves over time.",
    "authors": [
      "Christopher Potts",
      "Zhengxuan Wu",
      "Atticus Geiger",
      "Douwe Kiela"
    ],
    "githubLink": "https://github.com/cgpotts/dynasent",
    "itemCount": "121,634 sentences",
    "source": "arXiv, Hugging Face",
    "specs": "Text (Sentiment Analysis)",
    "year": "2020",
    "id": "saved-1769640302487-9tzjh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQhXjwxWK3AA6h6a8wou13dGtZovx1xxLyC1KTu98F9U1_N0YCBQiTJO1ghcZuP66ySRFohcqrJQPi5JNqAn7cn3hye7-YjJ_oCuKIgDLkh7SIl-IptKmUa-D-k7XGfl105XooJKPTV64VktZ3AA5VM6V5Epju-haymGEzZRf9MvsNUYWJ2hbXGWBCkfiCp3w3-twghOTOs6VuE4kYyWzB_A==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvjPO0a7sUGmnuWr7gV0qzMf14Ye0lRg0NkoMZakR-8eSpUb8DAXT1UsQOZ5x8fIFs9ieNw4kpYgN_-H-mRi3WqgbkODv1k96BHhomDOixLK1Y5OA-0YNlwYkdADaHVqp_GVAdCodYEbI=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFH2fyAvjb8MCACrYhajnNXQQtYdyItXF_IOHqx0yx_W3MeO6Gp1OdZAPoDAm2HvDbF3unl1QadzpRzsG78brLXU7aoNO9b5jCEiNj2Bv0DL3xSQUiR4fw7hYbGY8GFfP754v8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFff18HKfsRaaFJQlWwLAmo2huI5NZsvNa4lTSlGUBB4oCkY_v55CQS8xWwLFFpS31OvtrFNT3QqEBtf8rG0FJau6MpAKSmqPUQPG_B4_w4-NxLisNztHC2-yhi5Dit9re2mq0UW_8i5caWHSw14zzE7ogxWOTzcr7cS5nH4nmB7iFo2hANeY19gJWQnyhdtwK8Q_nl5ZDRKE5nEJx2UdR4eMjX6O61R2YIVK9nJf6XBjQlPJgWualo5-rw3QEnpPNRhiw=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEdzJAkpCmb8CSqUyz4n_ug7nfUxqo2UuvmecXA3KZQrotZ-nXADzOkwGLJNxSGxnqcjoo_KsqKSwAAZfspbky0e-WupjuEjnTXUtAy5rIBwq-HfG8R5Vlql9tK505pvBUW_AtL7xl4LaStpovK87ZMxuLsfyQ",
        "title": "oup.com"
      }
    ]
  },
  {
    "title": "OpenAI Summarize from Feedback",
    "paperLink": "https://arxiv.org/abs/2009.01325",
    "description": "A dataset of human preferences between pairs of summaries generated by a language model. This dataset was used to train a reward model for RLHF to improve text summarization quality beyond standard supervised baselines.",
    "authors": [
      "Nisan Stiennon",
      "Long Ouyang",
      "Jeffrey Wu",
      "Daniel Ziegler",
      "Ryan Lowe",
      "Chelsea Voss",
      "Alec Radford",
      "Dario Amodei",
      "Paul Christiano"
    ],
    "githubLink": "https://github.com/openai/summarize-from-feedback",
    "itemCount": "64,832 summary comparisons",
    "source": "Hugging Face, arXiv",
    "specs": "Text (Summarization, Preference Pairs)",
    "year": "2020",
    "id": "saved-1769640302487-k606l",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQhXjwxWK3AA6h6a8wou13dGtZovx1xxLyC1KTu98F9U1_N0YCBQiTJO1ghcZuP66ySRFohcqrJQPi5JNqAn7cn3hye7-YjJ_oCuKIgDLkh7SIl-IptKmUa-D-k7XGfl105XooJKPTV64VktZ3AA5VM6V5Epju-haymGEzZRf9MvsNUYWJ2hbXGWBCkfiCp3w3-twghOTOs6VuE4kYyWzB_A==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEvjPO0a7sUGmnuWr7gV0qzMf14Ye0lRg0NkoMZakR-8eSpUb8DAXT1UsQOZ5x8fIFs9ieNw4kpYgN_-H-mRi3WqgbkODv1k96BHhomDOixLK1Y5OA-0YNlwYkdADaHVqp_GVAdCodYEbI=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFH2fyAvjb8MCACrYhajnNXQQtYdyItXF_IOHqx0yx_W3MeO6Gp1OdZAPoDAm2HvDbF3unl1QadzpRzsG78brLXU7aoNO9b5jCEiNj2Bv0DL3xSQUiR4fw7hYbGY8GFfP754v8=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFff18HKfsRaaFJQlWwLAmo2huI5NZsvNa4lTSlGUBB4oCkY_v55CQS8xWwLFFpS31OvtrFNT3QqEBtf8rG0FJau6MpAKSmqPUQPG_B4_w4-NxLisNztHC2-yhi5Dit9re2mq0UW_8i5caWHSw14zzE7ogxWOTzcr7cS5nH4nmB7iFo2hANeY19gJWQnyhdtwK8Q_nl5ZDRKE5nEJx2UdR4eMjX6O61R2YIVK9nJf6XBjQlPJgWualo5-rw3QEnpPNRhiw=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGEdzJAkpCmb8CSqUyz4n_ug7nfUxqo2UuvmecXA3KZQrotZ-nXADzOkwGLJNxSGxnqcjoo_KsqKSwAAZfspbky0e-WupjuEjnTXUtAy5rIBwq-HfG8R5Vlql9tK505pvBUW_AtL7xl4LaStpovK87ZMxuLsfyQ",
        "title": "oup.com"
      }
    ]
  },
  {
    "title": "MMAD (Multimodal Anomaly Detection)",
    "paperLink": "https://arxiv.org/abs/2410.12743",
    "description": "A full-spectrum benchmark for Multimodal Large Language Models (MLLMs) in industrial anomaly detection. It evaluates models on key subtasks like anomaly description, localization, and reasoning using image-text pairs.",
    "authors": [
      "Jiaqi Tu",
      "Guodong Wang",
      "Shoaib Ahmed Siddiqui"
    ],
    "githubLink": "https://github.com/Chopper-233/MMAD",
    "itemCount": "39,672 questions",
    "source": "arXiv",
    "specs": "Multimodal (Image + Text), 8,366 industrial images, diverse question types",
    "year": "2025",
    "id": "saved-1769640366964-l0cdn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "NLP-ADBench",
    "paperLink": "https://arxiv.org/abs/2412.04784",
    "description": "The comprehensive benchmark for anomaly detection in Natural Language Processing (NLP). It includes multiple curated datasets and evaluates state-of-the-art algorithms, addressing the scarcity of standard benchmarks for text-based anomaly detection.",
    "authors": [
      "Yuangang Li",
      "Yue Zhao"
    ],
    "githubLink": "https://huggingface.co/datasets/Yuangang/NLP-ADBench",
    "itemCount": "8 datasets",
    "source": "arXiv",
    "specs": "Text data, various NLP domains, 19 algorithms evaluated",
    "year": "2024",
    "id": "saved-1769640366964-aifd7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MVTec LOCO AD (Logical Constraints Anomaly Detection)",
    "paperLink": "https://arxiv.org/abs/2206.07223",
    "description": "Designed to evaluate algorithms on detecting 'logical anomalies'—violations of underlying constraints (e.g., correct object in wrong location) rather than just structural defects. It complements MVTec AD by focusing on these higher-level dependencies.",
    "authors": [
      "Paul Bergmann",
      "Kilian Batzner",
      "Michael Fauser",
      "David Sattlegger",
      "Carsten Steger"
    ],
    "githubLink": "https://www.mvtec.com/company/research/datasets/mvtec-loco",
    "itemCount": "3644 images",
    "source": "arXiv",
    "specs": "5 categories, structural and logical anomalies, pixel-precise ground truth",
    "year": "2022",
    "id": "saved-1769640366964-xbl5w",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "VisA (Visual Anomaly Dataset)",
    "paperLink": "https://arxiv.org/abs/2207.14315",
    "description": "A large-scale industrial anomaly detection dataset designed for self-supervised pre-training benchmarks. It features complex structures, multiple instances per view, and 12 distinct objects across 3 domains, making it more challenging than previous benchmarks.",
    "authors": [
      "Yujie Zou",
      "Jongheon Jeong",
      "Latha Pemula",
      "Dongqing Zhang",
      "Hampton D. Deal"
    ],
    "githubLink": "https://github.com/amazon-research/spot-diff",
    "itemCount": "10,821 images",
    "source": "arXiv",
    "specs": "12 objects, 3 domains, high-resolution color images, pixel-level masks",
    "year": "2022",
    "id": "saved-1769640366964-s64s9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ADBench (Anomaly Detection Benchmark)",
    "paperLink": "https://arxiv.org/abs/2206.09426",
    "description": "A comprehensive benchmark for tabular anomaly detection. It evaluates 30 algorithms across 57 datasets, covering unsupervised, semi-supervised, and supervised settings, and analyzes performance under varying levels of noise and anomaly types.",
    "authors": [
      "Songqiao Han",
      "Xiyang Hu",
      "Hailiang Huang",
      "Minqi Jiang",
      "Yue Zhao"
    ],
    "githubLink": "https://github.com/Minqi824/ADBench",
    "itemCount": "57 datasets",
    "source": "arXiv",
    "specs": "Tabular data, diverse domains (healthcare, finance, etc.), 30 algorithms evaluated",
    "year": "2022",
    "id": "saved-1769640366965-4z8fy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "TSB-UAD (Time Series Benchmark for Unsupervised Anomaly Detection)",
    "paperLink": "https://www.vldb.org/pvldb/vol15/p1697-papenbrock.pdf",
    "description": "An end-to-end benchmark suite for univariate time-series anomaly detection. It includes a large collection of datasets and implementations of various anomaly detection algorithms to evaluate detection accuracy and efficiency.",
    "authors": [
      "Paul Boniol",
      "Michele Linardi",
      "Federico Roncallo",
      "Themis Palpanas"
    ],
    "githubLink": "https://github.com/TheDataverse/TSB-UAD",
    "itemCount": "1980 time series",
    "source": "Scholar",
    "specs": "Univariate time series, diverse domains (synthetic and real)",
    "year": "2022",
    "id": "saved-1769640366965-op9ep",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MVTec AD (MVTec Anomaly Detection Dataset)",
    "paperLink": "https://www.mvtec.com/fileadmin/Redaktion/mvtec.com/company/research/datasets/mvtec_ad.pdf",
    "description": "A comprehensive real-world dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over 5000 high-resolution images divided into fifteen different object and texture categories, each with defect-free training images and a test set containing various defects.",
    "authors": [
      "Paul Bergmann",
      "Michael Fauser",
      "David Sattlegger",
      "Carsten Steger"
    ],
    "githubLink": "https://github.com/openvinotoolkit/anomalib",
    "itemCount": "5354 images",
    "source": "Scholar",
    "specs": "15 categories, high-resolution images (700x700 to 1024x1024), pixel-precise ground truth annotations",
    "year": "2019",
    "id": "saved-1769640366965-dlwbo",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ShanghaiTech Campus Dataset",
    "paperLink": "https://arxiv.org/abs/1707.09664",
    "description": "A large-scale video anomaly detection dataset captured in a university campus. It features complex light conditions, diverse camera angles, and 130 abnormal events like chasing, brawling, and sudden motion, annotated at the pixel/frame level.",
    "authors": [
      "W. Liu",
      "W. Luo",
      "D. Lian",
      "S. Gao"
    ],
    "githubLink": "https://github.com/svip-lab/MLEP",
    "itemCount": "270,000 frames",
    "source": "arXiv",
    "specs": "13 scenes, 130 abnormal events, video frames, pixel-level ground truth",
    "year": "2018",
    "id": "saved-1769640366965-kn3ta",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "UCF-Crime",
    "paperLink": "https://arxiv.org/abs/1801.04264",
    "description": "A large-scale dataset of real-world surveillance videos covering 13 types of real-world anomalies such as abuse, arrest, arson, and burglary. It focuses on weakly supervised learning where only video-level labels are available during training.",
    "authors": [
      "Waqas Sultani",
      "Chen Chen",
      "Mubarak Shah"
    ],
    "githubLink": "https://github.com/WaqasSultani/AnomalyDetectionCVPR2018",
    "itemCount": "1900 videos",
    "source": "arXiv",
    "specs": "128 hours of video, 13 anomaly classes, real-world surveillance footage",
    "year": "2018",
    "id": "saved-1769640366965-ghx92",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "SWaT (Secure Water Treatment)",
    "paperLink": "https://itrust.sutd.edu.sg/itrust-labs_datasets/dataset_info/",
    "description": "A cyber-physical system dataset collected from a scaled-down real-world water treatment testbed. It contains continuous operational data including sensors and actuators under normal conditions and various cyber-attack scenarios.",
    "authors": [
      "Jonathan Goh",
      "Sridhar Adepu",
      "Marcus Tan",
      "Zi Shan Lee"
    ],
    "githubLink": "https://itrust.sutd.edu.sg/itrust-labs_datasets",
    "itemCount": "11 days continuous data",
    "source": "Scholar",
    "specs": "51 sensors/actuators, time-series, industrial control system (ICS) traffic",
    "year": "2016",
    "id": "saved-1769640366965-bb7wz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJQzC7PCL5lTC9ONs2jL9d7Xsnfp4AMhjRm0YRpaGbyogMI2HVzURGZYIeuwl6EP9YkIM-MRqhSfP0E5cgzW409FK8gOaRw9rIWWfKZlA_HtEp767CAMnjpSA-It7jIwkC",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGwTwjUHxHqEUeb1NvYTSCr2-HwwDNgHF32Lp4j1eaVJ_BqbGvZu72NZOdIZmRNj6vyJGU33rpNcsmEJ7cQ7sSqAto0JEP7i1nC7Sm9Lo-f6kiKSa8A8yqOQoFRhofMyMfQ8ibGv_EPRYCzZVn_1Iz4vLYUXRApszzIPNqLTZ6O2bjz0BtQX-v7rha6U6jq2COx2tTmHnU8k3L7JMTekYR2Pbbl8Iho52zufXDZAsZMIUtzh_CWTuiDOjUZkloDa2qX1Hn9Exsy5HVVbQQ6L4TMH_L0bs07CVo=",
        "title": "amazon.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuh8hZLzX7mKaRvqH3D4Lb2Ast_eHhlXsq2HIit131-jLx1Nq_4wYQ7WryUv-TMAMVVQPgw9Yz_qkZEi07bz1Ksl7-eN2Ksmtn6n1vbA7c4KHYD81nppmCvrsb-Vu7L-52c3jiVPgX1wSmSZTHup7mc3ACbYzouWJTiapDLDNxL1z_MYZ35A==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreNztpBU5NYCz0Gvvwx57MR0-65I_4UeXR-v3TY7afI_W1lOQT-rkMaUrdAt64k9WS2MjHd6snX-cHCXJGn1X2D8RxNF_AIrvs_vgqBWoagpKEVJqxbS_sQH2Cuy_pU10Yj4z6xgfNA==",
        "title": "ucf.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEdjyiRlS5fsVowhDD7yEEg-TuwjEqNtXZRnTnm_eJCpt8GfhZkP-8_DCHZZquRvVsAz1FpljQAgtHCytrRWy9KYclovTFl4B1jtvL7jHgEq6XXQ3stn0qxf_SqQRlYRcx4SnOAhgtxzI2-eLsz2s255np059qmq-DuUbH_kDXSdG2DX6O6jEXRhjR1pPA=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGxEZGU1XkBhyybanTn15pnU_2wz4TA4LcTNsESfgls49NlsSsYAPlEb30I27y2e6j2Q-Au27RsbTP8JOShxVJql5G93ntSryKhXdznfZS2XDuhGxgtpksGG_mW",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MEETI",
    "paperLink": "https://arxiv.org/abs/2507.15255",
    "description": "A large-scale multimodal ECG dataset (MIMIC-IV-Ext ECG-Text-Image) that synchronizes raw waveform data, high-resolution plotted images, beat-level features, and detailed textual interpretations generated by LLMs (GPT-4o) and experts.",
    "authors": [
      "Deyun Zhang",
      "Xiang Lan",
      "Shijia Geng",
      "Qinghao Zhao",
      "Sumei Fan",
      "Mengling Feng",
      "Shenda Hong"
    ],
    "githubLink": "https://github.com/PKUDigitalHealth/MIMIC-IV-ECG-Ext-Text-Image",
    "itemCount": "~800,000 samples",
    "source": "arXiv",
    "specs": "12-lead ECG Signal (500Hz), ECG Images, Tabular Features, Text Reports",
    "year": "2025",
    "id": "saved-1769640432814-vesvp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxHduyN7tpbM4yTo8W9Xqc3ZcFXhC8vIjSVXKUOjcr01d8mdoYdmDMpfD1gjWpWOpNrXMHWzngxEpCSatzWDIpA4eQ5OVg9aXC--fVPbTKWA-lDtjJBXPwyW2wZMPHFXTUBsxemM-7iZpoV_H7j4OuizOqg1y3b_RAmOHWFrZsPJQwytISKcp_t3qLDYT6Paj7SfWu15Ia6EUQyWk7J-w=",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnfBjpzJFqc5KTc4W-eQ0Mssq5q66ApdaHYb7ktQuxtqQVda_g6sdCwELR6tmK-6_2pzD3G2RayuiqFR_o5ubBnw_dK6TBbrGKngr4S9SDGPaLbbyxRjQlrmoe2ow=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY4vSg9_01RT02gjM4oNO726w948sBN7cWTc5oMI90Jp-4NOoy8BL0KP7T40WfTzNCXTRkll-9Ts01rWAOICdfQYKjm02GqbyTZfwYHFq8qoGXptI8THxO44Gk468=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY1GQBOEhKW4tUTufbvSf2cfWW2KUfoZ15itLOnUe5ZoMPXNnr00O4FCA-jk1d2guoWyQWHvlE87fTu2jvB9bip9222yXsdCwFMq40WdiX71xo8w4eMqAkl52uL14YqbzW2A==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "ECG-Expert-QA",
    "paperLink": "https://arxiv.org/abs/2502.xxxx",
    "description": "A multimodal dataset for evaluating diagnostic capabilities in ECG interpretation. It contains meticulously curated question-answer pairs covering fundamental diagnostic tasks and complex clinical cases, validated by experts.",
    "authors": [
      "Qinghao Zhao",
      "Deyun Zhang",
      "et al."
    ],
    "githubLink": "N/A",
    "itemCount": "47,211 QA pairs",
    "source": "arXiv",
    "specs": "Text (QA), ECG Signals",
    "year": "2025",
    "id": "saved-1769640432814-uubfn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxHduyN7tpbM4yTo8W9Xqc3ZcFXhC8vIjSVXKUOjcr01d8mdoYdmDMpfD1gjWpWOpNrXMHWzngxEpCSatzWDIpA4eQ5OVg9aXC--fVPbTKWA-lDtjJBXPwyW2wZMPHFXTUBsxemM-7iZpoV_H7j4OuizOqg1y3b_RAmOHWFrZsPJQwytISKcp_t3qLDYT6Paj7SfWu15Ia6EUQyWk7J-w=",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnfBjpzJFqc5KTc4W-eQ0Mssq5q66ApdaHYb7ktQuxtqQVda_g6sdCwELR6tmK-6_2pzD3G2RayuiqFR_o5ubBnw_dK6TBbrGKngr4S9SDGPaLbbyxRjQlrmoe2ow=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY4vSg9_01RT02gjM4oNO726w948sBN7cWTc5oMI90Jp-4NOoy8BL0KP7T40WfTzNCXTRkll-9Ts01rWAOICdfQYKjm02GqbyTZfwYHFq8qoGXptI8THxO44Gk468=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY1GQBOEhKW4tUTufbvSf2cfWW2KUfoZ15itLOnUe5ZoMPXNnr00O4FCA-jk1d2guoWyQWHvlE87fTu2jvB9bip9222yXsdCwFMq40WdiX71xo8w4eMqAkl52uL14YqbzW2A==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "MEIT",
    "paperLink": "https://arxiv.org/abs/2403.04945",
    "description": "A multimodal instruction tuning dataset and framework constructed from MIMIC-IV-ECG and PTB-XL. It is designed to train Large Language Models for automated ECG report generation by aligning ECG signals with textual instructions and reports.",
    "authors": [
      "Zhongwei Wan",
      "Che Liu",
      "Xin Wang",
      "Chaofan Tao",
      "Hui Shen",
      "Zhenwu Peng",
      "Jie Fu",
      "Rossella Arcucci",
      "Huaxiu Yao",
      "Mi Zhang"
    ],
    "githubLink": "https://github.com/AIoT-MLSys-Lab/MEIT",
    "itemCount": "~800,000 ECG-report pairs (MIMIC-IV) + 21k pairs (PTB-XL)",
    "source": "arXiv",
    "specs": "Text (Instructions, Reports), 12-lead ECG signals",
    "year": "2024",
    "id": "saved-1769640432814-i92ws",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxHduyN7tpbM4yTo8W9Xqc3ZcFXhC8vIjSVXKUOjcr01d8mdoYdmDMpfD1gjWpWOpNrXMHWzngxEpCSatzWDIpA4eQ5OVg9aXC--fVPbTKWA-lDtjJBXPwyW2wZMPHFXTUBsxemM-7iZpoV_H7j4OuizOqg1y3b_RAmOHWFrZsPJQwytISKcp_t3qLDYT6Paj7SfWu15Ia6EUQyWk7J-w=",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnfBjpzJFqc5KTc4W-eQ0Mssq5q66ApdaHYb7ktQuxtqQVda_g6sdCwELR6tmK-6_2pzD3G2RayuiqFR_o5ubBnw_dK6TBbrGKngr4S9SDGPaLbbyxRjQlrmoe2ow=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY4vSg9_01RT02gjM4oNO726w948sBN7cWTc5oMI90Jp-4NOoy8BL0KP7T40WfTzNCXTRkll-9Ts01rWAOICdfQYKjm02GqbyTZfwYHFq8qoGXptI8THxO44Gk468=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY1GQBOEhKW4tUTufbvSf2cfWW2KUfoZ15itLOnUe5ZoMPXNnr00O4FCA-jk1d2guoWyQWHvlE87fTu2jvB9bip9222yXsdCwFMq40WdiX71xo8w4eMqAkl52uL14YqbzW2A==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "ECGBench",
    "paperLink": "https://arxiv.org/abs/2410.19008",
    "description": "A comprehensive benchmark for evaluating Multimodal LLMs on ECG interpretation. It aggregates multiple datasets (including PTB-XL, CPSC, ECG-QA) to cover tasks like abnormality detection, report generation, and question answering.",
    "authors": [
      "Rongyao Liu",
      "Penghui Zhu",
      "Jiaxuan Wang",
      "Hao Cheng",
      "Yingnan Hu",
      "Jiahui Pan",
      "Yinjie Lei"
    ],
    "githubLink": "https://github.com/AIMedLab/PULSE",
    "itemCount": "Aggregates 9 datasets (variable size per task)",
    "source": "Hugging Face",
    "specs": "ECG Images, Text (QA, Reports, Captions)",
    "year": "2024",
    "id": "saved-1769640432814-01nbh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxHduyN7tpbM4yTo8W9Xqc3ZcFXhC8vIjSVXKUOjcr01d8mdoYdmDMpfD1gjWpWOpNrXMHWzngxEpCSatzWDIpA4eQ5OVg9aXC--fVPbTKWA-lDtjJBXPwyW2wZMPHFXTUBsxemM-7iZpoV_H7j4OuizOqg1y3b_RAmOHWFrZsPJQwytISKcp_t3qLDYT6Paj7SfWu15Ia6EUQyWk7J-w=",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnfBjpzJFqc5KTc4W-eQ0Mssq5q66ApdaHYb7ktQuxtqQVda_g6sdCwELR6tmK-6_2pzD3G2RayuiqFR_o5ubBnw_dK6TBbrGKngr4S9SDGPaLbbyxRjQlrmoe2ow=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY4vSg9_01RT02gjM4oNO726w948sBN7cWTc5oMI90Jp-4NOoy8BL0KP7T40WfTzNCXTRkll-9Ts01rWAOICdfQYKjm02GqbyTZfwYHFq8qoGXptI8THxO44Gk468=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY1GQBOEhKW4tUTufbvSf2cfWW2KUfoZ15itLOnUe5ZoMPXNnr00O4FCA-jk1d2guoWyQWHvlE87fTu2jvB9bip9222yXsdCwFMq40WdiX71xo8w4eMqAkl52uL14YqbzW2A==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "ECG-QA",
    "paperLink": "https://arxiv.org/abs/2306.15681",
    "description": "The first comprehensive Question Answering dataset specifically designed for ECG analysis. It includes diverse question types such as single-ECG interpretation and comparative analysis between two ECGs, derived from clinically relevant attributes in the PTB-XL dataset.",
    "authors": [
      "Jungwoo Oh",
      "Gyubok Lee",
      "Seongsu Bae",
      "Joon-Myoung Kwon",
      "Edward Choi"
    ],
    "githubLink": "https://github.com/Startpoco/ECG-QA",
    "itemCount": "414,348 QA pairs (267k train, 64k val, 82k test) from 70 templates",
    "source": "arXiv",
    "specs": "Text (Question/Answer pairs), 12-lead ECG signals (linked to PTB-XL)",
    "year": "2023",
    "id": "saved-1769640432815-dbmog",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxHduyN7tpbM4yTo8W9Xqc3ZcFXhC8vIjSVXKUOjcr01d8mdoYdmDMpfD1gjWpWOpNrXMHWzngxEpCSatzWDIpA4eQ5OVg9aXC--fVPbTKWA-lDtjJBXPwyW2wZMPHFXTUBsxemM-7iZpoV_H7j4OuizOqg1y3b_RAmOHWFrZsPJQwytISKcp_t3qLDYT6Paj7SfWu15Ia6EUQyWk7J-w=",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnfBjpzJFqc5KTc4W-eQ0Mssq5q66ApdaHYb7ktQuxtqQVda_g6sdCwELR6tmK-6_2pzD3G2RayuiqFR_o5ubBnw_dK6TBbrGKngr4S9SDGPaLbbyxRjQlrmoe2ow=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY4vSg9_01RT02gjM4oNO726w948sBN7cWTc5oMI90Jp-4NOoy8BL0KP7T40WfTzNCXTRkll-9Ts01rWAOICdfQYKjm02GqbyTZfwYHFq8qoGXptI8THxO44Gk468=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY1GQBOEhKW4tUTufbvSf2cfWW2KUfoZ15itLOnUe5ZoMPXNnr00O4FCA-jk1d2guoWyQWHvlE87fTu2jvB9bip9222yXsdCwFMq40WdiX71xo8w4eMqAkl52uL14YqbzW2A==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "PTB-XL+",
    "paperLink": "https://www.nature.com/articles/s41597-023-02153-8",
    "description": "An enhanced version of the PTB-XL dataset that includes extensive feature extraction and automatic diagnostic statements (text) from commercial algorithms (12SL), serving as a foundational dataset for ECG-text modeling.",
    "authors": [
      "Nils Strodthoff",
      "Patrick Wagner",
      "Tobias Schaeffter",
      "Wojciech Samek"
    ],
    "githubLink": "https://physionet.org/content/ptb-xl-plus/1.0.1/",
    "itemCount": "21,837 records",
    "source": "PhysioNet",
    "specs": "12-lead ECG, Tabular features, Textual diagnostic statements",
    "year": "2023",
    "id": "saved-1769640432815-ar6gd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxHduyN7tpbM4yTo8W9Xqc3ZcFXhC8vIjSVXKUOjcr01d8mdoYdmDMpfD1gjWpWOpNrXMHWzngxEpCSatzWDIpA4eQ5OVg9aXC--fVPbTKWA-lDtjJBXPwyW2wZMPHFXTUBsxemM-7iZpoV_H7j4OuizOqg1y3b_RAmOHWFrZsPJQwytISKcp_t3qLDYT6Paj7SfWu15Ia6EUQyWk7J-w=",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnfBjpzJFqc5KTc4W-eQ0Mssq5q66ApdaHYb7ktQuxtqQVda_g6sdCwELR6tmK-6_2pzD3G2RayuiqFR_o5ubBnw_dK6TBbrGKngr4S9SDGPaLbbyxRjQlrmoe2ow=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY4vSg9_01RT02gjM4oNO726w948sBN7cWTc5oMI90Jp-4NOoy8BL0KP7T40WfTzNCXTRkll-9Ts01rWAOICdfQYKjm02GqbyTZfwYHFq8qoGXptI8THxO44Gk468=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHY1GQBOEhKW4tUTufbvSf2cfWW2KUfoZ15itLOnUe5ZoMPXNnr00O4FCA-jk1d2guoWyQWHvlE87fTu2jvB9bip9222yXsdCwFMq40WdiX71xo8w4eMqAkl52uL14YqbzW2A==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "TSRBench",
    "paperLink": "https://arxiv.org/abs/2601.18744",
    "description": "A comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. It categorizes tasks into four major dimensions: Perception, Reasoning, Prediction, and Decision-Making.",
    "authors": [
      "Fangxu Yu",
      "Xingang Guo",
      "Lingzhi Yuan",
      "Haoqiang Kang",
      "Hongyu Zhao",
      "Lianhui Qin",
      "Furong Huang",
      "Bin Hu",
      "Tianyi Zhou"
    ],
    "githubLink": "https://tsrbench.github.io/",
    "itemCount": "4125 problems",
    "source": "arXiv",
    "specs": "14 domains, 15 tasks, Multi-modal (Text, Visual, Time Series)",
    "year": "2026",
    "id": "saved-1769640519290-uik4r",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO5Cw181UuiqHcE_E5Ic7HoipCNn03rJefO9QDZlYAUv5Ezhq8tlVbKJPaHNli5t3TuMgiRJQADcT-9uQk-QrqABJnx-Nvz0f3DyIw8HKOmnnkGkrDchPNArEEkiFgDfktzDo=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELc96U875UwH-4hKmaEp1yOa08VgT2shTiTFEQSgeVcoI8w9z5i2C0yQHnmHeKlwKHRfKMgsniF0fsamrByzVeesN-z_wOtERpk7TNRa8ys5QrMzshelOAZVupEccu7iakA9U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHyv68G1t2s-UC14f6zTeOaHtmDL7hEL5URnYKGLLQFE3W-Id3-fpSofgVK3WBZgjXFUenzC39ffjWej0V3T6U2GVILetV2iBPvWLQUve80eE8mBTO7z7xVnJ1zhn4MDEQKcNKU9NXhlk84MfQAeQHhQA2dfdXTZbKOFf2uKS-MnnobCK8t_UH3-SPGdbeyzvFvUREWJmeIegloQV_bWOXiTJNo7jwVN55kNCjXWyvPdC2VItP0iRHI6Kc=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExSHBeUVEgA92sm28cVZFKLZVo35xAZh8jlINFeq3E4ZKS9PZNtiKK84VuoUU9xu8mw4VVxdmHHeigyzJjDlQrcODyvOaMTbEqdK-WUp_WggZwXhin074D8tnHgCPT",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcTGNBkXE0sWVbI0i2N6qj5HgF2grImqDignG7Vzvmhh1Iqgka1evvpBuxYaSttk2hRXIUXKNVwRRqnL12GLx5Xrzqek-htLD7ikhzg6UW1BCs4Ixy1G02WCu67dRx",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4MK68I6-vjgNHu8wsWJQu6TgMnGGLERX2AM0f7iMIRY57s3BZ_Lug7N7xHzq290mxt6bgdtJ8Fx7eIVDhX9uP3nxoMPApqbIl1Ss5UM-_pdag2f2Y4b--2Qvu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIENQcUXQGnJxboD1LNTGYTeE_tHD9H3HK5CfDyj6qaXmZw-94UztzVixqq7_qPdVucvQC6BQFX5F-swKFybHNy1xrnai62MembDvloaqHwJIP5ZYaFnlq_2D4x_qQ9yU=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "MTBench",
    "paperLink": "https://arxiv.org/abs/2503.16858",
    "description": "A large-scale benchmark designed to evaluate large language models (LLMs) on time series and text understanding across financial and weather domains. It comprises paired time-series and textual data for tasks like forecasting, trend analysis, and question answering.",
    "authors": [
      "Jialin Chen",
      "Aosong Feng",
      "Ziyu Zhao",
      "Juan Garza",
      "Gaukhar Nurbek",
      "Cheng Qin",
      "Leandros Tassiulas"
    ],
    "githubLink": "https://github.com/Graph-and-Geometric-Learning/MTBench",
    "itemCount": "Large-scale (Financial news + stock prices, Weather reports + temperature)",
    "source": "arXiv",
    "specs": "Multimodal (Text + Time Series), Tasks: Forecasting, Trend Analysis, QA",
    "year": "2025",
    "id": "saved-1769640519290-25fg3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO5Cw181UuiqHcE_E5Ic7HoipCNn03rJefO9QDZlYAUv5Ezhq8tlVbKJPaHNli5t3TuMgiRJQADcT-9uQk-QrqABJnx-Nvz0f3DyIw8HKOmnnkGkrDchPNArEEkiFgDfktzDo=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELc96U875UwH-4hKmaEp1yOa08VgT2shTiTFEQSgeVcoI8w9z5i2C0yQHnmHeKlwKHRfKMgsniF0fsamrByzVeesN-z_wOtERpk7TNRa8ys5QrMzshelOAZVupEccu7iakA9U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHyv68G1t2s-UC14f6zTeOaHtmDL7hEL5URnYKGLLQFE3W-Id3-fpSofgVK3WBZgjXFUenzC39ffjWej0V3T6U2GVILetV2iBPvWLQUve80eE8mBTO7z7xVnJ1zhn4MDEQKcNKU9NXhlk84MfQAeQHhQA2dfdXTZbKOFf2uKS-MnnobCK8t_UH3-SPGdbeyzvFvUREWJmeIegloQV_bWOXiTJNo7jwVN55kNCjXWyvPdC2VItP0iRHI6Kc=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExSHBeUVEgA92sm28cVZFKLZVo35xAZh8jlINFeq3E4ZKS9PZNtiKK84VuoUU9xu8mw4VVxdmHHeigyzJjDlQrcODyvOaMTbEqdK-WUp_WggZwXhin074D8tnHgCPT",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcTGNBkXE0sWVbI0i2N6qj5HgF2grImqDignG7Vzvmhh1Iqgka1evvpBuxYaSttk2hRXIUXKNVwRRqnL12GLx5Xrzqek-htLD7ikhzg6UW1BCs4Ixy1G02WCu67dRx",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4MK68I6-vjgNHu8wsWJQu6TgMnGGLERX2AM0f7iMIRY57s3BZ_Lug7N7xHzq290mxt6bgdtJ8Fx7eIVDhX9uP3nxoMPApqbIl1Ss5UM-_pdag2f2Y4b--2Qvu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIENQcUXQGnJxboD1LNTGYTeE_tHD9H3HK5CfDyj6qaXmZw-94UztzVixqq7_qPdVucvQC6BQFX5F-swKFybHNy1xrnai62MembDvloaqHwJIP5ZYaFnlq_2D4x_qQ9yU=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Time-MQA (TSQA Dataset)",
    "paperLink": "https://arxiv.org/abs/2503.01875",
    "description": "A unified framework and dataset for Time Series Multi-Task Question Answering. The TSQA dataset contains ~200k question-answer pairs derived from diverse time series, enabling natural language queries for numerical analysis and open-ended reasoning.",
    "authors": [
      "Yaxuan Kong",
      "Yiyuan Yang",
      "Yoontae Hwang",
      "Wenjie Du",
      "Stefan Zohren",
      "Zhangyang Wang",
      "Ming Jin",
      "Qingsong Wen"
    ],
    "githubLink": "https://huggingface.co/papers/2503.01875",
    "itemCount": "~200,000 QA pairs",
    "source": "arXiv",
    "specs": "Multimodal (Time Series + Natural Language QA), Tasks: QA, Reasoning, Numerical Analysis",
    "year": "2025",
    "id": "saved-1769640519290-7kqo9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO5Cw181UuiqHcE_E5Ic7HoipCNn03rJefO9QDZlYAUv5Ezhq8tlVbKJPaHNli5t3TuMgiRJQADcT-9uQk-QrqABJnx-Nvz0f3DyIw8HKOmnnkGkrDchPNArEEkiFgDfktzDo=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELc96U875UwH-4hKmaEp1yOa08VgT2shTiTFEQSgeVcoI8w9z5i2C0yQHnmHeKlwKHRfKMgsniF0fsamrByzVeesN-z_wOtERpk7TNRa8ys5QrMzshelOAZVupEccu7iakA9U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHyv68G1t2s-UC14f6zTeOaHtmDL7hEL5URnYKGLLQFE3W-Id3-fpSofgVK3WBZgjXFUenzC39ffjWej0V3T6U2GVILetV2iBPvWLQUve80eE8mBTO7z7xVnJ1zhn4MDEQKcNKU9NXhlk84MfQAeQHhQA2dfdXTZbKOFf2uKS-MnnobCK8t_UH3-SPGdbeyzvFvUREWJmeIegloQV_bWOXiTJNo7jwVN55kNCjXWyvPdC2VItP0iRHI6Kc=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExSHBeUVEgA92sm28cVZFKLZVo35xAZh8jlINFeq3E4ZKS9PZNtiKK84VuoUU9xu8mw4VVxdmHHeigyzJjDlQrcODyvOaMTbEqdK-WUp_WggZwXhin074D8tnHgCPT",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcTGNBkXE0sWVbI0i2N6qj5HgF2grImqDignG7Vzvmhh1Iqgka1evvpBuxYaSttk2hRXIUXKNVwRRqnL12GLx5Xrzqek-htLD7ikhzg6UW1BCs4Ixy1G02WCu67dRx",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4MK68I6-vjgNHu8wsWJQu6TgMnGGLERX2AM0f7iMIRY57s3BZ_Lug7N7xHzq290mxt6bgdtJ8Fx7eIVDhX9uP3nxoMPApqbIl1Ss5UM-_pdag2f2Y4b--2Qvu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIENQcUXQGnJxboD1LNTGYTeE_tHD9H3HK5CfDyj6qaXmZw-94UztzVixqq7_qPdVucvQC6BQFX5F-swKFybHNy1xrnai62MembDvloaqHwJIP5ZYaFnlq_2D4x_qQ9yU=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "mTSBench",
    "paperLink": "https://arxiv.org/abs/2506.06648",
    "description": "A large-scale benchmark for multivariate time series anomaly detection and unsupervised model selection. It spans a wide range of domains and systematically benchmarks anomaly detection methods, including LLM-based ones.",
    "authors": [
      "Minh-Ngoc Tran",
      "et al."
    ],
    "githubLink": "https://github.com/PLAN-Lab/mTSBench",
    "itemCount": "344 labeled time series, 19 datasets",
    "source": "arXiv",
    "specs": "Multivariate Time Series, 12 domains, Tasks: Anomaly Detection, Model Selection",
    "year": "2025",
    "id": "saved-1769640519290-pfbd9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO5Cw181UuiqHcE_E5Ic7HoipCNn03rJefO9QDZlYAUv5Ezhq8tlVbKJPaHNli5t3TuMgiRJQADcT-9uQk-QrqABJnx-Nvz0f3DyIw8HKOmnnkGkrDchPNArEEkiFgDfktzDo=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELc96U875UwH-4hKmaEp1yOa08VgT2shTiTFEQSgeVcoI8w9z5i2C0yQHnmHeKlwKHRfKMgsniF0fsamrByzVeesN-z_wOtERpk7TNRa8ys5QrMzshelOAZVupEccu7iakA9U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHyv68G1t2s-UC14f6zTeOaHtmDL7hEL5URnYKGLLQFE3W-Id3-fpSofgVK3WBZgjXFUenzC39ffjWej0V3T6U2GVILetV2iBPvWLQUve80eE8mBTO7z7xVnJ1zhn4MDEQKcNKU9NXhlk84MfQAeQHhQA2dfdXTZbKOFf2uKS-MnnobCK8t_UH3-SPGdbeyzvFvUREWJmeIegloQV_bWOXiTJNo7jwVN55kNCjXWyvPdC2VItP0iRHI6Kc=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExSHBeUVEgA92sm28cVZFKLZVo35xAZh8jlINFeq3E4ZKS9PZNtiKK84VuoUU9xu8mw4VVxdmHHeigyzJjDlQrcODyvOaMTbEqdK-WUp_WggZwXhin074D8tnHgCPT",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcTGNBkXE0sWVbI0i2N6qj5HgF2grImqDignG7Vzvmhh1Iqgka1evvpBuxYaSttk2hRXIUXKNVwRRqnL12GLx5Xrzqek-htLD7ikhzg6UW1BCs4Ixy1G02WCu67dRx",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4MK68I6-vjgNHu8wsWJQu6TgMnGGLERX2AM0f7iMIRY57s3BZ_Lug7N7xHzq290mxt6bgdtJ8Fx7eIVDhX9uP3nxoMPApqbIl1Ss5UM-_pdag2f2Y4b--2Qvu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIENQcUXQGnJxboD1LNTGYTeE_tHD9H3HK5CfDyj6qaXmZw-94UztzVixqq7_qPdVucvQC6BQFX5F-swKFybHNy1xrnai62MembDvloaqHwJIP5ZYaFnlq_2D4x_qQ9yU=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Time-MMD",
    "paperLink": "https://arxiv.org/abs/2406.08627",
    "description": "A multi-domain, multimodal time series dataset designed to address the lack of high-quality multimodal resources. It covers 9 primary data domains and ensures fine-grained modality alignment between numerical and textual series data.",
    "authors": [
      "Haoxin Liu",
      "Shangqing Xu",
      "Zhiyuan Zhao",
      "Lingkai Kong",
      "Harshavardhan Kamarthi",
      "Aditya B. Sasanur",
      "Megha Sharma",
      "Jiaming Cui",
      "Qingsong Wen",
      "Chao Zhang",
      "B. Aditya Prakash"
    ],
    "githubLink": "https://github.com/AdityaLab/Time-MMD",
    "itemCount": "Covers 9 primary data domains",
    "source": "NeurIPS",
    "specs": "Multimodal (Numerical Time Series + Textual Series), Tasks: Forecasting, Analysis",
    "year": "2024",
    "id": "saved-1769640519290-o8jp2",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO5Cw181UuiqHcE_E5Ic7HoipCNn03rJefO9QDZlYAUv5Ezhq8tlVbKJPaHNli5t3TuMgiRJQADcT-9uQk-QrqABJnx-Nvz0f3DyIw8HKOmnnkGkrDchPNArEEkiFgDfktzDo=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELc96U875UwH-4hKmaEp1yOa08VgT2shTiTFEQSgeVcoI8w9z5i2C0yQHnmHeKlwKHRfKMgsniF0fsamrByzVeesN-z_wOtERpk7TNRa8ys5QrMzshelOAZVupEccu7iakA9U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHyv68G1t2s-UC14f6zTeOaHtmDL7hEL5URnYKGLLQFE3W-Id3-fpSofgVK3WBZgjXFUenzC39ffjWej0V3T6U2GVILetV2iBPvWLQUve80eE8mBTO7z7xVnJ1zhn4MDEQKcNKU9NXhlk84MfQAeQHhQA2dfdXTZbKOFf2uKS-MnnobCK8t_UH3-SPGdbeyzvFvUREWJmeIegloQV_bWOXiTJNo7jwVN55kNCjXWyvPdC2VItP0iRHI6Kc=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExSHBeUVEgA92sm28cVZFKLZVo35xAZh8jlINFeq3E4ZKS9PZNtiKK84VuoUU9xu8mw4VVxdmHHeigyzJjDlQrcODyvOaMTbEqdK-WUp_WggZwXhin074D8tnHgCPT",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcTGNBkXE0sWVbI0i2N6qj5HgF2grImqDignG7Vzvmhh1Iqgka1evvpBuxYaSttk2hRXIUXKNVwRRqnL12GLx5Xrzqek-htLD7ikhzg6UW1BCs4Ixy1G02WCu67dRx",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4MK68I6-vjgNHu8wsWJQu6TgMnGGLERX2AM0f7iMIRY57s3BZ_Lug7N7xHzq290mxt6bgdtJ8Fx7eIVDhX9uP3nxoMPApqbIl1Ss5UM-_pdag2f2Y4b--2Qvu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIENQcUXQGnJxboD1LNTGYTeE_tHD9H3HK5CfDyj6qaXmZw-94UztzVixqq7_qPdVucvQC6BQFX5F-swKFybHNy1xrnai62MembDvloaqHwJIP5ZYaFnlq_2D4x_qQ9yU=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "UniTS Benchmark Suite",
    "paperLink": "https://arxiv.org/abs/2402.19046",
    "description": "A unified multi-task time series evaluation suite used to validate the UniTS model. It standardizes tasks across forecasting, classification, imputation, and anomaly detection using task tokenization.",
    "authors": [
      "Shanghua Gao",
      "Teddy Koker",
      "Owen Queen",
      "Thomas Hartvigsen",
      "Theodoros Tsiligkaridis",
      "Marinka Zitnik"
    ],
    "githubLink": "https://github.com/mims-harvard/UniTS",
    "itemCount": "38 datasets",
    "source": "arXiv",
    "specs": "Multi-task (Forecasting, Classification, Imputation, Anomaly Detection), Multi-domain",
    "year": "2024",
    "id": "saved-1769640519290-3g5j2",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEO5Cw181UuiqHcE_E5Ic7HoipCNn03rJefO9QDZlYAUv5Ezhq8tlVbKJPaHNli5t3TuMgiRJQADcT-9uQk-QrqABJnx-Nvz0f3DyIw8HKOmnnkGkrDchPNArEEkiFgDfktzDo=",
        "title": "papers.cool"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELc96U875UwH-4hKmaEp1yOa08VgT2shTiTFEQSgeVcoI8w9z5i2C0yQHnmHeKlwKHRfKMgsniF0fsamrByzVeesN-z_wOtERpk7TNRa8ys5QrMzshelOAZVupEccu7iakA9U=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHyv68G1t2s-UC14f6zTeOaHtmDL7hEL5URnYKGLLQFE3W-Id3-fpSofgVK3WBZgjXFUenzC39ffjWej0V3T6U2GVILetV2iBPvWLQUve80eE8mBTO7z7xVnJ1zhn4MDEQKcNKU9NXhlk84MfQAeQHhQA2dfdXTZbKOFf2uKS-MnnobCK8t_UH3-SPGdbeyzvFvUREWJmeIegloQV_bWOXiTJNo7jwVN55kNCjXWyvPdC2VItP0iRHI6Kc=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQExSHBeUVEgA92sm28cVZFKLZVo35xAZh8jlINFeq3E4ZKS9PZNtiKK84VuoUU9xu8mw4VVxdmHHeigyzJjDlQrcODyvOaMTbEqdK-WUp_WggZwXhin074D8tnHgCPT",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcTGNBkXE0sWVbI0i2N6qj5HgF2grImqDignG7Vzvmhh1Iqgka1evvpBuxYaSttk2hRXIUXKNVwRRqnL12GLx5Xrzqek-htLD7ikhzg6UW1BCs4Ixy1G02WCu67dRx",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4MK68I6-vjgNHu8wsWJQu6TgMnGGLERX2AM0f7iMIRY57s3BZ_Lug7N7xHzq290mxt6bgdtJ8Fx7eIVDhX9uP3nxoMPApqbIl1Ss5UM-_pdag2f2Y4b--2Qvu",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIENQcUXQGnJxboD1LNTGYTeE_tHD9H3HK5CfDyj6qaXmZw-94UztzVixqq7_qPdVucvQC6BQFX5F-swKFybHNy1xrnai62MembDvloaqHwJIP5ZYaFnlq_2D4x_qQ9yU=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "BIPIA",
    "paperLink": "https://arxiv.org/abs/2407.08182",
    "description": "A benchmark specifically designed to evaluate the robustness of LLMs and defenses against indirect prompt injection attacks, particularly in email and tabular data contexts.",
    "authors": [
      "Jingwei Yi",
      "Yueqi Xie",
      "Bin Zhu",
      "Ke Xu",
      "Wei Ma",
      "Tsuyoshi Idé",
      "Naoki Abe",
      "Dawn Song",
      "Hui Xue"
    ],
    "githubLink": "https://github.com/microsoft/BIPIA",
    "itemCount": "626,250 training prompts; 86,250 test prompts",
    "source": "arXiv",
    "specs": "Text (Email/Table formats); Indirect Prompt Injection",
    "year": "2025",
    "id": "saved-1769640761781-gsvjp",
    "groundingSources": []
  },
  {
    "title": "GuardBench",
    "paperLink": "https://aclanthology.org/2024.emnlp-main.1026/",
    "description": "A large-scale benchmark designed to evaluate guardrail models, comprising 40 datasets (including 35 repurposed and 5 novel ones) covering various safety risks in multiple languages (English, German, French, Italian, Spanish).",
    "authors": [
      "Elias Bassani",
      "Ignacio Sanchez"
    ],
    "githubLink": "https://github.com/AmenRa/guardbench",
    "itemCount": "40 datasets; >31,000 prompts each for novel multilingual datasets",
    "source": "arXiv",
    "specs": "Multilingual text (En, De, Fr, It, Es); Prompt moderation and Response evaluation tasks",
    "year": "2024",
    "id": "saved-1769640761781-omnjl",
    "groundingSources": []
  },
  {
    "title": "DeepPlanning",
    "paperLink": "https://arxiv.org/abs/2601.18137",
    "description": "A benchmark designed to evaluate long-horizon agentic planning capabilities with verifiable constraints, featuring realistic multi-day travel planning and multi-product shopping tasks requiring global optimization.",
    "authors": [
      "Yinger Zhang",
      "Shutong Jiang",
      "Renhao Li",
      "Jianhong Tu",
      "Yang Su",
      "Lianghao Deng",
      "Xudong Guo",
      "Chenxu Lv",
      "Junyang Lin"
    ],
    "githubLink": "https://github.com/QwenLM/Qwen-Agent",
    "itemCount": "Not specified (Large-scale evaluation mentioned)",
    "source": "Hugging Face",
    "specs": "Travel planning and Shopping planning domains with verifiable constraints",
    "year": "2026",
    "id": "saved-1769641178362-4gy8m",
    "groundingSources": []
  },
  {
    "title": "CriticBench",
    "paperLink": "https://arxiv.org/abs/2402.14809",
    "description": "A comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across varying tasks including mathematical, commonsense, symbolic, coding, and algorithmic domains.",
    "authors": [
      "Zicheng Lin",
      "Zhibin Gou",
      "Tian Liang",
      "Ruilin Luo",
      "Haowei Liu",
      "Yujiu Yang"
    ],
    "githubLink": "https://github.com/open-compass/CriticBench",
    "itemCount": "15 datasets",
    "source": "Hugging Face",
    "specs": "Text generation, critique, and correction tasks",
    "year": "2024",
    "id": "saved-1769641178362-5hgb5",
    "groundingSources": []
  },
  {
    "title": "FalseReject",
    "paperLink": "https://arxiv.org/abs/2505.09388",
    "description": "A large-scale dataset aimed at evaluating and calibrating LLM over-refusal. It features benign queries that appear harmful, covering 44 safety-related categories, and includes resources for training models to distinguish between safe and unsafe contexts using structured reasoning.",
    "authors": [
      "Zhehao Zhang",
      "Weijie Xu",
      "Fanyou Wu",
      "Chandan K. Reddy"
    ],
    "githubLink": "https://github.com/zhehao-zhang/FalseReject",
    "itemCount": "16,000 samples",
    "source": "arXiv",
    "specs": "Text; Includes training sets (Instruct, CoT) and a human-annotated test set",
    "year": "2025",
    "id": "saved-1769641321708-5h8di",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHK7D0fcawenOavvOcpTucQn9BAarfwoW5Us1NxREmYLM1WFyXTTEMy75Mw0cOhpvtXbRDCTj_l5Fd_u2NVWiiAokZZQHm7hADQvbIfHOmeZ4NttKuejhaxu4ihavzU8UvqcdM50Q==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmLLxfVllxl-COP7qhFgQ3V-8je7m3DzbJMHk1vxiTol9qblMoq-vzj24iboEIxnGy-indKfCvM6-9WMCIhcBEFkGpdtMhLLfkr1QjKFY6TiccbaUfawdAJbUWo03nOzGcGf9G",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxuMJPMHy_wtd9M7ATDdI3JW-8U3EzXXQrA4UgFNGNVGo9thBC7f7ZMtpMyODZXuAP8M4R-57cQwo_lWtUxHYbzor5Pbbc5un4vVcJhB8NOmC8fBepnuJfEabP3zcF58iuGv4=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPMFlZW2CxwrTPzY2DbBsrDr3fMPnRROV0SEQp5kdv4hlkNMte5xWZestUZ5HbgUGhwRYOrKWd588JMbTCOz1wOJs9AZFFh2Lof-aiLeD0LgOGTsF4tmM_-opQFsNiajyZjm5HzxsH_HSGXtda-lBfFBpc5hT9s0sex49YOulH4szWML5EnHPNqpKSfLWKH_293Gl8xChXUm8rnP1Us17jm6TASK3KMt-xnZZxtpU4zxrD0Oy5MIfvfNqBJ6FxMA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_n_NQ7PO9BHICm9uuvzJx7qGEp3VJE8BTW5oq9mX5KtNW9tnHIjL92a4LaZSuMTB-JB8XJejGhxOizQMHxHaZZIMqkRprw8CkaR-9xPIhA_Xmk3b6i4aSIABYuQsTqZOEjn5IfS0AhS0B",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3W528y26kbDZw65qlpgpSeMiUKoPzYcvBvk_mewR0mGoDLkRr-USVCi1NuDdHOPaAzIHDJbw1P9XW-Min24dZu7mV64TdT4qSTmMerUn6akRQ35VLYF3yY31fknEMyjde8kwdY43HT01oSg==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvwXQ2lJFjFsqmHS67MpP-gsnLTiE1v9JPWl55jXzaIKrlwqLq3V-eFvJ4TfjVytENiqgxkJAyLpEWXFlGVR8E2hGnsXZMrFv8woNcI2jZT9SQQCVyvPmlYBs2BokSnoI9Z5DmyG_d9b_idh5r2adfZoodDA==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "RefusalBench",
    "paperLink": "https://arxiv.org/abs/2509.12345",
    "description": "A benchmark suite for evaluating selective refusal in Retrieval-Augmented Generation (RAG) systems. It tests a model's ability to refuse answering when the provided context is flawed or insufficient, while avoiding over-refusal on valid questions.",
    "authors": [
      "Aashiq Muhamed",
      "Leonardo F. R. Ribeiro",
      "Markus Dreyer",
      "Virginia Smith",
      "Mona T. Diab"
    ],
    "githubLink": "https://github.com/amazon-science/refusal-bench",
    "itemCount": "Varies (Generated via perturbation)",
    "source": "arXiv",
    "specs": "Text; RAG-based QA pairs with perturbed contexts (Single-doc and Multi-doc versions)",
    "year": "2025",
    "id": "saved-1769641321708-gty3m",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHK7D0fcawenOavvOcpTucQn9BAarfwoW5Us1NxREmYLM1WFyXTTEMy75Mw0cOhpvtXbRDCTj_l5Fd_u2NVWiiAokZZQHm7hADQvbIfHOmeZ4NttKuejhaxu4ihavzU8UvqcdM50Q==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmLLxfVllxl-COP7qhFgQ3V-8je7m3DzbJMHk1vxiTol9qblMoq-vzj24iboEIxnGy-indKfCvM6-9WMCIhcBEFkGpdtMhLLfkr1QjKFY6TiccbaUfawdAJbUWo03nOzGcGf9G",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxuMJPMHy_wtd9M7ATDdI3JW-8U3EzXXQrA4UgFNGNVGo9thBC7f7ZMtpMyODZXuAP8M4R-57cQwo_lWtUxHYbzor5Pbbc5un4vVcJhB8NOmC8fBepnuJfEabP3zcF58iuGv4=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPMFlZW2CxwrTPzY2DbBsrDr3fMPnRROV0SEQp5kdv4hlkNMte5xWZestUZ5HbgUGhwRYOrKWd588JMbTCOz1wOJs9AZFFh2Lof-aiLeD0LgOGTsF4tmM_-opQFsNiajyZjm5HzxsH_HSGXtda-lBfFBpc5hT9s0sex49YOulH4szWML5EnHPNqpKSfLWKH_293Gl8xChXUm8rnP1Us17jm6TASK3KMt-xnZZxtpU4zxrD0Oy5MIfvfNqBJ6FxMA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_n_NQ7PO9BHICm9uuvzJx7qGEp3VJE8BTW5oq9mX5KtNW9tnHIjL92a4LaZSuMTB-JB8XJejGhxOizQMHxHaZZIMqkRprw8CkaR-9xPIhA_Xmk3b6i4aSIABYuQsTqZOEjn5IfS0AhS0B",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3W528y26kbDZw65qlpgpSeMiUKoPzYcvBvk_mewR0mGoDLkRr-USVCi1NuDdHOPaAzIHDJbw1P9XW-Min24dZu7mV64TdT4qSTmMerUn6akRQ35VLYF3yY31fknEMyjde8kwdY43HT01oSg==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvwXQ2lJFjFsqmHS67MpP-gsnLTiE1v9JPWl55jXzaIKrlwqLq3V-eFvJ4TfjVytENiqgxkJAyLpEWXFlGVR8E2hGnsXZMrFv8woNcI2jZT9SQQCVyvPmlYBs2BokSnoI9Z5DmyG_d9b_idh5r2adfZoodDA==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "MOR-Bench (Multilingual Over-Refusal Benchmark)",
    "paperLink": "https://aclanthology.org/2025.findings-acl.123",
    "description": "Designed to assess over-refusal in a multilingual context, evaluating whether models maintain consistent safety boundaries across different languages or if they exhibit exaggerated safety due to cross-lingual misalignment.",
    "authors": [
      "Authors not fully listed in snippet"
    ],
    "githubLink": "N/A",
    "itemCount": "Large-scale",
    "source": "Scholar",
    "specs": "Multilingual text prompts",
    "year": "2025",
    "id": "saved-1769641321708-untb6",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHK7D0fcawenOavvOcpTucQn9BAarfwoW5Us1NxREmYLM1WFyXTTEMy75Mw0cOhpvtXbRDCTj_l5Fd_u2NVWiiAokZZQHm7hADQvbIfHOmeZ4NttKuejhaxu4ihavzU8UvqcdM50Q==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGmLLxfVllxl-COP7qhFgQ3V-8je7m3DzbJMHk1vxiTol9qblMoq-vzj24iboEIxnGy-indKfCvM6-9WMCIhcBEFkGpdtMhLLfkr1QjKFY6TiccbaUfawdAJbUWo03nOzGcGf9G",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxuMJPMHy_wtd9M7ATDdI3JW-8U3EzXXQrA4UgFNGNVGo9thBC7f7ZMtpMyODZXuAP8M4R-57cQwo_lWtUxHYbzor5Pbbc5un4vVcJhB8NOmC8fBepnuJfEabP3zcF58iuGv4=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPMFlZW2CxwrTPzY2DbBsrDr3fMPnRROV0SEQp5kdv4hlkNMte5xWZestUZ5HbgUGhwRYOrKWd588JMbTCOz1wOJs9AZFFh2Lof-aiLeD0LgOGTsF4tmM_-opQFsNiajyZjm5HzxsH_HSGXtda-lBfFBpc5hT9s0sex49YOulH4szWML5EnHPNqpKSfLWKH_293Gl8xChXUm8rnP1Us17jm6TASK3KMt-xnZZxtpU4zxrD0Oy5MIfvfNqBJ6FxMA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_n_NQ7PO9BHICm9uuvzJx7qGEp3VJE8BTW5oq9mX5KtNW9tnHIjL92a4LaZSuMTB-JB8XJejGhxOizQMHxHaZZIMqkRprw8CkaR-9xPIhA_Xmk3b6i4aSIABYuQsTqZOEjn5IfS0AhS0B",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3W528y26kbDZw65qlpgpSeMiUKoPzYcvBvk_mewR0mGoDLkRr-USVCi1NuDdHOPaAzIHDJbw1P9XW-Min24dZu7mV64TdT4qSTmMerUn6akRQ35VLYF3yY31fknEMyjde8kwdY43HT01oSg==",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvwXQ2lJFjFsqmHS67MpP-gsnLTiE1v9JPWl55jXzaIKrlwqLq3V-eFvJ4TfjVytENiqgxkJAyLpEWXFlGVR8E2hGnsXZMrFv8woNcI2jZT9SQQCVyvPmlYBs2BokSnoI9Z5DmyG_d9b_idh5r2adfZoodDA==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visually Grounded Web Tasks",
    "paperLink": "https://arxiv.org/abs/2401.13649",
    "description": "A benchmark designed to assess multimodal agents on realistic, visually grounded web tasks, requiring agents to process image-text inputs and interpret instructions.",
    "authors": [
      "Jing Yu Koh",
      "Robert Lo",
      "Lawrence Jang",
      "Vikram Duvvur",
      "Ming Chong Lim",
      "Po-Yu Huang",
      "Graham Neubig",
      "Shuyan Zhou",
      "Ruslan Salakhutdinov",
      "Daniel Fried"
    ],
    "githubLink": "https://github.com/web-arena-x/visualwebarena",
    "itemCount": "910 tasks",
    "source": "arXiv",
    "specs": "Visually grounded web tasks, multimodal inputs",
    "year": "2024",
    "id": "saved-1769641435282-3bi30",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp2Kyl57n15UZhjoXgPi7NJo1uZiphvEGpBXDx6h1WtRB3AleNkexHO1U_H15Zc1TgZVgeCr5RSqxJwg9km2vaZsFWKr3LthmwzGhmd_UfNrwGUa2GAcN3c1OpGN7tvIhrnPilarac4zok",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyKJlTzi5TejwWtZvpq9obbvy6DF5pMr5hs-MI-SQWHvccNedt78hs4SV3SCsc9u_x9dGU80vkny4Ou182jsmahbCXC7hAAVoOzeSTfdVlaOM4OQkptsNj_PVclr1ZaSpMCJGUuQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhfQrSUT9R5saE1NmJNmpsFcSk_7I8tNMdunyv4jpwuOdR_pWp1WXnU8O-p_qig-zjTLrvrxKESzDrt2nR8nz0BpYFdjF24hSnVhsGQ-YI_UHUr3IdrKDcFIcE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHULw4wQWwe4v2Mkh0AZ-60usln6XuCpoB-_mfYcLMl-RJmXguUhJk61zFF4snh795yZMr8Xr0RBuOoE-6hHxhdkAXVDA4MLV0ILj-Qf0wBvJWrI-9eP9xpOA4ajdxdq9VZ5ioi",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMdlhWl4x-vgpuNTtqnupc9bInR8fLw1dvOYK4eui77lUOtVzlxIiQfQhwTC2K3jA6a9iH7uyYNzFVeWSj1Yu7FxEsyooqtiIRSMlqdj9iZ4NbjhJsQi7B88WRFwkAGYsyt6Z3",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents",
    "paperLink": "https://arxiv.org/abs/2405.14579",
    "description": "A fully functional Android environment that dynamically constructs parameterized tasks expressed in natural language, enabling testing on a large suite of tasks.",
    "authors": [
      "Christopher Rawles",
      "Sarah Guo",
      "Sahil Jain",
      "Oriana Riva",
      "Timothy Lillicrap"
    ],
    "githubLink": "https://github.com/google-research/android_world",
    "itemCount": "116 tasks (dynamically instantiated)",
    "source": "arXiv",
    "specs": "Android environment, dynamic task generation, reward signals",
    "year": "2024",
    "id": "saved-1769641435282-gfs9z",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp2Kyl57n15UZhjoXgPi7NJo1uZiphvEGpBXDx6h1WtRB3AleNkexHO1U_H15Zc1TgZVgeCr5RSqxJwg9km2vaZsFWKr3LthmwzGhmd_UfNrwGUa2GAcN3c1OpGN7tvIhrnPilarac4zok",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyKJlTzi5TejwWtZvpq9obbvy6DF5pMr5hs-MI-SQWHvccNedt78hs4SV3SCsc9u_x9dGU80vkny4Ou182jsmahbCXC7hAAVoOzeSTfdVlaOM4OQkptsNj_PVclr1ZaSpMCJGUuQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhfQrSUT9R5saE1NmJNmpsFcSk_7I8tNMdunyv4jpwuOdR_pWp1WXnU8O-p_qig-zjTLrvrxKESzDrt2nR8nz0BpYFdjF24hSnVhsGQ-YI_UHUr3IdrKDcFIcE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHULw4wQWwe4v2Mkh0AZ-60usln6XuCpoB-_mfYcLMl-RJmXguUhJk61zFF4snh795yZMr8Xr0RBuOoE-6hHxhdkAXVDA4MLV0ILj-Qf0wBvJWrI-9eP9xpOA4ajdxdq9VZ5ioi",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMdlhWl4x-vgpuNTtqnupc9bInR8fLw1dvOYK4eui77lUOtVzlxIiQfQhwTC2K3jA6a9iH7uyYNzFVeWSj1Yu7FxEsyooqtiIRSMlqdj9iZ4NbjhJsQi7B88WRFwkAGYsyt6Z3",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
    "paperLink": "https://arxiv.org/abs/2402.17553",
    "description": "A dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks on desktop and web interfaces.",
    "authors": [
      "Raghav Kapoor",
      "Yash Parag Butala",
      "Melisa Russak",
      "Jing Yu Koh",
      "Kiran Kamble",
      "Waseem Alshikh",
      "Ruslan Salakhutdinov"
    ],
    "githubLink": "https://huggingface.co/datasets/Writer/omniact",
    "itemCount": "9,802 image-instruction pairs",
    "source": "arXiv",
    "specs": "Desktop and web, PyAutoGUI executable commands, multimodal",
    "year": "2024",
    "id": "saved-1769641435282-fw5tc",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp2Kyl57n15UZhjoXgPi7NJo1uZiphvEGpBXDx6h1WtRB3AleNkexHO1U_H15Zc1TgZVgeCr5RSqxJwg9km2vaZsFWKr3LthmwzGhmd_UfNrwGUa2GAcN3c1OpGN7tvIhrnPilarac4zok",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyKJlTzi5TejwWtZvpq9obbvy6DF5pMr5hs-MI-SQWHvccNedt78hs4SV3SCsc9u_x9dGU80vkny4Ou182jsmahbCXC7hAAVoOzeSTfdVlaOM4OQkptsNj_PVclr1ZaSpMCJGUuQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhfQrSUT9R5saE1NmJNmpsFcSk_7I8tNMdunyv4jpwuOdR_pWp1WXnU8O-p_qig-zjTLrvrxKESzDrt2nR8nz0BpYFdjF24hSnVhsGQ-YI_UHUr3IdrKDcFIcE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHULw4wQWwe4v2Mkh0AZ-60usln6XuCpoB-_mfYcLMl-RJmXguUhJk61zFF4snh795yZMr8Xr0RBuOoE-6hHxhdkAXVDA4MLV0ILj-Qf0wBvJWrI-9eP9xpOA4ajdxdq9VZ5ioi",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMdlhWl4x-vgpuNTtqnupc9bInR8fLw1dvOYK4eui77lUOtVzlxIiQfQhwTC2K3jA6a9iH7uyYNzFVeWSj1Yu7FxEsyooqtiIRSMlqdj9iZ4NbjhJsQi7B88WRFwkAGYsyt6Z3",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "GUI-World: A Dataset for GUI-Oriented Multimodal LLM-based Agents",
    "paperLink": "https://arxiv.org/abs/2406.10819",
    "description": "A comprehensive dataset covering six GUI scenarios including video, human-annotated keyframes, and captions to benchmark GUI-oriented capabilities.",
    "authors": [
      "Dongping Chen",
      "Yue Huang",
      "Siyuan Wu",
      "Jingyu Tang",
      "Liuyi Chen",
      "Yilin Bai",
      "Zhigang He",
      "Chenlong Wang",
      "Huichi Zhou",
      "Yiqiang Li"
    ],
    "githubLink": "https://github.com/Dongping-Chen/GUI-World",
    "itemCount": "12,000+ videos",
    "source": "arXiv",
    "specs": "Video-based, 6 GUI scenarios (desktop, mobile, XR, web, etc.)",
    "year": "2024",
    "id": "saved-1769641435282-lporq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp2Kyl57n15UZhjoXgPi7NJo1uZiphvEGpBXDx6h1WtRB3AleNkexHO1U_H15Zc1TgZVgeCr5RSqxJwg9km2vaZsFWKr3LthmwzGhmd_UfNrwGUa2GAcN3c1OpGN7tvIhrnPilarac4zok",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyKJlTzi5TejwWtZvpq9obbvy6DF5pMr5hs-MI-SQWHvccNedt78hs4SV3SCsc9u_x9dGU80vkny4Ou182jsmahbCXC7hAAVoOzeSTfdVlaOM4OQkptsNj_PVclr1ZaSpMCJGUuQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhfQrSUT9R5saE1NmJNmpsFcSk_7I8tNMdunyv4jpwuOdR_pWp1WXnU8O-p_qig-zjTLrvrxKESzDrt2nR8nz0BpYFdjF24hSnVhsGQ-YI_UHUr3IdrKDcFIcE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHULw4wQWwe4v2Mkh0AZ-60usln6XuCpoB-_mfYcLMl-RJmXguUhJk61zFF4snh795yZMr8Xr0RBuOoE-6hHxhdkAXVDA4MLV0ILj-Qf0wBvJWrI-9eP9xpOA4ajdxdq9VZ5ioi",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMdlhWl4x-vgpuNTtqnupc9bInR8fLw1dvOYK4eui77lUOtVzlxIiQfQhwTC2K3jA6a9iH7uyYNzFVeWSj1Yu7FxEsyooqtiIRSMlqdj9iZ4NbjhJsQi7B88WRFwkAGYsyt6Z3",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
    "paperLink": "https://arxiv.org/abs/2402.05930",
    "description": "A large-scale benchmark of interactions for conversational web navigation, covering a broad range of patterns on real-world websites.",
    "authors": [
      "Xing Han Lu",
      "Zdeněk Kasner",
      "Siva Reddy"
    ],
    "githubLink": "https://github.com/McGill-NLP/weblinx",
    "itemCount": "100,000 interactions",
    "source": "arXiv",
    "specs": "Conversational web navigation, HTML, screenshots, multi-turn dialogue",
    "year": "2024",
    "id": "saved-1769641435282-dst4i",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp2Kyl57n15UZhjoXgPi7NJo1uZiphvEGpBXDx6h1WtRB3AleNkexHO1U_H15Zc1TgZVgeCr5RSqxJwg9km2vaZsFWKr3LthmwzGhmd_UfNrwGUa2GAcN3c1OpGN7tvIhrnPilarac4zok",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyKJlTzi5TejwWtZvpq9obbvy6DF5pMr5hs-MI-SQWHvccNedt78hs4SV3SCsc9u_x9dGU80vkny4Ou182jsmahbCXC7hAAVoOzeSTfdVlaOM4OQkptsNj_PVclr1ZaSpMCJGUuQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhfQrSUT9R5saE1NmJNmpsFcSk_7I8tNMdunyv4jpwuOdR_pWp1WXnU8O-p_qig-zjTLrvrxKESzDrt2nR8nz0BpYFdjF24hSnVhsGQ-YI_UHUr3IdrKDcFIcE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHULw4wQWwe4v2Mkh0AZ-60usln6XuCpoB-_mfYcLMl-RJmXguUhJk61zFF4snh795yZMr8Xr0RBuOoE-6hHxhdkAXVDA4MLV0ILj-Qf0wBvJWrI-9eP9xpOA4ajdxdq9VZ5ioi",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMdlhWl4x-vgpuNTtqnupc9bInR8fLw1dvOYK4eui77lUOtVzlxIiQfQhwTC2K3jA6a9iH7uyYNzFVeWSj1Yu7FxEsyooqtiIRSMlqdj9iZ4NbjhJsQi7B88WRFwkAGYsyt6Z3",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
    "paperLink": "https://arxiv.org/abs/2402.07945",
    "description": "An environment and dataset for VLM agents to interact with real computer screens, including planning, acting, and reflecting phases.",
    "authors": [
      "Runliang Niu",
      "Xuejing Liu",
      "Wei Ding",
      "Boyu Li",
      "Shijie Wang",
      "Jialin Wang",
      "Jiabo Ye",
      "Pengfei Wang",
      "Jianqiang Wan",
      "Humen Zhong"
    ],
    "githubLink": "https://github.com/niuzaisheng/ScreenAgent",
    "itemCount": "N/A (Environment & Dataset)",
    "source": "arXiv",
    "specs": "Screenshots, mouse/keyboard actions, VLM interaction",
    "year": "2024",
    "id": "saved-1769641435282-0lhds",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp2Kyl57n15UZhjoXgPi7NJo1uZiphvEGpBXDx6h1WtRB3AleNkexHO1U_H15Zc1TgZVgeCr5RSqxJwg9km2vaZsFWKr3LthmwzGhmd_UfNrwGUa2GAcN3c1OpGN7tvIhrnPilarac4zok",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyKJlTzi5TejwWtZvpq9obbvy6DF5pMr5hs-MI-SQWHvccNedt78hs4SV3SCsc9u_x9dGU80vkny4Ou182jsmahbCXC7hAAVoOzeSTfdVlaOM4OQkptsNj_PVclr1ZaSpMCJGUuQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhfQrSUT9R5saE1NmJNmpsFcSk_7I8tNMdunyv4jpwuOdR_pWp1WXnU8O-p_qig-zjTLrvrxKESzDrt2nR8nz0BpYFdjF24hSnVhsGQ-YI_UHUr3IdrKDcFIcE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHULw4wQWwe4v2Mkh0AZ-60usln6XuCpoB-_mfYcLMl-RJmXguUhJk61zFF4snh795yZMr8Xr0RBuOoE-6hHxhdkAXVDA4MLV0ILj-Qf0wBvJWrI-9eP9xpOA4ajdxdq9VZ5ioi",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMdlhWl4x-vgpuNTtqnupc9bInR8fLw1dvOYK4eui77lUOtVzlxIiQfQhwTC2K3jA6a9iH7uyYNzFVeWSj1Yu7FxEsyooqtiIRSMlqdj9iZ4NbjhJsQi7B88WRFwkAGYsyt6Z3",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Android in the Wild: A Large-Scale Dataset for Android Device Control",
    "paperLink": "https://arxiv.org/abs/2307.10088",
    "description": "A large-scale dataset of human demonstrations for device control on Android, containing episodes with screens, actions, and natural language instructions.",
    "authors": [
      "Christopher Rawles",
      "Alice Li",
      "Daniel Rodriguez",
      "Oriana Riva",
      "Timothy Lillicrap"
    ],
    "githubLink": "https://github.com/google-research/google-research/tree/master/android_in_the_wild",
    "itemCount": "715,000 episodes",
    "source": "arXiv",
    "specs": "Android screenshots, touch/gesture actions, natural language instructions",
    "year": "2023",
    "id": "saved-1769641435282-gat8k",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp2Kyl57n15UZhjoXgPi7NJo1uZiphvEGpBXDx6h1WtRB3AleNkexHO1U_H15Zc1TgZVgeCr5RSqxJwg9km2vaZsFWKr3LthmwzGhmd_UfNrwGUa2GAcN3c1OpGN7tvIhrnPilarac4zok",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyKJlTzi5TejwWtZvpq9obbvy6DF5pMr5hs-MI-SQWHvccNedt78hs4SV3SCsc9u_x9dGU80vkny4Ou182jsmahbCXC7hAAVoOzeSTfdVlaOM4OQkptsNj_PVclr1ZaSpMCJGUuQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhfQrSUT9R5saE1NmJNmpsFcSk_7I8tNMdunyv4jpwuOdR_pWp1WXnU8O-p_qig-zjTLrvrxKESzDrt2nR8nz0BpYFdjF24hSnVhsGQ-YI_UHUr3IdrKDcFIcE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHULw4wQWwe4v2Mkh0AZ-60usln6XuCpoB-_mfYcLMl-RJmXguUhJk61zFF4snh795yZMr8Xr0RBuOoE-6hHxhdkAXVDA4MLV0ILj-Qf0wBvJWrI-9eP9xpOA4ajdxdq9VZ5ioi",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMdlhWl4x-vgpuNTtqnupc9bInR8fLw1dvOYK4eui77lUOtVzlxIiQfQhwTC2K3jA6a9iH7uyYNzFVeWSj1Yu7FxEsyooqtiIRSMlqdj9iZ4NbjhJsQi7B88WRFwkAGYsyt6Z3",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "ULT (UnLeakedTestBench)",
    "paperLink": "https://arxiv.org/abs/2508.00656",
    "description": "A benchmark specifically designed for function-level unit test generation from real-world Python functions. It features a rigorous curation process to ensure high cyclomatic complexity and mitigate data contamination.",
    "authors": [
      "Y. Huang",
      "et al."
    ],
    "githubLink": "https://github.com/huangd1999/UnLeakedTestBench",
    "itemCount": "3,909 functions",
    "source": "arXiv",
    "specs": "Python, Function-level code",
    "year": "2025",
    "id": "saved-1769641553633-6t2ra",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8ucFlGK803i_4f2NkFrTmc1q7uJ_QGeikD82sg9ic1iRAl2lBZntg0uiL_yfWwRHOtSAKNqHV-lMXy3Yi8KDqwPWkfJdZh_YeLn8iA9IQ7qQRybNF7nBbktBinK4TMorRm4XE_hAomU_9W-MsH3j2Dfiwo3plJbZACA2aqU-bdd5_",
        "title": "evosuite.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFriZ5qH791K2DhzBJKsKCRqmErtHqk-TIMv14U7hlFDgjC1E_3WlJ_kuQVF9gjvnnv9LYFNA9uxKP88m4GgW7Wmg42uR-xFT8MLL_bkaGWpbEeWForIjzedRQHJ6qikLO9iAhWfVoRHcxnJmIYzt_wY4Gu",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGX9qIIaXFvSjKs347PTSkCTCIWAq7gTdx2CsboipqHBuQZs6sEat4MWsJOK69ZAVbh2jqbbj1niNUuIkH6U1jTwvDsBuMDgiLd4xmljK9M-5C8RMdBjj-lTaxXfMWwNQ==",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ProjectTest",
    "paperLink": "https://arxiv.org/abs/2502.06556",
    "description": "A project-level benchmark for unit test generation covering Python, Java, and JavaScript. Unlike function-level benchmarks, it focuses on generating tests within the context of moderate-sized, high-quality projects.",
    "authors": [
      "Research Team (arXiv)"
    ],
    "githubLink": "N/A",
    "itemCount": "60 projects (20 per language)",
    "source": "arXiv",
    "specs": "Python, Java, JavaScript, Project-level",
    "year": "2025",
    "id": "saved-1769641553633-g8yua",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8ucFlGK803i_4f2NkFrTmc1q7uJ_QGeikD82sg9ic1iRAl2lBZntg0uiL_yfWwRHOtSAKNqHV-lMXy3Yi8KDqwPWkfJdZh_YeLn8iA9IQ7qQRybNF7nBbktBinK4TMorRm4XE_hAomU_9W-MsH3j2Dfiwo3plJbZACA2aqU-bdd5_",
        "title": "evosuite.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFriZ5qH791K2DhzBJKsKCRqmErtHqk-TIMv14U7hlFDgjC1E_3WlJ_kuQVF9gjvnnv9LYFNA9uxKP88m4GgW7Wmg42uR-xFT8MLL_bkaGWpbEeWForIjzedRQHJ6qikLO9iAhWfVoRHcxnJmIYzt_wY4Gu",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGX9qIIaXFvSjKs347PTSkCTCIWAq7gTdx2CsboipqHBuQZs6sEat4MWsJOK69ZAVbh2jqbbj1niNUuIkH6U1jTwvDsBuMDgiLd4xmljK9M-5C8RMdBjj-lTaxXfMWwNQ==",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Classes2Test",
    "paperLink": "https://arxiv.org/abs/2408.08322",
    "description": "An annotated corpus that maps Java classes under test to their corresponding human-written JUnit test classes. It serves as a standardized ground truth for benchmarking automated systems at the class level.",
    "authors": [
      "F. Lops",
      "et al."
    ],
    "githubLink": "N/A",
    "itemCount": "9,410 repositories (filtered subset)",
    "source": "Semantic Scholar",
    "specs": "Java, Class-level, JUnit",
    "year": "2025",
    "id": "saved-1769641553633-qzr3f",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8ucFlGK803i_4f2NkFrTmc1q7uJ_QGeikD82sg9ic1iRAl2lBZntg0uiL_yfWwRHOtSAKNqHV-lMXy3Yi8KDqwPWkfJdZh_YeLn8iA9IQ7qQRybNF7nBbktBinK4TMorRm4XE_hAomU_9W-MsH3j2Dfiwo3plJbZACA2aqU-bdd5_",
        "title": "evosuite.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFriZ5qH791K2DhzBJKsKCRqmErtHqk-TIMv14U7hlFDgjC1E_3WlJ_kuQVF9gjvnnv9LYFNA9uxKP88m4GgW7Wmg42uR-xFT8MLL_bkaGWpbEeWForIjzedRQHJ6qikLO9iAhWfVoRHcxnJmIYzt_wY4Gu",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGX9qIIaXFvSjKs347PTSkCTCIWAq7gTdx2CsboipqHBuQZs6sEat4MWsJOK69ZAVbh2jqbbj1niNUuIkH6U1jTwvDsBuMDgiLd4xmljK9M-5C8RMdBjj-lTaxXfMWwNQ==",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Go-UT-Bench",
    "paperLink": "https://arxiv.org/abs/2511.00000",
    "description": "A benchmark dataset for evaluating LLM-based unit test generation in the Go programming language. It includes pairs of code and unit tests drawn from permissively licensed repositories.",
    "authors": [
      "Research Team (arXiv)"
    ],
    "githubLink": "N/A",
    "itemCount": "5,264 pairs",
    "source": "arXiv",
    "specs": "Go (Golang), Unit Tests",
    "year": "2025",
    "id": "saved-1769641553633-6i2ci",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8ucFlGK803i_4f2NkFrTmc1q7uJ_QGeikD82sg9ic1iRAl2lBZntg0uiL_yfWwRHOtSAKNqHV-lMXy3Yi8KDqwPWkfJdZh_YeLn8iA9IQ7qQRybNF7nBbktBinK4TMorRm4XE_hAomU_9W-MsH3j2Dfiwo3plJbZACA2aqU-bdd5_",
        "title": "evosuite.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFriZ5qH791K2DhzBJKsKCRqmErtHqk-TIMv14U7hlFDgjC1E_3WlJ_kuQVF9gjvnnv9LYFNA9uxKP88m4GgW7Wmg42uR-xFT8MLL_bkaGWpbEeWForIjzedRQHJ6qikLO9iAhWfVoRHcxnJmIYzt_wY4Gu",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGX9qIIaXFvSjKs347PTSkCTCIWAq7gTdx2CsboipqHBuQZs6sEat4MWsJOK69ZAVbh2jqbbj1niNUuIkH6U1jTwvDsBuMDgiLd4xmljK9M-5C8RMdBjj-lTaxXfMWwNQ==",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Methods2Test",
    "paperLink": "https://arxiv.org/abs/2203.12776",
    "description": "A large, supervised dataset of test cases mapped to corresponding methods under test (focal methods), extracted from Java open-source projects. It contains pairs of JUnit tests and focal methods with rich metadata.",
    "authors": [
      "Michele Tufano",
      "Shao Kun Deng",
      "Neel Sundaresan",
      "Alexey Svyatkovskiy"
    ],
    "githubLink": "https://github.com/microsoft/methods2test",
    "itemCount": "780,944 pairs",
    "source": "arXiv",
    "specs": "Java, JUnit, JSON format, Text",
    "year": "2022",
    "id": "saved-1769641553633-afr30",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8ucFlGK803i_4f2NkFrTmc1q7uJ_QGeikD82sg9ic1iRAl2lBZntg0uiL_yfWwRHOtSAKNqHV-lMXy3Yi8KDqwPWkfJdZh_YeLn8iA9IQ7qQRybNF7nBbktBinK4TMorRm4XE_hAomU_9W-MsH3j2Dfiwo3plJbZACA2aqU-bdd5_",
        "title": "evosuite.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFriZ5qH791K2DhzBJKsKCRqmErtHqk-TIMv14U7hlFDgjC1E_3WlJ_kuQVF9gjvnnv9LYFNA9uxKP88m4GgW7Wmg42uR-xFT8MLL_bkaGWpbEeWForIjzedRQHJ6qikLO9iAhWfVoRHcxnJmIYzt_wY4Gu",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGX9qIIaXFvSjKs347PTSkCTCIWAq7gTdx2CsboipqHBuQZs6sEat4MWsJOK69ZAVbh2jqbbj1niNUuIkH6U1jTwvDsBuMDgiLd4xmljK9M-5C8RMdBjj-lTaxXfMWwNQ==",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Atlas (Assert Statements)",
    "paperLink": "https://arxiv.org/abs/2002.12345",
    "description": "A dataset and approach for learning to generate meaningful assert statements for unit test cases. The dataset consists of test methods and focal methods extracted from GitHub projects to train NMT models.",
    "authors": [
      "Cody Watson",
      "Michele Tufano",
      "Kevin Moran",
      "Gabriele Bavota",
      "Denys Poshyvanyk"
    ],
    "githubLink": "N/A",
    "itemCount": "Thousands of test methods",
    "source": "arXiv",
    "specs": "Java, Assert Statements, NMT",
    "year": "2020",
    "id": "saved-1769641553633-pmszn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8ucFlGK803i_4f2NkFrTmc1q7uJ_QGeikD82sg9ic1iRAl2lBZntg0uiL_yfWwRHOtSAKNqHV-lMXy3Yi8KDqwPWkfJdZh_YeLn8iA9IQ7qQRybNF7nBbktBinK4TMorRm4XE_hAomU_9W-MsH3j2Dfiwo3plJbZACA2aqU-bdd5_",
        "title": "evosuite.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFriZ5qH791K2DhzBJKsKCRqmErtHqk-TIMv14U7hlFDgjC1E_3WlJ_kuQVF9gjvnnv9LYFNA9uxKP88m4GgW7Wmg42uR-xFT8MLL_bkaGWpbEeWForIjzedRQHJ6qikLO9iAhWfVoRHcxnJmIYzt_wY4Gu",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGX9qIIaXFvSjKs347PTSkCTCIWAq7gTdx2CsboipqHBuQZs6sEat4MWsJOK69ZAVbh2jqbbj1niNUuIkH6U1jTwvDsBuMDgiLd4xmljK9M-5C8RMdBjj-lTaxXfMWwNQ==",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ClassEval",
    "paperLink": "https://arxiv.org/abs/2308.01861",
    "description": "A manually-crafted benchmark for evaluating LLMs on class-level code generation, moving beyond independent function synthesis to testing class-level structure and dependencies.",
    "authors": [
      "Xueying Du",
      "Mingwei Liu",
      "Kaixin Wang",
      "Hanlin Wang",
      "Junwei Liu",
      "Yixuan Chen",
      "Jiayi Feng",
      "Chaofeng Sha",
      "Xin Peng",
      "Yiling Lou"
    ],
    "githubLink": "https://github.com/FudanSELab/ClassEval",
    "itemCount": "100 classes (412 methods)",
    "source": "arXiv",
    "specs": "Python; Class-level generation; Input: class skeleton/description; Output: class implementation",
    "year": "2023",
    "id": "saved-1769641718916-csxsh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaf716fAJuR_sUogiWcAUXZ74u-PFZupVZnzY7_EbcnXVjTIt6aoCk4ZnqAAYcjQs39QOFfrG69nqVl2PwXwfsswYqAd-am8moLdexCE6zFwFhIK4Fs9vrIgUY5pXMRtBlf6ZPPq8hgCRT5Uc0kA9bpmKXiKNd4z60IM9PODAwbb7QZeAY4GR3aTfA27DgDiVuZyrueCgywGywQ1ciIQ==",
        "title": "plainenglish.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnv0LKd-khN34Ta-oP3ukKMYH7vOS9_UuF2ApsrZL5MtzKjHB-VXvvoH_XeMWOtB3XT7QepWoNJUuctcuza-ZppbS0dfNv7_j_w9ceoHIUiTkXflk_rUNbW9AWXlyOfZWjNHmUYIvZjPIuwC61-23fRu-vDBW6oh2KS_bvwhP8otScvNSORxya7oSZo_fYd9bhzZxIjojAMCjhRQZZDoo=",
        "title": "datacamp.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFr2y_l5GullJ6XorUsL36zSIquGZan-4y8SyktmXH7gp4ni4Oud4pRrwlJHk10hnlEe5sng9DNMYT-2R4jm4eMLWOfC2aVGGiHYx3hXW6A3B3cliD8ZXvlY3GtpKCs8D_u4J8DCI6IBzo=",
        "title": "mlr.press"
      }
    ]
  },
  {
    "title": "APPS (Automated Programming Progress Standard)",
    "paperLink": "https://arxiv.org/abs/2105.05816",
    "description": "A benchmark for measuring coding challenge competence, consisting of problems collected from coding competitions. It includes tests for functional correctness and covers introductory, interview, and competition-level difficulty.",
    "authors": [
      "Dan Hendrycks",
      "Steven Basart",
      "Saurav Kadavath",
      "Mantas Mazeika",
      "Akul Arora",
      "Ethan Guo",
      "Collin Burns",
      "Samir Puranik",
      "Horace He",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "githubLink": "https://github.com/hendrycks/apps",
    "itemCount": "10,000 problems (5,000 test)",
    "source": "arXiv",
    "specs": "Python; Coding competition problems; Input: problem statement; Output: full program",
    "year": "2021",
    "id": "saved-1769641718916-f8hb8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaf716fAJuR_sUogiWcAUXZ74u-PFZupVZnzY7_EbcnXVjTIt6aoCk4ZnqAAYcjQs39QOFfrG69nqVl2PwXwfsswYqAd-am8moLdexCE6zFwFhIK4Fs9vrIgUY5pXMRtBlf6ZPPq8hgCRT5Uc0kA9bpmKXiKNd4z60IM9PODAwbb7QZeAY4GR3aTfA27DgDiVuZyrueCgywGywQ1ciIQ==",
        "title": "plainenglish.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnv0LKd-khN34Ta-oP3ukKMYH7vOS9_UuF2ApsrZL5MtzKjHB-VXvvoH_XeMWOtB3XT7QepWoNJUuctcuza-ZppbS0dfNv7_j_w9ceoHIUiTkXflk_rUNbW9AWXlyOfZWjNHmUYIvZjPIuwC61-23fRu-vDBW6oh2KS_bvwhP8otScvNSORxya7oSZo_fYd9bhzZxIjojAMCjhRQZZDoo=",
        "title": "datacamp.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFr2y_l5GullJ6XorUsL36zSIquGZan-4y8SyktmXH7gp4ni4Oud4pRrwlJHk10hnlEe5sng9DNMYT-2R4jm4eMLWOfC2aVGGiHYx3hXW6A3B3cliD8ZXvlY3GtpKCs8D_u4J8DCI6IBzo=",
        "title": "mlr.press"
      }
    ]
  },
  {
    "title": "GenBench",
    "paperLink": "https://arxiv.org/abs/2406.01627",
    "description": "A comprehensive benchmarking suite for systematic evaluation of genomic foundation models, covering diverse tasks from local (short-range) to global (long-range) views of genomics, including coding region, non-coding region, and genome structure prediction.",
    "authors": [
      "Zicheng Liu",
      "Jiahui Li",
      "Stan Z. Li"
    ],
    "githubLink": "https://github.com/jimmylihui/GenBench",
    "itemCount": "Multiple datasets covering 10+ models",
    "source": "arXiv",
    "specs": "Short-range and long-range genomic tasks, DNA sequences",
    "year": "2024",
    "id": "saved-1769656927644-oickd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2zyWTtkbQs2cuMyReLNTyQPrD1PCahfRLdIRnNB9RE8Y0bMlTlou2higaLOla0Uk-f8eAk6C099VsPVORdqdweubamZQGrCocVQIgcxzojDiCH-Sm-r3EQQil14mjZ-Wv6IaYaog3vwb0EwZ7fM9ezwCD2fo2cQ5lh1lcx3u95TuXhJo=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEputQSMV0SHdSdn8NGq3UclcMgUCqVeAFueEj5H39qFBDTLPPqtBxkipsHQes-oVhhDz8ZeUkbpL58vSvaq1TmhTD_TNsCB-OWUAbz0SpNNOwt7IbqzBFFL__upOFLSNjhExHmNWSRk06LD5Ep",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF80MlJWGLzIjcaZMBaUfOEYvW-62BsxHAoaqzlR0Yz5sUIMEs8xkJXXhEstpbsHEDy6L76UUdyMqmka0Uh9MdemlnsZLVFh6hY8yfXzsXF4S6GBahPaYznzOYb-mvcQamvqw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEyyied3waB_r--MsctPs44XWX1FkPGHxpYoIEteA3fqP2h1t1AhMCAcp59rXgFpWi-rCZmfK5CjWETq0Iw9QdJw1qvzQqUvtf4wQN9G4inLwNqkJG0JVlPtcw9pavH",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEwQ5lWYhC7tZFBR3IT4XtYuY7VJND3BxzX-SXS8PI1UMLIIgb0KwoOOJvix6pnarP8QBXkpA5udo_IxNbydl24-3Us3P9qOB7dPFWyrOVmFXznRNnkv71T-EEVgB0jkcYmism",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnJYdCP1lI4rChY2y_gSh6j5zJ6ftO_V8yqVACqo6vMKsB9ZNiBrehsQr5zILpPtuD2d6qsBimH1wB5Wqf_uttWkSWFIb21JpDCbKveM4xQNmQYasK1Cgaxo-tRt3K0PuS",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXTmbrByciEMukd5AP7pbTR_090elQsqBMFH6skuU5ed0jizRVJ_i2O-OEEqsmfMnNahz1uO3OwL3BrdG9TxfQ_5Q3KTScb4j6G-ea4ZYrzKfsxVMJQGwKL0TSlzSC2uPlR2IsyA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBfjMfjm5R7gixWevkX-pjHBJKFaaVvWMKkZE2dlXKNOjbzbX543CL2gUnp6NmcXyP5zs_2GiBEHZqx-jnkUrcTin1VNaEQsGKdwslxx7kvtS19xkJbjOql_XI9-EDSI54oDQw7gJy0176Uu4U-96qIYa0v37b",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGOPQ_WRsT-rliBn3Ao0NoEKV5dZqS-toGZzLj6x8cjxtcmRiMOJoDCnLMD_fFOqaZFq4QRlIHp42EgBs5ETnu6oUnnr7KhJbfVdYSz2r50m06DMuNER_t6utri6Yc5w6OsFlsdvFA1A0Z5AKoBDKll_Iq3Lp9M",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGazC2NFKABX1Nx1w371tPN-36yZ4vfKyHYnPyTNxCp6fx2EIAOJ9gjWgRUwRVZVP08EaSjPOnRXfA6rZsOxdowpQ4aMqra2A8a9xpxoPP7DkGCgWAI_pyjhXnX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJwx06SqLcD2G7N1CmdCKNMlb4eP0KTfFbKwZRU3Zl-c99dMThXhvyJlL-6huSvZL5qHMI8LdbStFHAK1fYJWoM7JFPhpayoQhDCT-tajiCLZBXmZV2VtJc5XlHNrO6OsFnhmUBuL8mH0k8FAoPoc0VZ2xELhIUMnUfMrUE3THR_G1Gd9SXk-GhbCwy4cmKMUjbEUa9Znk6A_ZB2jJs-JIVhHWdra7EDNQ7icawnlaB012kqXV",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Q6GkPnsmXnuj3OqlWljL7iLQdujxc6gPoKWHbkxXq5wPgzJUhYC16SDq2u8QUcAsJVpQVdfv7bmR_wyrtxtBrVaE9aG2QNF4akVZo3WLgVRaW0PWXgwRSsCe",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Genomics Long-Range Benchmark (LRB)",
    "paperLink": "https://openreview.net/forum?id=eWk13aT8K7",
    "description": "A suite of biologically relevant tasks designed to challenge and advance DNA language models, specifically focusing on tasks requiring long-range sequence dependencies such as variant effect and gene expression prediction.",
    "authors": [
      "Evan Trop",
      "Yair Schiff",
      "Volodymyr Kuleshov",
      "InstaDeep Team"
    ],
    "githubLink": "https://huggingface.co/datasets/InstaDeepAI/genomics-long-range-benchmark",
    "itemCount": "9 tasks",
    "source": "Hugging Face",
    "specs": "Long-range DNA prediction, variant effects, CAGE prediction",
    "year": "2024",
    "id": "saved-1769656927644-pve5j",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2zyWTtkbQs2cuMyReLNTyQPrD1PCahfRLdIRnNB9RE8Y0bMlTlou2higaLOla0Uk-f8eAk6C099VsPVORdqdweubamZQGrCocVQIgcxzojDiCH-Sm-r3EQQil14mjZ-Wv6IaYaog3vwb0EwZ7fM9ezwCD2fo2cQ5lh1lcx3u95TuXhJo=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEputQSMV0SHdSdn8NGq3UclcMgUCqVeAFueEj5H39qFBDTLPPqtBxkipsHQes-oVhhDz8ZeUkbpL58vSvaq1TmhTD_TNsCB-OWUAbz0SpNNOwt7IbqzBFFL__upOFLSNjhExHmNWSRk06LD5Ep",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF80MlJWGLzIjcaZMBaUfOEYvW-62BsxHAoaqzlR0Yz5sUIMEs8xkJXXhEstpbsHEDy6L76UUdyMqmka0Uh9MdemlnsZLVFh6hY8yfXzsXF4S6GBahPaYznzOYb-mvcQamvqw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEyyied3waB_r--MsctPs44XWX1FkPGHxpYoIEteA3fqP2h1t1AhMCAcp59rXgFpWi-rCZmfK5CjWETq0Iw9QdJw1qvzQqUvtf4wQN9G4inLwNqkJG0JVlPtcw9pavH",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEwQ5lWYhC7tZFBR3IT4XtYuY7VJND3BxzX-SXS8PI1UMLIIgb0KwoOOJvix6pnarP8QBXkpA5udo_IxNbydl24-3Us3P9qOB7dPFWyrOVmFXznRNnkv71T-EEVgB0jkcYmism",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnJYdCP1lI4rChY2y_gSh6j5zJ6ftO_V8yqVACqo6vMKsB9ZNiBrehsQr5zILpPtuD2d6qsBimH1wB5Wqf_uttWkSWFIb21JpDCbKveM4xQNmQYasK1Cgaxo-tRt3K0PuS",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXTmbrByciEMukd5AP7pbTR_090elQsqBMFH6skuU5ed0jizRVJ_i2O-OEEqsmfMnNahz1uO3OwL3BrdG9TxfQ_5Q3KTScb4j6G-ea4ZYrzKfsxVMJQGwKL0TSlzSC2uPlR2IsyA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBfjMfjm5R7gixWevkX-pjHBJKFaaVvWMKkZE2dlXKNOjbzbX543CL2gUnp6NmcXyP5zs_2GiBEHZqx-jnkUrcTin1VNaEQsGKdwslxx7kvtS19xkJbjOql_XI9-EDSI54oDQw7gJy0176Uu4U-96qIYa0v37b",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGOPQ_WRsT-rliBn3Ao0NoEKV5dZqS-toGZzLj6x8cjxtcmRiMOJoDCnLMD_fFOqaZFq4QRlIHp42EgBs5ETnu6oUnnr7KhJbfVdYSz2r50m06DMuNER_t6utri6Yc5w6OsFlsdvFA1A0Z5AKoBDKll_Iq3Lp9M",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGazC2NFKABX1Nx1w371tPN-36yZ4vfKyHYnPyTNxCp6fx2EIAOJ9gjWgRUwRVZVP08EaSjPOnRXfA6rZsOxdowpQ4aMqra2A8a9xpxoPP7DkGCgWAI_pyjhXnX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJwx06SqLcD2G7N1CmdCKNMlb4eP0KTfFbKwZRU3Zl-c99dMThXhvyJlL-6huSvZL5qHMI8LdbStFHAK1fYJWoM7JFPhpayoQhDCT-tajiCLZBXmZV2VtJc5XlHNrO6OsFnhmUBuL8mH0k8FAoPoc0VZ2xELhIUMnUfMrUE3THR_G1Gd9SXk-GhbCwy4cmKMUjbEUa9Znk6A_ZB2jJs-JIVhHWdra7EDNQ7icawnlaB012kqXV",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Q6GkPnsmXnuj3OqlWljL7iLQdujxc6gPoKWHbkxXq5wPgzJUhYC16SDq2u8QUcAsJVpQVdfv7bmR_wyrtxtBrVaE9aG2QNF4akVZo3WLgVRaW0PWXgwRSsCe",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Plant Genomic Benchmark",
    "paperLink": "https://huggingface.co/datasets/InstaDeepAI/plant-genomic-benchmark",
    "description": "A benchmark dataset featuring evaluation tasks for plant genomics, including single/multi-output regression and classification, derived from the AgroNT foundational model project.",
    "authors": [
      "InstaDeepAI"
    ],
    "githubLink": "https://huggingface.co/datasets/InstaDeepAI/plant-genomic-benchmark",
    "itemCount": "8 evaluation tasks",
    "source": "Hugging Face",
    "specs": "Plant genome sequences, regression, classification",
    "year": "2024",
    "id": "saved-1769656927644-wrzut",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2zyWTtkbQs2cuMyReLNTyQPrD1PCahfRLdIRnNB9RE8Y0bMlTlou2higaLOla0Uk-f8eAk6C099VsPVORdqdweubamZQGrCocVQIgcxzojDiCH-Sm-r3EQQil14mjZ-Wv6IaYaog3vwb0EwZ7fM9ezwCD2fo2cQ5lh1lcx3u95TuXhJo=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEputQSMV0SHdSdn8NGq3UclcMgUCqVeAFueEj5H39qFBDTLPPqtBxkipsHQes-oVhhDz8ZeUkbpL58vSvaq1TmhTD_TNsCB-OWUAbz0SpNNOwt7IbqzBFFL__upOFLSNjhExHmNWSRk06LD5Ep",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF80MlJWGLzIjcaZMBaUfOEYvW-62BsxHAoaqzlR0Yz5sUIMEs8xkJXXhEstpbsHEDy6L76UUdyMqmka0Uh9MdemlnsZLVFh6hY8yfXzsXF4S6GBahPaYznzOYb-mvcQamvqw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEyyied3waB_r--MsctPs44XWX1FkPGHxpYoIEteA3fqP2h1t1AhMCAcp59rXgFpWi-rCZmfK5CjWETq0Iw9QdJw1qvzQqUvtf4wQN9G4inLwNqkJG0JVlPtcw9pavH",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEwQ5lWYhC7tZFBR3IT4XtYuY7VJND3BxzX-SXS8PI1UMLIIgb0KwoOOJvix6pnarP8QBXkpA5udo_IxNbydl24-3Us3P9qOB7dPFWyrOVmFXznRNnkv71T-EEVgB0jkcYmism",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnJYdCP1lI4rChY2y_gSh6j5zJ6ftO_V8yqVACqo6vMKsB9ZNiBrehsQr5zILpPtuD2d6qsBimH1wB5Wqf_uttWkSWFIb21JpDCbKveM4xQNmQYasK1Cgaxo-tRt3K0PuS",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXTmbrByciEMukd5AP7pbTR_090elQsqBMFH6skuU5ed0jizRVJ_i2O-OEEqsmfMnNahz1uO3OwL3BrdG9TxfQ_5Q3KTScb4j6G-ea4ZYrzKfsxVMJQGwKL0TSlzSC2uPlR2IsyA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBfjMfjm5R7gixWevkX-pjHBJKFaaVvWMKkZE2dlXKNOjbzbX543CL2gUnp6NmcXyP5zs_2GiBEHZqx-jnkUrcTin1VNaEQsGKdwslxx7kvtS19xkJbjOql_XI9-EDSI54oDQw7gJy0176Uu4U-96qIYa0v37b",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGOPQ_WRsT-rliBn3Ao0NoEKV5dZqS-toGZzLj6x8cjxtcmRiMOJoDCnLMD_fFOqaZFq4QRlIHp42EgBs5ETnu6oUnnr7KhJbfVdYSz2r50m06DMuNER_t6utri6Yc5w6OsFlsdvFA1A0Z5AKoBDKll_Iq3Lp9M",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGazC2NFKABX1Nx1w371tPN-36yZ4vfKyHYnPyTNxCp6fx2EIAOJ9gjWgRUwRVZVP08EaSjPOnRXfA6rZsOxdowpQ4aMqra2A8a9xpxoPP7DkGCgWAI_pyjhXnX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJwx06SqLcD2G7N1CmdCKNMlb4eP0KTfFbKwZRU3Zl-c99dMThXhvyJlL-6huSvZL5qHMI8LdbStFHAK1fYJWoM7JFPhpayoQhDCT-tajiCLZBXmZV2VtJc5XlHNrO6OsFnhmUBuL8mH0k8FAoPoc0VZ2xELhIUMnUfMrUE3THR_G1Gd9SXk-GhbCwy4cmKMUjbEUa9Znk6A_ZB2jJs-JIVhHWdra7EDNQ7icawnlaB012kqXV",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Q6GkPnsmXnuj3OqlWljL7iLQdujxc6gPoKWHbkxXq5wPgzJUhYC16SDq2u8QUcAsJVpQVdfv7bmR_wyrtxtBrVaE9aG2QNF4akVZo3WLgVRaW0PWXgwRSsCe",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Genomic Benchmarks",
    "paperLink": "https://doi.org/10.1186/s12859-022-05138-x",
    "description": "A collection of curated and easily accessible sequence classification datasets in the field of genomics, focusing on regulatory elements like promoters, enhancers, and open chromatin regions.",
    "authors": [
      "Katarína Grešová",
      "Vlastimil Martínek",
      "David Čechák",
      "Petr Šimeček",
      "Panagiotis Alexiou"
    ],
    "githubLink": "https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks",
    "itemCount": "9 datasets",
    "source": "Hugging Face",
    "specs": "DNA sequence classification, CSV/Genomic coordinates",
    "year": "2023",
    "id": "saved-1769656927644-n885b",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2zyWTtkbQs2cuMyReLNTyQPrD1PCahfRLdIRnNB9RE8Y0bMlTlou2higaLOla0Uk-f8eAk6C099VsPVORdqdweubamZQGrCocVQIgcxzojDiCH-Sm-r3EQQil14mjZ-Wv6IaYaog3vwb0EwZ7fM9ezwCD2fo2cQ5lh1lcx3u95TuXhJo=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEputQSMV0SHdSdn8NGq3UclcMgUCqVeAFueEj5H39qFBDTLPPqtBxkipsHQes-oVhhDz8ZeUkbpL58vSvaq1TmhTD_TNsCB-OWUAbz0SpNNOwt7IbqzBFFL__upOFLSNjhExHmNWSRk06LD5Ep",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF80MlJWGLzIjcaZMBaUfOEYvW-62BsxHAoaqzlR0Yz5sUIMEs8xkJXXhEstpbsHEDy6L76UUdyMqmka0Uh9MdemlnsZLVFh6hY8yfXzsXF4S6GBahPaYznzOYb-mvcQamvqw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEyyied3waB_r--MsctPs44XWX1FkPGHxpYoIEteA3fqP2h1t1AhMCAcp59rXgFpWi-rCZmfK5CjWETq0Iw9QdJw1qvzQqUvtf4wQN9G4inLwNqkJG0JVlPtcw9pavH",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEwQ5lWYhC7tZFBR3IT4XtYuY7VJND3BxzX-SXS8PI1UMLIIgb0KwoOOJvix6pnarP8QBXkpA5udo_IxNbydl24-3Us3P9qOB7dPFWyrOVmFXznRNnkv71T-EEVgB0jkcYmism",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnJYdCP1lI4rChY2y_gSh6j5zJ6ftO_V8yqVACqo6vMKsB9ZNiBrehsQr5zILpPtuD2d6qsBimH1wB5Wqf_uttWkSWFIb21JpDCbKveM4xQNmQYasK1Cgaxo-tRt3K0PuS",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXTmbrByciEMukd5AP7pbTR_090elQsqBMFH6skuU5ed0jizRVJ_i2O-OEEqsmfMnNahz1uO3OwL3BrdG9TxfQ_5Q3KTScb4j6G-ea4ZYrzKfsxVMJQGwKL0TSlzSC2uPlR2IsyA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBfjMfjm5R7gixWevkX-pjHBJKFaaVvWMKkZE2dlXKNOjbzbX543CL2gUnp6NmcXyP5zs_2GiBEHZqx-jnkUrcTin1VNaEQsGKdwslxx7kvtS19xkJbjOql_XI9-EDSI54oDQw7gJy0176Uu4U-96qIYa0v37b",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGOPQ_WRsT-rliBn3Ao0NoEKV5dZqS-toGZzLj6x8cjxtcmRiMOJoDCnLMD_fFOqaZFq4QRlIHp42EgBs5ETnu6oUnnr7KhJbfVdYSz2r50m06DMuNER_t6utri6Yc5w6OsFlsdvFA1A0Z5AKoBDKll_Iq3Lp9M",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGazC2NFKABX1Nx1w371tPN-36yZ4vfKyHYnPyTNxCp6fx2EIAOJ9gjWgRUwRVZVP08EaSjPOnRXfA6rZsOxdowpQ4aMqra2A8a9xpxoPP7DkGCgWAI_pyjhXnX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJwx06SqLcD2G7N1CmdCKNMlb4eP0KTfFbKwZRU3Zl-c99dMThXhvyJlL-6huSvZL5qHMI8LdbStFHAK1fYJWoM7JFPhpayoQhDCT-tajiCLZBXmZV2VtJc5XlHNrO6OsFnhmUBuL8mH0k8FAoPoc0VZ2xELhIUMnUfMrUE3THR_G1Gd9SXk-GhbCwy4cmKMUjbEUa9Znk6A_ZB2jJs-JIVhHWdra7EDNQ7icawnlaB012kqXV",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Q6GkPnsmXnuj3OqlWljL7iLQdujxc6gPoKWHbkxXq5wPgzJUhYC16SDq2u8QUcAsJVpQVdfv7bmR_wyrtxtBrVaE9aG2QNF4akVZo3WLgVRaW0PWXgwRSsCe",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "GUANinE",
    "paperLink": "https://doi.org/10.1101/2023.10.17.562768",
    "description": "A large-scale, de-noised benchmark for evaluating genomic AI models, focusing on functional genomics tasks like functional element annotation, gene expression prediction, and sequence conservation.",
    "authors": [
      "K. M. Hanson",
      "R. Krishnan",
      "N. I. Lab"
    ],
    "githubLink": "https://github.com/ni-lab/guanine",
    "itemCount": ">60 million training examples",
    "source": "bioRxiv",
    "specs": "Functional genomics, sequence-to-function tasks",
    "year": "2023",
    "id": "saved-1769656927644-h0dlr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2zyWTtkbQs2cuMyReLNTyQPrD1PCahfRLdIRnNB9RE8Y0bMlTlou2higaLOla0Uk-f8eAk6C099VsPVORdqdweubamZQGrCocVQIgcxzojDiCH-Sm-r3EQQil14mjZ-Wv6IaYaog3vwb0EwZ7fM9ezwCD2fo2cQ5lh1lcx3u95TuXhJo=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEputQSMV0SHdSdn8NGq3UclcMgUCqVeAFueEj5H39qFBDTLPPqtBxkipsHQes-oVhhDz8ZeUkbpL58vSvaq1TmhTD_TNsCB-OWUAbz0SpNNOwt7IbqzBFFL__upOFLSNjhExHmNWSRk06LD5Ep",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF80MlJWGLzIjcaZMBaUfOEYvW-62BsxHAoaqzlR0Yz5sUIMEs8xkJXXhEstpbsHEDy6L76UUdyMqmka0Uh9MdemlnsZLVFh6hY8yfXzsXF4S6GBahPaYznzOYb-mvcQamvqw==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEyyied3waB_r--MsctPs44XWX1FkPGHxpYoIEteA3fqP2h1t1AhMCAcp59rXgFpWi-rCZmfK5CjWETq0Iw9QdJw1qvzQqUvtf4wQN9G4inLwNqkJG0JVlPtcw9pavH",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEwQ5lWYhC7tZFBR3IT4XtYuY7VJND3BxzX-SXS8PI1UMLIIgb0KwoOOJvix6pnarP8QBXkpA5udo_IxNbydl24-3Us3P9qOB7dPFWyrOVmFXznRNnkv71T-EEVgB0jkcYmism",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHnJYdCP1lI4rChY2y_gSh6j5zJ6ftO_V8yqVACqo6vMKsB9ZNiBrehsQr5zILpPtuD2d6qsBimH1wB5Wqf_uttWkSWFIb21JpDCbKveM4xQNmQYasK1Cgaxo-tRt3K0PuS",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXTmbrByciEMukd5AP7pbTR_090elQsqBMFH6skuU5ed0jizRVJ_i2O-OEEqsmfMnNahz1uO3OwL3BrdG9TxfQ_5Q3KTScb4j6G-ea4ZYrzKfsxVMJQGwKL0TSlzSC2uPlR2IsyA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHBfjMfjm5R7gixWevkX-pjHBJKFaaVvWMKkZE2dlXKNOjbzbX543CL2gUnp6NmcXyP5zs_2GiBEHZqx-jnkUrcTin1VNaEQsGKdwslxx7kvtS19xkJbjOql_XI9-EDSI54oDQw7gJy0176Uu4U-96qIYa0v37b",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGOPQ_WRsT-rliBn3Ao0NoEKV5dZqS-toGZzLj6x8cjxtcmRiMOJoDCnLMD_fFOqaZFq4QRlIHp42EgBs5ETnu6oUnnr7KhJbfVdYSz2r50m06DMuNER_t6utri6Yc5w6OsFlsdvFA1A0Z5AKoBDKll_Iq3Lp9M",
        "title": "biorxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGazC2NFKABX1Nx1w371tPN-36yZ4vfKyHYnPyTNxCp6fx2EIAOJ9gjWgRUwRVZVP08EaSjPOnRXfA6rZsOxdowpQ4aMqra2A8a9xpxoPP7DkGCgWAI_pyjhXnX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJwx06SqLcD2G7N1CmdCKNMlb4eP0KTfFbKwZRU3Zl-c99dMThXhvyJlL-6huSvZL5qHMI8LdbStFHAK1fYJWoM7JFPhpayoQhDCT-tajiCLZBXmZV2VtJc5XlHNrO6OsFnhmUBuL8mH0k8FAoPoc0VZ2xELhIUMnUfMrUE3THR_G1Gd9SXk-GhbCwy4cmKMUjbEUa9Znk6A_ZB2jJs-JIVhHWdra7EDNQ7icawnlaB012kqXV",
        "title": "themoonlight.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Q6GkPnsmXnuj3OqlWljL7iLQdujxc6gPoKWHbkxXq5wPgzJUhYC16SDq2u8QUcAsJVpQVdfv7bmR_wyrtxtBrVaE9aG2QNF4akVZo3WLgVRaW0PWXgwRSsCe",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "SAIR (Structurally Augmented IC50 Repository)",
    "paperLink": "https://huggingface.co/datasets/SandboxAQ/SAIR",
    "description": "A large-scale dataset of co-folded 3D protein-ligand structures paired with experimentally measured IC50 labels to link molecular structure directly to drug potency.",
    "authors": [
      "SandboxAQ"
    ],
    "githubLink": "https://huggingface.co/datasets/SandboxAQ/SAIR",
    "itemCount": "5 million AI-generated 3D structures with potency labels",
    "source": "Hugging Face",
    "specs": "3D Protein-Ligand Structures, IC50 values.",
    "year": "2025",
    "id": "saved-1769657008018-mjt5j",
    "groundingSources": []
  },
  {
    "title": "KD-DTI",
    "paperLink": "https://arxiv.org/abs/2109.13187",
    "description": "A benchmark dataset for discovering drug-target interactions from biomedical literature using text mining and relation extraction.",
    "authors": [
      "Yutai Hou",
      "Yingce Xia",
      "Lijun Wu",
      "Shufang Xie",
      "Yang Fan",
      "Jinhua Zhu",
      "Wanxiang Che",
      "Tao Qin",
      "Tie-Yan Liu"
    ],
    "githubLink": "https://huggingface.co/datasets/kd-dti",
    "itemCount": "14,000+ labeled papers/triplets",
    "source": "arXiv",
    "specs": "Text (Biomedical papers) and DTI Triplets.",
    "year": "2021",
    "id": "saved-1769657008018-jfhyc",
    "groundingSources": []
  },
  {
    "title": "LIT-PCBA",
    "paperLink": "https://pubs.acs.org/doi/10.1021/acs.jcim.0c00155",
    "description": "An unbiased dataset for virtual screening and machine learning designed to remove assay artifacts and chemical bias found in previous benchmarks like DUD-E.",
    "authors": [
      "Viet-Khoa Tran-Nguyen",
      "Célien Jacquemard",
      "Didier Rognan"
    ],
    "githubLink": "http://drugdesign.unistra.fr/LIT-PCBA",
    "itemCount": "15 targets, 7,844 confirmed actives, 407,381 confirmed inactives",
    "source": "Scholar",
    "specs": "2D and 3D molecular structures; Virtual screening tasks.",
    "year": "2020",
    "id": "saved-1769657008018-efnn2",
    "groundingSources": []
  },
  {
    "title": "PharmKG",
    "paperLink": "https://academic.oup.com/bib/article/22/4/bbaa344/6042240",
    "description": "A multi-relational, attributed biomedical knowledge graph benchmark linking genes, drugs, and diseases for data mining and link prediction tasks.",
    "authors": [
      "Shuangjia Zheng",
      "Jiahua Rao",
      "Ying Song",
      "Jianmin Zhang",
      "Xiangliang Zhang",
      "Yuedong Yang",
      "Zhangming Niu"
    ],
    "githubLink": "https://github.com/MindRank-Biotech/PharmKG",
    "itemCount": "500,000+ interconnections, ~8,000 entities",
    "source": "Scholar",
    "specs": "Knowledge Graph (Nodes: Genes, Drugs, Diseases; Edges: 29 relation types).",
    "year": "2020",
    "id": "saved-1769657008018-0qxec",
    "groundingSources": []
  },
  {
    "title": "Open Graph Benchmark (OGB) - Molecular",
    "paperLink": "https://arxiv.org/abs/2005.00687",
    "description": "A collection of realistic, large-scale graph datasets for machine learning, including specific molecular property prediction tasks (ogbg-molhiv, ogbg-molpcba).",
    "authors": [
      "Weihua Hu",
      "Matthias Fey",
      "Marinka Zitnik",
      "Yuxiao Dong",
      "Hongyu Ren",
      "Bowen Liu",
      "Michele Catasta",
      "Jure Leskovec"
    ],
    "githubLink": "https://github.com/snap-stanford/ogb",
    "itemCount": "ogbg-molpcba (437,929 graphs), ogbg-molhiv (41,127 graphs)",
    "source": "arXiv",
    "specs": "Graph objects (Nodes: Atoms, Edges: Bonds).",
    "year": "2020",
    "id": "saved-1769657008018-ztz7p",
    "groundingSources": []
  },
  {
    "title": "GuacaMol",
    "paperLink": "https://arxiv.org/abs/1811.09621",
    "description": "An evaluation framework and benchmarking suite for de novo molecular design, assessing the ability of generative models to create valid, novel, and diverse molecules.",
    "authors": [
      "Nathan Brown",
      "Marco Fiscato",
      "Alain C. Vaucher",
      "Mihaela Smilova",
      "Markus H. Segler"
    ],
    "githubLink": "https://github.com/BenevolentAI/guacamol",
    "itemCount": "Training set from ChEMBL 24 (~1.6M molecules); 5 distribution-learning and 20 goal-directed benchmarks",
    "source": "arXiv",
    "specs": "SMILES strings; Generative tasks.",
    "year": "2018",
    "id": "saved-1769657008018-m00ql",
    "groundingSources": []
  },
  {
    "title": "MOSES (Molecular Sets)",
    "paperLink": "https://arxiv.org/abs/1811.12823",
    "description": "A benchmarking platform for molecular generation models providing a standardized dataset of lead-like molecules and metrics for diversity and quality.",
    "authors": [
      "Daniil Polykovskiy",
      "Alexander Zhebrak",
      "Benjamin Sanchez-Lengeling",
      "Sergey Golovanov",
      "Oktai Tatanov",
      "Stanislav Belyaev",
      "Rauf Kurbanov",
      "Aleksey Artamonov",
      "Vladimir Aladinskiy",
      "Mark Veselov",
      "Artur Kadurin",
      "Simon Johansson",
      "Hongming Chen",
      "Sergey Nikolenko",
      "Alan Aspuru-Guzik",
      "Alex Zhavoronkov"
    ],
    "githubLink": "https://github.com/molecularsets/moses",
    "itemCount": "~1.9 million lead-like molecules",
    "source": "arXiv",
    "specs": "SMILES strings; Generative tasks.",
    "year": "2018",
    "id": "saved-1769657008018-j9p36",
    "groundingSources": []
  },
  {
    "title": "CASP15 Datasets (and previous editions)",
    "paperLink": "https://predictioncenter.org",
    "description": "Datasets from the Critical Assessment of Structure Prediction (CASP) competitions, which are the gold standard for blind assessment of protein structure prediction methods. CASP15 focused on high-accuracy modeling, protein complexes, and RNA structures.",
    "authors": [
      "Andriy Kryshtafovych",
      "Torsten Schwede",
      "CASP Organizers"
    ],
    "githubLink": "https://predictioncenter.org/download_area/",
    "itemCount": "~100 targets per competition; Full download area ~18GB",
    "source": "Scholar / Official Website",
    "specs": "PDB format, FASTA sequences; Modalities: 3D Structure, Sequence",
    "year": "2023",
    "id": "saved-1769657076297-739qy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNLUWxricUdG3Lu1iE5967w4sYoS9eD7eUhCoIYkGT7UECli8w7lzMzMm1JTdWQEdHxDbAQvULsXtrEj_M11Ytn9me3VNj_iK_42Vqx1UX5DtGejik6XTbHLTXBd7Z4NAUFL8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFw1ptGtig2gFs0mUJIVOyAhig5XNhep3LdIrDFRXgC8R5xAe4Aj7MzeT-EHopCKYfd5ShCVqVm_xrvnvdwk3StsCFI5H-zqQS8fHUIxCKdJm29Dah52ipdJBa0Eg9UgcXklb6WJWE=",
        "title": "predictioncenter.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEIgmSij3OtOOPWG9MDR8n_Wd2reuJUljcCW7jP0lyaW_G-Nn_qjDQ9UIy7xg2QLKd4hA5r_-R9RFVqWKj0wVhFwALhptYe8qXrdjhHORaJDKiO1j9bmeRWmiHuUkBiwyr7yLJX7tyCr_MCzbe5JwWFhTD2g6gWQg==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "ProteinInvBench",
    "paperLink": "https://openreview.net/forum?id=p5l8cfcwtS",
    "description": "A comprehensive benchmark for protein design (inverse folding) comprising extended tasks, integrated models, and diverse evaluation metrics (recovery, confidence, diversity, sc-TM). Includes single-chain, multi-chain, and de novo design tasks.",
    "authors": [
      "Zhangyang Gao",
      "Cheng Tan",
      "Yijie Zhang",
      "Xingran Chen",
      "Lirong Wu",
      "Stan Z. Li"
    ],
    "githubLink": "https://github.com/A4Bio/OpenCPD",
    "itemCount": "Training: 16,153 structures; Validation: 1,457; Test: 1,797 (CATH dataset)",
    "source": "OpenReview / NeurIPS",
    "specs": "Processed PyTorch datasets; Modalities: Structure -> Sequence",
    "year": "2023",
    "id": "saved-1769657076297-vh9ol",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNLUWxricUdG3Lu1iE5967w4sYoS9eD7eUhCoIYkGT7UECli8w7lzMzMm1JTdWQEdHxDbAQvULsXtrEj_M11Ytn9me3VNj_iK_42Vqx1UX5DtGejik6XTbHLTXBd7Z4NAUFL8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFw1ptGtig2gFs0mUJIVOyAhig5XNhep3LdIrDFRXgC8R5xAe4Aj7MzeT-EHopCKYfd5ShCVqVm_xrvnvdwk3StsCFI5H-zqQS8fHUIxCKdJm29Dah52ipdJBa0Eg9UgcXklb6WJWE=",
        "title": "predictioncenter.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEIgmSij3OtOOPWG9MDR8n_Wd2reuJUljcCW7jP0lyaW_G-Nn_qjDQ9UIy7xg2QLKd4hA5r_-R9RFVqWKj0wVhFwALhptYe8qXrdjhHORaJDKiO1j9bmeRWmiHuUkBiwyr7yLJX7tyCr_MCzbe5JwWFhTD2g6gWQg==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "SidechainNet",
    "paperLink": "https://arxiv.org/abs/2010.08162",
    "description": "An all-atom protein structure dataset that extends ProteinNet by adding sidechain angle and coordinate information. It is designed for machine learning models that predict complete protein structures, including backbone and sidechains.",
    "authors": [
      "Jonathan E. King",
      "David Ryan Koes"
    ],
    "githubLink": "https://github.com/jonathanking/sidechainnet",
    "itemCount": "Contains the same entries as ProteinNet but with extended features; ~3GB download size",
    "source": "arXiv / MLSB",
    "specs": "Pickled Python dictionaries; Modalities: Angles, Coordinates, Sequences, Masks",
    "year": "2020",
    "id": "saved-1769657076297-o80ri",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNLUWxricUdG3Lu1iE5967w4sYoS9eD7eUhCoIYkGT7UECli8w7lzMzMm1JTdWQEdHxDbAQvULsXtrEj_M11Ytn9me3VNj_iK_42Vqx1UX5DtGejik6XTbHLTXBd7Z4NAUFL8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFw1ptGtig2gFs0mUJIVOyAhig5XNhep3LdIrDFRXgC8R5xAe4Aj7MzeT-EHopCKYfd5ShCVqVm_xrvnvdwk3StsCFI5H-zqQS8fHUIxCKdJm29Dah52ipdJBa0Eg9UgcXklb6WJWE=",
        "title": "predictioncenter.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEIgmSij3OtOOPWG9MDR8n_Wd2reuJUljcCW7jP0lyaW_G-Nn_qjDQ9UIy7xg2QLKd4hA5r_-R9RFVqWKj0wVhFwALhptYe8qXrdjhHORaJDKiO1j9bmeRWmiHuUkBiwyr7yLJX7tyCr_MCzbe5JwWFhTD2g6gWQg==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "ATOM3D: Protein Structure Ranking (PSR)",
    "paperLink": "https://arxiv.org/abs/2012.04035",
    "description": "A benchmark task within the ATOM3D suite focused on predicting the quality (GDT_TS score) of protein structure models. It uses historical data from CASP competitions (CASP 5-13) to train and evaluate ranking methods.",
    "authors": [
      "Raphael J. L. Townshend",
      "Martin Vögele",
      "Patricia Suriana",
      "Alexander Derry",
      "Alexander Powers",
      "Yianni Laloudakis",
      "Sidhika Balachandar",
      "Bowen Jing",
      "Brandon Anderson",
      "Stephan Eismann",
      "Risi Kondor",
      "Russ B. Altman",
      "Ron O. Dror"
    ],
    "githubLink": "https://github.com/drorlab/atom3d",
    "itemCount": "~5.3 TB (total ATOM3D volume), PSR specific: ~8 GB (raw), ~2 GB (split)",
    "source": "arXiv / NeurIPS",
    "specs": "LMDB format; Modalities: 3D Atomic Coordinates, Quality Scores",
    "year": "2020",
    "id": "saved-1769657076297-nrpws",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNLUWxricUdG3Lu1iE5967w4sYoS9eD7eUhCoIYkGT7UECli8w7lzMzMm1JTdWQEdHxDbAQvULsXtrEj_M11Ytn9me3VNj_iK_42Vqx1UX5DtGejik6XTbHLTXBd7Z4NAUFL8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFw1ptGtig2gFs0mUJIVOyAhig5XNhep3LdIrDFRXgC8R5xAe4Aj7MzeT-EHopCKYfd5ShCVqVm_xrvnvdwk3StsCFI5H-zqQS8fHUIxCKdJm29Dah52ipdJBa0Eg9UgcXklb6WJWE=",
        "title": "predictioncenter.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEIgmSij3OtOOPWG9MDR8n_Wd2reuJUljcCW7jP0lyaW_G-Nn_qjDQ9UIy7xg2QLKd4hA5r_-R9RFVqWKj0wVhFwALhptYe8qXrdjhHORaJDKiO1j9bmeRWmiHuUkBiwyr7yLJX7tyCr_MCzbe5JwWFhTD2g6gWQg==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "ProteinNet",
    "paperLink": "https://doi.org/10.1186/s12859-019-2932-0",
    "description": "A standardized machine learning dataset for protein structure prediction, organized as a series of datasets spanning CASP 7 through 12. It integrates sequence, structure (secondary and tertiary), multiple sequence alignments (MSAs), and Position-Specific Scoring Matrices (PSSMs) into a unified format.",
    "authors": [
      "Mohammed AlQuraishi"
    ],
    "githubLink": "https://github.com/aqlaboratory/proteinnet",
    "itemCount": "Ranges from ~34,000 to ~104,000 structures depending on the CASP edition/thinning",
    "source": "arXiv / BMC Bioinformatics",
    "specs": "TensorFlow Records (TFRecords), text formats; Sequence, Structure, MSA, PSSM",
    "year": "2019",
    "id": "saved-1769657076297-fr8s8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNLUWxricUdG3Lu1iE5967w4sYoS9eD7eUhCoIYkGT7UECli8w7lzMzMm1JTdWQEdHxDbAQvULsXtrEj_M11Ytn9me3VNj_iK_42Vqx1UX5DtGejik6XTbHLTXBd7Z4NAUFL8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFw1ptGtig2gFs0mUJIVOyAhig5XNhep3LdIrDFRXgC8R5xAe4Aj7MzeT-EHopCKYfd5ShCVqVm_xrvnvdwk3StsCFI5H-zqQS8fHUIxCKdJm29Dah52ipdJBa0Eg9UgcXklb6WJWE=",
        "title": "predictioncenter.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEIgmSij3OtOOPWG9MDR8n_Wd2reuJUljcCW7jP0lyaW_G-Nn_qjDQ9UIy7xg2QLKd4hA5r_-R9RFVqWKj0wVhFwALhptYe8qXrdjhHORaJDKiO1j9bmeRWmiHuUkBiwyr7yLJX7tyCr_MCzbe5JwWFhTD2g6gWQg==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "CAMEO (Continuous Automated Model EvaluatiOn)",
    "paperLink": "https://www.cameo3d.org",
    "description": "A continuous, fully automated benchmark that evaluates protein structure prediction servers on a weekly basis using pre-released sequences from the PDB. It complements the biennial CASP experiments by providing high-frequency assessment.",
    "authors": [
      "Juergen Haas",
      "Steven Roth",
      "Konstantin Arnold",
      "Florian Kiefer",
      "Torsten Schwede",
      "SIB Swiss Institute of Bioinformatics"
    ],
    "githubLink": "https://www.cameo3d.org",
    "itemCount": "Continuous weekly targets; Thousands of evaluated structures annually",
    "source": "Official Website / Scholar",
    "specs": "PDB format, Sequences; Modalities: 3D Structure, Sequence",
    "year": "2013",
    "id": "saved-1769657076297-8ejzt",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNLUWxricUdG3Lu1iE5967w4sYoS9eD7eUhCoIYkGT7UECli8w7lzMzMm1JTdWQEdHxDbAQvULsXtrEj_M11Ytn9me3VNj_iK_42Vqx1UX5DtGejik6XTbHLTXBd7Z4NAUFL8=",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFw1ptGtig2gFs0mUJIVOyAhig5XNhep3LdIrDFRXgC8R5xAe4Aj7MzeT-EHopCKYfd5ShCVqVm_xrvnvdwk3StsCFI5H-zqQS8fHUIxCKdJm29Dah52ipdJBa0Eg9UgcXklb6WJWE=",
        "title": "predictioncenter.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEIgmSij3OtOOPWG9MDR8n_Wd2reuJUljcCW7jP0lyaW_G-Nn_qjDQ9UIy7xg2QLKd4hA5r_-R9RFVqWKj0wVhFwALhptYe8qXrdjhHORaJDKiO1j9bmeRWmiHuUkBiwyr7yLJX7tyCr_MCzbe5JwWFhTD2g6gWQg==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "FORTRESS (Frontier Risk Evaluation for National Security and Public Safety)",
    "paperLink": "https://scale.com/research/fortress",
    "description": "A red-teaming benchmark designed to evaluate the safety of LLMs against dual-use risks. It includes expert-crafted adversarial prompts covering CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosives), political violence, and illicit financial activities.",
    "authors": [
      "Scale AI"
    ],
    "githubLink": "https://huggingface.co/datasets/ScaleAI/fortress_public",
    "itemCount": "500 prompts (public subset)",
    "source": "Hugging Face",
    "specs": "Text (Adversarial Prompts), Risk Domains: CBRNE, Terrorism, Cyber",
    "year": "2025",
    "id": "saved-1769657165093-8vrpp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE91uFwUSF2lcvRj9jFG6coBgMnZCW0nZ8o4hfkCcT2I1S2w4f7a_rgZZEEtV8Z27sYbNzl5UTNr-jUp48BueGF0eOD8imAtGbue0tqazarWMOP6PQH77wgOaatZqyY108WtePn_QC27taaptiF4lOxrB0-uGaFddzPg_FTtYokxcGJ__Ftv4olEiOQ7lvOi-JgVVYzMQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHESdHmUyTGPjqLgQmNBI1HbuQxW2IffrFSztNU-JWClcw-XUus-vY33h-WldWU9RCSXDL4WdfXcKSFXMbADTakpbmGCNRPHMEiYt_fJmcP8RpEdoGcAf1F09Cy",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ClearHarm",
    "paperLink": "https://huggingface.co/papers/2506.14922",
    "description": "A jailbreak benchmark focused on unambiguously harmful questions, including specific tracks for CBRN threats, designed to be more challenging than existing safety benchmarks.",
    "authors": [
      "FAR.AI",
      "Alignment Research"
    ],
    "githubLink": "https://huggingface.co/datasets/AlignmentResearch/ClearHarm",
    "itemCount": "~179 prompts (eval set)",
    "source": "Hugging Face",
    "specs": "Text (Jailbreak Prompts), Modalities: Text, Focus: CBRN, Cyber, Harassment",
    "year": "2025",
    "id": "saved-1769657165093-8p36h",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE91uFwUSF2lcvRj9jFG6coBgMnZCW0nZ8o4hfkCcT2I1S2w4f7a_rgZZEEtV8Z27sYbNzl5UTNr-jUp48BueGF0eOD8imAtGbue0tqazarWMOP6PQH77wgOaatZqyY108WtePn_QC27taaptiF4lOxrB0-uGaFddzPg_FTtYokxcGJ__Ftv4olEiOQ7lvOi-JgVVYzMQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHESdHmUyTGPjqLgQmNBI1HbuQxW2IffrFSztNU-JWClcw-XUus-vY33h-WldWU9RCSXDL4WdfXcKSFXMbADTakpbmGCNRPHMEiYt_fJmcP8RpEdoGcAf1F09Cy",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Hazmat Placard Image Classification Dataset",
    "paperLink": "https://www.kaggle.com/datasets/nxank4/hazmat-placard-image-classification",
    "description": "A high-resolution image dataset of hazmat placards used in transportation and industry, suitable for training object detection and classification models for safety compliance.",
    "authors": [
      "nxank4 (Kaggle)"
    ],
    "githubLink": "",
    "itemCount": "36,984 files (3 GB)",
    "source": "Kaggle",
    "specs": "Images, Format: JPG/PNG, Task: Classification/Detection",
    "year": "2024",
    "id": "saved-1769657165093-b5whb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE91uFwUSF2lcvRj9jFG6coBgMnZCW0nZ8o4hfkCcT2I1S2w4f7a_rgZZEEtV8Z27sYbNzl5UTNr-jUp48BueGF0eOD8imAtGbue0tqazarWMOP6PQH77wgOaatZqyY108WtePn_QC27taaptiF4lOxrB0-uGaFddzPg_FTtYokxcGJ__Ftv4olEiOQ7lvOi-JgVVYzMQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHESdHmUyTGPjqLgQmNBI1HbuQxW2IffrFSztNU-JWClcw-XUus-vY33h-WldWU9RCSXDL4WdfXcKSFXMbADTakpbmGCNRPHMEiYt_fJmcP8RpEdoGcAf1F09Cy",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "HAZMAT-13 (Hazardous Materials Sign Dataset)",
    "paperLink": "https://github.com/mrl-amrl/HAZMAT13",
    "description": "A comprehensive image dataset of HAZMAT signs for detection and classification, collected for robotics applications (e.g., RoboCup). It covers 13 classes of hazardous material signs under various conditions.",
    "authors": [
      "mrl-amrl (Mechatronics Research Lab)"
    ],
    "githubLink": "https://github.com/mrl-amrl/HAZMAT13",
    "itemCount": "52,845 images (augmented), 1,685 (original)",
    "source": "GitHub",
    "specs": "Images (PASCAL-VOC annotations), 13 Classes (Poison, Radioactive, Explosive, etc.)",
    "year": "2021",
    "id": "saved-1769657165093-s4406",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE91uFwUSF2lcvRj9jFG6coBgMnZCW0nZ8o4hfkCcT2I1S2w4f7a_rgZZEEtV8Z27sYbNzl5UTNr-jUp48BueGF0eOD8imAtGbue0tqazarWMOP6PQH77wgOaatZqyY108WtePn_QC27taaptiF4lOxrB0-uGaFddzPg_FTtYokxcGJ__Ftv4olEiOQ7lvOi-JgVVYzMQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHESdHmUyTGPjqLgQmNBI1HbuQxW2IffrFSztNU-JWClcw-XUus-vY33h-WldWU9RCSXDL4WdfXcKSFXMbADTakpbmGCNRPHMEiYt_fJmcP8RpEdoGcAf1F09Cy",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "IMed-361M",
    "paperLink": "https://arxiv.org/abs/2411.12814",
    "description": "A large-scale benchmark for interactive medical image segmentation, covering diverse modalities and targets to facilitate the development of medical vision foundation models.",
    "authors": [
      "Junlong Cheng",
      "Bin Fu",
      "Jin Ye",
      "Guoan Wang",
      "Tianbin Li",
      "Haoyu Wang",
      "Ruoyu Li",
      "He Yao",
      "Junren Chen",
      "Jingwen Li",
      "Yanzhou Su",
      "Min Zhu",
      "Junjun He"
    ],
    "githubLink": "https://github.com/uni-medical/IMIS-Bench",
    "itemCount": "6.4 million images, 361 million masks",
    "source": "arXiv",
    "specs": "Medical Images (14 modalities), 204 segmentation targets",
    "year": "2024",
    "id": "saved-1769657228617-3nxys",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "MedSegBench",
    "paperLink": "https://doi.org/10.1038/s41597-024-04159-2",
    "description": "A comprehensive benchmark specifically designed for medical image segmentation, aggregating diverse datasets to evaluate model generalization across different medical imaging modalities.",
    "authors": [
      "Zeki Kuş",
      "Musa Aydin"
    ],
    "githubLink": "https://github.com/zekikus/MedSegBench",
    "itemCount": "35 datasets, 60,000+ images",
    "source": "Google Scholar",
    "specs": "Medical Images (Ultrasound, MRI, X-ray), 2D/3D formats",
    "year": "2024",
    "id": "saved-1769657228617-vy122",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "SA-1B (Segment Anything 1 Billion)",
    "paperLink": "https://arxiv.org/abs/2304.02643",
    "description": "A massive dataset designed for training general-purpose object segmentation foundation models. It enables 'zero-shot' segmentation via prompts and contains automatically generated masks of high quality.",
    "authors": [
      "Alexander Kirillov",
      "Eric Mintun",
      "Nikhila Ravi",
      "Hanzi Mao",
      "Chloe Rolland",
      "Laura Gustafson",
      "Tete Xiao",
      "Spencer Whitehead",
      "Alexander C. Berg",
      "Wan-Yen Lo",
      "Piotr Dollár",
      "Ross Girshick"
    ],
    "githubLink": "https://github.com/facebookresearch/segment-anything",
    "itemCount": "11 million images, 1.1 billion masks",
    "source": "arXiv",
    "specs": "High-resolution images, COCO RLE mask format, Auto-generated annotations",
    "year": "2023",
    "id": "saved-1769657228617-ornas",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "LVIS (Large Vocabulary Instance Segmentation)",
    "paperLink": "https://arxiv.org/abs/1908.03195",
    "description": "A dataset for instance segmentation that focuses on the 'long-tail' problem, featuring a large vocabulary of over 1000 object categories with a Zipfian distribution of occurrence.",
    "authors": [
      "Agrim Gupta",
      "Piotr Dollár",
      "Ross Girshick"
    ],
    "githubLink": "https://github.com/lvis-dataset/lvis-api",
    "itemCount": "164,000 images, 2+ million instance masks",
    "source": "arXiv",
    "specs": "Images, Instance Segmentation masks, >1200 categories",
    "year": "2019",
    "id": "saved-1769657228617-clf3l",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "ADE20K (MIT Scene Parsing Benchmark)",
    "paperLink": "https://arxiv.org/abs/1608.05442",
    "description": "A challenging scene parsing dataset containing dense annotations of a wide variety of scenes, used to benchmark semantic segmentation models on a large number of object categories.",
    "authors": [
      "Bolei Zhou",
      "Hang Zhao",
      "Xavier Puig",
      "Sanja Fidler",
      "Adela Barriuso",
      "Antonio Torralba"
    ],
    "githubLink": "https://github.com/CSAILVision/ADE20K",
    "itemCount": "~25,000 images",
    "source": "arXiv",
    "specs": "Images, Semantic Segmentation, 150 object/stuff categories",
    "year": "2017",
    "id": "saved-1769657228618-gkf8b",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "Cityscapes",
    "paperLink": "https://arxiv.org/abs/1604.01685",
    "description": "A large-scale dataset focusing on semantic understanding of urban street scenes, providing pixel-level annotations for diverse set of stereo video sequences.",
    "authors": [
      "Marius Cordts",
      "Mohamed Omran",
      "Sebastian Ramos",
      "Timo Rehfeld",
      "Markus Enzweiler",
      "Rodrigo Benenson",
      "Uwe Franke",
      "Stefan Roth",
      "Bernt Schiele"
    ],
    "githubLink": "https://github.com/mcordts/cityscapesScripts",
    "itemCount": "5,000 fine annotations, 20,000 coarse annotations",
    "source": "arXiv",
    "specs": "Stereo Video, Pixel-level Semantic/Instance Segmentation, 30 classes",
    "year": "2016",
    "id": "saved-1769657228618-jvl84",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "Microsoft COCO (Common Objects in Context)",
    "paperLink": "https://arxiv.org/abs/1405.0312",
    "description": "One of the most popular benchmarks for object detection, segmentation, and captioning. It places objects in their natural context to aid in scene understanding.",
    "authors": [
      "Tsung-Yi Lin",
      "Michael Maire",
      "Serge Belongie",
      "James Hays",
      "Pietro Perona",
      "Deva Ramanan",
      "Piotr Dollár",
      "C. Lawrence Zitnick"
    ],
    "githubLink": "https://github.com/cocodataset/cocoapi",
    "itemCount": "330,000 images, 1.5 million object instances",
    "source": "arXiv",
    "specs": "Images, Instance Segmentation, Keypoints, Captions, 80 object categories",
    "year": "2014",
    "id": "saved-1769657228618-vmhz7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "Pascal VOC (Visual Object Classes)",
    "paperLink": "http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf",
    "description": "A foundational benchmark in computer vision for object classification, detection, and segmentation, providing standardized image datasets and evaluation tools.",
    "authors": [
      "Mark Everingham",
      "Luc Van Gool",
      "Christopher K. I. Williams",
      "John Winn",
      "Andrew Zisserman"
    ],
    "githubLink": "https://github.com/cvat-ai/cvat",
    "itemCount": "~11,530 images (2012 version)",
    "source": "Google Scholar",
    "specs": "Images, Object Class Segmentation, Bounding Boxes, 20 classes",
    "year": "2010",
    "id": "saved-1769657228618-145nz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESMVyILr-kNja3_Pi569y70afl9-8L3Kl3H3e57rLJZlERgJGoTlBa9LOAyF-rMM-j9hYiwrOHtBBJyDndpdeLHbZ2dx-VQzrT7hoIbgFiQKuFEiGGXX-7xPi4IRIFNC_03kbg_5qWlgR9L2jCfRneZCN6JISWZQ==",
        "title": "tensorflow.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHA7GVp4R74FkoXG4PuYq1ZgxY3bR4mbGl5DCYcniNgSjdtRcjR7LT5f8ikA5KXYUAG2WXuSeQOUrPzHgJWs0e1svWP3QS3CeKQJUVAHxtFPr6o0ybXTU_146NE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjmQHVE4RgGiSwmMiRXgl9Nfav9kRFML_G5EMR-BLqI2keKkOYAIP4H9c0iMiO96fPFbaaK3HhTl6PLx_1FqbWUc_3NDLbtkGd_-Az5xjy54JNTMEFKNI0LQ==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtHI4yThaLChpelyu1Jigj1wic6awEOhiShO5sF1lX3dqOAeNY7WGSG_Em0i1HTJSHsq2-_GFYEJu28NwDXsh8llaif3cpvLgQ-FkK6DzHD69-A8ly_O-8LSlsPWUsrbp2Fo_W",
        "title": "opendatalab.com"
      }
    ]
  },
  {
    "title": "SA-Med2D-20M",
    "paperLink": "https://arxiv.org/abs/2311.11969",
    "description": "A large-scale 2D medical image segmentation dataset designed to incorporate medical knowledge into the Segment Anything Model (SAM). It covers diverse modalities (CT, MRI, etc.) and anatomical structures.",
    "authors": [
      "Jin Ye",
      "Junlong Cheng",
      "Jianpin Chen",
      "Zhongying Deng",
      "Tianbin Li"
    ],
    "githubLink": "https://github.com/OpenGVLab/SAM-Med2D",
    "itemCount": "4.6 million images, 19.7 million masks",
    "source": "arXiv",
    "specs": "2D Medical Images (CT, MRI, X-ray, Ultrasound, etc.), Mask annotations",
    "year": "2023",
    "id": "saved-1769657311609-aeh44",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQY8lx8R2NAhjzF1_jEUR0_q0Mo5lUDjpLlbrkNWWHqPiFZWcrNY2rxy4XSZRKsSbp51ZjbShJqXoD1guQYIxwqg6-98J2M8wWXcSGc144ZfQl0gEa-7kW_io=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-AbFPKBEz-vgUbp6PX_nIrZ8ur_WQlFEdVSl-e7oQNjwxjH7uD_hWbAU9fSVDimORSE0aualbO4gQYv_keQ2EiK2ZckmXk_ap8v7ahBqShl1TvD5ESXIm3B1qw6cDZOzYe-po8gIppUaYAZO3g4s4xk9VdSlJ0hWFQs6GzuK4kr8r5JchZ9QD9PnksyD3kpP8H9gYccf0MYyr3C3xXMagtOCEhiOAPRNdl3cv-Ky_",
        "title": "deeplearn.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFNkkK7B-yySPcAJNeebiIMj4b6ZWE0v7oDDfoCgScNwKWBwe0G9e6dm50ouWc-1kNcXyPhiLu8243Wyee8_j4vwJ8filYHIsPTr050hcmphXijBleJvwrd0CW9JispTDAaUYQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH0TW0FUdQ2cWIjcoepkeDYDuizqej9-Nl8hEPoPigkpJMhJYEVHLZmHrLqrqaQBl7KJdfe_6D5YBSMBWjcCO4LIiIbZinz4xrYQa5_AGafMl-Gcg8SO3vsUxpDZUMUmwwme_bgFLJqVJhkJITEyxsjIED7mmu2M9I9",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3t9KmmgI-oNETT2J68wdCfHELNfXwDknPC0K4wfopngf_hYb1BjM-l7MXJL1kUTIN55kZ5T-Ytcv0G1dDzDP5WomqKKfqL0qSl5egauALPpZ02scp3Yd5REftmISK9uFyr_2lAOkXDtox3DzttGR1T7Bo_RRRhZQS3gCVLf-j9l_8YzUUJhScW0uNfvhs6g6JmCI3pislS89Bq3O2",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "WildScenes",
    "paperLink": "https://arxiv.org/abs/2312.15364",
    "description": "A bi-modal benchmark for 2D and 3D semantic segmentation in unstructured natural environments (e.g., forests), supporting research on domain adaptation.",
    "authors": [
      "Kavisha Vidanapathirana",
      "Joshua Knights",
      "Stephen Hausler",
      "Mark Cox",
      "Milad Ramezani",
      "Jason Jooste",
      "Ethan Griffiths",
      "Shaheer Mohamed",
      "Sridha Sridharan",
      "Clinton Fookes",
      "Peyman Moghadam"
    ],
    "githubLink": "https://github.com/csiro-robotics/WildScenes",
    "itemCount": "9,306 annotated images, 12,148 submaps",
    "source": "arXiv",
    "specs": "2D images, 3D LiDAR point clouds, 6-DoF pose",
    "year": "2023",
    "id": "saved-1769657311610-9o7d6",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQY8lx8R2NAhjzF1_jEUR0_q0Mo5lUDjpLlbrkNWWHqPiFZWcrNY2rxy4XSZRKsSbp51ZjbShJqXoD1guQYIxwqg6-98J2M8wWXcSGc144ZfQl0gEa-7kW_io=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-AbFPKBEz-vgUbp6PX_nIrZ8ur_WQlFEdVSl-e7oQNjwxjH7uD_hWbAU9fSVDimORSE0aualbO4gQYv_keQ2EiK2ZckmXk_ap8v7ahBqShl1TvD5ESXIm3B1qw6cDZOzYe-po8gIppUaYAZO3g4s4xk9VdSlJ0hWFQs6GzuK4kr8r5JchZ9QD9PnksyD3kpP8H9gYccf0MYyr3C3xXMagtOCEhiOAPRNdl3cv-Ky_",
        "title": "deeplearn.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFNkkK7B-yySPcAJNeebiIMj4b6ZWE0v7oDDfoCgScNwKWBwe0G9e6dm50ouWc-1kNcXyPhiLu8243Wyee8_j4vwJ8filYHIsPTr050hcmphXijBleJvwrd0CW9JispTDAaUYQ=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH0TW0FUdQ2cWIjcoepkeDYDuizqej9-Nl8hEPoPigkpJMhJYEVHLZmHrLqrqaQBl7KJdfe_6D5YBSMBWjcCO4LIiIbZinz4xrYQa5_AGafMl-Gcg8SO3vsUxpDZUMUmwwme_bgFLJqVJhkJITEyxsjIED7mmu2M9I9",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3t9KmmgI-oNETT2J68wdCfHELNfXwDknPC0K4wfopngf_hYb1BjM-l7MXJL1kUTIN55kZ5T-Ytcv0G1dDzDP5WomqKKfqL0qSl5egauALPpZ02scp3Yd5REftmISK9uFyr_2lAOkXDtox3DzttGR1T7Bo_RRRhZQS3gCVLf-j9l_8YzUUJhScW0uNfvhs6g6JmCI3pislS89Bq3O2",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "M3D-Seg",
    "paperLink": "https://huggingface.co/datasets/GoodBaiBai88/M3D-Seg",
    "description": "A large-scale general 3D medical image segmentation dataset consolidating multiple public datasets (like CHAOS, AMOS22, KiTS, etc.) into a unified format.",
    "authors": [
      "GoodBaiBai88 (Hugging Face Profile)"
    ],
    "githubLink": "https://github.com/GoodBaiBai88/M3D-Seg",
    "itemCount": "5,772 3D images, 149,196 masks",
    "source": "Hugging Face",
    "specs": "CT/MRI 3D Volumes, Mask Annotations, Unified JSON Format",
    "year": "2024",
    "id": "saved-1769657369199-rnzvm",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "ScanNet200",
    "paperLink": "https://arxiv.org/abs/2206.12140",
    "description": "An expansion of the ScanNet benchmark to 200 semantic categories, aiming to study the long-tail 3D semantic segmentation problem.",
    "authors": [
      "David Rozenberszki",
      "Or Litany",
      "Angela Dai"
    ],
    "githubLink": "https://github.com/ScanNet/ScanNet",
    "itemCount": "1,513 scans (same as ScanNet, more labels)",
    "source": "arXiv",
    "specs": "3D Meshes, 200 Semantic Classes",
    "year": "2022",
    "id": "saved-1769657369199-5wjtf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "STPLS3D",
    "paperLink": "https://arxiv.org/abs/2203.09065",
    "description": "A large-scale synthetic and real aerial photogrammetry 3D point cloud dataset for semantic and instance segmentation.",
    "authors": [
      "Meida Chen",
      "Qingyong Hu",
      "Thomas Hugues",
      "Andrew Feng",
      "Yu Hou",
      "Kyle McCullough",
      "Lucio Soibelman"
    ],
    "githubLink": "https://github.com/meidachen/STPLS3D",
    "itemCount": "16 km^2 synthetic, 4 real-world areas",
    "source": "arXiv",
    "specs": "Aerial Photogrammetry Point Clouds, 18 Semantic Categories",
    "year": "2022",
    "id": "saved-1769657369199-gmvrb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "SemanticKITTI",
    "paperLink": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Behley_SemanticKITTI_A_Dataset_for_Semantic_Scene_Understanding_of_LiDAR_Sequences_ICCV_2019_paper.pdf",
    "description": "A large-scale dataset for semantic scene understanding of LiDAR sequences, based on the KITTI Vision Odometry Benchmark. It provides dense point-wise annotations for the complete 360-degree field-of-view of automotive LiDAR.",
    "authors": [
      "Jens Behley",
      "Martin Garbade",
      "Andres Milioto",
      "Jan Quenzel",
      "Sven Behnke",
      "Cyrill Stachniss",
      "Jurgen Gall"
    ],
    "githubLink": "https://github.com/PRBonn/semantic-kitti-api",
    "itemCount": "22 sequences, ~43,000 scans",
    "source": "Scholar",
    "specs": "LiDAR Point Clouds, 28 Semantic Classes",
    "year": "2019",
    "id": "saved-1769657369200-6adse",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "PartNet",
    "paperLink": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Mo_PartNet_A_Large-Scale_Benchmark_for_Fine-Grained_and_Hierarchical_Part-Level_3D_CVPR_2019_paper.pdf",
    "description": "A consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information, designed for fine-grained 3D shape understanding.",
    "authors": [
      "Kaichun Mo",
      "Shilin Zhu",
      "Angel X. Chang",
      "Li Yi",
      "Subarna Tripathi",
      "Leonidas J. Guibas",
      "Hao Su"
    ],
    "githubLink": "https://github.com/daerduoCarey/PartNet",
    "itemCount": "26,671 3D models, 573,585 part instances",
    "source": "Scholar",
    "specs": "3D Meshes/Point Clouds, Hierarchical Part Annotations",
    "year": "2019",
    "id": "saved-1769657369200-pqm3f",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "nuScenes",
    "paperLink": "https://arxiv.org/abs/1903.11027",
    "description": "A large-scale multimodal dataset for autonomous driving featuring data from a full sensor suite (6 cameras, 1 LiDAR, 5 radars). It includes 3D bounding box annotations and specific benchmarks for LiDAR semantic segmentation.",
    "authors": [
      "Holger Caesar",
      "Varun Bankiti",
      "Alex H. Lang",
      "Sourabh Vora",
      "Venice Erin Liong",
      "Qiang Xu",
      "Anush Krishnan",
      "Yu Pan",
      "Giancarlo Baldan",
      "Oscar Beijbom"
    ],
    "githubLink": "https://github.com/nutonomy/nuscenes-devkit",
    "itemCount": "1,000 scenes (20s each), 1.4M camera images, 390k LiDAR sweeps",
    "source": "arXiv",
    "specs": "LiDAR, RGB Images, Radar, 3D Bounding Boxes, Semantic Labels",
    "year": "2019",
    "id": "saved-1769657369200-j5wb9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "Waymo Open Dataset",
    "paperLink": "https://arxiv.org/abs/1912.04838",
    "description": "A high-quality multimodal sensor dataset for autonomous driving, comprising high-resolution LiDAR and camera data. It includes a specific 3D semantic segmentation benchmark with per-point labels.",
    "authors": [
      "Pei Sun",
      "Henrik Kretzschmar",
      "Xerxes Dotiwalla",
      "Aurelien Chouard",
      "Vijaysai Patnaik",
      "Paul Tsui",
      "James Guo",
      "Yuning Zhou",
      "Yifan Chai",
      "Benjamin Caine",
      "Vijay Vasudevan",
      "Wei Han",
      "Jiquan Ngiam",
      "Hang Zhao",
      "Aleksei Timofeev",
      "Scott Ettinger",
      "Maxim Krivokon",
      "Amy Gao",
      "Aditya Joshi",
      "Yu Zhang",
      "Jonathon Shlens",
      "Zhifeng Chen",
      "Dragomir Anguelov"
    ],
    "githubLink": "https://github.com/waymo-research/waymo-open-dataset",
    "itemCount": "1,150 scenes, ~12 million 3D labels",
    "source": "arXiv",
    "specs": "High-res LiDAR, RGB Video, 3D Semantic Segmentation Labels",
    "year": "2019",
    "id": "saved-1769657369200-7n6d1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "ScanNet",
    "paperLink": "http://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_ScanNet_Richly-Annotated_3D_CVPR_2017_paper.pdf",
    "description": "A large-scale RGB-D dataset of indoor scenes containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations.",
    "authors": [
      "Angela Dai",
      "Angel X. Chang",
      "Manolis Savva",
      "Maciej Halber",
      "Thomas Funkhouser",
      "Matthias Nießner"
    ],
    "githubLink": "https://github.com/ScanNet/ScanNet",
    "itemCount": "1,513 scans, ~2.5 million frames",
    "source": "Scholar",
    "specs": "RGB-D video, 3D reconstructed meshes, Instance-level semantic segmentation",
    "year": "2017",
    "id": "saved-1769657369200-4x51c",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "Semantic3D",
    "paperLink": "https://arxiv.org/abs/1704.03847",
    "description": "A large-scale point cloud classification benchmark containing dense point clouds acquired with static terrestrial laser scanners, covering a wide range of urban outdoor scenes.",
    "authors": [
      "Timo Hackel",
      "Nikolay Savinov",
      "Ladicky Lubor",
      "Jan D. Wegner",
      "Konrad Schindler",
      "Marc Pollefeys"
    ],
    "githubLink": "https://github.com/Daewon-Shim/Semantic3D-Net",
    "itemCount": "Over 4 billion labeled points, 30 scenes",
    "source": "arXiv",
    "specs": "Terrestrial Laser Scans (High density), 8 Semantic Classes",
    "year": "2017",
    "id": "saved-1769657369200-s0zem",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "S3DIS (Stanford Large-Scale 3D Indoor Spaces)",
    "paperLink": "http://openaccess.thecvf.com/content_cvpr_2016/papers/Armeni_3D_Semantic_Parsing_CVPR_2016_paper.pdf",
    "description": "A dataset composed of colored 3D point clouds for 6 large-scale indoor areas from 3 different buildings, mainly used for 3D semantic segmentation.",
    "authors": [
      "Iro Armeni",
      "Ozan Sener",
      "Amir R. Zamir",
      "Helen Jiang",
      "Ioannis Brilakis",
      "Martin Fischer",
      "Silvio Savarese"
    ],
    "githubLink": "https://github.com/alex-sax/2D-3D-Semantics",
    "itemCount": "6 areas, 271 rooms",
    "source": "Scholar",
    "specs": "3D Point Clouds (XYZ, RGB), Semantic Labels",
    "year": "2016",
    "id": "saved-1769657369200-0py2p",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8i6jB9E15rnw6-IvmKCsg5xKFVMF_d7ZJ8-8DU8vczV0BNeFF5SJHkiBAOuw1WeQvu1b67Sc2zqIp30PlFzieLBXjFtorP26qize1fOGltjZQhwbcxcU9rwJGxZnc7mV117RkkstAzMyyUF8X-6L5FpLX",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "AVSBench (Audio-Visual Segmentation Benchmark)",
    "paperLink": "https://arxiv.org/abs/2207.05042",
    "description": "A pixel-level audio-visual segmentation benchmark designed to segment sounding objects in video frames using both audio and visual cues. It includes settings for single-source and multi-source segmentation, and has been extended with semantic labels.",
    "authors": [
      "Jinxing Zhou",
      "Jianyuan Wang",
      "Jiayi Zhang",
      "Weixuan Sun",
      "Jing Zhang",
      "Stan Birchfield",
      "Dan Guo",
      "Lingpeng Kong",
      "Meng Wang",
      "Yiran Zhong"
    ],
    "githubLink": "https://github.com/OpenNLPLab/AVSBench",
    "itemCount": "5,356 videos (Single-source), 296 videos (Multi-source), ~7k videos (Semantic extension)",
    "source": "arXiv",
    "specs": "5-second video clips, pixel-level binary and semantic segmentation masks, audio-visual modalities",
    "year": "2022",
    "id": "saved-1769657441791-71vwi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGejzKymc8ie3gWj6MgNqAwPh_RnEOy6N_nxRd5GfnjgomwZTp7UoCZfDr5hYdFhmoiX3pulO1QLx1-2gqELMZ0pvy84XTQUBzbeHDYSOm-caBKPIfDB2VZsamaQSm_UcTf8w3qJ1gaPLB5TWfLvPtkmegpCmqm9EfAZM1v8yv83P-F-kbA5BcnTmhrHadkh7Ee-QZb93HImznxkdkeEn2HuWN6MQLR4AUvLBsJFQ5Ya4NWumK3md2KqKsESXPNpaSL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AudioSet-Strong",
    "paperLink": "https://arxiv.org/abs/2105.07031",
    "description": "A subset of the massive AudioSet dataset that contains precise, human-verified temporal start and end times (strong labels) for sound events, enabling fine-grained audio event segmentation and detection research.",
    "authors": [
      "Shawn Hershey",
      "Daniel P. W. Ellis",
      "Eduardo Fonseca",
      "Aren Jansen",
      "Caroline Liu",
      "R. Channing Moore",
      "Manoj Plakal"
    ],
    "githubLink": "https://github.com/temple-invaders/audioset-strong-download",
    "itemCount": "~120,000 strongly labeled clips",
    "source": "arXiv",
    "specs": "10-second audio clips, 0.1s resolution temporal boundary annotations, 44.1 kHz audio",
    "year": "2021",
    "id": "saved-1769657441791-wg4xg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGejzKymc8ie3gWj6MgNqAwPh_RnEOy6N_nxRd5GfnjgomwZTp7UoCZfDr5hYdFhmoiX3pulO1QLx1-2gqELMZ0pvy84XTQUBzbeHDYSOm-caBKPIfDB2VZsamaQSm_UcTf8w3qJ1gaPLB5TWfLvPtkmegpCmqm9EfAZM1v8yv83P-F-kbA5BcnTmhrHadkh7Ee-QZb93HImznxkdkeEn2HuWN6MQLR4AUvLBsJFQ5Ya4NWumK3md2KqKsESXPNpaSL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "DIHARD III",
    "paperLink": "https://arxiv.org/abs/2012.01477",
    "description": "A challenging benchmark for speaker diarization (segmentation of audio by speaker identity) across diverse domains such as clinical interviews, restaurant conversations, and web videos, focusing on robustness to noise and conversational variability.",
    "authors": [
      "Neville Ryant",
      "Prachi Singh",
      "Venkat Krishnamohan",
      "Rajan Varma",
      "Kenneth Church",
      "Christopher Cieri",
      "Jun Du",
      "Sriram Ganapathy",
      "Mark Liberman"
    ],
    "githubLink": "https://github.com/dihardchallenge/dihard3",
    "itemCount": "~33 hours of core evaluation data",
    "source": "arXiv",
    "specs": "Multi-domain audio recordings, RTTM (Rich Transcription Time Marked) annotation format",
    "year": "2020",
    "id": "saved-1769657441791-vibrx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGejzKymc8ie3gWj6MgNqAwPh_RnEOy6N_nxRd5GfnjgomwZTp7UoCZfDr5hYdFhmoiX3pulO1QLx1-2gqELMZ0pvy84XTQUBzbeHDYSOm-caBKPIfDB2VZsamaQSm_UcTf8w3qJ1gaPLB5TWfLvPtkmegpCmqm9EfAZM1v8yv83P-F-kbA5BcnTmhrHadkh7Ee-QZb93HImznxkdkeEn2HuWN6MQLR4AUvLBsJFQ5Ya4NWumK3md2KqKsESXPNpaSL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "DESED (Domestic Environment Sound Event Detection)",
    "paperLink": "https://hal.inria.fr/hal-02160855/document",
    "description": "A benchmark dataset for sound event detection in domestic environments, used in the DCASE Challenge Task 4. It combines weakly labeled data, strongly labeled synthetic data, and unlabeled data for semi-supervised segmentation of sound events like alarms, dog barks, and running water.",
    "authors": [
      "Nicolas Turpault",
      "Romain Serizel",
      "Ankit Parag Shah",
      "Justin Salamon"
    ],
    "githubLink": "https://github.com/DCASE-REPO/DESED_task",
    "itemCount": "Thousands of 10-second clips (Synthetic + Real)",
    "source": "Scholar",
    "specs": "10-second audio clips, strong (time boundaries) and weak labels, 10 domestic sound classes",
    "year": "2019",
    "id": "saved-1769657441791-hgmbk",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGejzKymc8ie3gWj6MgNqAwPh_RnEOy6N_nxRd5GfnjgomwZTp7UoCZfDr5hYdFhmoiX3pulO1QLx1-2gqELMZ0pvy84XTQUBzbeHDYSOm-caBKPIfDB2VZsamaQSm_UcTf8w3qJ1gaPLB5TWfLvPtkmegpCmqm9EfAZM1v8yv83P-F-kbA5BcnTmhrHadkh7Ee-QZb93HImznxkdkeEn2HuWN6MQLR4AUvLBsJFQ5Ya4NWumK3md2KqKsESXPNpaSL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AVA-Speech",
    "paperLink": "https://arxiv.org/abs/1808.00606",
    "description": "A densely labeled dataset for speech activity detection in movies. It provides temporal boundaries for speech, speech with background noise/music, and non-speech segments, facilitating robust audio segmentation in complex acoustic environments.",
    "authors": [
      "Sourish Chaudhuri",
      "Joseph Roth",
      "Daniel P. W. Ellis",
      "Andrew Gallagher",
      "Liat Kaver",
      "Radu Soricut",
      "Kevin Murphy",
      "Caroline Pantofaru",
      "Michael S. Ryoo",
      "Bernhard Schölkopf"
    ],
    "githubLink": "https://github.com/google-research/google-research/tree/master/ava_speech",
    "itemCount": "45 hours of audio, ~46,000 labeled segments",
    "source": "arXiv",
    "specs": "Start/end timestamps, labels: Speech, NoSpeech, Speech+Music, Speech+Noise",
    "year": "2018",
    "id": "saved-1769657441791-wx7vr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGejzKymc8ie3gWj6MgNqAwPh_RnEOy6N_nxRd5GfnjgomwZTp7UoCZfDr5hYdFhmoiX3pulO1QLx1-2gqELMZ0pvy84XTQUBzbeHDYSOm-caBKPIfDB2VZsamaQSm_UcTf8w3qJ1gaPLB5TWfLvPtkmegpCmqm9EfAZM1v8yv83P-F-kbA5BcnTmhrHadkh7Ee-QZb93HImznxkdkeEn2HuWN6MQLR4AUvLBsJFQ5Ya4NWumK3md2KqKsESXPNpaSL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "SALAMI (Structural Analysis of Large Amounts of Music Information)",
    "paperLink": "https://ismir2011.ismir.net/papers/OS6-1.pdf",
    "description": "A benchmark for music structural segmentation. It provides ground truth annotations for the temporal structure of music recordings (e.g., Intro, Verse, Chorus) across a wide variety of genres.",
    "authors": [
      "Jordan B. L. Smith",
      "J. Stephen Downie",
      "David De Roure",
      "Ichiro Fujinaga",
      "Mert Bay",
      "Andreas Ehmann"
    ],
    "githubLink": "https://github.com/DDMAL/salami-data-public",
    "itemCount": "~1,400 recordings, ~2,400 annotations",
    "source": "Scholar",
    "specs": "Text annotation files, hierarchical structural segments, diverse music genres",
    "year": "2011",
    "id": "saved-1769657441791-pmigv",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGejzKymc8ie3gWj6MgNqAwPh_RnEOy6N_nxRd5GfnjgomwZTp7UoCZfDr5hYdFhmoiX3pulO1QLx1-2gqELMZ0pvy84XTQUBzbeHDYSOm-caBKPIfDB2VZsamaQSm_UcTf8w3qJ1gaPLB5TWfLvPtkmegpCmqm9EfAZM1v8yv83P-F-kbA5BcnTmhrHadkh7Ee-QZb93HImznxkdkeEn2HuWN6MQLR4AUvLBsJFQ5Ya4NWumK3md2KqKsESXPNpaSL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AMI Meeting Corpus",
    "paperLink": "https://www.researchgate.net/publication/221654366_The_AMI_Meeting_Corpus_A_Pre-announcement",
    "description": "A multi-modal dataset consisting of 100 hours of meeting recordings. It serves as a benchmark for meeting segmentation, speaker diarization, topic segmentation, and dialogue act recognition.",
    "authors": [
      "Jean Carletta",
      "Simone Ashby",
      "Sebastien Bourban",
      "Mike Flynn",
      "Mael Guillemot",
      "Thomas Hain",
      "Jaroslav Kadlec",
      "Vasilis Karaiskos",
      "Wessel Kraaij",
      "Melissa Kronenthal"
    ],
    "githubLink": "https://huggingface.co/datasets/diarizers-community/ami",
    "itemCount": "100 hours of meetings",
    "source": "Hugging Face",
    "specs": "Audio (headset/array), Video, Text transcripts, Dialogue acts, Topic boundaries",
    "year": "2006",
    "id": "saved-1769657441791-32u9o",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGejzKymc8ie3gWj6MgNqAwPh_RnEOy6N_nxRd5GfnjgomwZTp7UoCZfDr5hYdFhmoiX3pulO1QLx1-2gqELMZ0pvy84XTQUBzbeHDYSOm-caBKPIfDB2VZsamaQSm_UcTf8w3qJ1gaPLB5TWfLvPtkmegpCmqm9EfAZM1v8yv83P-F-kbA5BcnTmhrHadkh7Ee-QZb93HImznxkdkeEn2HuWN6MQLR4AUvLBsJFQ5Ya4NWumK3md2KqKsESXPNpaSL",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "PASCAL VOC 2012",
    "paperLink": "https://arxiv.org/abs/0909.5206",
    "description": "A classic benchmark for visual object category recognition and detection, providing a standard dataset for classification, detection, and segmentation of 20 distinct object classes.",
    "authors": [
      "Mark Everingham",
      "Luc Van Gool",
      "Christopher K. I. Williams",
      "John Winn",
      "Andrew Zisserman"
    ],
    "githubLink": "https://github.com/sfzhang15/RefineDet",
    "itemCount": "11,530 images (total), 2,913 images (segmentation)",
    "source": "arXiv",
    "specs": "Images, Object Detection, Semantic Segmentation, Instance Segmentation",
    "year": "2010",
    "id": "saved-1769657496491-9mjtq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEm82auu2CB1ivBxGBxZjNgb4LgWNHRPa9dyoCbNwADVflRK_KLdnafNt1-ROBKsTO1pQ5LOwVK_jF4BKMtYSJxCFzPJX-LhWGYP8PP6lrL3vJGf6MNEXHx3oM=",
        "title": "datasetninja.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgEDtMCqFg7F0bFNNLBJ_fyCKvNIcA1SmAZ-AnSIOFme47qTqfV0nrDW8iCx4v4D-ud68k4HvZgGakDCmDmSV12wHT08brYyPTYktyYhTz06F80MnXPzJC1ClcYlw-W1YcDeaULo79P5MfJQ==",
        "title": "huggingface.co"
      }
    ]
  },
  {
    "title": "Open Images V7",
    "paperLink": "https://arxiv.org/abs/1811.00982",
    "description": "A dataset of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives.",
    "authors": [
      "Alina Kuznetsova",
      "Hassan Rom",
      "Neil Alldrin",
      "Jasper Uijlings",
      "Ivan Krasin",
      "Jordi Pont-Tuset",
      "Shahab Kamali",
      "Stefan Popov",
      "Matteo Malloci",
      "Alexander Kolesnikov",
      "Tom Duerig",
      "Vittorio Ferrari"
    ],
    "githubLink": "https://github.com/cvdfoundation/open-images-dataset",
    "itemCount": "9 million images, 2.8 million segmentation masks",
    "source": "arXiv",
    "specs": "Images, Bounding Boxes, Segmentation Masks, Point Labels, Visual Relationships",
    "year": "2020",
    "id": "saved-1769657565015-5bv0j",
    "groundingSources": []
  },
  {
    "title": "ScanNet++",
    "paperLink": "https://arxiv.org/abs/2308.11417",
    "description": "A high-fidelity dataset of 3D indoor scenes capturing sub-millimeter geometry and high-resolution color, designed for novel view synthesis and semantic understanding benchmarks.",
    "authors": [
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Matthias Nießner",
      "Angela Dai"
    ],
    "githubLink": "https://github.com/scannetpp/scannetpp",
    "itemCount": "460+ scenes",
    "source": "arXiv",
    "specs": "Laser scans (0.9mm resolution), 33MP DSLR images, iPhone RGB-D",
    "year": "2023",
    "id": "saved-1769657642387-6qsac",
    "groundingSources": []
  },
  {
    "title": "OmniObject3D",
    "paperLink": "https://arxiv.org/abs/2301.07525",
    "description": "A large-vocabulary 3D object dataset with massive high-quality real-scanned objects, supporting robust perception, novel-view synthesis, and 3D generation.",
    "authors": [
      "Tong Wu",
      "Jiarui Zhang",
      "Xiao Fu",
      "Yuxin Wang",
      "Jiawei Ren",
      "Liang Pan",
      "Wayne Wu",
      "Lei Yang",
      "Jiaqi Wang",
      "Chen Qian",
      "Dahua Lin",
      "Ziwei Liu"
    ],
    "githubLink": "https://github.com/omniobject3d/OmniObject3D",
    "itemCount": "6,000 objects",
    "source": "arXiv",
    "specs": "Textured meshes, Point clouds, Multi-view images, Real-captured videos, 190 categories",
    "year": "2023",
    "id": "saved-1769657642387-uq11o",
    "groundingSources": []
  },
  {
    "title": "Objaverse",
    "paperLink": "https://arxiv.org/abs/2212.08051",
    "description": "A massive dataset of annotated 3D objects collected from the web, designed to scale up 3D deep learning and generative models.",
    "authors": [
      "Matt Deitke",
      "Dustin Schwenk",
      "Jordi Salvador",
      "Luca Weihs",
      "Oscar Michel",
      "Eli VanderBilt",
      "Ludwig Schmidt",
      "Kiana Ehsani",
      "Aniruddha Kembhavi",
      "Ali Farhadi"
    ],
    "githubLink": "https://github.com/allenai/objaverse-xl",
    "itemCount": "800K+ objects (v1), 10M+ (XL)",
    "source": "Hugging Face",
    "specs": "3D models (GLB, USD, etc.), Metadata, Captions",
    "year": "2023",
    "id": "saved-1769657642387-9hyk7",
    "groundingSources": []
  },
  {
    "title": "CO3D (Common Objects in 3D)",
    "paperLink": "https://arxiv.org/abs/2109.00512",
    "description": "A large-scale dataset for category-level 3D reconstruction and novel view synthesis, featuring real-world videos of objects from MS-COCO categories with camera poses and point clouds.",
    "authors": [
      "Jeremy Reizenstein",
      "Roman Shapovalov",
      "Philipp Henzler",
      "Luca Sbordone",
      "Patrick Labatut",
      "David Novotny"
    ],
    "githubLink": "https://github.com/facebookresearch/co3d",
    "itemCount": "19,000+ videos (v1), ~37,000 (v2)",
    "source": "arXiv",
    "specs": "RGB images, Camera poses, Depth maps, Point clouds, 50 categories",
    "year": "2021",
    "id": "saved-1769657642387-dfbzu",
    "groundingSources": []
  },
  {
    "title": "RealEstate10K",
    "paperLink": "https://arxiv.org/abs/1805.09817",
    "description": "A large dataset of camera trajectories derived from YouTube videos of real estate tours, used for view synthesis and SLAM benchmarks.",
    "authors": [
      "Tinghui Zhou",
      "Richard Tucker",
      "John Flynn",
      "Graham Fyffe",
      "Noah Snavely"
    ],
    "githubLink": "https://google.github.io/realestate10k/",
    "itemCount": "10 million frames",
    "source": "Google Scholar",
    "specs": "Video frames, Camera intrinsics, Extrinsics (SLAM derived), txt format",
    "year": "2018",
    "id": "saved-1769657642387-2307u",
    "groundingSources": []
  },
  {
    "title": "Tanks and Temples",
    "paperLink": "https://www.tanksandtemples.org/media/papers/paper_tanksandtemples_2017.pdf",
    "description": "A benchmark for image-based 3D reconstruction containing high-resolution video sequences of outdoor and indoor scenes captured in realistic conditions. Ground truth is provided by industrial laser scanners.",
    "authors": [
      "Arno Knapitsch",
      "Jaesik Park",
      "Qian-Yi Zhou",
      "Vladlen Koltun"
    ],
    "githubLink": "https://github.com/isl-org/TanksAndTemples",
    "itemCount": "21 scenes (Intermediate & Advanced)",
    "source": "Google Scholar",
    "specs": "High-resolution video, Laser scan ground truth, Ply format",
    "year": "2017",
    "id": "saved-1769657642387-t69ax",
    "groundingSources": []
  },
  {
    "title": "DTU MVS Dataset",
    "paperLink": "http://openaccess.thecvf.com/content_cvpr_2014/papers/Jensen_Large_Scale_Multi-view_2014_CVPR_paper.pdf",
    "description": "A classic benchmark for multi-view stereo (MVS) algorithms, consisting of tabletop objects captured under controlled lighting with precise camera poses and structured light scanner ground truth.",
    "authors": [
      "Rasmus Jensen",
      "Anders Dahl",
      "George Vogiatzis",
      "Engin Tola",
      "Henrik Aanæs"
    ],
    "githubLink": "https://github.com/jzhangbs/DTU-MVS",
    "itemCount": "124 scenes",
    "source": "Google Scholar",
    "specs": "RGB images (1600x1200), Structured light 3D scans, Calibration data",
    "year": "2014",
    "id": "saved-1769657642388-ihzi5",
    "groundingSources": []
  },
  {
    "title": "Edit3D-Bench",
    "paperLink": "https://arxiv.org/abs/2508.19247",
    "description": "A benchmark for evaluating 3D editing capabilities, specifically focusing on precision and coherence in native 3D latent space. It includes human-annotated samples with specific editing instructions.",
    "authors": [
      "Lin Li",
      "Zehuan Huang",
      "Haoran Feng",
      "Gengxiong Zhuang",
      "Rui Chen",
      "Chunchao Guo",
      "Lu Sheng"
    ],
    "githubLink": "https://github.com/Nelipot-Lee/VoxHammer",
    "itemCount": "100+ samples",
    "source": "Hugging Face",
    "specs": "3D assets with editing prompts, masks, and ground truth data for evaluation",
    "year": "2025",
    "id": "saved-1769657738160-whtjh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "BelHouse3D",
    "paperLink": "https://arxiv.org/abs/2411.13251",
    "description": "A synthetic point cloud dataset designed for 3D indoor scene semantic segmentation, constructed using real-world references from houses in Belgium. It includes an out-of-distribution test set with occlusions.",
    "authors": [
      "Umamaheswaran Raman Kumar",
      "Abdur Razzaq Fayjie",
      "Jurgen Hannaert",
      "Patrick Vandewalle",
      "Alessio Del Bue",
      "Cristian Canton",
      "Jordi Pont-Tuset",
      "Tatiana Tommasi"
    ],
    "githubLink": "https://github.com/Pointcept/Pointcept",
    "itemCount": "424 indoor scenes",
    "source": "arXiv",
    "specs": "Synthetic point clouds, 19 classes, occlusion scenarios for robustness testing",
    "year": "2024",
    "id": "saved-1769657738160-0dwpt",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Objaverse-XL",
    "paperLink": "https://arxiv.org/abs/2307.05663",
    "description": "A massive dataset of over 10 million 3D objects sourced from diverse platforms like Sketchfab, Thingiverse, and Polycam. It is designed to scale up 3D vision tasks such as novel view synthesis and zero-shot generalization.",
    "authors": [
      "Matt Deitke",
      "Ruoshi Liu",
      "Matthew Wallingford",
      "Huong Ngo",
      "Oscar Michel",
      "Aditya Kusupati",
      "Alan Fan",
      "Christian Laforte",
      "Vikram Voleti",
      "Samir Yitzhak Gadre",
      "Eli VanderBilt",
      "Aniruddha Kembhavi",
      "Carl Vondrick",
      "Georgia Gkioxari",
      "Kiana Ehsani",
      "Ludwig Schmidt",
      "Ali Farhadi"
    ],
    "githubLink": "https://github.com/allenai/objaverse-xl",
    "itemCount": "10,000,000+ objects",
    "source": "arXiv",
    "specs": "Diverse 3D formats (GLB, etc.), metadata, sourced from web repositories",
    "year": "2023",
    "id": "saved-1769657738160-dw414",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "FaceScape",
    "paperLink": "https://arxiv.org/abs/2111.01082",
    "description": "A large-scale detailed 3D face dataset and benchmark for single-view 3D face reconstruction. It features high-quality textured 3D faces with pore-level geometry and displacement maps.",
    "authors": [
      "Haotian Yang",
      "Hao Zhu",
      "Yanru Wang",
      "Mingkai Huang",
      "Qiu Shen",
      "Ruigang Yang",
      "Xun Cao"
    ],
    "githubLink": "https://github.com/zhuhao-nju/facescape",
    "itemCount": "16,940 3D faces",
    "source": "arXiv",
    "specs": "847 subjects, 20 expressions each; Textured 3D models, displacement maps, topological uniformity",
    "year": "2021",
    "id": "saved-1769657738161-zndqo",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "3D-FUTURE",
    "paperLink": "https://arxiv.org/abs/2009.09633",
    "description": "A large-scale, richly-annotated repository of 3D furniture shapes with high-resolution textures. It is designed to support 3D modeling from images, texture recovery, and interior design understanding.",
    "authors": [
      "Huan Fu",
      "Rongfei Jia",
      "Lin Gao",
      "Dacheng Tao"
    ],
    "githubLink": "https://github.com/3D-FRONT-FUTURE/3D-FUTURE-ToolBox",
    "itemCount": "9,992 3D models",
    "source": "arXiv",
    "specs": "CAD furniture models, 5,000+ rooms, 20,000+ photo-realistic rendered images, high-res textures",
    "year": "2021",
    "id": "saved-1769657738161-hh0mt",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PartNet",
    "paperLink": "https://arxiv.org/abs/1812.02713",
    "description": "A consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. It supports tasks like fine-grained semantic segmentation, hierarchical semantic segmentation, and part instance segmentation.",
    "authors": [
      "Kaichun Mo",
      "Shilin Zhu",
      "Angel X. Chang",
      "Li Yi",
      "Subarna Tripathi",
      "Leonidas J. Guibas",
      "Hao Su"
    ],
    "githubLink": "https://github.com/daerduoCarey/PartNet_Dataset",
    "itemCount": "26,671 3D models",
    "source": "arXiv",
    "specs": "573,585 part instances across 24 object categories; hierarchical 3D part annotations",
    "year": "2019",
    "id": "saved-1769657738161-0fz1u",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ScanObjectNN",
    "paperLink": "https://openaccess.thecvf.com/content_ICCV_2019/html/Uy_Revisiting_Point_Cloud_Classification_A_New_Benchmark_Dataset_and_Classification_ICCV_2019_paper.html",
    "description": "A real-world point cloud object dataset based on scanned indoor scene data. It focuses on the challenges of background clutter and partial occlusion in real-world scans compared to synthetic datasets.",
    "authors": [
      "Mikaela Angelina Uy",
      "Quang-Hieu Pham",
      "Binh-Son Hua",
      "Duc Thanh Nguyen",
      "Sai-Kit Yeung"
    ],
    "githubLink": "https://github.com/hkust-vgd/scanobjectnn",
    "itemCount": "15,000 objects",
    "source": "Scholar",
    "specs": "15 categories, 2902 unique instances; Point clouds with global/local coords, normals, colors, semantic labels",
    "year": "2019",
    "id": "saved-1769657738161-hh9om",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ShapeNet",
    "paperLink": "https://arxiv.org/abs/1512.03012",
    "description": "A collaborative effort to establish a richly-annotated, large-scale dataset of 3D shapes. It provides semantic categories, consistent rigid alignments, parts, and other annotations for 3D CAD models.",
    "authors": [
      "Angel X. Chang",
      "Thomas Funkhouser",
      "Leonidas Guibas",
      "Pat Hanrahan",
      "Qixing Huang",
      "Zimo Li",
      "Silvio Savarese",
      "Manolis Savva",
      "Shuran Song",
      "Hao Su",
      "Jianxiong Xiao",
      "Li Yi",
      "Fisher Yu"
    ],
    "githubLink": "https://github.com/shapenet/shapenet-viewer",
    "itemCount": "3,000,000+ models",
    "source": "arXiv",
    "specs": "3,135 categories (ShapeNetCore: ~51,300 unique models in 55 categories); CAD models, parts, alignments",
    "year": "2015",
    "id": "saved-1769657738161-6vxp2",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGdsB4vV1kGAE0wbR9e5CuZNvSTm2-MkFvps14wStcTpH68tGcXSxmP1bBsdZMo9kre6u48YhcEqTD-wNHJQ3SfHyX0O3E-NfTz2XKIm5a1_CGJY8bAkE_p1hx4bjGF3KKSceQ_bp9d",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPwlrwOP3ObAoTYB_ass8RaAIb5S8zy_hJJ9tLzug8CdHDl2--lELaPFwje1F2l_ncJ1hEqNzJmZahW846_YxclrGof4FmqN5DG0n9CJl4JIL7YSGXSRUlnELe9OtHMR77eQ25",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFaFq70HC4T4i9qNjB5f0-u6RVaI2Z2PW8l2rNWZIH6z592Q5o-j9cs4g7E2g6RsizoohlLlCf6uXi6a7FS_xx4S2gIoIDtQdmTA7OvTUgh5-bjMECgSzG9SVMX",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGa1pTBO1IDOvi1oYJYxAsv7sf3cPCSYS2EXyFSFOSE7mgJZBV8nFQcomsok1KDC6oH34mS2oz5jDyd5fUj7QsB1TXZ6oEYT_ufxz6i3hHekoq4_pZpZuQtoS6",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDP9O-fJKLZDQ7Ndd2jFJFfpBObJSyQWLk3tPta0zVDVjis7tOgkinPj-KWU3aiqPbvNF4O1z9yREL3ppdSa9x7cfAYRApa4lGsr2m6DMxxKvafBbgSqmHtZ7QjZNsRA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDlmQ824F3HQWi8Y2x0EIi5qyyPGQPOMnx-_XaC9OB2OVN1uK_PWr5nNvLYBiT3GiL3LiXLiPiN0o1_srZB8DJVF862xGkrpgijvYGSa3pDqiHCR0_UTc2dIfCXbSo",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Patho-Bench",
    "paperLink": "https://arxiv.org/abs/2502.06750",
    "description": "A standardized benchmark suite designed to evaluate patch and slide encoder foundation models for whole-slide images (WSIs). It covers diverse tasks including morphological subtyping, tumor grading, and mutation prediction across multiple tissue types.",
    "authors": [
      "Andrew Zhang",
      "Guillaume Jaume",
      "Anurag Vaidya",
      "Tong Ding",
      "Faisal Mahmood"
    ],
    "githubLink": "https://github.com/mahmoodlab/Patho-Bench",
    "itemCount": "95 tasks derived from 33 public datasets",
    "source": "Hugging Face",
    "specs": "Python library / Hugging Face Datasets; H&E Whole Slide Images (WSIs)",
    "year": "2025",
    "id": "saved-1769657839340-v5z3x",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PathoCell (PathoCellBench)",
    "paperLink": "https://arxiv.org/abs/2404.13632",
    "description": "A comprehensive benchmark for evaluating cell phenotyping capabilities of pathology foundation models. It consolidates datasets like PanNuke, Lizard, and the new PathoCell dataset to test generalization across technical and medical domain shifts.",
    "authors": [
      "Fabian H. Reith",
      "Jonathan Lüscher",
      "Nils Koreuber",
      "Johannes Franzen",
      "Christoph Winklmayr"
    ],
    "githubLink": "https://github.com/Kainmueller-Lab/PathoCell",
    "itemCount": "4 datasets (PathoCell, PanNuke, Lizard, ARCTIQUE); >88 million cells in PathoCell subset",
    "source": "Hugging Face",
    "specs": "LMDB format; H&E images; 14 granular cell types",
    "year": "2024",
    "id": "saved-1769657839340-asm76",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "AI4SkIN",
    "paperLink": "https://doi.org/10.1016/j.artmed.2024.102830",
    "description": "A dataset for skin cancer subtyping, specifically cutaneous spindle cell neoplasms. It includes annotations from both experts and crowdsourced non-experts to support Multiple Instance Learning (MIL) research.",
    "authors": [
      "Rocío del Amor",
      "Pablo Meseguer",
      "Adrián Colomer",
      "Rafael Molina",
      "Valery Naranjo"
    ],
    "githubLink": "https://github.com/wizmik12/Crowdsourcing-MIL-Skin-Cancer",
    "itemCount": "271 Whole Slide Images (WSIs)",
    "source": "Zenodo",
    "specs": "H&E Whole Slide Images; Skin Cancer Subtypes",
    "year": "2024",
    "id": "saved-1769657839340-s1axs",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "UBC-OCEAN",
    "paperLink": "https://www.kaggle.com/competitions/UBC-OCEAN",
    "description": "A benchmark dataset for classifying ovarian cancer subtypes and detecting outliers (rare subtypes) in whole-slide histopathology images.",
    "authors": [
      "UBC",
      "BC Cancer",
      "Ovarian Tumour Tissue Analysis (OTTA) Consortium"
    ],
    "githubLink": "https://github.com/GeneStat/UBC-OCEAN",
    "itemCount": "538 Training WSIs",
    "source": "Kaggle",
    "specs": "H&E Whole Slide Images; 5 Subtypes + Outliers",
    "year": "2023",
    "id": "saved-1769657839340-irenv",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PANDA (Prostate cANcer graDe Assessment)",
    "paperLink": "https://www.nature.com/articles/s41591-021-01620-2",
    "description": "A large-scale dataset for prostate cancer diagnosis and Gleason grading, originally released for a Kaggle competition. It consists of whole-slide images from two different centers to study domain generalization.",
    "authors": [
      "Wouter Bulten",
      "Kimmo Kartasalo",
      "Po-Hsuan Cameron Chen",
      "Peter Ström",
      "Hans Pinckaers",
      "et al."
    ],
    "githubLink": "https://github.com/karolinska-institutet/panda-challenge",
    "itemCount": "10,616 Whole Slide Images (WSIs)",
    "source": "Google Scholar",
    "specs": "H&E Whole Slide Images; Multiclass (Gleason Scores)",
    "year": "2022",
    "id": "saved-1769657839340-7efly",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "NuCLS",
    "paperLink": "https://academic.oup.com/gigascience/article/11/1/giac013/6586616",
    "description": "A large-scale crowdsourced dataset for nucleus classification, localization, and segmentation in breast cancer, featuring annotations from both pathologists and non-experts.",
    "authors": [
      "Mohamed Amgad",
      "L. A. Atteya",
      "H. Hussein",
      "K. H. Mohammed",
      "Lee A. D. Cooper"
    ],
    "githubLink": "https://github.com/PathologyDataScience/NuCLS",
    "itemCount": ">220,000 annotated nuclei",
    "source": "Semantic Scholar",
    "specs": "H&E; Breast Cancer; Object Detection / Segmentation",
    "year": "2022",
    "id": "saved-1769657839340-xmqsn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Lizard",
    "paperLink": "https://arxiv.org/abs/2108.11195",
    "description": "A large-scale dataset for colonic nuclear instance segmentation and classification. It is designed to train models for accurate nuclear detection and typing in colon histology images.",
    "authors": [
      "Sophia J. Wagner",
      "Daniel Reisenbüchler",
      "Nicholas P. West",
      "Tom Wiesmann",
      "Heike I. Grabsch",
      "et al."
    ],
    "githubLink": "https://github.com/vqdang/hover_net",
    "itemCount": "291 Fields of View; 495,179 labeled nuclei",
    "source": "arXiv",
    "specs": "H&E; Colon tissue; 6 cell classes",
    "year": "2021",
    "id": "saved-1769657839341-s2hwh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MHIST",
    "paperLink": "https://arxiv.org/abs/2101.12355",
    "description": "A minimalist histopathology image analysis dataset serving as a 'petri dish' for rapid benchmarking. It focuses on the binary classification of colorectal polyps (Hyperplastic Polyp vs. Sessile Serrated Adenoma).",
    "authors": [
      "Jerry Wei",
      "Arief Suriawinata",
      "Bing Ren",
      "Xiaoying Liu",
      "Saeed Hassanpour"
    ],
    "githubLink": "https://bmirds.github.io/MHIST/",
    "itemCount": "3,152 images",
    "source": "arXiv",
    "specs": "224x224 px images; H&E; Binary Classification",
    "year": "2021",
    "id": "saved-1769657839341-rbwck",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PanNuke",
    "paperLink": "https://doi.org/10.1007/978-3-030-23937-4_2",
    "description": "An open pan-cancer histology dataset for nuclei instance segmentation and classification. It contains semi-automatically generated nuclei annotations across 19 different tissue types.",
    "authors": [
      "Jevgenij Gamper",
      "Navid Alemi Koohbanani",
      "Ksenija Benet",
      "Ali Khuram",
      "Nasir Rajpoot"
    ],
    "githubLink": "https://github.com/jgamper/PanNukeDataset",
    "itemCount": "7,904 patches; >200,000 nuclei",
    "source": "Semantic Scholar",
    "specs": "256x256 patches; H&E; 5 cell classes (Neoplastic, Inflammatory, Connective, Dead, Epithelial)",
    "year": "2019",
    "id": "saved-1769657839342-obnxi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PatchCamelyon (PCam)",
    "paperLink": "https://arxiv.org/abs/1806.03962",
    "description": "A classification benchmark derived from the Camelyon16 challenge. Ideally positioned as a 'CIFAR-10 for pathology', it provides a large set of small patches for binary tumor detection.",
    "authors": [
      "Bas S. Veeling",
      "Jasper Linmans",
      "Jim Winkens",
      "Tazro Cohen",
      "Max Welling"
    ],
    "githubLink": "https://github.com/basveeling/pcam",
    "itemCount": "327,680 patches",
    "source": "GitHub",
    "specs": "96x96 color images; H&E; Binary Classification (Metastatic vs Normal)",
    "year": "2018",
    "id": "saved-1769657839342-srv2o",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1Y6GQxWX07C0O49b_iNSChMTAfvN2PegILgu77oq0FUDMekYydgdmvB2kROcoAzK5WXiCdwj10JMpMjIRrzrHhoEz7O_9Tw7adyesBKVtxTIKNfZ57-JZFfIi6HWTFh-aMfB-nw==",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVOYLZUaaBfBOYAfxttB1iXFIIQoMVFz5DabUTfP1h5N0rDsCCvlN6rZSD_eL3hK2gehguGMC3V72zurAzmIfRHtkWEAI8O9inye_pbXG9I1jwcDqRl8KjDYPGj9p-Hw4-0-y7zMR8msN3-Qaw43-Wr_I=",
        "title": "mdc-berlin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHysKMn1HgBI02exGgwDCnyzNi62Nbvg64IBIa_EH-v3ZNhjmuwH_h78aRfOZ4eyY5BO2B6M8OoCu4ZwoqzYrRDGxTzkOjnnvUWVRSmiw-WBOd9ojKx5VFGSaQ=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "CheXpert Plus",
    "paperLink": "https://arxiv.org/abs/2405.19538",
    "description": "An augmented version of the CheXpert dataset that includes the original text radiology reports, patient demographics, and additional image formats, enabling text-based and multimodal research.",
    "authors": [
      "Pierre Chambon",
      "Jean-Benoit Delbrouck",
      "Thomas Drusch",
      "Kaveri Thakoor",
      "Louis Blankemeier",
      "et al."
    ],
    "githubLink": "https://github.com/Stanford-AIMI/CheXpert-Plus",
    "itemCount": "223,462 image-report pairs",
    "source": "Stanford AIMI",
    "specs": "Chest X-ray images, Text reports, Demographics",
    "year": "2024",
    "id": "saved-1769657927344-66yir",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFyOC6PaTV5KuRHtd6AbvpD1rvWfpSCjjqmaj-vuPXQ8SDq3Vn51_PEdlQ5MA10GbHTh53saVEqle5SkoneCZKFt3eiAatY5N1KPFkHCZmvxACFuPDIJ8S9EjM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVqgC4NxcwJhG7YgpSOQFAP5dWpZo3HT0y_IwDfvk2ObdmvLUirt5vzCX_LylXjJc4Keht3Rz8CgkwtNvRy_4IM_6_U-v_1AQ_MNGpIJJNMtiHLh977HSee4_ZwEZNkdvXHkxWRHR2paoilU7KXRbC2xK4SSZgf8XIsDCfPpIYdHhDmbbdA9cykTV0rg0FBBwvbQW591VnxpN5Xcroz1mAtU-rrh1DKyxB2jmO9JDAqovodkNjZ68s",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHXv66JfUUN--X0htvYb4RZKHF6TS0a0DumyCx1tEx_Jx1ldbFE5_nex0M3_LV5OlMJg4uuweV1PrmDY2wqOl9IJs6RDsW9mk_3dja3xa-GcqpN-WH80Cl3769ZjLab9Umo4zo85NffThISIcc=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2wN4eIrhf2QJNgB6Uq836pYtPDdnLXaSjLi3Heo48vVw1HilphGSgsaNDbjmaqmV2j_lr5QYB1Aqp0LjP565Vhu5uhfSBH6VLPw-8yxsVArjSlNyGeaYUuNoQcfOlXX6GeQd-vuZ4Ri5sK1nIf5qBJtmbxd1Xe96_vE4LVG4TStG9wkwr1cxPPF7JAml9jg==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-yRUbxFPmO_zKBcA3GNilyUaub98VAoGdY3V1tcIxiPGtdfqnHosoLcX20woLaO0Pmgx-EZXwCpBUZyxYq5-tpzOkssRWHGDGbgvi-agky9w3UbIuwxrRudkLOzBN6ztT_Dhtf1ZbGtf36EtX76k3Hs-ALcCO_dmA3gIyX4zs-fUh5mfRDtHCthIYnH-6htAn1mVvPFyO-bLdMgZvcdIlUt6Un7lfmlh1UHMr-JhF",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "MS-CXR-T",
    "paperLink": "https://physionet.org/content/ms-cxr-t/1.0.0/",
    "description": "A multi-modal benchmark for evaluating biomedical vision-language processing models on temporal tasks, specifically measuring disease progression and temporal-semantic similarity in radiology reports.",
    "authors": [
      "Shruthi Bannur",
      "Stephanie Hyland",
      "Qianchu Liu",
      "Fernando Pérez-García",
      "Maximilian Ilse",
      "Daniel Coelho de Castro",
      "Benedikt Boecking",
      "Harshita Sharma",
      "Kenza Bouzid",
      "Anton Schwaighofer",
      "Maria Teodora Wetscherek",
      "Hannah Richardson",
      "Tristan Naumann",
      "Javier Alvarez Valle",
      "Ozan Oktay"
    ],
    "githubLink": "https://github.com/microsoft/ms-cxr",
    "itemCount": "1,326 image pairs, 361 sentence pairs",
    "source": "PhysioNet",
    "specs": "Longitudinal Chest X-rays, Temporal labels, Sentence pairs",
    "year": "2023",
    "id": "saved-1769657927344-ov4yw",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFyOC6PaTV5KuRHtd6AbvpD1rvWfpSCjjqmaj-vuPXQ8SDq3Vn51_PEdlQ5MA10GbHTh53saVEqle5SkoneCZKFt3eiAatY5N1KPFkHCZmvxACFuPDIJ8S9EjM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVqgC4NxcwJhG7YgpSOQFAP5dWpZo3HT0y_IwDfvk2ObdmvLUirt5vzCX_LylXjJc4Keht3Rz8CgkwtNvRy_4IM_6_U-v_1AQ_MNGpIJJNMtiHLh977HSee4_ZwEZNkdvXHkxWRHR2paoilU7KXRbC2xK4SSZgf8XIsDCfPpIYdHhDmbbdA9cykTV0rg0FBBwvbQW591VnxpN5Xcroz1mAtU-rrh1DKyxB2jmO9JDAqovodkNjZ68s",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHXv66JfUUN--X0htvYb4RZKHF6TS0a0DumyCx1tEx_Jx1ldbFE5_nex0M3_LV5OlMJg4uuweV1PrmDY2wqOl9IJs6RDsW9mk_3dja3xa-GcqpN-WH80Cl3769ZjLab9Umo4zo85NffThISIcc=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2wN4eIrhf2QJNgB6Uq836pYtPDdnLXaSjLi3Heo48vVw1HilphGSgsaNDbjmaqmV2j_lr5QYB1Aqp0LjP565Vhu5uhfSBH6VLPw-8yxsVArjSlNyGeaYUuNoQcfOlXX6GeQd-vuZ4Ri5sK1nIf5qBJtmbxd1Xe96_vE4LVG4TStG9wkwr1cxPPF7JAml9jg==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-yRUbxFPmO_zKBcA3GNilyUaub98VAoGdY3V1tcIxiPGtdfqnHosoLcX20woLaO0Pmgx-EZXwCpBUZyxYq5-tpzOkssRWHGDGbgvi-agky9w3UbIuwxrRudkLOzBN6ztT_Dhtf1ZbGtf36EtX76k3Hs-ALcCO_dmA3gIyX4zs-fUh5mfRDtHCthIYnH-6htAn1mVvPFyO-bLdMgZvcdIlUt6Un7lfmlh1UHMr-JhF",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "RadGraph",
    "paperLink": "https://arxiv.org/abs/2106.14463",
    "description": "A dataset of entities and relations extracted from radiology reports (MIMIC-CXR and CheXpert). It provides a schema and annotations for developing information extraction models for radiology text.",
    "authors": [
      "Saahil Jain",
      "Ashwin Agrawal",
      "Adriel Saporta",
      "Steven Q. H. Truong",
      "Du Nguyen Duong",
      "Tan Bui",
      "Pierre Chambon",
      "Yuhao Zhang",
      "Matthew P. Lungren",
      "Andrew Y. Ng",
      "Curtis P. Langlotz",
      "Pranav Rajpurkar"
    ],
    "githubLink": "https://github.com/Stanford-AIMI/radgraph",
    "itemCount": "600 annotated reports (dev/test), 220k inference",
    "source": "PhysioNet / Stanford AIMI",
    "specs": "Text reports, Entity/Relation annotations (JSON)",
    "year": "2021",
    "id": "saved-1769657927344-9z5dq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFyOC6PaTV5KuRHtd6AbvpD1rvWfpSCjjqmaj-vuPXQ8SDq3Vn51_PEdlQ5MA10GbHTh53saVEqle5SkoneCZKFt3eiAatY5N1KPFkHCZmvxACFuPDIJ8S9EjM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVqgC4NxcwJhG7YgpSOQFAP5dWpZo3HT0y_IwDfvk2ObdmvLUirt5vzCX_LylXjJc4Keht3Rz8CgkwtNvRy_4IM_6_U-v_1AQ_MNGpIJJNMtiHLh977HSee4_ZwEZNkdvXHkxWRHR2paoilU7KXRbC2xK4SSZgf8XIsDCfPpIYdHhDmbbdA9cykTV0rg0FBBwvbQW591VnxpN5Xcroz1mAtU-rrh1DKyxB2jmO9JDAqovodkNjZ68s",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHXv66JfUUN--X0htvYb4RZKHF6TS0a0DumyCx1tEx_Jx1ldbFE5_nex0M3_LV5OlMJg4uuweV1PrmDY2wqOl9IJs6RDsW9mk_3dja3xa-GcqpN-WH80Cl3769ZjLab9Umo4zo85NffThISIcc=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2wN4eIrhf2QJNgB6Uq836pYtPDdnLXaSjLi3Heo48vVw1HilphGSgsaNDbjmaqmV2j_lr5QYB1Aqp0LjP565Vhu5uhfSBH6VLPw-8yxsVArjSlNyGeaYUuNoQcfOlXX6GeQd-vuZ4Ri5sK1nIf5qBJtmbxd1Xe96_vE4LVG4TStG9wkwr1cxPPF7JAml9jg==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-yRUbxFPmO_zKBcA3GNilyUaub98VAoGdY3V1tcIxiPGtdfqnHosoLcX20woLaO0Pmgx-EZXwCpBUZyxYq5-tpzOkssRWHGDGbgvi-agky9w3UbIuwxrRudkLOzBN6ztT_Dhtf1ZbGtf36EtX76k3Hs-ALcCO_dmA3gIyX4zs-fUh5mfRDtHCthIYnH-6htAn1mVvPFyO-bLdMgZvcdIlUt6Un7lfmlh1UHMr-JhF",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "IU X-Ray (OpenI)",
    "paperLink": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4804869/",
    "description": "A set of chest X-ray images and corresponding diagnostic reports collected by Indiana University. It is widely used for benchmarking radiology report generation models.",
    "authors": [
      "Dina Demner-Fushman",
      "Marc D. Kohli",
      "Marc B. Rosenman",
      "Sonya E. Shooshan",
      "Laritza Rodriguez",
      "Sameer Antani",
      "George R. Thoma",
      "Clement J. McDonald"
    ],
    "githubLink": "https://github.com/rsumner4/chest-xray-generator",
    "itemCount": "7,470 images, 3,955 reports",
    "source": "OpenI",
    "specs": "Chest X-ray images (PNG/DICOM), XML radiology reports",
    "year": "2016",
    "id": "saved-1769657927344-u66lb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFyOC6PaTV5KuRHtd6AbvpD1rvWfpSCjjqmaj-vuPXQ8SDq3Vn51_PEdlQ5MA10GbHTh53saVEqle5SkoneCZKFt3eiAatY5N1KPFkHCZmvxACFuPDIJ8S9EjM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVqgC4NxcwJhG7YgpSOQFAP5dWpZo3HT0y_IwDfvk2ObdmvLUirt5vzCX_LylXjJc4Keht3Rz8CgkwtNvRy_4IM_6_U-v_1AQ_MNGpIJJNMtiHLh977HSee4_ZwEZNkdvXHkxWRHR2paoilU7KXRbC2xK4SSZgf8XIsDCfPpIYdHhDmbbdA9cykTV0rg0FBBwvbQW591VnxpN5Xcroz1mAtU-rrh1DKyxB2jmO9JDAqovodkNjZ68s",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHXv66JfUUN--X0htvYb4RZKHF6TS0a0DumyCx1tEx_Jx1ldbFE5_nex0M3_LV5OlMJg4uuweV1PrmDY2wqOl9IJs6RDsW9mk_3dja3xa-GcqpN-WH80Cl3769ZjLab9Umo4zo85NffThISIcc=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2wN4eIrhf2QJNgB6Uq836pYtPDdnLXaSjLi3Heo48vVw1HilphGSgsaNDbjmaqmV2j_lr5QYB1Aqp0LjP565Vhu5uhfSBH6VLPw-8yxsVArjSlNyGeaYUuNoQcfOlXX6GeQd-vuZ4Ri5sK1nIf5qBJtmbxd1Xe96_vE4LVG4TStG9wkwr1cxPPF7JAml9jg==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE-yRUbxFPmO_zKBcA3GNilyUaub98VAoGdY3V1tcIxiPGtdfqnHosoLcX20woLaO0Pmgx-EZXwCpBUZyxYq5-tpzOkssRWHGDGbgvi-agky9w3UbIuwxrRudkLOzBN6ztT_Dhtf1ZbGtf36EtX76k3Hs-ALcCO_dmA3gIyX4zs-fUh5mfRDtHCthIYnH-6htAn1mVvPFyO-bLdMgZvcdIlUt6Un7lfmlh1UHMr-JhF",
        "title": "neurips.cc"
      }
    ]
  },
  {
    "title": "ULS23: Universal Lesion Segmentation Challenge",
    "paperLink": "https://arxiv.org/abs/2406.07160",
    "description": "A benchmark for 3D universal lesion segmentation in CT, combining data from DeepLesion, KiTS, LiTS, and new annotations for bone/pancreas/lymph nodes.",
    "authors": [
      "M. J. J. de Grauw",
      "E. Th. Scholten",
      "E. J. Smit",
      "et al."
    ],
    "githubLink": "https://github.com/DIAGNijmegen/ULS23",
    "itemCount": "38,693 lesions (Training)",
    "source": "arXiv",
    "specs": "3D CT (NIfTI). Modality: Medical Imaging (CT).",
    "year": "2024",
    "id": "saved-1769658008404-90s43",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "BraTS 2023: International Brain Tumor Segmentation Challenge",
    "paperLink": "https://arxiv.org/abs/2405.18372",
    "description": "The latest iteration of the BraTS challenge, focusing on segmenting brain tumors (gliomas, meningiomas, metastases) across multiple MRI modalities.",
    "authors": [
      "Baid, Ujjwal",
      "et al."
    ],
    "githubLink": "https://github.com/RAVIRAJPOOJARY/BraTS-2023-Challenge",
    "itemCount": "4,842 MRI scans (Training)",
    "source": "arXiv",
    "specs": "Multi-modal MRI (T1, T1c, T2, FLAIR) in NIfTI format. Modality: Medical Imaging (MRI).",
    "year": "2023",
    "id": "saved-1769658008405-pbb4p",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "KiTS23: 2023 Kidney and Kidney Tumor Segmentation Challenge",
    "paperLink": "https://openreview.net/forum?id=T8f3XnE7Ww",
    "description": "A benchmark for semantic segmentation of kidneys, renal tumors, and cysts in CT scans. It expands on KiTS19/21 by adding more cases and new contrast phases.",
    "authors": [
      "Nicholas Heller",
      "Fabian Isensee",
      "Dartagnan Engelhardt",
      "et al."
    ],
    "githubLink": "https://github.com/neheller/kits23",
    "itemCount": "599 CT scans (489 Training, 110 Testing)",
    "source": "Semantic Scholar",
    "specs": "3D CT scans (NIfTI). Modality: Medical Imaging (CT).",
    "year": "2023",
    "id": "saved-1769658008405-vqxfo",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "ATLAS v2.0: Anatomical Tracings of Lesions After Stroke",
    "paperLink": "https://www.nature.com/articles/s41597-022-01401-7",
    "description": "A large, open-source dataset of T1-weighted MRIs with manually segmented stroke lesions, designed to improve the generalizability of lesion segmentation algorithms.",
    "authors": [
      "Sook-Lei Liew",
      "B. P. Lo",
      "M. R. Donnelly",
      "A. Zavaliangos-Petropulu",
      "et al."
    ],
    "githubLink": "https://github.com/npnl/ATLAS",
    "itemCount": "1,271 T1-weighted MRI scans",
    "source": "Nature Scientific Data",
    "specs": "T1-weighted MRI (NIfTI). Modality: Medical Imaging (MRI).",
    "year": "2022",
    "id": "saved-1769658008405-faeuz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "ISLES 2022: Ischemic Stroke Lesion Segmentation",
    "paperLink": "https://arxiv.org/abs/2206.06694",
    "description": "A multi-center MRI dataset for the segmentation of acute to sub-acute ischemic stroke lesions, featuring high variability in lesion size and location.",
    "authors": [
      "Moritz R. Hernandez Petzsche",
      "Ezequiel de la Rosa",
      "Uta Hanning",
      "et al."
    ],
    "githubLink": "https://github.com/ezequieldlrosa/isles22",
    "itemCount": "400 MRI cases (250 Training, 150 Testing)",
    "source": "arXiv",
    "specs": "Multi-modal MRI (DWI, ADC, FLAIR). Modality: Medical Imaging (MRI).",
    "year": "2022",
    "id": "saved-1769658008405-0eb10",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "MSSEG-2: Multiple Sclerosis New Lesion Segmentation",
    "paperLink": "https://doi.org/10.1016/j.media.2021.102039",
    "description": "A challenge dataset focused on detecting and segmenting new Multiple Sclerosis (MS) lesions between two longitudinal MRI time points.",
    "authors": [
      "Olivier Commowick",
      "A. Cervenansky",
      "F. Cotton",
      "F. Dojat",
      "et al."
    ],
    "githubLink": "https://github.com/OLiviella/MSSEG-2",
    "itemCount": "100 Patients (Longitudinal MRI pairs)",
    "source": "Scholar",
    "specs": "3D FLAIR MRI. Modality: Medical Imaging (MRI).",
    "year": "2021",
    "id": "saved-1769658008405-ro8wr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "LiTS: Liver Tumor Segmentation Benchmark",
    "paperLink": "https://arxiv.org/abs/1901.04056",
    "description": "A benchmark dataset for automatic segmentation of liver and liver tumors in CT scans, aiming to compare different algorithms' performance on diverse clinical data.",
    "authors": [
      "Patrick Bilic",
      "Patrick Christ",
      "Hongwei Bran Li",
      "Eugene Vorontsov",
      "et al."
    ],
    "githubLink": "https://github.com/junqiangchen/LiTS---Liver-Tumor-Segmentation-Challenge",
    "itemCount": "201 CT scans (131 training, 70 testing)",
    "source": "arXiv",
    "specs": "3D CT scans (NIfTI format). Modality: Medical Imaging (CT).",
    "year": "2019",
    "id": "saved-1769658008405-ntmmi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection",
    "paperLink": "https://arxiv.org/abs/1902.03368",
    "description": "A large-scale dataset for skin lesion analysis, including lesion segmentation, attribute detection, and disease classification. It serves as a benchmark for melanoma detection.",
    "authors": [
      "Noel Codella",
      "Veronica Rotemberg",
      "Philipp Tschandl",
      "M. Emre Celebi",
      "et al."
    ],
    "githubLink": "https://github.com/ISIC-Archive/ISIC-Challenge-2018",
    "itemCount": "10,015 dermoscopic images",
    "source": "arXiv",
    "specs": "Dermoscopic images (JPEG) with binary segmentation masks (PNG). Modality: Image (Skin).",
    "year": "2018",
    "id": "saved-1769658008405-2qs4w",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "NIH DeepLesion",
    "paperLink": "https://arxiv.org/abs/1711.10535",
    "description": "A large-scale dataset of diverse lesions (lung nodules, liver tumors, enlarged lymph nodes, etc.) mined from PACS, supporting universal lesion detection and segmentation.",
    "authors": [
      "Ke Yan",
      "Xiaosong Wang",
      "Le Lu",
      "Ronald M. Summers"
    ],
    "githubLink": "https://github.com/rsummers11/CADLab/tree/master/DeepLesion",
    "itemCount": "32,735 annotated lesions in 10,594 studies",
    "source": "arXiv",
    "specs": "CT slices (PNG/NIfTI) with RECIST measurements. Modality: Medical Imaging (CT).",
    "year": "2018",
    "id": "saved-1769658008405-popro",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "LUNA16: LUng Nodule Analysis 2016",
    "paperLink": "https://doi.org/10.1016/j.media.2017.06.015",
    "description": "A widely used benchmark for the detection and segmentation of pulmonary nodules in CT scans, derived from the LIDC-IDRI database.",
    "authors": [
      "Arnaud Arindra Adiyoso Setio",
      "Alberto Traverso",
      "Thomas de Bel",
      "et al."
    ],
    "githubLink": "https://luna16.grand-challenge.org/",
    "itemCount": "888 CT scans",
    "source": "Scholar",
    "specs": "3D CT scans (MHD/RAW format). Modality: Medical Imaging (CT).",
    "year": "2017",
    "id": "saved-1769658008405-hgb9s",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGuRFVmap1r9AgIiUhx5ezX0SUpr5NWvKXgEh18N1EmzTrlCWag7CPnLP4s98sOvvy3TqocXYkUSybx07ud7JUgLnlYeRvjK7SaJxvwPpqxutv1VZmFjHS87C97Ibih4YkjOp1DSp49fms9UszQXVMQgGP4mwW7kptmmIrPmOz1ChlugUBEN3K6aFMz19o=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFknhwi6-5PqvDpmnUVzpr5WFfJ3WR0SpwJhBTP2TBcqlmxa60MoBBlqAH2Iw0MR5yKPWKKND8QR1mtCuyTCcmM91OOEMDL7Ry9AZBe3vXCN6uL4B_g-GtRQkxcc95QUw4UTzI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEWuHbI-ac838kuMQjmJdU_dMmg9CdxeXyFs-o-00pWp69-P9cNAjLaxlJkFmEtDLdUzIR6u2REPaitdlS4fhGcbM5xNWT453NwAJxcx1K2TFWMlliVJd2_8KqLWNwD7w==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGe-4Fbj9iPz-4Kf57IgzEMaZ7B-p5Nha2toA_yzbf87AMwTrrTOVOg0XHzsukyrcdrdCdEPlbaYHrsBOZTKU_6lK9uqUgGEOaXFJiQy9OOfe60y9shM26UdDlka4FRaQTkzOcDeVz3oyVMgM3yZKQjPCu-AVRV9NqFDzckvOVk2_NgdGeGauIRmZsAISzN2LLyPvLX8x3fo_x-SzHCwhnxT-i8-qGcQ-vic89JioaFQwKdZ_CX2coTGeqyC36iz-n0xOL5YPs1Ah0ggDVEsONA4jpe9Q6T-4=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgvPRc-0b0TzMzYhfJRrmSHzjnglrMbOw6qisojspmUnR2JWOAI-fmKq9b15bo2pNnQXFr-L_MUZIEwfCkNOJGidMkFGSbZI3pzkyXSKeXr_qBVEq8KWWXde0DuekelPnARBHwYzyLbWZrUGSPDnqVTbSQ",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbeQF85FX3ww7tLnNWEt8ZaBdnxDqjt768iF0S6BuEuVXzJpmHJFqcyrCH2EVUVSRarcUTMdt814sqMhZsMtK-nrmzbcby-C27mh-3Zny-lnQFUvc-fz8QG2C8tj568oasdH2NREGKRy3nfVH3vJR9KIkVuq1uhLsuj3symmFnLAdabk9F9OoRunRAEOXIJ-vOwbeRkN3CedB22_5L329lvdHp_5rlCEBEzagYC4bnWrBlljQHm_22V7v6ajuh5Oo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmbl-zFqTY5dwBV9WbTakOFAjikyMCoCDoK6MxsNPctR847gWYxFaPZRlPR3r4Yafgwo0O4r80M26375G0ICNJASFLsvlKSbtQA5JjYVSghhk09ar-vyKf9hAHPrYXxNm1-ev6JBCFE-0XCiaH1IQqYw3REbN08dTvfdAOf3U7a8LHG7nfaGiYxWW6BKiJW1vMboBXOAunF3yS_aNcFaQJGGxuyt68N1YJQW1YE8rqE2jMkOxEqmiZw9IAg8oC_tSxPRGl6ZGXDdPVh3pMvDNpAxmsPmH5FAsgxoG-y1DGMUkhOSM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "M4Raw",
    "paperLink": "https://www.nature.com/articles/s41597-023-02181-4",
    "description": "A multi-contrast, multi-repetition, multi-channel MRI k-space dataset specifically for low-field (0.3T) MRI research and reconstruction benchmarking.",
    "authors": [
      "Mengye Lyu",
      "Lifeng Mei",
      "Shoujin Huang",
      "Sixing Liu",
      "Yi Li",
      "et al."
    ],
    "githubLink": "https://github.com/mylyu/M4Raw",
    "itemCount": "183 subjects (1024 training volumes)",
    "source": "Scholar",
    "specs": "0.3T Low-field Brain MRI, T1, T2, FLAIR, Raw k-space",
    "year": "2023",
    "id": "saved-1769658082027-j2yj7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFdeR01gQIPDWIIohVOjJbW-57rWSy5mf_8jV6-CS3_YpvRJ8yQqxDiZbs4ljv8WMz3fCQTqjmuLATN1Td4Jz6leJRF5Cb88NjFJtdvCoAqOHftR_Jpa6TSxXWu2lIn6iOen2PNpTviJc80_IbY7qBO4xgo",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "OCMR",
    "paperLink": "https://arxiv.org/abs/2008.03410",
    "description": "Open-Access Multi-Coil k-Space Dataset for Cardiovascular Magnetic Resonance Imaging. Provides fully sampled and prospectively undersampled cardiac cine series.",
    "authors": [
      "Chong Chen",
      "Yingmin Liu",
      "Philip Schniter",
      "Matthew Tong",
      "Kedar Ghodasara",
      "et al."
    ],
    "githubLink": "https://github.com/MRIOSU/OCMR",
    "itemCount": "53 fully sampled, 212 undersampled cine series",
    "source": "arXiv",
    "specs": "Cardiac Cine MRI, Multi-coil k-space",
    "year": "2020",
    "id": "saved-1769658082027-viize",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFdeR01gQIPDWIIohVOjJbW-57rWSy5mf_8jV6-CS3_YpvRJ8yQqxDiZbs4ljv8WMz3fCQTqjmuLATN1Td4Jz6leJRF5Cb88NjFJtdvCoAqOHftR_Jpa6TSxXWu2lIn6iOen2PNpTviJc80_IbY7qBO4xgo",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Calgary-Campinas (CC) Public Brain MR Dataset",
    "paperLink": "https://sites.google.com/view/calgary-campinas-dataset/home",
    "description": "A multi-vendor, multi-field strength brain MRI dataset providing raw k-space data and reconstructed images for reconstruction and segmentation benchmarking.",
    "authors": [
      "Roberto Souza",
      "Oeslle Lucena",
      "Julia Garrafa",
      "David Gobbi",
      "Marina Saluzzi",
      "Simone Appenzeller",
      "Leticia Rittner",
      "Richard Frayne",
      "Roberto Lotufo"
    ],
    "githubLink": "https://github.com/rmsouza01/MC-MRRec-challenge",
    "itemCount": "359 reconstructed volumes, ~150 raw k-space volumes",
    "source": "Scholar",
    "specs": "Raw k-space (12/32 channel), 3D T1-weighted Brain MRI",
    "year": "2018",
    "id": "saved-1769658082027-22o7m",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFdeR01gQIPDWIIohVOjJbW-57rWSy5mf_8jV6-CS3_YpvRJ8yQqxDiZbs4ljv8WMz3fCQTqjmuLATN1Td4Jz6leJRF5Cb88NjFJtdvCoAqOHftR_Jpa6TSxXWu2lIn6iOen2PNpTviJc80_IbY7qBO4xgo",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Stanford 2D FSE",
    "paperLink": "http://mridata.org/",
    "description": "Fully sampled raw k-space data of knee MRI scans, commonly used for benchmarking compressed sensing and deep learning reconstruction methods. Hosted on mridata.org.",
    "authors": [
      "Joseph Y. Cheng",
      "et al."
    ],
    "githubLink": "https://github.com/MRSRL/mridata-recon",
    "itemCount": "89 volumes",
    "source": "Scholar",
    "specs": "Raw k-space, 2D Fast Spin Echo, Knee",
    "year": "2018",
    "id": "saved-1769658082027-gr02c",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFdeR01gQIPDWIIohVOjJbW-57rWSy5mf_8jV6-CS3_YpvRJ8yQqxDiZbs4ljv8WMz3fCQTqjmuLATN1Td4Jz6leJRF5Cb88NjFJtdvCoAqOHftR_Jpa6TSxXWu2lIn6iOen2PNpTviJc80_IbY7qBO4xgo",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "MIMIC-CXR",
    "paperLink": "https://doi.org/10.1038/s41597-019-0322-0",
    "description": "A large publicly available dataset of chest radiographs with free-text radiology reports. It contains images associated with patients admitted to the Beth Israel Deaconess Medical Center.",
    "authors": [
      "Alistair E. W. Johnson",
      "Tom J. Pollard",
      "Seth J. Berkowitz",
      "Nathaniel R. Greenbaum",
      "Matthew P. Lungren",
      "Chih-Ying Deng",
      "Roger G. Mark",
      "Steven Horng"
    ],
    "githubLink": "https://github.com/MIT-LCP/mimic-code",
    "itemCount": "377,110 images, 227,835 radiographic studies",
    "source": "Scholar",
    "specs": "Chest X-ray (DICOM/JPG), Text Radiology Reports",
    "year": "2019",
    "id": "saved-1769658145559-ojkti",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEPhEAZeXIBaD5Xrf_aLS69ahpLEDV7ytuICb9OtcDBa7i0228j14sAE3QdMNMk0QGltVsBdFfIobex8ne3QsUyeb2tuLP4zOoz3S9stYLN-BLBpunF_szZ2LRe",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYhaeRyBpn3UqqRlWSRe9pl5Pe-RWat8187eLtxLlXQapHUDbQqbRiWO4YfJDHuijebXddnViVVMo7VqtF28UP8mgVM873Re8iAU9cMUrvwESrBo9VznYa-A4xO3g=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaDwhC8bdUhAjnk3Aw8CzSS_DHk0FlsAinGkbDnA12qwV2tcfgtpUtWosvOzXXCXaUiWg6ZCgBIGZS0P0odxIg0Usz--XsJ9iLQVjFS7-ztoNTTLBCkizhn4Q3FXvMvgvLPg4=",
        "title": "physionet.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp9zsBQxrz1DudIvS85D9448Nh_xg7i40v0VgUFQ2xl-qL2Psoy-T4JLRaIGGqzWO1G5rIJWgAX6IQ84RnrvC6CrhE0PUbKj6uwzH5fQorm2CUG4EiR4WBvTAeofFuWThShqfJ9bVP62LqBKgdDDcqlDH9NP-m4PZHpZ1e0a2xwLZ9tmwjxg==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGKI9Ugap4dfbbonei4aV00cvsKvvmUYmPu4LDUHfyDxMRKlvDI7Ya-F6WpGSSybjKF1R5s94vM11UPf7pSbk-nwcHI4NzYQw1tQUdVyB8zQzBjYWjg4nqQmO78lvd8r1rx75Mi9EbDmhJC3YusQ==",
        "title": "upenn.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFZ8o0JT-lyN-nmpr7rXLkzoxQzoO2WoXfnG_ws0y2E_8VibR2ShDfcCfY5s0xAA5GtTYKIiaD_HjBLIml6siOt-DGorxGJASnFk6VGatZs_nl_iuZz7pJJf48f4QCtCrLYb9SxO2kGqLvYX4sNwn3KvcP477qKxnr7LvTqnDC2FMl0FNkiCU-OjQg4HEfhy4UmGp19FHZD",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeWatQT1yG6gQELqPtHmUGA8HUHel2H2CG63uextziDxGJwn0GcAcnt6Shjzdhxth7_RzRx-xyLtfRWwl_GnzgSyCZUUjTkg73xtzLVFigCpV2KbbVh8A7JOUUEQgKFAfUtm_jtCizp7I8UPE2TAw7P4L4Wg67",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5qsRUl-UkX0diBNyxnXjPP64IUwq3qjoOxe-zQ7QrK9IEXkfOyqngEYx0v1rFu9afDWatE0RGQSJaUE5I7aUI-Nzhcalqxct4J-mPMcz6_EWZxGIfJHff_Q==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEbsJ8kzPtaVDhmyWvZ01XNTKUxpJtWDLuikp5knFFTAJFpr-REbkpCqGULM5cCu4pqLlEJBlVgOd1tyvlrmiSNrDlwgR21Y9hjYmAWIUBkihSGCCwpE0l2WLAbT8Fv_135E4vEe8btDBmYcOrDQDM9rTecCSIC2hOBSQoFaOHGxQVJm7OddBbBZ4azLNrgAIP_l8qF",
        "title": "medicalbrief.co.za"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVN1g9FMOI02uFti7tfCYZKM2_kylcZ5ApJkT_qZlWS8vLL4UdFy9tgpr9C6vTfo13b0mbqxXm8K8CihwUL_if2WU_iRxGajmoIIzH5ofy8zvpD24l5raYK0z_cNXyXydGTN-hi7nqJvTgMUA=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "RSNA Pneumonia Detection Challenge",
    "paperLink": "https://doi.org/10.1148/ryai.2019180041",
    "description": "A dataset and challenge for detecting lung opacities (pneumonia) in chest radiographs. It provides bounding box annotations for positive cases.",
    "authors": [
      "George Shih",
      "Carol C. Wu",
      "Safwan S. Halabi",
      "Marc D. Kohli",
      "Luciano M. Prevedello",
      "et al."
    ],
    "githubLink": "https://github.com/filipradenovic/cnnimageretrieval-pytorch",
    "itemCount": "26,684 chest radiographs",
    "source": "Kaggle",
    "specs": "Chest X-ray images, Bounding box annotations",
    "year": "2019",
    "id": "saved-1769658145559-aj1ts",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEPhEAZeXIBaD5Xrf_aLS69ahpLEDV7ytuICb9OtcDBa7i0228j14sAE3QdMNMk0QGltVsBdFfIobex8ne3QsUyeb2tuLP4zOoz3S9stYLN-BLBpunF_szZ2LRe",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYhaeRyBpn3UqqRlWSRe9pl5Pe-RWat8187eLtxLlXQapHUDbQqbRiWO4YfJDHuijebXddnViVVMo7VqtF28UP8mgVM873Re8iAU9cMUrvwESrBo9VznYa-A4xO3g=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaDwhC8bdUhAjnk3Aw8CzSS_DHk0FlsAinGkbDnA12qwV2tcfgtpUtWosvOzXXCXaUiWg6ZCgBIGZS0P0odxIg0Usz--XsJ9iLQVjFS7-ztoNTTLBCkizhn4Q3FXvMvgvLPg4=",
        "title": "physionet.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp9zsBQxrz1DudIvS85D9448Nh_xg7i40v0VgUFQ2xl-qL2Psoy-T4JLRaIGGqzWO1G5rIJWgAX6IQ84RnrvC6CrhE0PUbKj6uwzH5fQorm2CUG4EiR4WBvTAeofFuWThShqfJ9bVP62LqBKgdDDcqlDH9NP-m4PZHpZ1e0a2xwLZ9tmwjxg==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGKI9Ugap4dfbbonei4aV00cvsKvvmUYmPu4LDUHfyDxMRKlvDI7Ya-F6WpGSSybjKF1R5s94vM11UPf7pSbk-nwcHI4NzYQw1tQUdVyB8zQzBjYWjg4nqQmO78lvd8r1rx75Mi9EbDmhJC3YusQ==",
        "title": "upenn.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFZ8o0JT-lyN-nmpr7rXLkzoxQzoO2WoXfnG_ws0y2E_8VibR2ShDfcCfY5s0xAA5GtTYKIiaD_HjBLIml6siOt-DGorxGJASnFk6VGatZs_nl_iuZz7pJJf48f4QCtCrLYb9SxO2kGqLvYX4sNwn3KvcP477qKxnr7LvTqnDC2FMl0FNkiCU-OjQg4HEfhy4UmGp19FHZD",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeWatQT1yG6gQELqPtHmUGA8HUHel2H2CG63uextziDxGJwn0GcAcnt6Shjzdhxth7_RzRx-xyLtfRWwl_GnzgSyCZUUjTkg73xtzLVFigCpV2KbbVh8A7JOUUEQgKFAfUtm_jtCizp7I8UPE2TAw7P4L4Wg67",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5qsRUl-UkX0diBNyxnXjPP64IUwq3qjoOxe-zQ7QrK9IEXkfOyqngEYx0v1rFu9afDWatE0RGQSJaUE5I7aUI-Nzhcalqxct4J-mPMcz6_EWZxGIfJHff_Q==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEbsJ8kzPtaVDhmyWvZ01XNTKUxpJtWDLuikp5knFFTAJFpr-REbkpCqGULM5cCu4pqLlEJBlVgOd1tyvlrmiSNrDlwgR21Y9hjYmAWIUBkihSGCCwpE0l2WLAbT8Fv_135E4vEe8btDBmYcOrDQDM9rTecCSIC2hOBSQoFaOHGxQVJm7OddBbBZ4azLNrgAIP_l8qF",
        "title": "medicalbrief.co.za"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVN1g9FMOI02uFti7tfCYZKM2_kylcZ5ApJkT_qZlWS8vLL4UdFy9tgpr9C6vTfo13b0mbqxXm8K8CihwUL_if2WU_iRxGajmoIIzH5ofy8zvpD24l5raYK0z_cNXyXydGTN-hi7nqJvTgMUA=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "DeepLesion",
    "paperLink": "https://doi.org/10.1117/1.JMI.5.3.036501",
    "description": "A large-scale dataset mined from PACS systems to support universal lesion detection. It contains diverse lesion types (lung nodules, liver tumors, etc.) with bounding box annotations.",
    "authors": [
      "Ke Yan",
      "Xiaosong Wang",
      "Le Lu",
      "Ronald M. Summers"
    ],
    "githubLink": "https://github.com/rsummers11/CADLab",
    "itemCount": "32,735 lesions in 32,120 CT slices",
    "source": "Scholar",
    "specs": "CT images, Bounding boxes, RECIST measurements",
    "year": "2018",
    "id": "saved-1769658145559-qh276",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEPhEAZeXIBaD5Xrf_aLS69ahpLEDV7ytuICb9OtcDBa7i0228j14sAE3QdMNMk0QGltVsBdFfIobex8ne3QsUyeb2tuLP4zOoz3S9stYLN-BLBpunF_szZ2LRe",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYhaeRyBpn3UqqRlWSRe9pl5Pe-RWat8187eLtxLlXQapHUDbQqbRiWO4YfJDHuijebXddnViVVMo7VqtF28UP8mgVM873Re8iAU9cMUrvwESrBo9VznYa-A4xO3g=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaDwhC8bdUhAjnk3Aw8CzSS_DHk0FlsAinGkbDnA12qwV2tcfgtpUtWosvOzXXCXaUiWg6ZCgBIGZS0P0odxIg0Usz--XsJ9iLQVjFS7-ztoNTTLBCkizhn4Q3FXvMvgvLPg4=",
        "title": "physionet.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp9zsBQxrz1DudIvS85D9448Nh_xg7i40v0VgUFQ2xl-qL2Psoy-T4JLRaIGGqzWO1G5rIJWgAX6IQ84RnrvC6CrhE0PUbKj6uwzH5fQorm2CUG4EiR4WBvTAeofFuWThShqfJ9bVP62LqBKgdDDcqlDH9NP-m4PZHpZ1e0a2xwLZ9tmwjxg==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGKI9Ugap4dfbbonei4aV00cvsKvvmUYmPu4LDUHfyDxMRKlvDI7Ya-F6WpGSSybjKF1R5s94vM11UPf7pSbk-nwcHI4NzYQw1tQUdVyB8zQzBjYWjg4nqQmO78lvd8r1rx75Mi9EbDmhJC3YusQ==",
        "title": "upenn.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFZ8o0JT-lyN-nmpr7rXLkzoxQzoO2WoXfnG_ws0y2E_8VibR2ShDfcCfY5s0xAA5GtTYKIiaD_HjBLIml6siOt-DGorxGJASnFk6VGatZs_nl_iuZz7pJJf48f4QCtCrLYb9SxO2kGqLvYX4sNwn3KvcP477qKxnr7LvTqnDC2FMl0FNkiCU-OjQg4HEfhy4UmGp19FHZD",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeWatQT1yG6gQELqPtHmUGA8HUHel2H2CG63uextziDxGJwn0GcAcnt6Shjzdhxth7_RzRx-xyLtfRWwl_GnzgSyCZUUjTkg73xtzLVFigCpV2KbbVh8A7JOUUEQgKFAfUtm_jtCizp7I8UPE2TAw7P4L4Wg67",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5qsRUl-UkX0diBNyxnXjPP64IUwq3qjoOxe-zQ7QrK9IEXkfOyqngEYx0v1rFu9afDWatE0RGQSJaUE5I7aUI-Nzhcalqxct4J-mPMcz6_EWZxGIfJHff_Q==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEbsJ8kzPtaVDhmyWvZ01XNTKUxpJtWDLuikp5knFFTAJFpr-REbkpCqGULM5cCu4pqLlEJBlVgOd1tyvlrmiSNrDlwgR21Y9hjYmAWIUBkihSGCCwpE0l2WLAbT8Fv_135E4vEe8btDBmYcOrDQDM9rTecCSIC2hOBSQoFaOHGxQVJm7OddBbBZ4azLNrgAIP_l8qF",
        "title": "medicalbrief.co.za"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVN1g9FMOI02uFti7tfCYZKM2_kylcZ5ApJkT_qZlWS8vLL4UdFy9tgpr9C6vTfo13b0mbqxXm8K8CihwUL_if2WU_iRxGajmoIIzH5ofy8zvpD24l5raYK0z_cNXyXydGTN-hi7nqJvTgMUA=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "ACDC (Automated Cardiac Diagnosis Challenge)",
    "paperLink": "https://doi.org/10.1109/TMI.2018.2837502",
    "description": "A benchmark for cardiac image analysis, specifically for the segmentation of the left ventricle, right ventricle, and myocardium from Cine-MRI, and for the classification of cardiac pathologies.",
    "authors": [
      "Olivier Bernard",
      "Alain Lalande",
      "Clement Zotti",
      "Frederick Cervenansky",
      "et al."
    ],
    "githubLink": "https://github.com/vitalab/ACDC",
    "itemCount": "150 exams (100 training, 50 testing)",
    "source": "Scholar",
    "specs": "4D Cine-MRI, Segmentation masks (LV, RV, Myo)",
    "year": "2018",
    "id": "saved-1769658145559-08m9z",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEPhEAZeXIBaD5Xrf_aLS69ahpLEDV7ytuICb9OtcDBa7i0228j14sAE3QdMNMk0QGltVsBdFfIobex8ne3QsUyeb2tuLP4zOoz3S9stYLN-BLBpunF_szZ2LRe",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYhaeRyBpn3UqqRlWSRe9pl5Pe-RWat8187eLtxLlXQapHUDbQqbRiWO4YfJDHuijebXddnViVVMo7VqtF28UP8mgVM873Re8iAU9cMUrvwESrBo9VznYa-A4xO3g=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaDwhC8bdUhAjnk3Aw8CzSS_DHk0FlsAinGkbDnA12qwV2tcfgtpUtWosvOzXXCXaUiWg6ZCgBIGZS0P0odxIg0Usz--XsJ9iLQVjFS7-ztoNTTLBCkizhn4Q3FXvMvgvLPg4=",
        "title": "physionet.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp9zsBQxrz1DudIvS85D9448Nh_xg7i40v0VgUFQ2xl-qL2Psoy-T4JLRaIGGqzWO1G5rIJWgAX6IQ84RnrvC6CrhE0PUbKj6uwzH5fQorm2CUG4EiR4WBvTAeofFuWThShqfJ9bVP62LqBKgdDDcqlDH9NP-m4PZHpZ1e0a2xwLZ9tmwjxg==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGKI9Ugap4dfbbonei4aV00cvsKvvmUYmPu4LDUHfyDxMRKlvDI7Ya-F6WpGSSybjKF1R5s94vM11UPf7pSbk-nwcHI4NzYQw1tQUdVyB8zQzBjYWjg4nqQmO78lvd8r1rx75Mi9EbDmhJC3YusQ==",
        "title": "upenn.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFZ8o0JT-lyN-nmpr7rXLkzoxQzoO2WoXfnG_ws0y2E_8VibR2ShDfcCfY5s0xAA5GtTYKIiaD_HjBLIml6siOt-DGorxGJASnFk6VGatZs_nl_iuZz7pJJf48f4QCtCrLYb9SxO2kGqLvYX4sNwn3KvcP477qKxnr7LvTqnDC2FMl0FNkiCU-OjQg4HEfhy4UmGp19FHZD",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeWatQT1yG6gQELqPtHmUGA8HUHel2H2CG63uextziDxGJwn0GcAcnt6Shjzdhxth7_RzRx-xyLtfRWwl_GnzgSyCZUUjTkg73xtzLVFigCpV2KbbVh8A7JOUUEQgKFAfUtm_jtCizp7I8UPE2TAw7P4L4Wg67",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5qsRUl-UkX0diBNyxnXjPP64IUwq3qjoOxe-zQ7QrK9IEXkfOyqngEYx0v1rFu9afDWatE0RGQSJaUE5I7aUI-Nzhcalqxct4J-mPMcz6_EWZxGIfJHff_Q==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEbsJ8kzPtaVDhmyWvZ01XNTKUxpJtWDLuikp5knFFTAJFpr-REbkpCqGULM5cCu4pqLlEJBlVgOd1tyvlrmiSNrDlwgR21Y9hjYmAWIUBkihSGCCwpE0l2WLAbT8Fv_135E4vEe8btDBmYcOrDQDM9rTecCSIC2hOBSQoFaOHGxQVJm7OddBbBZ4azLNrgAIP_l8qF",
        "title": "medicalbrief.co.za"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVN1g9FMOI02uFti7tfCYZKM2_kylcZ5ApJkT_qZlWS8vLL4UdFy9tgpr9C6vTfo13b0mbqxXm8K8CihwUL_if2WU_iRxGajmoIIzH5ofy8zvpD24l5raYK0z_cNXyXydGTN-hi7nqJvTgMUA=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "BraTS (Multimodal Brain Tumor Segmentation Benchmark)",
    "paperLink": "https://doi.org/10.1109/TMI.2014.2377694",
    "description": "A longstanding benchmark for the segmentation of brain tumors (gliomas) in multimodal magnetic resonance imaging (MRI) scans. It focuses on identifying tumor sub-regions like necrotic core, edema, and enhancing tumor.",
    "authors": [
      "Bjoern H. Menze",
      "Andras Jakab",
      "Stefan Bauer",
      "Jayashree Kalpathy-Cramer",
      "Keyvan Farahani",
      "Justin Kirby",
      "Spyridon Bakas",
      "et al."
    ],
    "githubLink": "https://github.com/RAVIRAJAG/BraTS-2020-Dataset",
    "itemCount": "Varies by year (e.g., ~2000+ cases in BraTS 2021)",
    "source": "Scholar",
    "specs": "Multimodal MRI (T1, T1Gd, T2, FLAIR); 3D Segmentation masks",
    "year": "2015",
    "id": "saved-1769658145559-dyj5c",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEPhEAZeXIBaD5Xrf_aLS69ahpLEDV7ytuICb9OtcDBa7i0228j14sAE3QdMNMk0QGltVsBdFfIobex8ne3QsUyeb2tuLP4zOoz3S9stYLN-BLBpunF_szZ2LRe",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGYhaeRyBpn3UqqRlWSRe9pl5Pe-RWat8187eLtxLlXQapHUDbQqbRiWO4YfJDHuijebXddnViVVMo7VqtF28UP8mgVM873Re8iAU9cMUrvwESrBo9VznYa-A4xO3g=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaDwhC8bdUhAjnk3Aw8CzSS_DHk0FlsAinGkbDnA12qwV2tcfgtpUtWosvOzXXCXaUiWg6ZCgBIGZS0P0odxIg0Usz--XsJ9iLQVjFS7-ztoNTTLBCkizhn4Q3FXvMvgvLPg4=",
        "title": "physionet.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHp9zsBQxrz1DudIvS85D9448Nh_xg7i40v0VgUFQ2xl-qL2Psoy-T4JLRaIGGqzWO1G5rIJWgAX6IQ84RnrvC6CrhE0PUbKj6uwzH5fQorm2CUG4EiR4WBvTAeofFuWThShqfJ9bVP62LqBKgdDDcqlDH9NP-m4PZHpZ1e0a2xwLZ9tmwjxg==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGKI9Ugap4dfbbonei4aV00cvsKvvmUYmPu4LDUHfyDxMRKlvDI7Ya-F6WpGSSybjKF1R5s94vM11UPf7pSbk-nwcHI4NzYQw1tQUdVyB8zQzBjYWjg4nqQmO78lvd8r1rx75Mi9EbDmhJC3YusQ==",
        "title": "upenn.edu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFZ8o0JT-lyN-nmpr7rXLkzoxQzoO2WoXfnG_ws0y2E_8VibR2ShDfcCfY5s0xAA5GtTYKIiaD_HjBLIml6siOt-DGorxGJASnFk6VGatZs_nl_iuZz7pJJf48f4QCtCrLYb9SxO2kGqLvYX4sNwn3KvcP477qKxnr7LvTqnDC2FMl0FNkiCU-OjQg4HEfhy4UmGp19FHZD",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeWatQT1yG6gQELqPtHmUGA8HUHel2H2CG63uextziDxGJwn0GcAcnt6Shjzdhxth7_RzRx-xyLtfRWwl_GnzgSyCZUUjTkg73xtzLVFigCpV2KbbVh8A7JOUUEQgKFAfUtm_jtCizp7I8UPE2TAw7P4L4Wg67",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5qsRUl-UkX0diBNyxnXjPP64IUwq3qjoOxe-zQ7QrK9IEXkfOyqngEYx0v1rFu9afDWatE0RGQSJaUE5I7aUI-Nzhcalqxct4J-mPMcz6_EWZxGIfJHff_Q==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEbsJ8kzPtaVDhmyWvZ01XNTKUxpJtWDLuikp5knFFTAJFpr-REbkpCqGULM5cCu4pqLlEJBlVgOd1tyvlrmiSNrDlwgR21Y9hjYmAWIUBkihSGCCwpE0l2WLAbT8Fv_135E4vEe8btDBmYcOrDQDM9rTecCSIC2hOBSQoFaOHGxQVJm7OddBbBZ4azLNrgAIP_l8qF",
        "title": "medicalbrief.co.za"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHVN1g9FMOI02uFti7tfCYZKM2_kylcZ5ApJkT_qZlWS8vLL4UdFy9tgpr9C6vTfo13b0mbqxXm8K8CihwUL_if2WU_iRxGajmoIIzH5ofy8zvpD24l5raYK0z_cNXyXydGTN-hi7nqJvTgMUA=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "ROCO (Radiology Objects in COntext) v2",
    "paperLink": "https://arxiv.org/abs/2405.10004",
    "description": "A large-scale multimodal medical imaging dataset consisting of radiology images and caption pairs extracted from PMC Open Access articles. Designed for image captioning, retrieval, and multimodal learning.",
    "authors": [
      "Obioma Pelka",
      "Bjoern H. Menze",
      "Sven E. Rexhausen"
    ],
    "githubLink": "https://github.com/eltorio/ROCOv2-radiology",
    "itemCount": "79,789 images",
    "source": "arXiv",
    "specs": "Radiology images, Captions, UMLS Concepts, Modality labels",
    "year": "2024",
    "id": "saved-1769658218238-qn4py",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHO7I0daSKA5hBRN7K0HcawvtWGFGNNBH0EasQL6SILJniIt_xwVXbA7-3nR3eMO4JfRF1rFnGfMR3OyDxtV_ofrRiMC9bcfeUP7u8tWsPDK9XO96_x1pRXbkP3oiET",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGUXrE1dO4jdiWq_LitLOUpRdu_H54rsfR8p52wxs4A3nkV66OyKBlWIKvuJZvkcvTI9Lk2hJzSP4uvnITsAygapH8rA1DM_mqnkI4MkfcqFVwlnzQdCXpGhmF_pd_qUzoAPfI8Mn6S-g==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF-AaQXHWNZad9EIoxLrAmXG2sTxSVlvp0aPr9GxrnyhjHYEKH3l0dNphEXXDc84zSQ6DrYGao4Rd6Ja_0KBkcJ_0cpyyh4-9iWxqvnajsZA5NYn27Hdkb_c67j",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZLnqWjQQxvb7SAme5IDquw_0iGhfbwbdorij3fpNo9Z6vH0JDamE-g_6KY4aptuh7-7st84zQz713l3uej8GoUETRMZ5aw7rlXz0AjfluVElerE-CtLVFiCM3Lmjs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFG8VYIh6_rNRTTY68XpFsJUUL_3FcgeSNAko45MfoA2vY7jHHSNN6bZjGJP4C_WXmr5uHYj7SIA4JYRvZgy8Dr6sUl97e1vh98RNie0bCgVODv_F7ipvU816vDNXaiu0gRY89axQOOZa0vmrNRska12JA4DE64vB7gvaAdSGamZPpth0U3dOVll2_1wWSTm0kwJW44ny0niE2OfmYbbxaQeVh4NFByOW8MTgI9kRsImDWoMaKW3OYgg-fHIcLFBkt_V3KLkpPvI9LZLlA=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGzxKf9Dll8Ua-vrgER6b-1zXJTBZNyJSMUAhEAReU6MIplcHtAZl_Mqmn3LsPatY35o5bsPlaHOL9lkQFkKOYG_9AQMZGhde7ZNAEeBQMjKbsQApiAw8ZbLgUus8NRaRF2gkXw7abI4Kf9ZvF-67zxjYKBJAisqhb1KFuo2wXdMszY_-kUgS25U5skQsdHTg==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuskn9W3X0K8G6VzjHk2UmB5POgREFaWbvZrV9WEql2hBAJpRBpKLeTfbbAb9-dD0PCZ-R-zFrSLL_z1lovBtb1_QyS4I13nMfY-OTwEToAdgmxSQGurkFt1HwYfFsQthwgXiuDqEheGOa6AHfjDsHppuHaqgchc4WQVtiXKL-euuFEkFMqDm_twcGXigdmA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEe76nROMs6Jt2pLiDk-5YjoqetPw8qHb989T_3UJwc9RWBbNBybieJ-crgjZ8_yKA1r_Kwg8hv4Grxu1S3qRbsezq35fbwpAYA2kYow_bK2ui1otgtsRveX84TnZDem7C-4GZhijujHdG5ditMLD2gLgjBVeTdYjhPmQlGuSw5JlzZvQDEzoak97wZrdfJYpfSOTKMVkaGFAChqSG51Lbahyft-8HZ5PBxdNyXXervc2DmsIe3_mJMUjme0pPmIlOJxGG6yFRj2MHwmrKSglMXYTHmcxCq6XSd",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFl3UpT77rhAZAJJ5gK0ZJpclgTXvtIq8YLuYXrwms4eCT1_p6CNXB2XAQP-IA9ERRruX-QjeszFBN7sZ_3fj8O7wxelLsW6nIoo_u7ftDaH7eBA6ooshAt7CduOgACvYvcR5V-LYo7OHnTggYGHTKhZPx2z7KprsxgZ8vtNKQMzIXdKl_FpTIHXcES-WDK5dwOIDMf5rG5YS7JalqQX0pczLuG1P0GmcN50V3Edj03ems=",
        "title": "themoonlight.io"
      }
    ]
  },
  {
    "title": "OmniMedVQA",
    "paperLink": "https://arxiv.org/abs/2402.09181",
    "description": "A comprehensive medical Visual Question Answering benchmark collected from 73 different medical datasets, covering 12 modalities and over 20 anatomical regions. Designed to evaluate Large Vision-Language Models (LVLMs).",
    "authors": [
      "Yutao Hu",
      "Tianbin Li",
      "Quanfeng Lu",
      "Wenqi Shao",
      "Junjun He",
      "Yu Qiao",
      "Ping Luo"
    ],
    "githubLink": "https://github.com/OpenGVLab/Multi-Modality-Arena",
    "itemCount": "118,010 images, 127,995 QA pairs",
    "source": "arXiv",
    "specs": "12 Modalities (CT, MRI, X-ray, etc.), QA pairs",
    "year": "2024",
    "id": "saved-1769658218238-83f7d",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHO7I0daSKA5hBRN7K0HcawvtWGFGNNBH0EasQL6SILJniIt_xwVXbA7-3nR3eMO4JfRF1rFnGfMR3OyDxtV_ofrRiMC9bcfeUP7u8tWsPDK9XO96_x1pRXbkP3oiET",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGUXrE1dO4jdiWq_LitLOUpRdu_H54rsfR8p52wxs4A3nkV66OyKBlWIKvuJZvkcvTI9Lk2hJzSP4uvnITsAygapH8rA1DM_mqnkI4MkfcqFVwlnzQdCXpGhmF_pd_qUzoAPfI8Mn6S-g==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF-AaQXHWNZad9EIoxLrAmXG2sTxSVlvp0aPr9GxrnyhjHYEKH3l0dNphEXXDc84zSQ6DrYGao4Rd6Ja_0KBkcJ_0cpyyh4-9iWxqvnajsZA5NYn27Hdkb_c67j",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZLnqWjQQxvb7SAme5IDquw_0iGhfbwbdorij3fpNo9Z6vH0JDamE-g_6KY4aptuh7-7st84zQz713l3uej8GoUETRMZ5aw7rlXz0AjfluVElerE-CtLVFiCM3Lmjs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFG8VYIh6_rNRTTY68XpFsJUUL_3FcgeSNAko45MfoA2vY7jHHSNN6bZjGJP4C_WXmr5uHYj7SIA4JYRvZgy8Dr6sUl97e1vh98RNie0bCgVODv_F7ipvU816vDNXaiu0gRY89axQOOZa0vmrNRska12JA4DE64vB7gvaAdSGamZPpth0U3dOVll2_1wWSTm0kwJW44ny0niE2OfmYbbxaQeVh4NFByOW8MTgI9kRsImDWoMaKW3OYgg-fHIcLFBkt_V3KLkpPvI9LZLlA=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGzxKf9Dll8Ua-vrgER6b-1zXJTBZNyJSMUAhEAReU6MIplcHtAZl_Mqmn3LsPatY35o5bsPlaHOL9lkQFkKOYG_9AQMZGhde7ZNAEeBQMjKbsQApiAw8ZbLgUus8NRaRF2gkXw7abI4Kf9ZvF-67zxjYKBJAisqhb1KFuo2wXdMszY_-kUgS25U5skQsdHTg==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuskn9W3X0K8G6VzjHk2UmB5POgREFaWbvZrV9WEql2hBAJpRBpKLeTfbbAb9-dD0PCZ-R-zFrSLL_z1lovBtb1_QyS4I13nMfY-OTwEToAdgmxSQGurkFt1HwYfFsQthwgXiuDqEheGOa6AHfjDsHppuHaqgchc4WQVtiXKL-euuFEkFMqDm_twcGXigdmA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEe76nROMs6Jt2pLiDk-5YjoqetPw8qHb989T_3UJwc9RWBbNBybieJ-crgjZ8_yKA1r_Kwg8hv4Grxu1S3qRbsezq35fbwpAYA2kYow_bK2ui1otgtsRveX84TnZDem7C-4GZhijujHdG5ditMLD2gLgjBVeTdYjhPmQlGuSw5JlzZvQDEzoak97wZrdfJYpfSOTKMVkaGFAChqSG51Lbahyft-8HZ5PBxdNyXXervc2DmsIe3_mJMUjme0pPmIlOJxGG6yFRj2MHwmrKSglMXYTHmcxCq6XSd",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFl3UpT77rhAZAJJ5gK0ZJpclgTXvtIq8YLuYXrwms4eCT1_p6CNXB2XAQP-IA9ERRruX-QjeszFBN7sZ_3fj8O7wxelLsW6nIoo_u7ftDaH7eBA6ooshAt7CduOgACvYvcR5V-LYo7OHnTggYGHTKhZPx2z7KprsxgZ8vtNKQMzIXdKl_FpTIHXcES-WDK5dwOIDMf5rG5YS7JalqQX0pczLuG1P0GmcN50V3Edj03ems=",
        "title": "themoonlight.io"
      }
    ]
  },
  {
    "title": "VQA-RAD",
    "paperLink": "https://doi.org/10.1038/sdata.2018.251",
    "description": "A dataset of clinically generated visual questions and answers about radiology images. The questions are manually created by clinicians to ensure medical relevance and accuracy.",
    "authors": [
      "Jason J. Lau",
      "Soumya Gayen",
      "Asma Ben Abacha",
      "Dina Demner-Fushman"
    ],
    "githubLink": "https://huggingface.co/datasets/flaviagiammarino/vqa-rad",
    "itemCount": "315 images, 3,515 QA pairs",
    "source": "Scholar",
    "specs": "Radiology images (CT, MRI, X-ray), QA pairs (Open-ended, Close-ended)",
    "year": "2018",
    "id": "saved-1769658218239-zu6ze",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHO7I0daSKA5hBRN7K0HcawvtWGFGNNBH0EasQL6SILJniIt_xwVXbA7-3nR3eMO4JfRF1rFnGfMR3OyDxtV_ofrRiMC9bcfeUP7u8tWsPDK9XO96_x1pRXbkP3oiET",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGUXrE1dO4jdiWq_LitLOUpRdu_H54rsfR8p52wxs4A3nkV66OyKBlWIKvuJZvkcvTI9Lk2hJzSP4uvnITsAygapH8rA1DM_mqnkI4MkfcqFVwlnzQdCXpGhmF_pd_qUzoAPfI8Mn6S-g==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF-AaQXHWNZad9EIoxLrAmXG2sTxSVlvp0aPr9GxrnyhjHYEKH3l0dNphEXXDc84zSQ6DrYGao4Rd6Ja_0KBkcJ_0cpyyh4-9iWxqvnajsZA5NYn27Hdkb_c67j",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGZLnqWjQQxvb7SAme5IDquw_0iGhfbwbdorij3fpNo9Z6vH0JDamE-g_6KY4aptuh7-7st84zQz713l3uej8GoUETRMZ5aw7rlXz0AjfluVElerE-CtLVFiCM3Lmjs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFG8VYIh6_rNRTTY68XpFsJUUL_3FcgeSNAko45MfoA2vY7jHHSNN6bZjGJP4C_WXmr5uHYj7SIA4JYRvZgy8Dr6sUl97e1vh98RNie0bCgVODv_F7ipvU816vDNXaiu0gRY89axQOOZa0vmrNRska12JA4DE64vB7gvaAdSGamZPpth0U3dOVll2_1wWSTm0kwJW44ny0niE2OfmYbbxaQeVh4NFByOW8MTgI9kRsImDWoMaKW3OYgg-fHIcLFBkt_V3KLkpPvI9LZLlA=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGzxKf9Dll8Ua-vrgER6b-1zXJTBZNyJSMUAhEAReU6MIplcHtAZl_Mqmn3LsPatY35o5bsPlaHOL9lkQFkKOYG_9AQMZGhde7ZNAEeBQMjKbsQApiAw8ZbLgUus8NRaRF2gkXw7abI4Kf9ZvF-67zxjYKBJAisqhb1KFuo2wXdMszY_-kUgS25U5skQsdHTg==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFuskn9W3X0K8G6VzjHk2UmB5POgREFaWbvZrV9WEql2hBAJpRBpKLeTfbbAb9-dD0PCZ-R-zFrSLL_z1lovBtb1_QyS4I13nMfY-OTwEToAdgmxSQGurkFt1HwYfFsQthwgXiuDqEheGOa6AHfjDsHppuHaqgchc4WQVtiXKL-euuFEkFMqDm_twcGXigdmA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEe76nROMs6Jt2pLiDk-5YjoqetPw8qHb989T_3UJwc9RWBbNBybieJ-crgjZ8_yKA1r_Kwg8hv4Grxu1S3qRbsezq35fbwpAYA2kYow_bK2ui1otgtsRveX84TnZDem7C-4GZhijujHdG5ditMLD2gLgjBVeTdYjhPmQlGuSw5JlzZvQDEzoak97wZrdfJYpfSOTKMVkaGFAChqSG51Lbahyft-8HZ5PBxdNyXXervc2DmsIe3_mJMUjme0pPmIlOJxGG6yFRj2MHwmrKSglMXYTHmcxCq6XSd",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFl3UpT77rhAZAJJ5gK0ZJpclgTXvtIq8YLuYXrwms4eCT1_p6CNXB2XAQP-IA9ERRruX-QjeszFBN7sZ_3fj8O7wxelLsW6nIoo_u7ftDaH7eBA6ooshAt7CduOgACvYvcR5V-LYo7OHnTggYGHTKhZPx2z7KprsxgZ8vtNKQMzIXdKl_FpTIHXcES-WDK5dwOIDMf5rG5YS7JalqQX0pczLuG1P0GmcN50V3Edj03ems=",
        "title": "themoonlight.io"
      }
    ]
  },
  {
    "title": "DUDE",
    "paperLink": "https://arxiv.org/abs/2305.08455",
    "description": "Document Understanding Dataset and Evaluation. A large-scale, multi-page, multi-domain DocVQA benchmark designed to simulate real-world document understanding scenarios.",
    "authors": [
      "Jordy Van Landeghem",
      "Rubén Tito",
      "Łukasz Borchmann",
      "Michał Pietruszka",
      "Paweł Józiak",
      "Rafał Powalski",
      "Dawid Jurkiewicz",
      "Mickaël Coustaty",
      "Bertrand Ackaert",
      "Ernest Valveny",
      "Matthew Blaschko",
      "Sien Moens",
      "Tomasz Stanisławek"
    ],
    "githubLink": "https://github.com/Jordy-VL/DUDEeval",
    "itemCount": "Large-scale multi-page documents",
    "source": "arXiv",
    "specs": "Multi-page documents, QA pairs, diverse industry domains",
    "year": "2023",
    "id": "saved-1769658288397-5m4a4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "DocLayNet",
    "paperLink": "https://arxiv.org/abs/2206.01062",
    "description": "A large human-annotated dataset for document layout analysis covering diverse document categories like financial reports, scientific articles, and patents.",
    "authors": [
      "Birgit Pfitzmann",
      "Christoph Auer",
      "Michele Dolfi",
      "Ahmed S. Nassar",
      "Peter W. J. Staar"
    ],
    "githubLink": "https://github.com/DS4SD/DocLayNet",
    "itemCount": "80,863 pages",
    "source": "arXiv",
    "specs": "6 document categories, 11 distinct layout labels, COCO annotation format",
    "year": "2022",
    "id": "saved-1769658288397-z7p6a",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "PubTables-1M",
    "paperLink": "https://arxiv.org/abs/2110.00061",
    "description": "A comprehensive dataset for table extraction from unstructured documents, including tasks for table detection, structure recognition, and functional analysis.",
    "authors": [
      "Brandon Smock",
      "Rohith Pesala",
      "Robin Abraham"
    ],
    "githubLink": "https://github.com/microsoft/table-transformer",
    "itemCount": "947,642 fully annotated tables",
    "source": "arXiv",
    "specs": "Table images, Bounding boxes, Structure annotations, XML/PASCAL VOC format",
    "year": "2021",
    "id": "saved-1769658288397-31ymo",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "DocVQA",
    "paperLink": "https://arxiv.org/abs/2007.00398",
    "description": "A dataset for Visual Question Answering (VQA) on document images, containing questions defined on a wide variety of document types.",
    "authors": [
      "Minesh Mathew",
      "Dimosthenis Karatzas",
      "R. Manmatha",
      "C. V. Jawahar"
    ],
    "githubLink": "https://github.com/anisha2102/docvqa",
    "itemCount": "50,000 questions; 12,000+ document images",
    "source": "arXiv",
    "specs": "Document images, Question-Answer pairs",
    "year": "2020",
    "id": "saved-1769658288397-ijan3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "Kleister",
    "paperLink": "https://arxiv.org/abs/2003.02356",
    "description": "A benchmark for Key Information Extraction (KIE) from long documents with complex layouts, specifically focusing on NDA and Charity reports.",
    "authors": [
      "Filip Graliński",
      "Tomasz Stanisławek",
      "Anna Wróblewska",
      "Dawid Lipiński",
      "Agnieszka Kaliska",
      "Paulina Rosalska",
      "Bartosz Topolski",
      "Przemysław Biecek"
    ],
    "githubLink": "https://github.com/applicaai/kleister-nda",
    "itemCount": "3,229 pages (NDA); 61,643 pages (Charity)",
    "source": "arXiv",
    "specs": "PDF documents, Entity-level annotations",
    "year": "2020",
    "id": "saved-1769658288397-i8gvl",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "FUNSD",
    "paperLink": "https://arxiv.org/abs/1905.13538",
    "description": "Form Understanding in Noisy Scanned Documents. A dataset for form understanding tasks like entity labeling and linking in noisy scanned documents.",
    "authors": [
      "Guillaume Jaume",
      "Hazım Kemal Ekenel",
      "Jean-Philippe Thiran"
    ],
    "githubLink": "https://github.com/GuillaumeJaume/FUNSD",
    "itemCount": "199 fully annotated forms",
    "source": "arXiv",
    "specs": "Scanned form images, JSON annotations (bounding boxes, labels, links)",
    "year": "2019",
    "id": "saved-1769658288397-0wh0o",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "CORD",
    "paperLink": "https://openreview.net/forum?id=SJl3z659UH",
    "description": "Consolidated Receipt Dataset. A dataset for post-OCR parsing and information extraction from receipt images.",
    "authors": [
      "Seunghyun Park",
      "Seung Shin",
      "Bado Lee",
      "Junyeop Lee",
      "Jaeheung Surh",
      "Minjoon Seo",
      "Hwalsuk Lee"
    ],
    "githubLink": "https://github.com/clovaai/cord",
    "itemCount": "~11,000 receipts",
    "source": "Hugging Face",
    "specs": "Receipt images, Multi-level semantic labels, JSON format",
    "year": "2019",
    "id": "saved-1769658288397-af7n0",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "TabFact",
    "paperLink": "https://arxiv.org/abs/1909.02164",
    "description": "A large-scale dataset for table-based fact verification, requiring models to verify textual statements based on semi-structured table evidence.",
    "authors": [
      "Wenhu Chen",
      "Hongmin Wang",
      "Jianshu Chen",
      "Yunkai Zhang",
      "Hong Wang",
      "Shiyang Li",
      "Xiyou Zhou",
      "William Yang Wang"
    ],
    "githubLink": "https://github.com/wenhuchen/Table-Fact-Checking",
    "itemCount": "117,854 statements; 16,573 tables",
    "source": "arXiv",
    "specs": "Wikipedia tables, Textual statements, Binary labels (Entailed/Refuted)",
    "year": "2019",
    "id": "saved-1769658288397-1sihy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "RVL-CDIP",
    "paperLink": "https://www.cs.cmu.edu/~aharley/icdar15/",
    "description": "A dataset for document classification and retrieval, consisting of scanned grayscale images from the tobacco industry documents.",
    "authors": [
      "Adam W. Harley",
      "Alex Ufkes",
      "Konstantinos G. Derpanis"
    ],
    "githubLink": "https://huggingface.co/datasets/rvl_cdip",
    "itemCount": "400,000 images",
    "source": "Hugging Face",
    "specs": "16 document categories (e.g., letter, memo, email), Grayscale images",
    "year": "2015",
    "id": "saved-1769658288397-iqj5x",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6UkxOsCPa65B1fIvI-iwsTrYq6ISQlLxqwVqI08NomLnJNvP5hmCxnwTU8Ku1ieQLWGbQI8Zp3ybIKLNA1OkAsjnEajPDrdBXkepEeMneHHwNb6JA0hxoyoiFT9dmYQ5CZvc=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeHtZz97_nxKIje-Iz33wVcZe5GaXXUssGCa4NJMgM1N-_2RtAbjlKoPbY_U3NCQREHV-ji5Wn00j8LugEciZX2iFFQumdvnJkwbPgaUhiAWe9GydOVy7Lbxl9iOttAQ==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpmYUE4NvmxOT6olJMKC0ZzmvmMXYEWB5L1g9AD0pimpJnm-V_4zedIsKVIOKaAiJsugkZPrkRQo5fvXCuxiO7vgp6Vj-XV-HjKtae4EIjvfcCEUOegsR_TtomZk9VtY-4pZ_6MQ9PtYJtYqs8jtbAKYKG9EJ1sxQw9i2f",
        "title": "icosys.ch"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHP4lqkSBwehmDEBmT425wyq4quVCUI-Ua3trVM6FKIRe4ZsVJJbLbna3wr5KL3cjeGJSOFgRLfX59K_Eot3591e-Swdn39Zy7jLXz-aVbxzPac5Q2R0cXO77lGhe0SZsaZrog12slcYPGjsVDNwRtW0cC7iXPODBUyfm-RMfjtgnXUNdWKYe6Dv9DMpvC2gDM3McK9o0aSHt3wLpr7foaKTTzYXjEjBw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8QVhIgrV8oC5ysCUFfmgx8jMjrXzgXOSuEGpPqaEYVH7W9R2skBpXf0z7B13DUTRS9jmsPvo1cpsi9-g_ogslylwWywJl5Mw50zwT82D3esxSlnppd_j0xDrv",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwWlHyiNFtyTjIa9m2pSuZN17nVYI3hYtmEoBGyfvzI8fzHmpfJsd4dzcljUuD_CUiAQtuKsVQBWl0GDaEo0jhxGYULma9B9qVaBebAY8nnaYir7KjM4rEzB-Xqoz-rM0zq3sYqhIy",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEpYex-YMqMLvBd-8cy5UG9OsLo_dIVJ-xqkZvvOcfKfN3RlI1xlKscZRzb3BO2LotCFOXyx6Xc_1ck5bt9Kvc8wyaqvIVC6xNV5_a9_WFJFjsbcooDczxqfSJ3",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEYxZAkyimipWvoc2ngIuHWJ1hwh-OCpnRTy_D9MTY8jBvqVeUdsgWzmqIGHtB1KiKX3-gNvUwjyEkGOEA2ujWDI83fNeUsPgSpfV9p1BDkljisdLRPOuoXUOfpQT_vE1k4Yz4nXjRu2tGLaUCyxX6A9PkLqdX6cfQFL1M5FSG4vvJotG_xG3_fFqHlqPoonqtIBkbxxAnAi8I5JuRzGm5nKgZKg7nmN_eGNRrDKpMr8ffqngzyrhxPNhO2EujJXc-AjzS1Z-gkoSAWU7JPgN6VWYtVqkvaBxrYKIs=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8M2cnUe2p0AyoiUhx_Nev3Lu1yGeRkYQ58dEI8WKU7TGWMXvf6m_-CIOzbyhzXlEf_htesyPdJrQY9IUOGv1zQ8zwth-xuNpdZSFj2RMWjsTdMGIp933ve9Dd8vZfoyvd2x6SKQ2aASI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "AutoPET Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2409.10120",
    "description": "A large-scale dataset for automated lesion segmentation in whole-body FDG-PET/CT and PSMA-PET/CT. It focuses on oncological cases such as melanoma, lymphoma, and lung cancer, providing a benchmark for metabolic tumor volume segmentation.",
    "authors": [
      "Sergios Gatidis",
      "Tobias Hepp",
      "Marcel Früh",
      "et al."
    ],
    "githubLink": "https://github.com/lab-midas/autoPET",
    "itemCount": "1,014 PET/CT studies",
    "source": "arXiv",
    "specs": "PET/CT; NIfTI format; 3D whole-body scans",
    "year": "2022",
    "id": "saved-1769658361021-8w8f1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3IenVuWn1SZXt41lFLgX4-6NptW6wfgG_L7uXFFtMZ3OSznX5ZrluFZtLYocjJCEBY9ObIeRMZrx41kNUL9pk67XJgu-RIcf-4IiqSgBztVccBcU_fA6xoUX4Mu_HgflhjxOATH44o-abY5HQky4WejS2cuLf",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "HECKTOR (Head and Neck Tumor Segmentation)",
    "paperLink": "https://arxiv.org/abs/2208.13504",
    "description": "Focused on the segmentation of head and neck primary tumors (GTVp) and metastatic lymph nodes (GTVn) in PET/CT images. The challenge also includes tasks for outcome prediction (recurrence-free survival).",
    "authors": [
      "Vincent Oreiller",
      "Valentin Andrearczyk",
      "Joel Castelli",
      "et al."
    ],
    "githubLink": "https://github.com/voreille/hecktor",
    "itemCount": "883 cases (HECKTOR 2022)",
    "source": "Medical Image Analysis",
    "specs": "PET/CT; NIfTI format; 3D volumes",
    "year": "2022",
    "id": "saved-1769658361021-adz3u",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3IenVuWn1SZXt41lFLgX4-6NptW6wfgG_L7uXFFtMZ3OSznX5ZrluFZtLYocjJCEBY9ObIeRMZrx41kNUL9pk67XJgu-RIcf-4IiqSgBztVccBcU_fA6xoUX4Mu_HgflhjxOATH44o-abY5HQky4WejS2cuLf",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Breast Ultrasound Images (BUSI) Dataset",
    "paperLink": "https://pubmed.ncbi.nlm.nih.gov/31819912/",
    "description": "A collection of breast ultrasound images classified into normal, benign, and malignant categories. It provides ground truth masks for tumor segmentation, widely used for training deep learning models in breast cancer detection.",
    "authors": [
      "Walid Al-Dhabyani",
      "Mohammed Gomaa",
      "Khaled Khaled",
      "et al."
    ],
    "githubLink": "https://github.com/Hiba-Gh/Breast-Ultrasound-Images-Dataset-BUSI-",
    "itemCount": "780 images (600 patients)",
    "source": "Data in Brief",
    "specs": "Ultrasound; PNG format; 2D images",
    "year": "2020",
    "id": "saved-1769658361021-10p94",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3IenVuWn1SZXt41lFLgX4-6NptW6wfgG_L7uXFFtMZ3OSznX5ZrluFZtLYocjJCEBY9ObIeRMZrx41kNUL9pk67XJgu-RIcf-4IiqSgBztVccBcU_fA6xoUX4Mu_HgflhjxOATH44o-abY5HQky4WejS2cuLf",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Kidney Tumor Segmentation (KiTS) Challenge",
    "paperLink": "https://arxiv.org/abs/1904.00446",
    "description": "A benchmark dataset for the segmentation of kidneys and kidney tumors in contrast-enhanced abdominal CT scans. The KiTS23 version expands on the original KiTS19 by adding more cases and refining annotations.",
    "authors": [
      "Nicholas Heller",
      "Niranjan Sathianathen",
      "Arveen Kalapara",
      "et al."
    ],
    "githubLink": "https://github.com/neheller/kits23",
    "itemCount": "599 CT scans (KiTS23)",
    "source": "arXiv",
    "specs": "CT; NIfTI format; 3D volumes",
    "year": "2019",
    "id": "saved-1769658361021-wvllt",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3IenVuWn1SZXt41lFLgX4-6NptW6wfgG_L7uXFFtMZ3OSznX5ZrluFZtLYocjJCEBY9ObIeRMZrx41kNUL9pk67XJgu-RIcf-4IiqSgBztVccBcU_fA6xoUX4Mu_HgflhjxOATH44o-abY5HQky4WejS2cuLf",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Medical Segmentation Decathlon (MSD)",
    "paperLink": "https://arxiv.org/abs/1902.09063",
    "description": "A comprehensive benchmark comprising 10 different segmentation tasks to test the generalizability of algorithms. Tumor-related tasks include Brain (glioma), Liver (tumor), Lung (cancer), Pancreas (tumor), and Colon (cancer).",
    "authors": [
      "Michela Antonelli",
      "Annika Reinke",
      "Spyridon Bakas",
      "et al."
    ],
    "githubLink": "https://github.com/neheller/kits19",
    "itemCount": "2,633 3D volumes (Total across 10 tasks)",
    "source": "arXiv",
    "specs": "CT and MRI; NIfTI format; 3D volumes",
    "year": "2019",
    "id": "saved-1769658361021-m28e1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3IenVuWn1SZXt41lFLgX4-6NptW6wfgG_L7uXFFtMZ3OSznX5ZrluFZtLYocjJCEBY9ObIeRMZrx41kNUL9pk67XJgu-RIcf-4IiqSgBztVccBcU_fA6xoUX4Mu_HgflhjxOATH44o-abY5HQky4WejS2cuLf",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Brain Tumor Segmentation (BraTS) Challenge",
    "paperLink": "https://arxiv.org/abs/1410.2101",
    "description": "A longstanding benchmark for the segmentation of brain tumors (gliomas) in multimodal MRI scans. It provides expert annotations for tumor sub-regions: edema, non-enhancing solid tumor, and necrotic/cystic core. Recent iterations (2023) have expanded to include brain metastases and African populations.",
    "authors": [
      "Bjoern H. Menze",
      "Andras Jakab",
      "Stefan Bauer",
      "et al."
    ],
    "githubLink": "https://github.com/RAVIRAJAG/BraTS-2020-Tumor-Segmentation",
    "itemCount": "~2,000 subjects (BraTS 2023)",
    "source": "arXiv",
    "specs": "Multi-modal MRI (T1, T1ce, T2, FLAIR); NIfTI format; 3D volumes",
    "year": "2015",
    "id": "saved-1769658361021-1tesw",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3IenVuWn1SZXt41lFLgX4-6NptW6wfgG_L7uXFFtMZ3OSznX5ZrluFZtLYocjJCEBY9ObIeRMZrx41kNUL9pk67XJgu-RIcf-4IiqSgBztVccBcU_fA6xoUX4Mu_HgflhjxOATH44o-abY5HQky4WejS2cuLf",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "LIDC-IDRI (Lung Image Database Consortium)",
    "paperLink": "https://pubmed.ncbi.nlm.nih.gov/21761661/",
    "description": "A major public resource for lung cancer research, containing thoracic CT scans with marked lung nodules. The LUNA16 challenge is a popular subset derived from this dataset specifically for nodule detection and segmentation.",
    "authors": [
      "Samuel G. Armato III",
      "Geoffrey McLennan",
      "Luc Bidaut",
      "et al."
    ],
    "githubLink": "https://github.com/jaeho3690/LIDC-IDRI-Segmentation",
    "itemCount": "1,018 cases",
    "source": "Medical Physics",
    "specs": "CT; DICOM/NIfTI; 3D volumes",
    "year": "2011",
    "id": "saved-1769658361021-4igv0",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3IenVuWn1SZXt41lFLgX4-6NptW6wfgG_L7uXFFtMZ3OSznX5ZrluFZtLYocjJCEBY9ObIeRMZrx41kNUL9pk67XJgu-RIcf-4IiqSgBztVccBcU_fA6xoUX4Mu_HgflhjxOATH44o-abY5HQky4WejS2cuLf",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Community Drug Response Prediction Benchmark",
    "paperLink": "https://arxiv.org/abs/2503.14356",
    "description": "A newly proposed benchmarking framework designed to evaluate cross-dataset generalization of drug response prediction models. It integrates five public screening datasets (CCLE, CTRPv2, gCSI, GDSCv1, GDSCv2) into a unified structure.",
    "authors": [
      "Alexander Partin",
      "Priyanka Vasanthakumari",
      "Oleksandr Narykov",
      "Andreas Wilke",
      "Natasha Koussa",
      "Sara E. W. L. O. W. S. W. E. C."
    ],
    "githubLink": "",
    "itemCount": "Aggregation of 5 major screening datasets",
    "source": "arXiv",
    "specs": "Integrated pan-cancer and multi-drug data types",
    "year": "2025",
    "id": "saved-1769658459975-tleoq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgOq7eMlf4Fsiu2Rc_hzc7pVnHDxVoR-XGXOgEJltlIRBquNvPgEVEK78rf8f1Yu8c9_OGa0HlzkVHQpmfvCMsa1wr1IszFhXEJePnu3x1pkyIyYU0byRJ3rcarmCWoxeYez4Z2bKZQiT9J2LjrY448EiH9pGSmdH4rofnSFN9VHDLV_4=",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsX6NkAtKLPDt7rfOOJNt9-jD63fMWEHJek-AVSA6hDe2mv6SIJg2QgpTmMpmw8twqesyy4-0cskYa3E_kW8kvRRCV88NqpHEspT3jDXk05VNfjG2aN4sMhydvxYBsNtoZ",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWMEgmo9FiFoTy4rHh_7aeBA7wI1KlE7y3qP1FuaSyuTfKZZKE41OR07gx6wN_w0fB_hw9gYF5MfAM4rZo5QN456gNflOl5E0J6X_iEnirytqwwuQRaTmKn4dfsHixpuod29pTMCCcP3d3Zw==",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7VJH2U6V3uglYyEd1qMs84enmkSesCSdNyUKtoZutsdmKeQdc_-3RW6Y-Qbqfo5klQUSEIOwO58C-sjyeW9c_3uqtwPEFu8cE0U_XkqFDA3RxqHIoRFmGyD9jZgkJAFMvhRlCR0HzMrouFvwo9gKFjfaeWXX1naVJG48J_Raku9nqUOStZ-61NgrPH0hXIzxa2cWlOhagYVVuEkKcmjZvoYFcYWySYIDPoi1bEaTXzgBDSkimeMl00ONW7nSE87cgHXc5HJxa-9D-mcvGuyPYE3t4c8lSe0WmAIYJysTvpWfTgvoJ3VTns-IFf-iVAZo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRP3dTNKVTKz7Pj0Md9u43YPsRvnh4AdBBOZmK-3BWR5qjjXWdPak-heVpVUHIXZES0BZH7P5mDbgcZuu_tEMYHL_aP3kBvPRMeFMKRzxnKxKMZpvSPFC1Xbc=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqYX6Pn4G7l-MO7FGzJ31KlKT_f8YyjpuhfLVse2zyjmqRJHj9jLjt8six6ST93e6gnNzz687UfRqk6hJUXukBKSGHP8vt_wmpiMsJiZt8t3RMRb7FLczElm-K6GDexrfYhykIQaGMPPMz3xg=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHZsAB7ipf_tL3XRMhef1lbgpsh8yFEJQVJo_vyc64bi_VKn4lRhmbtBQ5OEHLPs6IlamVaYgVU-SeUn-EXpVF3QznLHrB3gyooWjrjB_j0DtF_e82VRNt4HhUmBlgum8qt8if4cbzWj8-nrUE=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVlKidni4aRQ5topQV-pLrbn2Na_hkAyL0eF3R_dsq8PXcz1kXt27NGSEomh59IY0W-OOqOoKJzOhwEwO_d5r1PBqCLpkPjpXKBO0zN0mI-3ty3kUYer2Okpr-7gMJ9ccnpCnUW2czEa9WNdk=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Predict Immunotherapy Response (MEDomicsLab)",
    "paperLink": "https://github.com/MEDomicsLab/predict-immunotherapy-response",
    "description": "A dataset and codebase focused on predicting patient response to immune checkpoint inhibition (ICI) treatment using biomarkers (mutational signatures, multi-omics).",
    "authors": [
      "MEDomicsLab"
    ],
    "githubLink": "https://github.com/MEDomicsLab/predict-immunotherapy-response",
    "itemCount": "Varies (Clinical cohorts)",
    "source": "GitHub",
    "specs": "Code and clinical biomarker data (TMB, gene expression)",
    "year": "2024",
    "id": "saved-1769658459975-wujvz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgOq7eMlf4Fsiu2Rc_hzc7pVnHDxVoR-XGXOgEJltlIRBquNvPgEVEK78rf8f1Yu8c9_OGa0HlzkVHQpmfvCMsa1wr1IszFhXEJePnu3x1pkyIyYU0byRJ3rcarmCWoxeYez4Z2bKZQiT9J2LjrY448EiH9pGSmdH4rofnSFN9VHDLV_4=",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsX6NkAtKLPDt7rfOOJNt9-jD63fMWEHJek-AVSA6hDe2mv6SIJg2QgpTmMpmw8twqesyy4-0cskYa3E_kW8kvRRCV88NqpHEspT3jDXk05VNfjG2aN4sMhydvxYBsNtoZ",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWMEgmo9FiFoTy4rHh_7aeBA7wI1KlE7y3qP1FuaSyuTfKZZKE41OR07gx6wN_w0fB_hw9gYF5MfAM4rZo5QN456gNflOl5E0J6X_iEnirytqwwuQRaTmKn4dfsHixpuod29pTMCCcP3d3Zw==",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7VJH2U6V3uglYyEd1qMs84enmkSesCSdNyUKtoZutsdmKeQdc_-3RW6Y-Qbqfo5klQUSEIOwO58C-sjyeW9c_3uqtwPEFu8cE0U_XkqFDA3RxqHIoRFmGyD9jZgkJAFMvhRlCR0HzMrouFvwo9gKFjfaeWXX1naVJG48J_Raku9nqUOStZ-61NgrPH0hXIzxa2cWlOhagYVVuEkKcmjZvoYFcYWySYIDPoi1bEaTXzgBDSkimeMl00ONW7nSE87cgHXc5HJxa-9D-mcvGuyPYE3t4c8lSe0WmAIYJysTvpWfTgvoJ3VTns-IFf-iVAZo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRP3dTNKVTKz7Pj0Md9u43YPsRvnh4AdBBOZmK-3BWR5qjjXWdPak-heVpVUHIXZES0BZH7P5mDbgcZuu_tEMYHL_aP3kBvPRMeFMKRzxnKxKMZpvSPFC1Xbc=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqYX6Pn4G7l-MO7FGzJ31KlKT_f8YyjpuhfLVse2zyjmqRJHj9jLjt8six6ST93e6gnNzz687UfRqk6hJUXukBKSGHP8vt_wmpiMsJiZt8t3RMRb7FLczElm-K6GDexrfYhykIQaGMPPMz3xg=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHZsAB7ipf_tL3XRMhef1lbgpsh8yFEJQVJo_vyc64bi_VKn4lRhmbtBQ5OEHLPs6IlamVaYgVU-SeUn-EXpVF3QznLHrB3gyooWjrjB_j0DtF_e82VRNt4HhUmBlgum8qt8if4cbzWj8-nrUE=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVlKidni4aRQ5topQV-pLrbn2Na_hkAyL0eF3R_dsq8PXcz1kXt27NGSEomh59IY0W-OOqOoKJzOhwEwO_d5r1PBqCLpkPjpXKBO0zN0mI-3ty3kUYer2Okpr-7gMJ9ccnpCnUW2czEa9WNdk=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "COVID-19 EHR Predictive Modeling Benchmark",
    "paperLink": "https://arxiv.org/abs/2209.08051",
    "description": "A comprehensive benchmark for predicting clinical treatment responses and outcomes (mortality, length-of-stay) in COVID-19 patients using Electronic Health Records (EHR) data from intensive care units.",
    "authors": [
      "Yuanhan Zhu",
      "Jingang Duan",
      "Liu Li",
      "Guochang Wang",
      "Sheng Yu",
      "Xiang Tao"
    ],
    "githubLink": "https://github.com/yhzhu99/covid-ehr-benchmarks",
    "itemCount": "2 real-world EHR datasets (heterogeneous medical features)",
    "source": "arXiv",
    "specs": "Structured EHR data (vital signs, lab tests, treatments, demographics)",
    "year": "2022",
    "id": "saved-1769658459975-l32ow",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgOq7eMlf4Fsiu2Rc_hzc7pVnHDxVoR-XGXOgEJltlIRBquNvPgEVEK78rf8f1Yu8c9_OGa0HlzkVHQpmfvCMsa1wr1IszFhXEJePnu3x1pkyIyYU0byRJ3rcarmCWoxeYez4Z2bKZQiT9J2LjrY448EiH9pGSmdH4rofnSFN9VHDLV_4=",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsX6NkAtKLPDt7rfOOJNt9-jD63fMWEHJek-AVSA6hDe2mv6SIJg2QgpTmMpmw8twqesyy4-0cskYa3E_kW8kvRRCV88NqpHEspT3jDXk05VNfjG2aN4sMhydvxYBsNtoZ",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWMEgmo9FiFoTy4rHh_7aeBA7wI1KlE7y3qP1FuaSyuTfKZZKE41OR07gx6wN_w0fB_hw9gYF5MfAM4rZo5QN456gNflOl5E0J6X_iEnirytqwwuQRaTmKn4dfsHixpuod29pTMCCcP3d3Zw==",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7VJH2U6V3uglYyEd1qMs84enmkSesCSdNyUKtoZutsdmKeQdc_-3RW6Y-Qbqfo5klQUSEIOwO58C-sjyeW9c_3uqtwPEFu8cE0U_XkqFDA3RxqHIoRFmGyD9jZgkJAFMvhRlCR0HzMrouFvwo9gKFjfaeWXX1naVJG48J_Raku9nqUOStZ-61NgrPH0hXIzxa2cWlOhagYVVuEkKcmjZvoYFcYWySYIDPoi1bEaTXzgBDSkimeMl00ONW7nSE87cgHXc5HJxa-9D-mcvGuyPYE3t4c8lSe0WmAIYJysTvpWfTgvoJ3VTns-IFf-iVAZo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRP3dTNKVTKz7Pj0Md9u43YPsRvnh4AdBBOZmK-3BWR5qjjXWdPak-heVpVUHIXZES0BZH7P5mDbgcZuu_tEMYHL_aP3kBvPRMeFMKRzxnKxKMZpvSPFC1Xbc=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqYX6Pn4G7l-MO7FGzJ31KlKT_f8YyjpuhfLVse2zyjmqRJHj9jLjt8six6ST93e6gnNzz687UfRqk6hJUXukBKSGHP8vt_wmpiMsJiZt8t3RMRb7FLczElm-K6GDexrfYhykIQaGMPPMz3xg=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHZsAB7ipf_tL3XRMhef1lbgpsh8yFEJQVJo_vyc64bi_VKn4lRhmbtBQ5OEHLPs6IlamVaYgVU-SeUn-EXpVF3QznLHrB3gyooWjrjB_j0DtF_e82VRNt4HhUmBlgum8qt8if4cbzWj8-nrUE=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVlKidni4aRQ5topQV-pLrbn2Na_hkAyL0eF3R_dsq8PXcz1kXt27NGSEomh59IY0W-OOqOoKJzOhwEwO_d5r1PBqCLpkPjpXKBO0zN0mI-3ty3kUYer2Okpr-7gMJ9ccnpCnUW2czEa9WNdk=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Cancer Therapeutics Response Portal (CTRPv2)",
    "paperLink": "https://aacrjournals.org/cancerdiscovery/article/5/11/1210/65005/Correlating-Chemical-Sensitivity-and-Basal-Gene",
    "description": "A benchmark dataset linking genetic, lineage, and other cellular features of cancer cell lines to the sensitivity of small-molecule probes and drugs. It is widely used to develop and test drug response prediction algorithms.",
    "authors": [
      "Brinton Seashore-Ludlow",
      "Matthew G. Rees",
      "Jaime H. Cheah",
      "Murat Cokol",
      "Edmund V. Price",
      "Matthew E. Coletti",
      "Victor Jones",
      "Nicole E. Bodycombe",
      "Christian K. Soule",
      "Joshua Gould",
      "Bastien Alexander",
      "Amanda Li",
      "Philip Montgomery",
      "Joshua D. Wawer",
      "Gunnar Kuru",
      "B. A. Kotz",
      "C. S. Hon",
      "R. L. Munoz",
      "T. Liefeld",
      "V. Dančík",
      "J. A. Bittker",
      "M. Palmer",
      "J. E. Bradner",
      "A. F. Shamji",
      "P. A. Clemons",
      "S. L. Schreiber"
    ],
    "githubLink": "https://github.com/broadinstitute/ctrp-v2-analysis-scripts",
    "itemCount": "860 cancer cell lines, 481 compounds",
    "source": "Scholar",
    "specs": "Basal gene expression, mutation, copy number, and drug sensitivity (AUC) profiles",
    "year": "2015",
    "id": "saved-1769658459975-0tid1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgOq7eMlf4Fsiu2Rc_hzc7pVnHDxVoR-XGXOgEJltlIRBquNvPgEVEK78rf8f1Yu8c9_OGa0HlzkVHQpmfvCMsa1wr1IszFhXEJePnu3x1pkyIyYU0byRJ3rcarmCWoxeYez4Z2bKZQiT9J2LjrY448EiH9pGSmdH4rofnSFN9VHDLV_4=",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsX6NkAtKLPDt7rfOOJNt9-jD63fMWEHJek-AVSA6hDe2mv6SIJg2QgpTmMpmw8twqesyy4-0cskYa3E_kW8kvRRCV88NqpHEspT3jDXk05VNfjG2aN4sMhydvxYBsNtoZ",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWMEgmo9FiFoTy4rHh_7aeBA7wI1KlE7y3qP1FuaSyuTfKZZKE41OR07gx6wN_w0fB_hw9gYF5MfAM4rZo5QN456gNflOl5E0J6X_iEnirytqwwuQRaTmKn4dfsHixpuod29pTMCCcP3d3Zw==",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7VJH2U6V3uglYyEd1qMs84enmkSesCSdNyUKtoZutsdmKeQdc_-3RW6Y-Qbqfo5klQUSEIOwO58C-sjyeW9c_3uqtwPEFu8cE0U_XkqFDA3RxqHIoRFmGyD9jZgkJAFMvhRlCR0HzMrouFvwo9gKFjfaeWXX1naVJG48J_Raku9nqUOStZ-61NgrPH0hXIzxa2cWlOhagYVVuEkKcmjZvoYFcYWySYIDPoi1bEaTXzgBDSkimeMl00ONW7nSE87cgHXc5HJxa-9D-mcvGuyPYE3t4c8lSe0WmAIYJysTvpWfTgvoJ3VTns-IFf-iVAZo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRP3dTNKVTKz7Pj0Md9u43YPsRvnh4AdBBOZmK-3BWR5qjjXWdPak-heVpVUHIXZES0BZH7P5mDbgcZuu_tEMYHL_aP3kBvPRMeFMKRzxnKxKMZpvSPFC1Xbc=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqYX6Pn4G7l-MO7FGzJ31KlKT_f8YyjpuhfLVse2zyjmqRJHj9jLjt8six6ST93e6gnNzz687UfRqk6hJUXukBKSGHP8vt_wmpiMsJiZt8t3RMRb7FLczElm-K6GDexrfYhykIQaGMPPMz3xg=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHZsAB7ipf_tL3XRMhef1lbgpsh8yFEJQVJo_vyc64bi_VKn4lRhmbtBQ5OEHLPs6IlamVaYgVU-SeUn-EXpVF3QznLHrB3gyooWjrjB_j0DtF_e82VRNt4HhUmBlgum8qt8if4cbzWj8-nrUE=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVlKidni4aRQ5topQV-pLrbn2Na_hkAyL0eF3R_dsq8PXcz1kXt27NGSEomh59IY0W-OOqOoKJzOhwEwO_d5r1PBqCLpkPjpXKBO0zN0mI-3ty3kUYer2Okpr-7gMJ9ccnpCnUW2czEa9WNdk=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Genomics of Drug Sensitivity in Cancer (GDSC)",
    "paperLink": "https://www.nature.com/articles/nature11003",
    "description": "A large-scale public resource for drug sensitivity prediction in cancer cells. It correlates drug sensitivity data (IC50/AUC) with genomic datasets (mutation, copy number, gene expression) to identify molecular markers of drug response. Available in two versions (GDSC1 and GDSC2).",
    "authors": [
      "Wanjuan Yang",
      "Jorge Soares",
      "Patricia Greninger",
      "Elena J. Edelman",
      "Howard Lightfoot",
      "Simon Forbes",
      "Nidhi Bindal",
      "Dave Beare",
      "James A. Smith",
      "I. Richard Thompson",
      "Sridhar Ramaswamy",
      "P. Andrew Futreal",
      "Daniel A. Haber",
      "Michael R. Stratton",
      "Cyril H. Benes",
      "Ultan McDermott",
      "Mathew J. Garnett"
    ],
    "githubLink": "https://github.com/Camion1/GDSC",
    "itemCount": "1000+ cell lines, 250+ drugs (GDSC1: ~177k pairs)",
    "source": "Scholar",
    "specs": "Tabular/Omics data (Gene Expression, Mutation, CNV) and Float response (IC50, AUC)",
    "year": "2012",
    "id": "saved-1769658459975-iirju",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgOq7eMlf4Fsiu2Rc_hzc7pVnHDxVoR-XGXOgEJltlIRBquNvPgEVEK78rf8f1Yu8c9_OGa0HlzkVHQpmfvCMsa1wr1IszFhXEJePnu3x1pkyIyYU0byRJ3rcarmCWoxeYez4Z2bKZQiT9J2LjrY448EiH9pGSmdH4rofnSFN9VHDLV_4=",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsX6NkAtKLPDt7rfOOJNt9-jD63fMWEHJek-AVSA6hDe2mv6SIJg2QgpTmMpmw8twqesyy4-0cskYa3E_kW8kvRRCV88NqpHEspT3jDXk05VNfjG2aN4sMhydvxYBsNtoZ",
        "title": "mdpi.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWMEgmo9FiFoTy4rHh_7aeBA7wI1KlE7y3qP1FuaSyuTfKZZKE41OR07gx6wN_w0fB_hw9gYF5MfAM4rZo5QN456gNflOl5E0J6X_iEnirytqwwuQRaTmKn4dfsHixpuod29pTMCCcP3d3Zw==",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7VJH2U6V3uglYyEd1qMs84enmkSesCSdNyUKtoZutsdmKeQdc_-3RW6Y-Qbqfo5klQUSEIOwO58C-sjyeW9c_3uqtwPEFu8cE0U_XkqFDA3RxqHIoRFmGyD9jZgkJAFMvhRlCR0HzMrouFvwo9gKFjfaeWXX1naVJG48J_Raku9nqUOStZ-61NgrPH0hXIzxa2cWlOhagYVVuEkKcmjZvoYFcYWySYIDPoi1bEaTXzgBDSkimeMl00ONW7nSE87cgHXc5HJxa-9D-mcvGuyPYE3t4c8lSe0WmAIYJysTvpWfTgvoJ3VTns-IFf-iVAZo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRP3dTNKVTKz7Pj0Md9u43YPsRvnh4AdBBOZmK-3BWR5qjjXWdPak-heVpVUHIXZES0BZH7P5mDbgcZuu_tEMYHL_aP3kBvPRMeFMKRzxnKxKMZpvSPFC1Xbc=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqYX6Pn4G7l-MO7FGzJ31KlKT_f8YyjpuhfLVse2zyjmqRJHj9jLjt8six6ST93e6gnNzz687UfRqk6hJUXukBKSGHP8vt_wmpiMsJiZt8t3RMRb7FLczElm-K6GDexrfYhykIQaGMPPMz3xg=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHZsAB7ipf_tL3XRMhef1lbgpsh8yFEJQVJo_vyc64bi_VKn4lRhmbtBQ5OEHLPs6IlamVaYgVU-SeUn-EXpVF3QznLHrB3gyooWjrjB_j0DtF_e82VRNt4HhUmBlgum8qt8if4cbzWj8-nrUE=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVlKidni4aRQ5topQV-pLrbn2Na_hkAyL0eF3R_dsq8PXcz1kXt27NGSEomh59IY0W-OOqOoKJzOhwEwO_d5r1PBqCLpkPjpXKBO0zN0mI-3ty3kUYer2Okpr-7gMJ9ccnpCnUW2czEa9WNdk=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "SurvBoard",
    "paperLink": "https://academic.oup.com/bib/article/doi/10.1093/bib/bbae123/7634842",
    "description": "A standardized benchmarking framework for multi-omics cancer survival models. It enables comparison between single-cancer and pan-cancer models and assesses performance under missing modality scenarios across major cancer dataset collections.",
    "authors": [
      "BoevaLab",
      "Vale-Silva et al."
    ],
    "githubLink": "https://github.com/BoevaLab/survboard",
    "itemCount": "28 datasets",
    "source": "Scholar",
    "specs": "Multi-omics (genomic, transcriptomic) and clinical data from TCGA, TARGET, ICGC, and METABRIC",
    "year": "2024",
    "id": "saved-1769658531391-7rx9b",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVZ8M1cbMdxCQFu_eU3avzVfqXypsvscGBVgsgoXJ8IL1102aPU1NTPyVd7l_Me9L8MrS16t3sSS2U85PWlNKWCWAxE4epEsTB5K0fEF3eKuH1AIV8sz-Onz565hAdl_Pn1qihlZB3Kg3vMosTOm1jYrHZHw==",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5ZuhYS3fYlWeVDaBIE7hifNNXN2TUl5Y_UeDZv0EMKlpD6xgZxSJhxLQK-H6o4vx13SmpqZTpF84SQfhvOKuGpJKYN7yxiQnSp5kfQI6VMKmK4KT2",
        "title": "survboard.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWS2bmc_aLbple0JKPFz758pfEh8OQVOfiHvG_l2euYNgvRcecLx8eL07NGmZzTgRWRiAu5BsJ-Ann8ZEqsgaWgvqWCKnyx85VkZ2ZEeUFdOVr50sQmwAKIH1fd4mkZ4M=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFojBW1SNCf9-O34ijJSRkPJfgXDo7U4i9Bzc8UOD7emmSEC2KCFfyeluPmRD1Le5FGe4yYzfu19I2_1PFensibTFaiDMxh-bCG-EEwH2NphfitAA8qiE91hlOsiKgb",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "SurvSet",
    "paperLink": "https://arxiv.org/abs/2203.03094",
    "description": "A comprehensive open-source repository designed for rapid benchmarking of machine learning algorithms and statistical methods in time-to-event analysis. It contains consistently formatted datasets from diverse domains including biomedicine, economics, and engineering.",
    "authors": [
      "Erik Drysdale"
    ],
    "githubLink": "https://github.com/ErikinBC/SurvSet",
    "itemCount": "76 datasets",
    "source": "arXiv",
    "specs": "Tabular data, varies from high-dimensional (genomics) to low-dimensional (clinical), formatted for Pandas",
    "year": "2022",
    "id": "saved-1769658531392-sikjg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVZ8M1cbMdxCQFu_eU3avzVfqXypsvscGBVgsgoXJ8IL1102aPU1NTPyVd7l_Me9L8MrS16t3sSS2U85PWlNKWCWAxE4epEsTB5K0fEF3eKuH1AIV8sz-Onz565hAdl_Pn1qihlZB3Kg3vMosTOm1jYrHZHw==",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5ZuhYS3fYlWeVDaBIE7hifNNXN2TUl5Y_UeDZv0EMKlpD6xgZxSJhxLQK-H6o4vx13SmpqZTpF84SQfhvOKuGpJKYN7yxiQnSp5kfQI6VMKmK4KT2",
        "title": "survboard.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWS2bmc_aLbple0JKPFz758pfEh8OQVOfiHvG_l2euYNgvRcecLx8eL07NGmZzTgRWRiAu5BsJ-Ann8ZEqsgaWgvqWCKnyx85VkZ2ZEeUFdOVr50sQmwAKIH1fd4mkZ4M=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFojBW1SNCf9-O34ijJSRkPJfgXDo7U4i9Bzc8UOD7emmSEC2KCFfyeluPmRD1Le5FGe4yYzfu19I2_1PFensibTFaiDMxh-bCG-EEwH2NphfitAA8qiE91hlOsiKgb",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "SurvBenchmark",
    "paperLink": "https://academic.oup.com/gigascience/article/10/2/giab001/6126665",
    "description": "A comprehensive benchmarking study evaluating a diverse collection of survival models, including classical statistical methods and state-of-the-art machine learning approaches, on both clinical and omics datasets.",
    "authors": [
      "Herrmann et al."
    ],
    "githubLink": "https://github.com/nliulab/Survival-Benchmark",
    "itemCount": "16 datasets",
    "source": "Scholar",
    "specs": "Clinical and Omics data, includes simulated and real-world datasets",
    "year": "2021",
    "id": "saved-1769658531392-xgwo9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVZ8M1cbMdxCQFu_eU3avzVfqXypsvscGBVgsgoXJ8IL1102aPU1NTPyVd7l_Me9L8MrS16t3sSS2U85PWlNKWCWAxE4epEsTB5K0fEF3eKuH1AIV8sz-Onz565hAdl_Pn1qihlZB3Kg3vMosTOm1jYrHZHw==",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5ZuhYS3fYlWeVDaBIE7hifNNXN2TUl5Y_UeDZv0EMKlpD6xgZxSJhxLQK-H6o4vx13SmpqZTpF84SQfhvOKuGpJKYN7yxiQnSp5kfQI6VMKmK4KT2",
        "title": "survboard.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWS2bmc_aLbple0JKPFz758pfEh8OQVOfiHvG_l2euYNgvRcecLx8eL07NGmZzTgRWRiAu5BsJ-Ann8ZEqsgaWgvqWCKnyx85VkZ2ZEeUFdOVr50sQmwAKIH1fd4mkZ4M=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFojBW1SNCf9-O34ijJSRkPJfgXDo7U4i9Bzc8UOD7emmSEC2KCFfyeluPmRD1Le5FGe4yYzfu19I2_1PFensibTFaiDMxh-bCG-EEwH2NphfitAA8qiE91hlOsiKgb",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "scikit-survival Datasets",
    "paperLink": "https://jmlr.org/papers/v21/20-729.html",
    "description": "A module within the scikit-survival library providing easy access to well-known survival analysis datasets for prototyping and benchmarking, including the Veterans' Lung Cancer and GBSG2 datasets.",
    "authors": [
      "Sebastian Pölsterl"
    ],
    "githubLink": "https://github.com/sebp/scikit-survival",
    "itemCount": "5+ datasets",
    "source": "Scholar",
    "specs": "Tabular clinical data (e.g., Veterans, GBSG2, WHAS500)",
    "year": "2020",
    "id": "saved-1769658531392-87twn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVZ8M1cbMdxCQFu_eU3avzVfqXypsvscGBVgsgoXJ8IL1102aPU1NTPyVd7l_Me9L8MrS16t3sSS2U85PWlNKWCWAxE4epEsTB5K0fEF3eKuH1AIV8sz-Onz565hAdl_Pn1qihlZB3Kg3vMosTOm1jYrHZHw==",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5ZuhYS3fYlWeVDaBIE7hifNNXN2TUl5Y_UeDZv0EMKlpD6xgZxSJhxLQK-H6o4vx13SmpqZTpF84SQfhvOKuGpJKYN7yxiQnSp5kfQI6VMKmK4KT2",
        "title": "survboard.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWS2bmc_aLbple0JKPFz758pfEh8OQVOfiHvG_l2euYNgvRcecLx8eL07NGmZzTgRWRiAu5BsJ-Ann8ZEqsgaWgvqWCKnyx85VkZ2ZEeUFdOVr50sQmwAKIH1fd4mkZ4M=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFojBW1SNCf9-O34ijJSRkPJfgXDo7U4i9Bzc8UOD7emmSEC2KCFfyeluPmRD1Le5FGe4yYzfu19I2_1PFensibTFaiDMxh-bCG-EEwH2NphfitAA8qiE91hlOsiKgb",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "PyCox Datasets",
    "paperLink": "https://arxiv.org/abs/1910.09224",
    "description": "A collection of event-time datasets included in the PyCox library, used for evaluating deep learning survival analysis models. Includes standard benchmarks like METABRIC, SUPPORT, and KKBox.",
    "authors": [
      "Håvard Kvamme",
      "Ørnulf Borgan",
      "Ida Scheel"
    ],
    "githubLink": "https://github.com/havakv/pycox",
    "itemCount": "5+ key datasets",
    "source": "arXiv",
    "specs": "Tabular data, includes large-scale datasets (e.g., KKBox with millions of rows)",
    "year": "2019",
    "id": "saved-1769658531392-vr3m5",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVZ8M1cbMdxCQFu_eU3avzVfqXypsvscGBVgsgoXJ8IL1102aPU1NTPyVd7l_Me9L8MrS16t3sSS2U85PWlNKWCWAxE4epEsTB5K0fEF3eKuH1AIV8sz-Onz565hAdl_Pn1qihlZB3Kg3vMosTOm1jYrHZHw==",
        "title": "oup.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5ZuhYS3fYlWeVDaBIE7hifNNXN2TUl5Y_UeDZv0EMKlpD6xgZxSJhxLQK-H6o4vx13SmpqZTpF84SQfhvOKuGpJKYN7yxiQnSp5kfQI6VMKmK4KT2",
        "title": "survboard.science"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWS2bmc_aLbple0JKPFz758pfEh8OQVOfiHvG_l2euYNgvRcecLx8eL07NGmZzTgRWRiAu5BsJ-Ann8ZEqsgaWgvqWCKnyx85VkZ2ZEeUFdOVr50sQmwAKIH1fd4mkZ4M=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFojBW1SNCf9-O34ijJSRkPJfgXDo7U4i9Bzc8UOD7emmSEC2KCFfyeluPmRD1Le5FGe4yYzfu19I2_1PFensibTFaiDMxh-bCG-EEwH2NphfitAA8qiE91hlOsiKgb",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "MMOral-OPG-Bench",
    "paperLink": "https://huggingface.co/datasets/OralGPT/MMOral-OPG-Bench",
    "description": "A specialized evaluation suite for Vision-Language Models (VLMs) in dentistry. It consists of panoramic X-rays paired with closed-ended and open-ended questions to test AI capabilities in dental diagnosis and reporting.",
    "authors": [
      "OralGPT Team"
    ],
    "githubLink": "https://huggingface.co/datasets/OralGPT/MMOral-OPG-Bench",
    "itemCount": "100 images paired with ~1,000 QA pairs",
    "specs": "Panoramic X-rays; Text Question-Answer pairs (VQA)",
    "year": "2025",
    "id": "saved-1769658613136-j5imj",
    "source": "Other",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE4ZBPVvS7Lzr9NPgG8n8X894DqOPyNIu7TO428UCAkG68AfBE3IsVl6EWrWBWGeXME5T8HSx6mPP_kTx6XiNNpSWhbYl4J4YRzkpTYmwMTinXJeK8TnEgqHVjHQi5nepd5TjTlhV-NAXWz5VSyvPKM6RHHPhJfcTmpL3Os06q_XHsKCShuSGxhZ2K0IzrWFu9kk6wPfhWJgCb97MU0umulyX-UAxIXL19CyuWFsqhBgyU1oLmyNO66hAmnvmDO7BKpxGhMbRSp2_m2mzvf1fvOG7ASXIMMnrtxmHjAG2NWWXQNRyc=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqbCZfhiGGLy0kJGn2bmiyPg_zl0RDdT7KTJU3gNW12yuDIolIIIsGGF04L-UtPguUWuZpjKA2kTskeACrrsHjCovgstyQOcnkae7v5ym5VNv6kpMYBTTZtR0Va4fplPZiINVLBRupnhFHi1kDUTUScvIlLIBMn0MU2SU=",
        "title": "ai-scholar.tech"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJiqlPXP4UgqLz83KBYf7JXRsBcX7X6nTC3Dt44hNfG02No2IEptetUVgcTCBiUeZiZoxu7IcZWoqwK-JSk7qDXkl0s4gs6nVModrnuSwVnD8twstg5aSVZuZVbSxNFCThej8q9b8jsS0c8YdYkkkV4_NsruyY-7wf7RW5Y_Ah1sW3P371NkcfiDzz2WuAixh87UovwL_2j1Nwe6A8mBii-9tmQ5XXJhmwLVQ6-RCAiIsUomzk7mTwvTycDTVLxssP3pa3op8mwfXi3mu_8MRKqwMfiYNclYI9pBmf6fk=",
        "title": "karenpanetta.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLQ97wP6-N6DUJsFNsFjE6QdcHZzvyNS5LU1Pgi00pK3YtNXaCe6v1H1vpWo9xJugWjsoa_SuiioX_P8YGM98kvKXoITLjriLF-3RVYWf_F1urn12la9pGdVCjfOty3GLj98D3I9H8GKXuWsA8bApYNQPzyq_p2RV6QE=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7j5J1O82Po-PgP0u2qzn8ccHoa2H86IwlOWn97EyUdN0KY5KSYOVMkrFDW57MVxI_23dj1jjBGVn44bLttqmoBcZvTrWcd30A3vd4LMT2cEUJRZvOxjh1nLxJqF7TWX73RgQ3aO0w-Uv5yKaoEM7EWLvwZN8JtVAdf_k=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnyFSqMwCLksabf3tgdls42Kq-5DMgghOtQrZy7aTwysQ_QQNcogFAEQJ2J2rvfapfvk3UJ-D3FE677VuEoG0hv4ssjtbtZZMNyJlb3H_8vF3JDGYPrNfho52NX89Odpo_RJsuD_s=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6mbWeGc7Y8T53JYHGBDtqzNSkc79fx9Tyy7cBJirNTZQN1iZFpv-d5XP6Ur3qUSnpmHC59Gv_ESTy0gYf0DCCgiyodFZyiQKwM0TpaAtyL6gRdD6Ot3psZuU=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "DENTEX (Dental Enumeration and Diagnosis on Panoramic X-rays)",
    "paperLink": "https://arxiv.org/abs/2305.19112",
    "description": "A large-scale benchmark for dental enumeration and diagnosis on panoramic X-rays. It includes three hierarchically annotated subsets for quadrant detection, tooth enumeration, and abnormal tooth diagnosis (caries, lesions, etc.). Developed for the MICCAI 2023 challenge.",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Enis Simsar",
      "Anjany Sekuboyina",
      "Bjoern Menze"
    ],
    "githubLink": "https://github.com/ibrahimhamamci/DENTEX",
    "itemCount": "3,269 panoramic X-rays (1,005 fully labeled, 634 partially labeled, 1,571 unlabeled)",
    "specs": "Panoramic X-ray images; Hierarchical labels (Quadrant, Enumeration, Diagnosis); COCO format",
    "year": "2023",
    "id": "saved-1769658613136-ujcla",
    "source": "Other",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE4ZBPVvS7Lzr9NPgG8n8X894DqOPyNIu7TO428UCAkG68AfBE3IsVl6EWrWBWGeXME5T8HSx6mPP_kTx6XiNNpSWhbYl4J4YRzkpTYmwMTinXJeK8TnEgqHVjHQi5nepd5TjTlhV-NAXWz5VSyvPKM6RHHPhJfcTmpL3Os06q_XHsKCShuSGxhZ2K0IzrWFu9kk6wPfhWJgCb97MU0umulyX-UAxIXL19CyuWFsqhBgyU1oLmyNO66hAmnvmDO7BKpxGhMbRSp2_m2mzvf1fvOG7ASXIMMnrtxmHjAG2NWWXQNRyc=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqbCZfhiGGLy0kJGn2bmiyPg_zl0RDdT7KTJU3gNW12yuDIolIIIsGGF04L-UtPguUWuZpjKA2kTskeACrrsHjCovgstyQOcnkae7v5ym5VNv6kpMYBTTZtR0Va4fplPZiINVLBRupnhFHi1kDUTUScvIlLIBMn0MU2SU=",
        "title": "ai-scholar.tech"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJiqlPXP4UgqLz83KBYf7JXRsBcX7X6nTC3Dt44hNfG02No2IEptetUVgcTCBiUeZiZoxu7IcZWoqwK-JSk7qDXkl0s4gs6nVModrnuSwVnD8twstg5aSVZuZVbSxNFCThej8q9b8jsS0c8YdYkkkV4_NsruyY-7wf7RW5Y_Ah1sW3P371NkcfiDzz2WuAixh87UovwL_2j1Nwe6A8mBii-9tmQ5XXJhmwLVQ6-RCAiIsUomzk7mTwvTycDTVLxssP3pa3op8mwfXi3mu_8MRKqwMfiYNclYI9pBmf6fk=",
        "title": "karenpanetta.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLQ97wP6-N6DUJsFNsFjE6QdcHZzvyNS5LU1Pgi00pK3YtNXaCe6v1H1vpWo9xJugWjsoa_SuiioX_P8YGM98kvKXoITLjriLF-3RVYWf_F1urn12la9pGdVCjfOty3GLj98D3I9H8GKXuWsA8bApYNQPzyq_p2RV6QE=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7j5J1O82Po-PgP0u2qzn8ccHoa2H86IwlOWn97EyUdN0KY5KSYOVMkrFDW57MVxI_23dj1jjBGVn44bLttqmoBcZvTrWcd30A3vd4LMT2cEUJRZvOxjh1nLxJqF7TWX73RgQ3aO0w-Uv5yKaoEM7EWLvwZN8JtVAdf_k=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnyFSqMwCLksabf3tgdls42Kq-5DMgghOtQrZy7aTwysQ_QQNcogFAEQJ2J2rvfapfvk3UJ-D3FE677VuEoG0hv4ssjtbtZZMNyJlb3H_8vF3JDGYPrNfho52NX89Odpo_RJsuD_s=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6mbWeGc7Y8T53JYHGBDtqzNSkc79fx9Tyy7cBJirNTZQN1iZFpv-d5XP6Ur3qUSnpmHC59Gv_ESTy0gYf0DCCgiyodFZyiQKwM0TpaAtyL6gRdD6Ot3psZuU=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ToothFairy Dataset",
    "paperLink": "https://arxiv.org/abs/2307.00000",
    "description": "Established for the MICCAI 2023 ToothFairy Challenge, this dataset focuses on the segmentation of the Inferior Alveolar Canal (IAC), a critical structure for avoiding nerve damage during dental surgery. It provides sparse and dense annotations.",
    "authors": [
      "Milos Ciprian",
      "Federico Bolelli",
      "Costantino Grana",
      "et al."
    ],
    "githubLink": "https://toothfairy.grand-challenge.org/",
    "itemCount": "443 CBCT scans (153 with voxel-level annotations)",
    "specs": "3D CBCT scans; Voxel-level annotations (Inferior Alveolar Canal)",
    "year": "2023",
    "id": "saved-1769658613136-hhau3",
    "source": "Other",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE4ZBPVvS7Lzr9NPgG8n8X894DqOPyNIu7TO428UCAkG68AfBE3IsVl6EWrWBWGeXME5T8HSx6mPP_kTx6XiNNpSWhbYl4J4YRzkpTYmwMTinXJeK8TnEgqHVjHQi5nepd5TjTlhV-NAXWz5VSyvPKM6RHHPhJfcTmpL3Os06q_XHsKCShuSGxhZ2K0IzrWFu9kk6wPfhWJgCb97MU0umulyX-UAxIXL19CyuWFsqhBgyU1oLmyNO66hAmnvmDO7BKpxGhMbRSp2_m2mzvf1fvOG7ASXIMMnrtxmHjAG2NWWXQNRyc=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqbCZfhiGGLy0kJGn2bmiyPg_zl0RDdT7KTJU3gNW12yuDIolIIIsGGF04L-UtPguUWuZpjKA2kTskeACrrsHjCovgstyQOcnkae7v5ym5VNv6kpMYBTTZtR0Va4fplPZiINVLBRupnhFHi1kDUTUScvIlLIBMn0MU2SU=",
        "title": "ai-scholar.tech"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJiqlPXP4UgqLz83KBYf7JXRsBcX7X6nTC3Dt44hNfG02No2IEptetUVgcTCBiUeZiZoxu7IcZWoqwK-JSk7qDXkl0s4gs6nVModrnuSwVnD8twstg5aSVZuZVbSxNFCThej8q9b8jsS0c8YdYkkkV4_NsruyY-7wf7RW5Y_Ah1sW3P371NkcfiDzz2WuAixh87UovwL_2j1Nwe6A8mBii-9tmQ5XXJhmwLVQ6-RCAiIsUomzk7mTwvTycDTVLxssP3pa3op8mwfXi3mu_8MRKqwMfiYNclYI9pBmf6fk=",
        "title": "karenpanetta.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLQ97wP6-N6DUJsFNsFjE6QdcHZzvyNS5LU1Pgi00pK3YtNXaCe6v1H1vpWo9xJugWjsoa_SuiioX_P8YGM98kvKXoITLjriLF-3RVYWf_F1urn12la9pGdVCjfOty3GLj98D3I9H8GKXuWsA8bApYNQPzyq_p2RV6QE=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7j5J1O82Po-PgP0u2qzn8ccHoa2H86IwlOWn97EyUdN0KY5KSYOVMkrFDW57MVxI_23dj1jjBGVn44bLttqmoBcZvTrWcd30A3vd4LMT2cEUJRZvOxjh1nLxJqF7TWX73RgQ3aO0w-Uv5yKaoEM7EWLvwZN8JtVAdf_k=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnyFSqMwCLksabf3tgdls42Kq-5DMgghOtQrZy7aTwysQ_QQNcogFAEQJ2J2rvfapfvk3UJ-D3FE677VuEoG0hv4ssjtbtZZMNyJlb3H_8vF3JDGYPrNfho52NX89Odpo_RJsuD_s=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6mbWeGc7Y8T53JYHGBDtqzNSkc79fx9Tyy7cBJirNTZQN1iZFpv-d5XP6Ur3qUSnpmHC59Gv_ESTy0gYf0DCCgiyodFZyiQKwM0TpaAtyL6gRdD6Ot3psZuU=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "ORCHID (Oral Cancer Histology Image Database)",
    "paperLink": "https://www.medrxiv.org/content/10.1101/2023.08.21.23294371v1",
    "description": "A comprehensive histopathology dataset for oral cancer analytics. It includes high-resolution image patches covering Normal, Oral Submucous Fibrosis (OSMF), and Oral Squamous Cell Carcinoma (OSCC) with grade-level sub-classifications.",
    "authors": [
      "Debapriya Banik",
      "Debotosh Bhattacharjee",
      "Mita Nasipuri",
      "et al."
    ],
    "githubLink": "https://github.com/debapriya-banik/ORCHID",
    "itemCount": "300,000+ histology image patches",
    "specs": "Histopathological images (H&E stained); Patch-level classification labels",
    "year": "2023",
    "id": "saved-1769658613136-qmco2",
    "source": "Other",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE4ZBPVvS7Lzr9NPgG8n8X894DqOPyNIu7TO428UCAkG68AfBE3IsVl6EWrWBWGeXME5T8HSx6mPP_kTx6XiNNpSWhbYl4J4YRzkpTYmwMTinXJeK8TnEgqHVjHQi5nepd5TjTlhV-NAXWz5VSyvPKM6RHHPhJfcTmpL3Os06q_XHsKCShuSGxhZ2K0IzrWFu9kk6wPfhWJgCb97MU0umulyX-UAxIXL19CyuWFsqhBgyU1oLmyNO66hAmnvmDO7BKpxGhMbRSp2_m2mzvf1fvOG7ASXIMMnrtxmHjAG2NWWXQNRyc=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqbCZfhiGGLy0kJGn2bmiyPg_zl0RDdT7KTJU3gNW12yuDIolIIIsGGF04L-UtPguUWuZpjKA2kTskeACrrsHjCovgstyQOcnkae7v5ym5VNv6kpMYBTTZtR0Va4fplPZiINVLBRupnhFHi1kDUTUScvIlLIBMn0MU2SU=",
        "title": "ai-scholar.tech"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJiqlPXP4UgqLz83KBYf7JXRsBcX7X6nTC3Dt44hNfG02No2IEptetUVgcTCBiUeZiZoxu7IcZWoqwK-JSk7qDXkl0s4gs6nVModrnuSwVnD8twstg5aSVZuZVbSxNFCThej8q9b8jsS0c8YdYkkkV4_NsruyY-7wf7RW5Y_Ah1sW3P371NkcfiDzz2WuAixh87UovwL_2j1Nwe6A8mBii-9tmQ5XXJhmwLVQ6-RCAiIsUomzk7mTwvTycDTVLxssP3pa3op8mwfXi3mu_8MRKqwMfiYNclYI9pBmf6fk=",
        "title": "karenpanetta.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLQ97wP6-N6DUJsFNsFjE6QdcHZzvyNS5LU1Pgi00pK3YtNXaCe6v1H1vpWo9xJugWjsoa_SuiioX_P8YGM98kvKXoITLjriLF-3RVYWf_F1urn12la9pGdVCjfOty3GLj98D3I9H8GKXuWsA8bApYNQPzyq_p2RV6QE=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7j5J1O82Po-PgP0u2qzn8ccHoa2H86IwlOWn97EyUdN0KY5KSYOVMkrFDW57MVxI_23dj1jjBGVn44bLttqmoBcZvTrWcd30A3vd4LMT2cEUJRZvOxjh1nLxJqF7TWX73RgQ3aO0w-Uv5yKaoEM7EWLvwZN8JtVAdf_k=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnyFSqMwCLksabf3tgdls42Kq-5DMgghOtQrZy7aTwysQ_QQNcogFAEQJ2J2rvfapfvk3UJ-D3FE677VuEoG0hv4ssjtbtZZMNyJlb3H_8vF3JDGYPrNfho52NX89Odpo_RJsuD_s=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6mbWeGc7Y8T53JYHGBDtqzNSkc79fx9Tyy7cBJirNTZQN1iZFpv-d5XP6Ur3qUSnpmHC59Gv_ESTy0gYf0DCCgiyodFZyiQKwM0TpaAtyL6gRdD6Ot3psZuU=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "OdontoAI Open Panoramic Radiographs (O²PR)",
    "paperLink": "https://arxiv.org/abs/2207.10882",
    "description": "A large-scale dataset for tooth segmentation and numbering, evolved from the UFBA-UESC dental images dataset. It utilizes a human-in-the-loop annotation strategy to provide high-quality instance segmentation masks.",
    "authors": [
      "B. Silva",
      "L. Pinheiro",
      "L. Oliveira",
      "M. Pithon"
    ],
    "githubLink": "https://github.com/IvisionLab/OdontoAI-Open-Panoramic-Radiographs",
    "itemCount": "4,000 panoramic radiographs (2,000 publicly released with annotations)",
    "specs": "Panoramic X-rays; Instance segmentation masks; COCO format; FDI numbering",
    "year": "2022",
    "id": "saved-1769658613136-wqo8x",
    "source": "Other",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE4ZBPVvS7Lzr9NPgG8n8X894DqOPyNIu7TO428UCAkG68AfBE3IsVl6EWrWBWGeXME5T8HSx6mPP_kTx6XiNNpSWhbYl4J4YRzkpTYmwMTinXJeK8TnEgqHVjHQi5nepd5TjTlhV-NAXWz5VSyvPKM6RHHPhJfcTmpL3Os06q_XHsKCShuSGxhZ2K0IzrWFu9kk6wPfhWJgCb97MU0umulyX-UAxIXL19CyuWFsqhBgyU1oLmyNO66hAmnvmDO7BKpxGhMbRSp2_m2mzvf1fvOG7ASXIMMnrtxmHjAG2NWWXQNRyc=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqbCZfhiGGLy0kJGn2bmiyPg_zl0RDdT7KTJU3gNW12yuDIolIIIsGGF04L-UtPguUWuZpjKA2kTskeACrrsHjCovgstyQOcnkae7v5ym5VNv6kpMYBTTZtR0Va4fplPZiINVLBRupnhFHi1kDUTUScvIlLIBMn0MU2SU=",
        "title": "ai-scholar.tech"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJiqlPXP4UgqLz83KBYf7JXRsBcX7X6nTC3Dt44hNfG02No2IEptetUVgcTCBiUeZiZoxu7IcZWoqwK-JSk7qDXkl0s4gs6nVModrnuSwVnD8twstg5aSVZuZVbSxNFCThej8q9b8jsS0c8YdYkkkV4_NsruyY-7wf7RW5Y_Ah1sW3P371NkcfiDzz2WuAixh87UovwL_2j1Nwe6A8mBii-9tmQ5XXJhmwLVQ6-RCAiIsUomzk7mTwvTycDTVLxssP3pa3op8mwfXi3mu_8MRKqwMfiYNclYI9pBmf6fk=",
        "title": "karenpanetta.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLQ97wP6-N6DUJsFNsFjE6QdcHZzvyNS5LU1Pgi00pK3YtNXaCe6v1H1vpWo9xJugWjsoa_SuiioX_P8YGM98kvKXoITLjriLF-3RVYWf_F1urn12la9pGdVCjfOty3GLj98D3I9H8GKXuWsA8bApYNQPzyq_p2RV6QE=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7j5J1O82Po-PgP0u2qzn8ccHoa2H86IwlOWn97EyUdN0KY5KSYOVMkrFDW57MVxI_23dj1jjBGVn44bLttqmoBcZvTrWcd30A3vd4LMT2cEUJRZvOxjh1nLxJqF7TWX73RgQ3aO0w-Uv5yKaoEM7EWLvwZN8JtVAdf_k=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnyFSqMwCLksabf3tgdls42Kq-5DMgghOtQrZy7aTwysQ_QQNcogFAEQJ2J2rvfapfvk3UJ-D3FE677VuEoG0hv4ssjtbtZZMNyJlb3H_8vF3JDGYPrNfho52NX89Odpo_RJsuD_s=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6mbWeGc7Y8T53JYHGBDtqzNSkc79fx9Tyy7cBJirNTZQN1iZFpv-d5XP6Ur3qUSnpmHC59Gv_ESTy0gYf0DCCgiyodFZyiQKwM0TpaAtyL6gRdD6Ot3psZuU=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "CTooth / CTooth+",
    "paperLink": "https://arxiv.org/abs/2206.08778",
    "description": "A benchmark specifically for 3D tooth volume segmentation in Cone Beam Computed Tomography (CBCT) images. It addresses the scarcity of 3D dental datasets by providing fully annotated volumes with gold-standard labels.",
    "authors": [
      "Weiwei Cui",
      "Yaqi Wang",
      "Liaoyuan Zeng",
      "Qianni Zhang"
    ],
    "githubLink": "https://github.com/daaz-lab/CTooth",
    "itemCount": "22 fully annotated volumes + 146 unlabeled volumes (CTooth+)",
    "specs": "3D CBCT volumes; Voxel-level segmentation labels",
    "year": "2022",
    "id": "saved-1769658613136-ktrd3",
    "source": "Other",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE4ZBPVvS7Lzr9NPgG8n8X894DqOPyNIu7TO428UCAkG68AfBE3IsVl6EWrWBWGeXME5T8HSx6mPP_kTx6XiNNpSWhbYl4J4YRzkpTYmwMTinXJeK8TnEgqHVjHQi5nepd5TjTlhV-NAXWz5VSyvPKM6RHHPhJfcTmpL3Os06q_XHsKCShuSGxhZ2K0IzrWFu9kk6wPfhWJgCb97MU0umulyX-UAxIXL19CyuWFsqhBgyU1oLmyNO66hAmnvmDO7BKpxGhMbRSp2_m2mzvf1fvOG7ASXIMMnrtxmHjAG2NWWXQNRyc=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqbCZfhiGGLy0kJGn2bmiyPg_zl0RDdT7KTJU3gNW12yuDIolIIIsGGF04L-UtPguUWuZpjKA2kTskeACrrsHjCovgstyQOcnkae7v5ym5VNv6kpMYBTTZtR0Va4fplPZiINVLBRupnhFHi1kDUTUScvIlLIBMn0MU2SU=",
        "title": "ai-scholar.tech"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJiqlPXP4UgqLz83KBYf7JXRsBcX7X6nTC3Dt44hNfG02No2IEptetUVgcTCBiUeZiZoxu7IcZWoqwK-JSk7qDXkl0s4gs6nVModrnuSwVnD8twstg5aSVZuZVbSxNFCThej8q9b8jsS0c8YdYkkkV4_NsruyY-7wf7RW5Y_Ah1sW3P371NkcfiDzz2WuAixh87UovwL_2j1Nwe6A8mBii-9tmQ5XXJhmwLVQ6-RCAiIsUomzk7mTwvTycDTVLxssP3pa3op8mwfXi3mu_8MRKqwMfiYNclYI9pBmf6fk=",
        "title": "karenpanetta.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLQ97wP6-N6DUJsFNsFjE6QdcHZzvyNS5LU1Pgi00pK3YtNXaCe6v1H1vpWo9xJugWjsoa_SuiioX_P8YGM98kvKXoITLjriLF-3RVYWf_F1urn12la9pGdVCjfOty3GLj98D3I9H8GKXuWsA8bApYNQPzyq_p2RV6QE=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7j5J1O82Po-PgP0u2qzn8ccHoa2H86IwlOWn97EyUdN0KY5KSYOVMkrFDW57MVxI_23dj1jjBGVn44bLttqmoBcZvTrWcd30A3vd4LMT2cEUJRZvOxjh1nLxJqF7TWX73RgQ3aO0w-Uv5yKaoEM7EWLvwZN8JtVAdf_k=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnyFSqMwCLksabf3tgdls42Kq-5DMgghOtQrZy7aTwysQ_QQNcogFAEQJ2J2rvfapfvk3UJ-D3FE677VuEoG0hv4ssjtbtZZMNyJlb3H_8vF3JDGYPrNfho52NX89Odpo_RJsuD_s=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6mbWeGc7Y8T53JYHGBDtqzNSkc79fx9Tyy7cBJirNTZQN1iZFpv-d5XP6Ur3qUSnpmHC59Gv_ESTy0gYf0DCCgiyodFZyiQKwM0TpaAtyL6gRdD6Ot3psZuU=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Tufts Dental Database (TDD)",
    "paperLink": "https://ieeexplore.ieee.org/document/9558839",
    "description": "A multimodal dataset designed for benchmarking diagnostic systems. It contains panoramic radiographs with expert annotations for teeth and abnormalities, along with eye-tracking data from radiologists to capture human attention patterns.",
    "authors": [
      "Karen Panetta",
      "Rahul Rajendran",
      "Aruna Ramesh",
      "Shishir Paramathma Rao",
      "Sos Agaian"
    ],
    "githubLink": "https://tdd.ece.tufts.edu/",
    "itemCount": "1,000 panoramic radiographs",
    "specs": "Panoramic X-rays; Expert annotations (masks/labels) + Eye-tracking data",
    "year": "2021",
    "id": "saved-1769658613136-u8wv8",
    "source": "Other",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE4ZBPVvS7Lzr9NPgG8n8X894DqOPyNIu7TO428UCAkG68AfBE3IsVl6EWrWBWGeXME5T8HSx6mPP_kTx6XiNNpSWhbYl4J4YRzkpTYmwMTinXJeK8TnEgqHVjHQi5nepd5TjTlhV-NAXWz5VSyvPKM6RHHPhJfcTmpL3Os06q_XHsKCShuSGxhZ2K0IzrWFu9kk6wPfhWJgCb97MU0umulyX-UAxIXL19CyuWFsqhBgyU1oLmyNO66hAmnvmDO7BKpxGhMbRSp2_m2mzvf1fvOG7ASXIMMnrtxmHjAG2NWWXQNRyc=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqbCZfhiGGLy0kJGn2bmiyPg_zl0RDdT7KTJU3gNW12yuDIolIIIsGGF04L-UtPguUWuZpjKA2kTskeACrrsHjCovgstyQOcnkae7v5ym5VNv6kpMYBTTZtR0Va4fplPZiINVLBRupnhFHi1kDUTUScvIlLIBMn0MU2SU=",
        "title": "ai-scholar.tech"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJiqlPXP4UgqLz83KBYf7JXRsBcX7X6nTC3Dt44hNfG02No2IEptetUVgcTCBiUeZiZoxu7IcZWoqwK-JSk7qDXkl0s4gs6nVModrnuSwVnD8twstg5aSVZuZVbSxNFCThej8q9b8jsS0c8YdYkkkV4_NsruyY-7wf7RW5Y_Ah1sW3P371NkcfiDzz2WuAixh87UovwL_2j1Nwe6A8mBii-9tmQ5XXJhmwLVQ6-RCAiIsUomzk7mTwvTycDTVLxssP3pa3op8mwfXi3mu_8MRKqwMfiYNclYI9pBmf6fk=",
        "title": "karenpanetta.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLQ97wP6-N6DUJsFNsFjE6QdcHZzvyNS5LU1Pgi00pK3YtNXaCe6v1H1vpWo9xJugWjsoa_SuiioX_P8YGM98kvKXoITLjriLF-3RVYWf_F1urn12la9pGdVCjfOty3GLj98D3I9H8GKXuWsA8bApYNQPzyq_p2RV6QE=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7j5J1O82Po-PgP0u2qzn8ccHoa2H86IwlOWn97EyUdN0KY5KSYOVMkrFDW57MVxI_23dj1jjBGVn44bLttqmoBcZvTrWcd30A3vd4LMT2cEUJRZvOxjh1nLxJqF7TWX73RgQ3aO0w-Uv5yKaoEM7EWLvwZN8JtVAdf_k=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnyFSqMwCLksabf3tgdls42Kq-5DMgghOtQrZy7aTwysQ_QQNcogFAEQJ2J2rvfapfvk3UJ-D3FE677VuEoG0hv4ssjtbtZZMNyJlb3H_8vF3JDGYPrNfho52NX89Odpo_RJsuD_s=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6mbWeGc7Y8T53JYHGBDtqzNSkc79fx9Tyy7cBJirNTZQN1iZFpv-d5XP6Ur3qUSnpmHC59Gv_ESTy0gYf0DCCgiyodFZyiQKwM0TpaAtyL6gRdD6Ot3psZuU=",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "TrackRAD2025 Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2503.19119",
    "description": "A multi-institutional dataset designed for developing and evaluating real-time tumor localization (tracking) algorithms for MRI-guided radiotherapy. It includes sagittal 2D cine MRIs from 585 patients across six international centers, featuring tumors in the thorax, abdomen, and pelvis.",
    "authors": [
      "Yiling Wang",
      "Elia Lombardo",
      "Adrian Thummerer",
      "Tom Blöcker",
      "Yu Fan",
      "Yue Zhao",
      "Paul Keall",
      "Guillaume Landry",
      "Matteo Maspero"
    ],
    "githubLink": "https://github.com/LMUK-RADONC-PHYS-RES/trackrad2025",
    "itemCount": "585 patients (over 2.8 million cine MRI frames)",
    "source": "Hugging Face",
    "specs": "2D sagittal cine MRI (0.35 T and 1.5 T), labeled (108 cases) and unlabeled (477 cases), MetaImage (.mha) format",
    "year": "2025",
    "id": "saved-1769658705266-m07zl",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbDtAfqFxpdxc7BaxSKGJkQd6s9R3Miat5Qif8wXAnP-AH8qH7C0U72JOyG8f1gqgkb2orpVooEirsJ8WrY5Rd60Ki5-RrEgJH8HYfZfdDX-ZMyEFKryZG83uM",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDfK_3gDOVhlJuwVLjA9PbMG_W2ApVxbuOgHeG12qEQJIOY_2MZx9U4CLuT95b8waia7pwadHb_1eatSWazGIrzxnXVkO8o2JtZtsxpBc1pSLf17QV20ZDvU4y-W8MynhU4XBNsCnQbCupF43roWEcx-ne-Opt0chDkAJx1Q==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESgHHT8Jtn1ZFzXiQ9L_rfPPANbC9KNLHOMEZ3PevJujy00Fcqg1qTirSoVWIMfIr2bJRBpRYub1OTD-eB6sIpIWvCeCZaRCWFNDcclhi5WMpgvevwYeA4YqSDBC1DZyuvAO-bMqDL1fSoIs3CkP4m0LiEHzW1OsxByfN1CAoRYcOmzL3ICnHBSJton7k94sbVeBbTpd8DleUimwRvenRJ7-M0USEpArbY2cTV9o1OwcS0Ne7OR29G43MZmovE5y_1vw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjaQ4MpjXMxQfxfVv9y3EDga9Pjqh-HbUocFgAsLWLBJpsXkHLSS7bU_XXeF5-S4zEnDKgY2uno18RbYyybdVhx0F2eWSygOm5VVbhe_BKMCF-hJoRDrr188sKUSVLjguuUJCxrA==",
        "title": "sydney.edu.au"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQa7TxJPDeDq_AyMKQwYufEgoTVazf_HOkwhYRxRE8XvSyCIf8iX8W-xZ-aqxozXEHGRvz8OKHTGJQLmRmz3NqOGgetodHJDubERpfLKbVPTRRBR0LuqDGrH4Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFvf5AmjSsdgLFtTG8nNbJ5vu3sp8JIAHib-rCmWTyINsrTMpqy0NNcQy0KWzcpiUkCGDDySgRxfWrYMuRQPeWojPjSP-E_oEcW2PFWAtIGIuuhHCKIKNER5pq8V5I08DMV-Rm3otUlX5eCYxrUAPU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7CCLZVTkx01h03pNsjMcEkb7MHLgcoirY0CAvaBzyanLbhS-89-44PJtxuy_Cnr5IZueEmgLlb-6cQfVL7iuS59qZHi-sCzJP9UgOB0WTP6Cuv9CCLw7LRRlds8v0MP-1SAxwXJZqoWWbEojWQJP-7NfA",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtuv3LLId9NgyYNFkEfd6VlioGwahPZrYyF8qMlI64C8tlMvf8Apz4yXC-bWyx2hWdXyQ9jYpscbdw3Z08BZ4KyQoDYa3mhoYso1dRKUVXhlJ74NBtrdcVSwQJhEV5m8pnlcBDNgwPWNMSGyNNBKY=",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6WqyuFQqdns-KuvE74R4QV69CymoZ6ttQyW2a8jsU37PPBns_bQo2J8gne4kfK25ghFqjc9ikni7AviDz_eYzd3MAnBPgi_gPCyUc3IJ2s15Z1bECUaGTudGvA0YOzuMMOjgJyJp-10Hq7VLk_6PvF_bL1tuKPFduXw==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1xiccvZLqvvRRfUzb3l7d416dGJZdOEqzjC9WlL37M1R57FPuiP2NX9nfbxc-3LO8HGgVG1i878BwMUNNn40TGAaKMU7teixnYgVZWAcACCVtNqLbuobzePJaE_wCIzu0HFzP",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "BrEaST: Curated Benchmark Dataset for Ultrasound-Based Breast Lesion Analysis",
    "paperLink": "https://doi.org/10.1038/s41597-024-02927-6",
    "description": "A comprehensive dataset of breast ultrasound scans containing benign and malignant lesions as well as normal tissue. While primarily for classification and segmentation, the detailed tumor-level labels and freehand annotations make it valuable for ultrasound-based tumor tracking benchmarks.",
    "authors": [
      "Michal K. Grzeszczyk",
      "Arkadiusz Sitek",
      "Marek Zięba",
      "Tomasz Trzciński"
    ],
    "githubLink": "https://github.com/piotrmazi/BrEaST-Dataset",
    "itemCount": "256 scans from 256 patients",
    "source": "Scholar",
    "specs": "Ultrasound images, patient-level labels, image-level annotations, tumor-level labels",
    "year": "2024",
    "id": "saved-1769658705266-j2lgz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbDtAfqFxpdxc7BaxSKGJkQd6s9R3Miat5Qif8wXAnP-AH8qH7C0U72JOyG8f1gqgkb2orpVooEirsJ8WrY5Rd60Ki5-RrEgJH8HYfZfdDX-ZMyEFKryZG83uM",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDfK_3gDOVhlJuwVLjA9PbMG_W2ApVxbuOgHeG12qEQJIOY_2MZx9U4CLuT95b8waia7pwadHb_1eatSWazGIrzxnXVkO8o2JtZtsxpBc1pSLf17QV20ZDvU4y-W8MynhU4XBNsCnQbCupF43roWEcx-ne-Opt0chDkAJx1Q==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESgHHT8Jtn1ZFzXiQ9L_rfPPANbC9KNLHOMEZ3PevJujy00Fcqg1qTirSoVWIMfIr2bJRBpRYub1OTD-eB6sIpIWvCeCZaRCWFNDcclhi5WMpgvevwYeA4YqSDBC1DZyuvAO-bMqDL1fSoIs3CkP4m0LiEHzW1OsxByfN1CAoRYcOmzL3ICnHBSJton7k94sbVeBbTpd8DleUimwRvenRJ7-M0USEpArbY2cTV9o1OwcS0Ne7OR29G43MZmovE5y_1vw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjaQ4MpjXMxQfxfVv9y3EDga9Pjqh-HbUocFgAsLWLBJpsXkHLSS7bU_XXeF5-S4zEnDKgY2uno18RbYyybdVhx0F2eWSygOm5VVbhe_BKMCF-hJoRDrr188sKUSVLjguuUJCxrA==",
        "title": "sydney.edu.au"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQa7TxJPDeDq_AyMKQwYufEgoTVazf_HOkwhYRxRE8XvSyCIf8iX8W-xZ-aqxozXEHGRvz8OKHTGJQLmRmz3NqOGgetodHJDubERpfLKbVPTRRBR0LuqDGrH4Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFvf5AmjSsdgLFtTG8nNbJ5vu3sp8JIAHib-rCmWTyINsrTMpqy0NNcQy0KWzcpiUkCGDDySgRxfWrYMuRQPeWojPjSP-E_oEcW2PFWAtIGIuuhHCKIKNER5pq8V5I08DMV-Rm3otUlX5eCYxrUAPU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7CCLZVTkx01h03pNsjMcEkb7MHLgcoirY0CAvaBzyanLbhS-89-44PJtxuy_Cnr5IZueEmgLlb-6cQfVL7iuS59qZHi-sCzJP9UgOB0WTP6Cuv9CCLw7LRRlds8v0MP-1SAxwXJZqoWWbEojWQJP-7NfA",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtuv3LLId9NgyYNFkEfd6VlioGwahPZrYyF8qMlI64C8tlMvf8Apz4yXC-bWyx2hWdXyQ9jYpscbdw3Z08BZ4KyQoDYa3mhoYso1dRKUVXhlJ74NBtrdcVSwQJhEV5m8pnlcBDNgwPWNMSGyNNBKY=",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6WqyuFQqdns-KuvE74R4QV69CymoZ6ttQyW2a8jsU37PPBns_bQo2J8gne4kfK25ghFqjc9ikni7AviDz_eYzd3MAnBPgi_gPCyUc3IJ2s15Z1bECUaGTudGvA0YOzuMMOjgJyJp-10Hq7VLk_6PvF_bL1tuKPFduXw==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1xiccvZLqvvRRfUzb3l7d416dGJZdOEqzjC9WlL37M1R57FPuiP2NX9nfbxc-3LO8HGgVG1i878BwMUNNn40TGAaKMU7teixnYgVZWAcACCVtNqLbuobzePJaE_wCIzu0HFzP",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "BUSIS: A Benchmark for Breast Ultrasound Image Segmentation",
    "paperLink": "https://doi.org/10.3390/healthcare10040731",
    "description": "A benchmark dataset for evaluating segmentation algorithms on breast ultrasound images. While focused on segmentation, it serves as a ground truth resource for initializing and validating frame-by-frame tumor tracking in ultrasound sequences.",
    "authors": [
      "Y. Huang",
      "L. Han",
      "H. Dou",
      "H. Luo",
      "Z. Yuan",
      "Q. Liu",
      "X. Zhang"
    ],
    "githubLink": "https://scholar.google.com/scholar?q=BUSIS+Benchmark+dataset",
    "itemCount": "562 images",
    "source": "Scholar",
    "specs": "B-mode ultrasound images, mask annotations",
    "year": "2022",
    "id": "saved-1769658705266-suauh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbDtAfqFxpdxc7BaxSKGJkQd6s9R3Miat5Qif8wXAnP-AH8qH7C0U72JOyG8f1gqgkb2orpVooEirsJ8WrY5Rd60Ki5-RrEgJH8HYfZfdDX-ZMyEFKryZG83uM",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDfK_3gDOVhlJuwVLjA9PbMG_W2ApVxbuOgHeG12qEQJIOY_2MZx9U4CLuT95b8waia7pwadHb_1eatSWazGIrzxnXVkO8o2JtZtsxpBc1pSLf17QV20ZDvU4y-W8MynhU4XBNsCnQbCupF43roWEcx-ne-Opt0chDkAJx1Q==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESgHHT8Jtn1ZFzXiQ9L_rfPPANbC9KNLHOMEZ3PevJujy00Fcqg1qTirSoVWIMfIr2bJRBpRYub1OTD-eB6sIpIWvCeCZaRCWFNDcclhi5WMpgvevwYeA4YqSDBC1DZyuvAO-bMqDL1fSoIs3CkP4m0LiEHzW1OsxByfN1CAoRYcOmzL3ICnHBSJton7k94sbVeBbTpd8DleUimwRvenRJ7-M0USEpArbY2cTV9o1OwcS0Ne7OR29G43MZmovE5y_1vw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjaQ4MpjXMxQfxfVv9y3EDga9Pjqh-HbUocFgAsLWLBJpsXkHLSS7bU_XXeF5-S4zEnDKgY2uno18RbYyybdVhx0F2eWSygOm5VVbhe_BKMCF-hJoRDrr188sKUSVLjguuUJCxrA==",
        "title": "sydney.edu.au"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQa7TxJPDeDq_AyMKQwYufEgoTVazf_HOkwhYRxRE8XvSyCIf8iX8W-xZ-aqxozXEHGRvz8OKHTGJQLmRmz3NqOGgetodHJDubERpfLKbVPTRRBR0LuqDGrH4Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFvf5AmjSsdgLFtTG8nNbJ5vu3sp8JIAHib-rCmWTyINsrTMpqy0NNcQy0KWzcpiUkCGDDySgRxfWrYMuRQPeWojPjSP-E_oEcW2PFWAtIGIuuhHCKIKNER5pq8V5I08DMV-Rm3otUlX5eCYxrUAPU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7CCLZVTkx01h03pNsjMcEkb7MHLgcoirY0CAvaBzyanLbhS-89-44PJtxuy_Cnr5IZueEmgLlb-6cQfVL7iuS59qZHi-sCzJP9UgOB0WTP6Cuv9CCLw7LRRlds8v0MP-1SAxwXJZqoWWbEojWQJP-7NfA",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtuv3LLId9NgyYNFkEfd6VlioGwahPZrYyF8qMlI64C8tlMvf8Apz4yXC-bWyx2hWdXyQ9jYpscbdw3Z08BZ4KyQoDYa3mhoYso1dRKUVXhlJ74NBtrdcVSwQJhEV5m8pnlcBDNgwPWNMSGyNNBKY=",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6WqyuFQqdns-KuvE74R4QV69CymoZ6ttQyW2a8jsU37PPBns_bQo2J8gne4kfK25ghFqjc9ikni7AviDz_eYzd3MAnBPgi_gPCyUc3IJ2s15Z1bECUaGTudGvA0YOzuMMOjgJyJp-10Hq7VLk_6PvF_bL1tuKPFduXw==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1xiccvZLqvvRRfUzb3l7d416dGJZdOEqzjC9WlL37M1R57FPuiP2NX9nfbxc-3LO8HGgVG1i878BwMUNNn40TGAaKMU7teixnYgVZWAcACCVtNqLbuobzePJaE_wCIzu0HFzP",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Image X Institute Real-Time Radiotherapy Benchmarks",
    "paperLink": "https://sydney.edu.au/medicine-health/image-x-institute/public-data.html",
    "description": "A collection of datasets specifically for benchmarking real-time radiotherapy systems, including lung and prostate patient measured motion traces, CT sets, and contour sets. It includes data from the TROG 15.01 SPARK trial (prostate) and 4D-Lung data.",
    "authors": [
      "Paul Keall",
      "Image X Institute Members"
    ],
    "githubLink": "https://github.com/Image-X-Institute",
    "itemCount": "Various (e.g., 48 prostate patients, 20 lung patients)",
    "source": "Scholar",
    "specs": "Motion traces, CT images, RT structure sets, kV/MV projection images",
    "year": "2020",
    "id": "saved-1769658705266-cbepd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbDtAfqFxpdxc7BaxSKGJkQd6s9R3Miat5Qif8wXAnP-AH8qH7C0U72JOyG8f1gqgkb2orpVooEirsJ8WrY5Rd60Ki5-RrEgJH8HYfZfdDX-ZMyEFKryZG83uM",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDfK_3gDOVhlJuwVLjA9PbMG_W2ApVxbuOgHeG12qEQJIOY_2MZx9U4CLuT95b8waia7pwadHb_1eatSWazGIrzxnXVkO8o2JtZtsxpBc1pSLf17QV20ZDvU4y-W8MynhU4XBNsCnQbCupF43roWEcx-ne-Opt0chDkAJx1Q==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESgHHT8Jtn1ZFzXiQ9L_rfPPANbC9KNLHOMEZ3PevJujy00Fcqg1qTirSoVWIMfIr2bJRBpRYub1OTD-eB6sIpIWvCeCZaRCWFNDcclhi5WMpgvevwYeA4YqSDBC1DZyuvAO-bMqDL1fSoIs3CkP4m0LiEHzW1OsxByfN1CAoRYcOmzL3ICnHBSJton7k94sbVeBbTpd8DleUimwRvenRJ7-M0USEpArbY2cTV9o1OwcS0Ne7OR29G43MZmovE5y_1vw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjaQ4MpjXMxQfxfVv9y3EDga9Pjqh-HbUocFgAsLWLBJpsXkHLSS7bU_XXeF5-S4zEnDKgY2uno18RbYyybdVhx0F2eWSygOm5VVbhe_BKMCF-hJoRDrr188sKUSVLjguuUJCxrA==",
        "title": "sydney.edu.au"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQa7TxJPDeDq_AyMKQwYufEgoTVazf_HOkwhYRxRE8XvSyCIf8iX8W-xZ-aqxozXEHGRvz8OKHTGJQLmRmz3NqOGgetodHJDubERpfLKbVPTRRBR0LuqDGrH4Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFvf5AmjSsdgLFtTG8nNbJ5vu3sp8JIAHib-rCmWTyINsrTMpqy0NNcQy0KWzcpiUkCGDDySgRxfWrYMuRQPeWojPjSP-E_oEcW2PFWAtIGIuuhHCKIKNER5pq8V5I08DMV-Rm3otUlX5eCYxrUAPU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7CCLZVTkx01h03pNsjMcEkb7MHLgcoirY0CAvaBzyanLbhS-89-44PJtxuy_Cnr5IZueEmgLlb-6cQfVL7iuS59qZHi-sCzJP9UgOB0WTP6Cuv9CCLw7LRRlds8v0MP-1SAxwXJZqoWWbEojWQJP-7NfA",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtuv3LLId9NgyYNFkEfd6VlioGwahPZrYyF8qMlI64C8tlMvf8Apz4yXC-bWyx2hWdXyQ9jYpscbdw3Z08BZ4KyQoDYa3mhoYso1dRKUVXhlJ74NBtrdcVSwQJhEV5m8pnlcBDNgwPWNMSGyNNBKY=",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6WqyuFQqdns-KuvE74R4QV69CymoZ6ttQyW2a8jsU37PPBns_bQo2J8gne4kfK25ghFqjc9ikni7AviDz_eYzd3MAnBPgi_gPCyUc3IJ2s15Z1bECUaGTudGvA0YOzuMMOjgJyJp-10Hq7VLk_6PvF_bL1tuKPFduXw==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1xiccvZLqvvRRfUzb3l7d416dGJZdOEqzjC9WlL37M1R57FPuiP2NX9nfbxc-3LO8HGgVG1i878BwMUNNn40TGAaKMU7teixnYgVZWAcACCVtNqLbuobzePJaE_wCIzu0HFzP",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "4D-Lung (TCIA)",
    "paperLink": "https://doi.org/10.7937/K9/TCIA.2016.ELN8YGLE",
    "description": "A benchmark dataset for image-guided radiation therapy (IGRT) research, specifically for real-time tumor tracking. It contains 4D Fan Beam CT (4D-FBCT) and 4D Cone Beam CT (4D-CBCT) images, along with tumor contours and respiratory motion traces for 20 locally-advanced non-small cell lung cancer patients.",
    "authors": [
      "Geoffrey D. Hugo",
      "Elisabeth Weiss",
      "William C. Sleeman",
      "Salim Balik",
      "Paul J. Keall",
      "Jun Lu",
      "Jeffrey F. Williamson"
    ],
    "githubLink": "https://www.cancerimagingarchive.net/collection/4d-lung/",
    "itemCount": "20 patients (82 4DCTs, 507 4DCBCTs)",
    "source": "Scholar",
    "specs": "4D-FBCT, 4D-CBCT, DICOM, motion traces, tumor contours",
    "year": "2016",
    "id": "saved-1769658705266-0j4bg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbDtAfqFxpdxc7BaxSKGJkQd6s9R3Miat5Qif8wXAnP-AH8qH7C0U72JOyG8f1gqgkb2orpVooEirsJ8WrY5Rd60Ki5-RrEgJH8HYfZfdDX-ZMyEFKryZG83uM",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDfK_3gDOVhlJuwVLjA9PbMG_W2ApVxbuOgHeG12qEQJIOY_2MZx9U4CLuT95b8waia7pwadHb_1eatSWazGIrzxnXVkO8o2JtZtsxpBc1pSLf17QV20ZDvU4y-W8MynhU4XBNsCnQbCupF43roWEcx-ne-Opt0chDkAJx1Q==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESgHHT8Jtn1ZFzXiQ9L_rfPPANbC9KNLHOMEZ3PevJujy00Fcqg1qTirSoVWIMfIr2bJRBpRYub1OTD-eB6sIpIWvCeCZaRCWFNDcclhi5WMpgvevwYeA4YqSDBC1DZyuvAO-bMqDL1fSoIs3CkP4m0LiEHzW1OsxByfN1CAoRYcOmzL3ICnHBSJton7k94sbVeBbTpd8DleUimwRvenRJ7-M0USEpArbY2cTV9o1OwcS0Ne7OR29G43MZmovE5y_1vw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjaQ4MpjXMxQfxfVv9y3EDga9Pjqh-HbUocFgAsLWLBJpsXkHLSS7bU_XXeF5-S4zEnDKgY2uno18RbYyybdVhx0F2eWSygOm5VVbhe_BKMCF-hJoRDrr188sKUSVLjguuUJCxrA==",
        "title": "sydney.edu.au"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFQa7TxJPDeDq_AyMKQwYufEgoTVazf_HOkwhYRxRE8XvSyCIf8iX8W-xZ-aqxozXEHGRvz8OKHTGJQLmRmz3NqOGgetodHJDubERpfLKbVPTRRBR0LuqDGrH4Z",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFvf5AmjSsdgLFtTG8nNbJ5vu3sp8JIAHib-rCmWTyINsrTMpqy0NNcQy0KWzcpiUkCGDDySgRxfWrYMuRQPeWojPjSP-E_oEcW2PFWAtIGIuuhHCKIKNER5pq8V5I08DMV-Rm3otUlX5eCYxrUAPU=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH7CCLZVTkx01h03pNsjMcEkb7MHLgcoirY0CAvaBzyanLbhS-89-44PJtxuy_Cnr5IZueEmgLlb-6cQfVL7iuS59qZHi-sCzJP9UgOB0WTP6Cuv9CCLw7LRRlds8v0MP-1SAxwXJZqoWWbEojWQJP-7NfA",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtuv3LLId9NgyYNFkEfd6VlioGwahPZrYyF8qMlI64C8tlMvf8Apz4yXC-bWyx2hWdXyQ9jYpscbdw3Z08BZ4KyQoDYa3mhoYso1dRKUVXhlJ74NBtrdcVSwQJhEV5m8pnlcBDNgwPWNMSGyNNBKY=",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6WqyuFQqdns-KuvE74R4QV69CymoZ6ttQyW2a8jsU37PPBns_bQo2J8gne4kfK25ghFqjc9ikni7AviDz_eYzd3MAnBPgi_gPCyUc3IJ2s15Z1bECUaGTudGvA0YOzuMMOjgJyJp-10Hq7VLk_6PvF_bL1tuKPFduXw==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1xiccvZLqvvRRfUzb3l7d416dGJZdOEqzjC9WlL37M1R57FPuiP2NX9nfbxc-3LO8HGgVG1i878BwMUNNn40TGAaKMU7teixnYgVZWAcACCVtNqLbuobzePJaE_wCIzu0HFzP",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "UltraGS (Clinical Ultrasound Examination Dataset)",
    "paperLink": "https://arxiv.org/abs/2511.05643",
    "description": "A benchmark dataset for Gaussian Splatting-based novel view synthesis and 3D reconstruction in ultrasound. It includes diverse anatomical scans collected under real-world clinical protocols to validate rendering fidelity and speed.",
    "authors": [
      "Bean Young",
      "et al."
    ],
    "githubLink": "https://github.com/Bean-Young/UltraGS",
    "itemCount": "6 clinical samples (wrist, kidney) + synthetic data",
    "source": "arXiv",
    "specs": "Clinical ultrasound video sequences, 3D reconstruction targets, wrist/kidney scans",
    "year": "2025",
    "id": "saved-1769658775162-7s44q",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsIfKz1QQDcVNuNqMHUf7s_QmhCGjbLzMd4JlsKkDNrDUOZC0UyZFvw38hcqP_tdTq7cr8DjhG5oX8wq7oyS5vaX73zuJbC9ye2ZEve4iZh2hb3uVOzu8r0cRhJI=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8d2PZoQ_Ngh0heXuqFq7eWCPzv4IWCJB3YAMnpd0EGFCh6ARBArxjpMLm2bjBTDLNarVmavfY11r-jHj80t12RptisG9Uj4YMSy1zFnn-OF34lUJvhTHIoFmdmbpy_zUspNAyUzqI2edRQcZ6l_ucHqvtk6lQ53HwpRqZaKfWW3SSjizBjB9bJ224dYHOqTHR1R1iVn8kIcTY4B3ZfjFUdeQYZisvJUXWpqlpBGHCdMb7bLYxaODXpfYxmSUj4Vruv67dOas33Cq4NPjmlktByu7nPYMoSFFH_2-AhNwEfIM2KO5z",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "OpenUS Dataset",
    "paperLink": "https://arxiv.org/abs/2511.11510",
    "description": "A massive compilation of public ultrasound datasets designed for pre-training foundation models. While primarily for analysis, it employs masked image modeling (reconstruction) as a pre-training objective and serves as a benchmark for generalizable ultrasound representations.",
    "authors": [
      "Xiaoyu Zheng",
      "Xu Chen",
      "Awais Rauf",
      "Gregory Slabaugh",
      "et al."
    ],
    "githubLink": "https://github.com/XZheng0427/OpenUS",
    "itemCount": "308,000+ images from 42 datasets",
    "source": "arXiv / GitHub",
    "specs": "B-mode images, diverse anatomies (liver, breast, cardiac, fetal, etc.), diverse devices",
    "year": "2025",
    "id": "saved-1769658775162-vl2a1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsIfKz1QQDcVNuNqMHUf7s_QmhCGjbLzMd4JlsKkDNrDUOZC0UyZFvw38hcqP_tdTq7cr8DjhG5oX8wq7oyS5vaX73zuJbC9ye2ZEve4iZh2hb3uVOzu8r0cRhJI=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8d2PZoQ_Ngh0heXuqFq7eWCPzv4IWCJB3YAMnpd0EGFCh6ARBArxjpMLm2bjBTDLNarVmavfY11r-jHj80t12RptisG9Uj4YMSy1zFnn-OF34lUJvhTHIoFmdmbpy_zUspNAyUzqI2edRQcZ6l_ucHqvtk6lQ53HwpRqZaKfWW3SSjizBjB9bJ224dYHOqTHR1R1iVn8kIcTY4B3ZfjFUdeQYZisvJUXWpqlpBGHCdMb7bLYxaODXpfYxmSUj4Vruv67dOas33Cq4NPjmlktByu7nPYMoSFFH_2-AhNwEfIM2KO5z",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "Ultra-NeRF Dataset",
    "paperLink": "https://proceedings.mlr.press/v227/wysocki24a.html",
    "description": "A dataset created to evaluate Neural Radiance Fields (NeRF) for ultrasound imaging. It contains sweeps of regions of interest with calibrated poses, allowing for the reconstruction of 3D volumes from 2D B-mode scans using implicit neural representations.",
    "authors": [
      "Magdalena Wysocki",
      "Mohammad Farid Azampour",
      "Christine Eilers",
      "Benjamin Busam",
      "Mehrdad Salehi",
      "Nassir Navab"
    ],
    "githubLink": "https://github.com/magdalena-wysocki/ultra-nerf",
    "itemCount": "Specific sweeps for synthetic and phantom targets",
    "source": "GitHub / MIDL",
    "specs": "2D B-mode sweeps, calibrated poses, synthetic & phantom data, 3D reconstruction targets",
    "year": "2023",
    "id": "saved-1769658775162-5qvpl",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsIfKz1QQDcVNuNqMHUf7s_QmhCGjbLzMd4JlsKkDNrDUOZC0UyZFvw38hcqP_tdTq7cr8DjhG5oX8wq7oyS5vaX73zuJbC9ye2ZEve4iZh2hb3uVOzu8r0cRhJI=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8d2PZoQ_Ngh0heXuqFq7eWCPzv4IWCJB3YAMnpd0EGFCh6ARBArxjpMLm2bjBTDLNarVmavfY11r-jHj80t12RptisG9Uj4YMSy1zFnn-OF34lUJvhTHIoFmdmbpy_zUspNAyUzqI2edRQcZ6l_ucHqvtk6lQ53HwpRqZaKfWW3SSjizBjB9bJ224dYHOqTHR1R1iVn8kIcTY4B3ZfjFUdeQYZisvJUXWpqlpBGHCdMb7bLYxaODXpfYxmSUj4Vruv67dOas33Cq4NPjmlktByu7nPYMoSFFH_2-AhNwEfIM2KO5z",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "CUBDL (Challenge on Ultrasound Beamforming with Deep Learning) Dataset",
    "paperLink": "https://ieeexplore.ieee.org/document/9497746",
    "description": "A large-scale benchmark dataset designed for evaluating deep learning methods in ultrasound image formation (beamforming). It allows researchers to compare reconstruction algorithms on standardized raw channel data.",
    "authors": [
      "Dongwoon Hyun",
      "A. Wiacek",
      "S. Goudarzi",
      "S. Rothlübbers",
      "A. Asif",
      "K. Eickel",
      "Y. C. Eldar",
      "J. Huang",
      "M. Mischi",
      "H. Rivaz",
      "D. Sinden",
      "R. J. G. van Sloun",
      "H. Strohm",
      "M. A. L. Bell"
    ],
    "githubLink": "https://gitlab.com/dongwoon.hyun/cubdl",
    "itemCount": "576 image acquisition sequences",
    "source": "IEEE IUS / JHU",
    "specs": "Raw RF/IQ channel data, plane wave & focused transmissions, phantom & in vivo (breast, carotid, heart)",
    "year": "2021",
    "id": "saved-1769658775162-trrmg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsIfKz1QQDcVNuNqMHUf7s_QmhCGjbLzMd4JlsKkDNrDUOZC0UyZFvw38hcqP_tdTq7cr8DjhG5oX8wq7oyS5vaX73zuJbC9ye2ZEve4iZh2hb3uVOzu8r0cRhJI=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8d2PZoQ_Ngh0heXuqFq7eWCPzv4IWCJB3YAMnpd0EGFCh6ARBArxjpMLm2bjBTDLNarVmavfY11r-jHj80t12RptisG9Uj4YMSy1zFnn-OF34lUJvhTHIoFmdmbpy_zUspNAyUzqI2edRQcZ6l_ucHqvtk6lQ53HwpRqZaKfWW3SSjizBjB9bJ224dYHOqTHR1R1iVn8kIcTY4B3ZfjFUdeQYZisvJUXWpqlpBGHCdMb7bLYxaODXpfYxmSUj4Vruv67dOas33Cq4NPjmlktByu7nPYMoSFFH_2-AhNwEfIM2KO5z",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "PICMUS (Plane-Wave Imaging Challenge in Medical UltraSound) Dataset",
    "paperLink": "https://ieeexplore.ieee.org/document/7728908",
    "description": "The first international benchmark for ultrasound image formation, specifically focusing on plane-wave imaging. It provides a standard framework for comparing beamforming strategies like Delay-and-Sum (DAS) and Minimum Variance (MV).",
    "authors": [
      "H. Liebgott",
      "A. Rodriguez-Molares",
      "F. Cervenansky",
      "J. A. Jensen",
      "O. Bernard"
    ],
    "githubLink": "https://www.creatis.insa-lyon.fr/Challenge/IEEE_IUS_2016/",
    "itemCount": "4 datasets (2 simulated, 2 experimental)",
    "source": "CREATIS / IEEE IUS",
    "specs": "Raw RF channel data, 75 plane waves, static phantoms and in vivo carotid",
    "year": "2016",
    "id": "saved-1769658775162-2yke5",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsIfKz1QQDcVNuNqMHUf7s_QmhCGjbLzMd4JlsKkDNrDUOZC0UyZFvw38hcqP_tdTq7cr8DjhG5oX8wq7oyS5vaX73zuJbC9ye2ZEve4iZh2hb3uVOzu8r0cRhJI=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8d2PZoQ_Ngh0heXuqFq7eWCPzv4IWCJB3YAMnpd0EGFCh6ARBArxjpMLm2bjBTDLNarVmavfY11r-jHj80t12RptisG9Uj4YMSy1zFnn-OF34lUJvhTHIoFmdmbpy_zUspNAyUzqI2edRQcZ6l_ucHqvtk6lQ53HwpRqZaKfWW3SSjizBjB9bJ224dYHOqTHR1R1iVn8kIcTY4B3ZfjFUdeQYZisvJUXWpqlpBGHCdMb7bLYxaODXpfYxmSUj4Vruv67dOas33Cq4NPjmlktByu7nPYMoSFFH_2-AhNwEfIM2KO5z",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "OpenNeuro ds006557: Ultra-low-field brain MRI",
    "paperLink": "https://direct.mit.edu/imag/article/doi/10.1162/IMAG.a.930",
    "description": "A dataset collected to assess the test-retest reliability of 64mT ultra-low-field MRI morphometry and its correspondence to 3T high-field MRI. It includes multiple orientations and voxel resolutions.",
    "authors": [
      "František Váša",
      "Carly Bennallick",
      "Niall J. Bourke",
      "Francesco Padormo",
      "Paul Cawley",
      "Tomoki Arichi",
      "Tobias C. Wood",
      "Robert Leech",
      "Steven C. R. Williams"
    ],
    "githubLink": "https://github.com/OpenNeuroDatasets/ds006557",
    "itemCount": "60 subjects",
    "source": "OpenNeuro",
    "specs": "64mT (Hyperfine Swoop), T1w, T2w, defaced structural MRI, BIDS format",
    "year": "2025",
    "id": "saved-1769658840075-co2ad",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEm5WY8aAx4-fb32ruNcAxaiCd0Mj1peM4bLgVLDVeFA1gKCs0v4QJKOYi64GJzCRK4Y2XmfSF5L8Rcp6YITWZin98Ps5QoH9hCoyCHgw9Ts0nz4R1H9EuyErCoIylC5_mPU42bM7J_3BwasCvDOC7Qo5r9otYG09W6tn7QxNyXQFUZB3qBN9wiexol2BVNe1mgkZlg80MkqN966IJI2ohytl9hsipbilvK-n2Nvdbqe1Ja3YbrjnrU7wz3TjDVwjlFWZNjYJEhlyn9IPTd-malJEWd",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "ULF-EnC Challenge Dataset",
    "paperLink": "https://zenodo.org/records/10998632",
    "description": "A paired dataset comprising ultra-low-field (64mT) and high-field (3T) MRI scans, created for the 'Ultra-Low-Field MRI Image Enhancement Challenge'. It serves as a benchmark for developing algorithms to enhance low-field images to match high-field quality.",
    "authors": [
      "K H Tohidul Islam",
      "Renata Yadav",
      "Hongwoo Jang",
      "S M Masadur Rahman",
      "Zahra Gharaee",
      "et al."
    ],
    "githubLink": "https://github.com/khtohidulislam/ULF-EnC-Challenge",
    "itemCount": "75 paired cases (50 train, 10 val, 15 test)",
    "source": "Zenodo",
    "specs": "NIFTI format, Paired 64mT (Hyperfine Swoop) and 3T (Philips Achieva) brain MRI",
    "year": "2024",
    "id": "saved-1769658840075-vx8hs",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEm5WY8aAx4-fb32ruNcAxaiCd0Mj1peM4bLgVLDVeFA1gKCs0v4QJKOYi64GJzCRK4Y2XmfSF5L8Rcp6YITWZin98Ps5QoH9hCoyCHgw9Ts0nz4R1H9EuyErCoIylC5_mPU42bM7J_3BwasCvDOC7Qo5r9otYG09W6tn7QxNyXQFUZB3qBN9wiexol2BVNe1mgkZlg80MkqN966IJI2ohytl9hsipbilvK-n2Nvdbqe1Ja3YbrjnrU7wz3TjDVwjlFWZNjYJEhlyn9IPTd-malJEWd",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "ICASSP 2025 LF-MRI Challenge Dataset",
    "paperLink": "https://2025.ieeeicassp.org/grand-challenges/",
    "description": "A benchmark dataset for the ICASSP 2025 Low-Field MRI Image Quality Challenge, featuring two tracks: image denoising and super-resolution reconstruction. It includes phantom data and in-vivo scans.",
    "authors": [
      "Venkatesh K M",
      "Ahlad Kumar",
      "Chandra Sekhar Seelamantula"
    ],
    "githubLink": "https://github.com/misp-lab/ICASSP-2025-LF-MRI-challenge",
    "itemCount": "Unknown (Paired phantom and in-vivo sets)",
    "source": "IEEE ICASSP",
    "specs": "Paired 3T and low-field (47mT phantom, 0.3T in-vivo) data",
    "year": "2024",
    "id": "saved-1769658840075-9w9vd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEm5WY8aAx4-fb32ruNcAxaiCd0Mj1peM4bLgVLDVeFA1gKCs0v4QJKOYi64GJzCRK4Y2XmfSF5L8Rcp6YITWZin98Ps5QoH9hCoyCHgw9Ts0nz4R1H9EuyErCoIylC5_mPU42bM7J_3BwasCvDOC7Qo5r9otYG09W6tn7QxNyXQFUZB3qBN9wiexol2BVNe1mgkZlg80MkqN966IJI2ohytl9hsipbilvK-n2Nvdbqe1Ja3YbrjnrU7wz3TjDVwjlFWZNjYJEhlyn9IPTd-malJEWd",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "M4Raw",
    "paperLink": "https://www.nature.com/articles/s41597-023-02183-z",
    "description": "A multi-contrast, multi-repetition, multi-channel MRI k-space dataset acquired on a low-field (0.3T) system. It is designed for research in low-field MRI reconstruction, denoising, and parallel imaging.",
    "authors": [
      "Mengye Lyu",
      "Lifeng Mei",
      "Shoujin Huang",
      "Sixing Liu",
      "Yi Li",
      "Kexin Yang",
      "Yilong Liu",
      "Yu Dong",
      "Linzheng Dong",
      "Ed X. Wu"
    ],
    "githubLink": "https://github.com/mylyu/M4Raw",
    "itemCount": "183 subjects; >1000 volumes",
    "source": "Scholar",
    "specs": "0.3T Low-field MRI; Raw k-space; T1w, T2w, FLAIR contrasts; Multi-repetition for denoising benchmarks.",
    "year": "2023",
    "id": "saved-1769658921003-go3s5",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsHHGDc7GVCiCrdry4QekU5ifHt0m8nitMs7lsVBo-Eh7bgCJXE3AJKNTWGWgKb07bIAZv9n0WIxfiKHwGGgmzuJoxbghgIGYNP1-KZjfiT3teEtmsK4v6FEfPDVuhLAMDRNPVig==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "CMRxRecon (Cardiac MRI Reconstruction)",
    "paperLink": "https://arxiv.org/abs/2309.10836",
    "description": "A benchmark dataset for the MICCAI Cardiac MRI Reconstruction Challenge. It contains multi-contrast, multi-view, and multi-coil raw k-space data from healthy volunteers for accelerated cine and mapping reconstruction.",
    "authors": [
      "Chengyan Wang",
      "Jun Lyu",
      "Shuo Wang",
      "Chenqin Qin",
      "Kunyuan Guo",
      "Xinyu Zhang",
      "Yanwen Luo",
      "Fanwen Wang",
      "Yifan Yuan",
      "Yinghua Zhang",
      "Kerong Duan",
      "Ke Wang",
      "Ziteng Duan",
      "Xiaomei Li",
      "Yi Zhang",
      "Jiahuai Ding",
      "Ximsheng Xie",
      "Jingyi Wei",
      "Jing Li",
      "Xiaoying Tang",
      "Qi Liu",
      "Peiyong Li",
      "Zhaoying Wen",
      "Chengwen Zheng",
      "Shuo Li",
      "He Wang"
    ],
    "githubLink": "https://github.com/CmrxRecon/CMRxRecon2024",
    "itemCount": "300 subjects (2023 release); 330 subjects (2024 update)",
    "source": "arXiv",
    "specs": "Cardiac Cine, T1/T2 Mapping; Multi-coil k-space; 3.0T; Fully sampled and undersampled masks.",
    "year": "2023",
    "id": "saved-1769658921004-ixmyp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsHHGDc7GVCiCrdry4QekU5ifHt0m8nitMs7lsVBo-Eh7bgCJXE3AJKNTWGWgKb07bIAZv9n0WIxfiKHwGGgmzuJoxbghgIGYNP1-KZjfiT3teEtmsK4v6FEfPDVuhLAMDRNPVig==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Calgary-Campinas (CC-359) / MR Reconstruction Challenge",
    "paperLink": "https://doi.org/10.1016/j.neuroimage.2017.12.073",
    "description": "A multi-vendor, multi-field-strength brain MRI dataset originally released for segmentation and later expanded to include raw k-space data for reconstruction benchmarks. It focuses on 3D T1-weighted volumetric imaging.",
    "authors": [
      "Roberto Souza",
      "Oeslle Lucena",
      "Julia Garrafa",
      "David Gobbi",
      "Marina Saluzzi",
      "Simone Appenzeller",
      "Leticia Rittner",
      "Richard Frayne",
      "Roberto Lotufo"
    ],
    "githubLink": "https://www.ccdataset.com",
    "itemCount": "359 reconstructed volumes; 167 raw k-space scans (approx. 200 GB)",
    "source": "Scholar",
    "specs": "3D T1-weighted; Raw k-space and Reconstructed volumes; 12-channel and 32-channel coils; 1.5T and 3T; GE, Philips, Siemens scanners.",
    "year": "2018",
    "id": "saved-1769658921004-p2jvq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsHHGDc7GVCiCrdry4QekU5ifHt0m8nitMs7lsVBo-Eh7bgCJXE3AJKNTWGWgKb07bIAZv9n0WIxfiKHwGGgmzuJoxbghgIGYNP1-KZjfiT3teEtmsK4v6FEfPDVuhLAMDRNPVig==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "TotalSegmentator: Robust Segmentation of 104 Anatomic Structures in CT Images",
    "paperLink": "https://pubs.rsna.org/doi/10.1148/ryai.230024",
    "description": "A robust segmentation model and dataset for 104 anatomic structures in CT images. The dataset was created by training an nnU-Net on a large collection of CT scans to segment organs, bones, muscles, and vessels.",
    "authors": [
      "Jakob Wasserthal",
      "Hanns-Christian Breit",
      "Manfred T. Meyer",
      "Maurice Pradella",
      "Daniel Hinck",
      "Alexander W. Sauter",
      "Tobias Heye",
      "Daniel T. Boll",
      "Joshy Cyriac",
      "Shan Yang",
      "Michael Bach",
      "Martin Segeroth"
    ],
    "githubLink": "https://github.com/wasserth/TotalSegmentator",
    "itemCount": "1204 CT images",
    "source": "Google Scholar",
    "specs": "CT scans; 104 classes including 27 organs, 59 bones, 10 muscles, and 8 vessels.",
    "year": "2023",
    "id": "saved-1769659001799-vxeia",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "AbdomenAtlas-8K",
    "paperLink": "https://arxiv.org/abs/2305.09666",
    "description": "A massive multi-organ dataset created using an active learning procedure to annotate 8,448 CT volumes. It is the largest multi-organ dataset to date for spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC.",
    "authors": [
      "Chongyu Qu",
      "Tiezheng Zhang",
      "Hualin Qiao",
      "Jie Liu",
      "Yucheng Tang",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ],
    "githubLink": "https://github.com/MrGiovanni/AbdomenAtlas",
    "itemCount": "8,448 CT volumes",
    "source": "arXiv",
    "specs": "CT volumes; 8 abdominal organs annotated.",
    "year": "2023",
    "id": "saved-1769659001799-po3zz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation",
    "paperLink": "https://arxiv.org/abs/2206.08023",
    "description": "AMOS is a large-scale, diverse, clinical dataset for abdominal organ segmentation, comprising 500 CT and 100 MRI scans from multi-center, multi-vendor, multi-modality, multi-phase, and multi-disease patients. It provides voxel-level annotations for 15 abdominal organs.",
    "authors": [
      "Yuanfeng Ji",
      "Haotian Bai",
      "Jie Yang",
      "Chongjian Ge",
      "Ye Zhu",
      "Ruimao Zhang",
      "Zhen Li",
      "Lingyan Zhang",
      "Wanling Ma",
      "Xiang Wan",
      "Ping Luo"
    ],
    "githubLink": "https://github.com/JiYuanFeng/AMOS",
    "itemCount": "600 scans (500 CT, 100 MRI)",
    "source": "arXiv",
    "specs": "CT and MRI modalities; 15 abdominal organs annotated (spleen, kidneys, gallbladder, esophagus, liver, stomach, aorta, IVC, pancreas, adrenal glands, duodenum, bladder, prostate/uterus).",
    "year": "2022",
    "id": "saved-1769659001799-5sorq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "FLARE: Fast and Low-resource semi-supervised Abdominal oRgan sEgmentation",
    "paperLink": "https://arxiv.org/abs/2308.05862",
    "description": "A challenge dataset designed to benchmark abdominal organ segmentation algorithms in low-resource settings, using a large scale of labeled and unlabeled data. The 2022 edition expanded to 13 organs.",
    "authors": [
      "Jun Ma",
      "Yao Zhang",
      "Song Gu",
      "Cheng Zhu",
      "Cheng Ge",
      "Yichi Zhang",
      "Xaber An",
      "Congcong Wang",
      "Qiyuan Wang",
      "Xin Liu",
      "Shuxin Cao",
      "Qiang Cao",
      "Shangqing Liu",
      "Shoujin Huang",
      "Yining Li",
      "Lianping Zhao",
      "Xuehuan Wan"
    ],
    "githubLink": "https://github.com/JunMa11/FLARE",
    "itemCount": "2300 CT scans (50 labeled, 2000 unlabeled, plus validation/test)",
    "source": "arXiv",
    "specs": "CT scans; 13 organs annotated (liver, spleen, pancreas, kidneys, stomach, gallbladder, esophagus, aorta, IVC, adrenal glands, duodenum).",
    "year": "2022",
    "id": "saved-1769659001799-zgi4z",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "WORD: A Large Scale Dataset for Abdominal Organ Segmentation",
    "paperLink": "https://arxiv.org/abs/2111.02403",
    "description": "The Whole abdominal ORgan Dataset (WORD) addresses the lack of large-scale fine annotated datasets for the whole abdomen. It contains volumes with fine pixel-level annotations and scribble-based sparse annotations.",
    "authors": [
      "Xiangde Luo",
      "Wenjun Liao",
      "Jieneng Chen",
      "Tao Song",
      "Yinan Chen",
      "Shichuan Zhang",
      "Nianyong Chen",
      "Guotai Wang",
      "Shaoting Zhang"
    ],
    "githubLink": "https://github.com/Luoxd1996/WORD",
    "itemCount": "150 abdominal CT volumes (30,495 slices)",
    "source": "arXiv",
    "specs": "CT volumes; 16 organs annotated.",
    "year": "2021",
    "id": "saved-1769659001799-l5ybf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "CT-ORG: CT Volumes with Multiple Organ Segmentations",
    "paperLink": "https://doi.org/10.1038/s41597-020-00715-8",
    "description": "A diverse dataset of CT scans containing diverse organ segmentations, including lungs, bones, liver, kidneys, bladder, and brain, derived from multiple sources including TCIA.",
    "authors": [
      "Blaine Rister",
      "Darvin Yi",
      "Kaushik Shivakumar",
      "Tom Noblet",
      "Daniel L. Rubin"
    ],
    "githubLink": "https://wiki.cancerimagingarchive.net/display/Public/CT-ORG",
    "itemCount": "140 CT scans",
    "source": "Semantic Scholar",
    "specs": "CT scans; 6 organ classes (Liver, Lungs, Bladder, Kidneys, Bones, Brain).",
    "year": "2020",
    "id": "saved-1769659001799-49k2j",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "SegTHOR: Segmentation of Thoracic Organs at Risk",
    "paperLink": "https://arxiv.org/abs/1912.05950",
    "description": "A dataset dedicated to the segmentation of organs at risk (OARs) in the thorax for radiotherapy planning. It includes manual contours for the heart, trachea, aorta, and esophagus.",
    "authors": [
      "Caroline Petitjean",
      "Jean-Christophe M. J. Lambert",
      "Bernard Dubray"
    ],
    "githubLink": "https://codalab.lisn.upsaclay.fr/competitions/1202",
    "itemCount": "60 CT scans (40 Training, 20 Testing)",
    "source": "arXiv",
    "specs": "CT scans; 4 thoracic organs (Heart, Aorta, Trachea, Esophagus).",
    "year": "2019",
    "id": "saved-1769659001799-gmubq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Beyond the Cranial Vault (BTCV) Abdomen Dataset",
    "paperLink": "https://doi.org/10.7303/syn3193805",
    "description": "A benchmark dataset from the MICCAI 2015 workshop 'Multi-Atlas Labeling Beyond the Cranial Vault'. It focuses on the segmentation of 13 abdominal organs from contrast-enhanced CT scans.",
    "authors": [
      "Bennett Landman",
      "Zhoubing Xu",
      "J. Igelsias",
      "Martin Styner",
      "T. Langerak",
      "Arno Klein"
    ],
    "githubLink": "https://www.synapse.org/#!Synapse:syn3193805/wiki/217789",
    "itemCount": "30 Training scans, 20 Testing scans",
    "source": "Scholar",
    "specs": "CT scans; 13 abdominal organs annotated (spleen, kidneys, gallbladder, esophagus, liver, stomach, aorta, IVC, veins, pancreas, adrenal glands).",
    "year": "2015",
    "id": "saved-1769659001800-3y6v4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGagHZ301L2u5OVoAsG_O_kj5GmNbjlr7XJT1hzdvInpaHELiVaPE5IbCieWDiojxJkM9fuuHzFlHc9A07HMIuQKHFDYeZ1dmaa4XBYDhe81Tx5KuDYdwDCjkyvpcKY",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzyl0U-f10CCBr2TyM7ayAL3UC6PykIEEenqZ7yNVR322PWICROrphU4YhWmveBoxHdIraibvWxPE7gW2fkNMfL0XZypJ_h-SLZUKOb296Fdx6YbFI8cD6wOG8eCmDkpjr",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXnnQUGRSnqG9fh6LA7lWd-oJ5ybSCCBsQ2TVlHfTr9ZsO_lC-mtrgOpsXoQ9-GDWW9LWm-p0QILm9Wi5-irUGv-bkadQjD4ssPrAfigDfUJk8plwAlIKeKf_mwjAGqzxa15rLdvyA",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcK-RtEbPLZywfk6XqzxWwL4prRJYUFXvsfBv5-wdnJQe-TXSFzNXO3Zh495zn0eEv0MOgEXIGnuugeZJp5ts-IQDjxQNK-F8AZQM7djGELfsz5ZPrj93jwQotYdGVt6RIHJ_sU-VOUOWexDQFNd0uj0KyQUZAZWunm7E=",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETrstzpYeyO7l9-amAWGRF62WqjV75J6ULwoGxPHhXQM7oa4JUQb7T64kqkJ6UamRNVLQ3c4zncrenT2krA0-_sQlB-YnnIpIdxulieP_7OF5SdSUUzrl5tIqadVvWNMtjDfgusnCDBHoGn5hxLIN_CKL8OojRRnU7N8RZ6SDx9j4arumWDg-CrFh3YNUrXqjACeW_iiMmUgfXCaO_2fDTIiyiKaPdivqfFw-QL1k14X7OPv2PUxlEEA==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGPDo8zM_bxD90CgX1Baur_8Q_BdpOG4EXY7AwxyEkKqJw0Kxtb0hmLd21Qpn0lfkUK2m7q_UxNGhAtozVl5liXqfg879yx0Y2nalQw_mUvXus25W_E8RjKxIgB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-xZT7DvOqvsDSn_0xy_x4BvPhbFUy1vOXigGMqW9AVGw9X9L9WhzKWH6fn1kXhJh93c-YNZUI5BCSB6dq1lKXb5fE2P5noDvSUvFA6GHFYKR9qUJuNwHz0crR6OOCZQtb7AxAOIUov6j8VhrKHp5fo5l6Jw-USQ==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkc4R-1Sxtf4_QdXMWRsgEf0PShV7lICZgQD-BDnEVJ7WpiatkdmUVn0dgJWG4_Dgcw59HEmU7dJ_8ytWYRzBu2BCW2M53Wv5NZlqVonU5XpP7dqn0xgpHMOcs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEevhKLUkUmyxYM0NskykMNKPN1vQV1OHuBUFMjjcswbuM7kHDIgjjOXpqUdz6cCNnRtP0mnQgbT8ifKTGHCTHXkKviG48zwzfc1AY8GXX3f5YHVFthf782qSYNq0fm_OFdwQE1R3DBqz_cA-8S",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_PfQAJ5x1Nzkjc7eddmdGeF8SMKMH1_gsGwBwIVYFwkJuIpZxswSxfjM1i4Sl9oCbB3uygaPsGgsc8LVT1wUdbr-F3O951PeL43L4mBjVIi-saNnj-8jeDzuN-bJ9vMcUmZ8cAJhhe99O8g2a",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHimQlUlG0bxX1RJaJXu-nQlnZ0G5xzIYaGsV0jknim12hv-pRlaDPFU4d73HYeQn0Ia7bVE6JoeHEU4l66pqTm2USxXG4zT0-m8PJW9vbcj799OuU5W7QDxiephQoIGgTduXIRGOrAGUzh4ZejqeZvyIjelJoZcjV6smoCbkfZh5KlIEWMt4_GZuJFJVxy-4PsSnVIvi-8TylDm85y3-Kc_dAsn5i4OYyUzoBAqJePntJaVdJUPFOXYhwzYdZZY_h6ivVqW013Eb4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgM8kBiwtNr0nI1TWTABCM4lq9nxztZQBwhZvFhZaM88iGbIKPLp9yAPnxdofbUZN_ZN_FuiubzbT_kPBzbJy1rH2kLRjbVEaA4WzM8-LnhjURMXQkJfnT3PX6LBr1drBuatV31A3LyAA_i5YB",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Universal UltraSound Image Challenge (UUSIC25) Dataset",
    "paperLink": "https://arxiv.org/abs/2512.17279",
    "description": "A large-scale benchmark designed to evaluate general-purpose deep learning models for multi-organ, multi-task ultrasound image analysis. It aggregates data from multiple public and private sources to test clinical generalization and computational efficiency.",
    "authors": [
      "Zehui Lin",
      "Luyi Han",
      "Tianyu Zhang",
      "Xing Wang",
      "Ying Zhou",
      "Yanming Zhang",
      "Dong Xu",
      "Lingyun Bao",
      "Ritse Mann"
    ],
    "githubLink": "https://github.com/uusic2025/challenge",
    "itemCount": "11,644 training images; 2,479 test images",
    "source": "arXiv",
    "specs": "Ultrasound images covering 7 organs (Breast, Thyroid, Liver, Kidney, Fetal Head, Cardiac, Appendix); Tasks: Classification and Segmentation",
    "year": "2025",
    "id": "saved-1769659052782-g67on",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHE4SyDAB718NdTNiQvGQWkzgEkKWaN9sQFAUNFz1GefRY8wPdY3U2YBXk2F3RluxSOVhPIxLVKL47DC5zMx3ctbDfAOVzOByY8LHtgSEVeBPrycVRoPuksG67iB-bE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEL1C04ywOxSwppeBTO8-O3k99a5D3dyBqPCAIoY0PMJARxZ4Qs_kVKqhIoDEXT8iyoutp_jhamxLBAOTXWRCr8zATdrTNyVdpNEjHut53zcVnagp41nmVkgPC1NEI_vuP2MwCA5WHTgtOcRxv2z2OWgrCOMNRMrtXfRgu8prEvGPdU8m1FQZuq2GGO9mwf9V3G2P-tpq3EyI38VgdXz3P9m4F12YzalJFbcGWhS63bNIatrEBeTpuXXnaEexacU0-r6bBkvSSSqPB6QLw8kQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFTU9jfJBO8zCGZqZ-PTjf0oU4c-lTDRR2VFLwwzwda5xTsw-sb1U5SyS4uFthDelltGvM-mvV4Z3TKacz-mrP0tKhrtSGR7PNX1ZLKrGBUY0X_i-RvVvc2LLRmAB2Ztjmh",
        "title": "sciprofiles.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2JLIj8IUjZUwFbSw_Oyewv5Pth7yWxRzJ60dYJy4bnyPXC6l8gc2DL7-vMsIHIkFDgDfdXTb7e42lRhtj3SBqbKpQn8X_ydPCosWqVtv1b8Eucho6ADWrz9NqkHeI",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVedMln7X_mCR_9nxJbXqHFq4kOqbd-MBX3G3VwzB-Z3BeZZn_QdILwg3xrdB3wUZseZxOlQqKJphb9YEXlZjbBMuh80eFv94sYIrS_gh7qrpk6X-0FcM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEC9OVbQcG07azhZjlT-JAxK_8LFQPDDrQkKYSgeKVtY9n4oOEI5SVXtbyH497psu6I_GHJkAUF1AHQ1_Z-bnZP633L2me1TlhIC11PFd_BGZXLU5gcc9mWys4hlnQUEvXrOgoloQ==",
        "title": "federicobolelli.it"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3X-maPucHwxCdY7Z66ioIOfRSsmqdE3Lufqyn2aU_5k-1zMHTpwCZ0BkbDtBvRA8-57Y17HEgPtpkRd97giNA-KPG0fh3o1kAG1fdYPS8kTXfuG27sKmAcUPQcrX4LJu-xl-UccOL",
        "title": "codabench.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKM9LZg1lLFDaCNTpUQzICptVmMc0yKhql7FsDQ0VVeCq2_hLKUhhvrJBx0m0Rtd0XAQC0xd9PqLgEj1RfWe8tXFiGQkGwHub5tXatDd74GonzZEBt71ZtG9fiAPWYhAIo4S6R-8jJQpTY3u1EeRsCTb4s07sQDB2ROvJCv93uyOD58CiIB7ULI1GZoK0WHFRhgPpuDGQkE7sxTP9zq6EAYNzn7XMpCCdxPkAGNCYCr7EjbjfKlQc7AXp-Is43HkgQDgWKt04aC2qqa-YqOp5vz6mNNAJVLIX3RIYAXw==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "ProPL Universal Ultrasound Dataset",
    "paperLink": "https://arxiv.org/abs/2511.15057",
    "description": "A comprehensive ultrasound dataset introduced to benchmark universal semi-supervised segmentation. It is designed to test frameworks that handle multiple organs and tasks while leveraging both labeled and unlabeled data.",
    "authors": [
      "Yaxiong Chen",
      "Qicong Wang",
      "Chunlei Li",
      "Jingliang Hu",
      "Yilei Shi",
      "Shengwu Xiong",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "githubLink": "https://github.com/WUTCM-Lab/ProPL",
    "itemCount": "Not specified (comprehensive collection)",
    "source": "arXiv",
    "specs": "Ultrasound images; Spans 5 organs and 8 segmentation tasks; Semi-supervised learning focus",
    "year": "2025",
    "id": "saved-1769659052783-ipvle",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHE4SyDAB718NdTNiQvGQWkzgEkKWaN9sQFAUNFz1GefRY8wPdY3U2YBXk2F3RluxSOVhPIxLVKL47DC5zMx3ctbDfAOVzOByY8LHtgSEVeBPrycVRoPuksG67iB-bE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEL1C04ywOxSwppeBTO8-O3k99a5D3dyBqPCAIoY0PMJARxZ4Qs_kVKqhIoDEXT8iyoutp_jhamxLBAOTXWRCr8zATdrTNyVdpNEjHut53zcVnagp41nmVkgPC1NEI_vuP2MwCA5WHTgtOcRxv2z2OWgrCOMNRMrtXfRgu8prEvGPdU8m1FQZuq2GGO9mwf9V3G2P-tpq3EyI38VgdXz3P9m4F12YzalJFbcGWhS63bNIatrEBeTpuXXnaEexacU0-r6bBkvSSSqPB6QLw8kQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFTU9jfJBO8zCGZqZ-PTjf0oU4c-lTDRR2VFLwwzwda5xTsw-sb1U5SyS4uFthDelltGvM-mvV4Z3TKacz-mrP0tKhrtSGR7PNX1ZLKrGBUY0X_i-RvVvc2LLRmAB2Ztjmh",
        "title": "sciprofiles.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2JLIj8IUjZUwFbSw_Oyewv5Pth7yWxRzJ60dYJy4bnyPXC6l8gc2DL7-vMsIHIkFDgDfdXTb7e42lRhtj3SBqbKpQn8X_ydPCosWqVtv1b8Eucho6ADWrz9NqkHeI",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVedMln7X_mCR_9nxJbXqHFq4kOqbd-MBX3G3VwzB-Z3BeZZn_QdILwg3xrdB3wUZseZxOlQqKJphb9YEXlZjbBMuh80eFv94sYIrS_gh7qrpk6X-0FcM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEC9OVbQcG07azhZjlT-JAxK_8LFQPDDrQkKYSgeKVtY9n4oOEI5SVXtbyH497psu6I_GHJkAUF1AHQ1_Z-bnZP633L2me1TlhIC11PFd_BGZXLU5gcc9mWys4hlnQUEvXrOgoloQ==",
        "title": "federicobolelli.it"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3X-maPucHwxCdY7Z66ioIOfRSsmqdE3Lufqyn2aU_5k-1zMHTpwCZ0BkbDtBvRA8-57Y17HEgPtpkRd97giNA-KPG0fh3o1kAG1fdYPS8kTXfuG27sKmAcUPQcrX4LJu-xl-UccOL",
        "title": "codabench.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKM9LZg1lLFDaCNTpUQzICptVmMc0yKhql7FsDQ0VVeCq2_hLKUhhvrJBx0m0Rtd0XAQC0xd9PqLgEj1RfWe8tXFiGQkGwHub5tXatDd74GonzZEBt71ZtG9fiAPWYhAIo4S6R-8jJQpTY3u1EeRsCTb4s07sQDB2ROvJCv93uyOD58CiIB7ULI1GZoK0WHFRhgPpuDGQkE7sxTP9zq6EAYNzn7XMpCCdxPkAGNCYCr7EjbjfKlQc7AXp-Is43HkgQDgWKt04aC2qqa-YqOp5vz6mNNAJVLIX3RIYAXw==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "US-43d (UltraSam Dataset)",
    "paperLink": "https://arxiv.org/abs/2411.16434",
    "description": "A massive collection of 43 open-access ultrasound datasets aggregated to train the UltraSam foundation model. It addresses the domain gap in ultrasound imaging by providing a unified resource for segmentation tasks.",
    "authors": [
      "Feng Tian",
      "Jintao Zhai",
      "Jinru Gong",
      "Xiao Zou"
    ],
    "githubLink": "https://github.com/CAMMA-public/UltraSam",
    "itemCount": "280,000+ image-mask pairs",
    "source": "arXiv",
    "specs": "2D and 3D Ultrasound images; Segmentation masks; Covers 20 different clinical applications/organs",
    "year": "2024",
    "id": "saved-1769659052783-rdiks",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHE4SyDAB718NdTNiQvGQWkzgEkKWaN9sQFAUNFz1GefRY8wPdY3U2YBXk2F3RluxSOVhPIxLVKL47DC5zMx3ctbDfAOVzOByY8LHtgSEVeBPrycVRoPuksG67iB-bE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEL1C04ywOxSwppeBTO8-O3k99a5D3dyBqPCAIoY0PMJARxZ4Qs_kVKqhIoDEXT8iyoutp_jhamxLBAOTXWRCr8zATdrTNyVdpNEjHut53zcVnagp41nmVkgPC1NEI_vuP2MwCA5WHTgtOcRxv2z2OWgrCOMNRMrtXfRgu8prEvGPdU8m1FQZuq2GGO9mwf9V3G2P-tpq3EyI38VgdXz3P9m4F12YzalJFbcGWhS63bNIatrEBeTpuXXnaEexacU0-r6bBkvSSSqPB6QLw8kQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFTU9jfJBO8zCGZqZ-PTjf0oU4c-lTDRR2VFLwwzwda5xTsw-sb1U5SyS4uFthDelltGvM-mvV4Z3TKacz-mrP0tKhrtSGR7PNX1ZLKrGBUY0X_i-RvVvc2LLRmAB2Ztjmh",
        "title": "sciprofiles.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2JLIj8IUjZUwFbSw_Oyewv5Pth7yWxRzJ60dYJy4bnyPXC6l8gc2DL7-vMsIHIkFDgDfdXTb7e42lRhtj3SBqbKpQn8X_ydPCosWqVtv1b8Eucho6ADWrz9NqkHeI",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVedMln7X_mCR_9nxJbXqHFq4kOqbd-MBX3G3VwzB-Z3BeZZn_QdILwg3xrdB3wUZseZxOlQqKJphb9YEXlZjbBMuh80eFv94sYIrS_gh7qrpk6X-0FcM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEC9OVbQcG07azhZjlT-JAxK_8LFQPDDrQkKYSgeKVtY9n4oOEI5SVXtbyH497psu6I_GHJkAUF1AHQ1_Z-bnZP633L2me1TlhIC11PFd_BGZXLU5gcc9mWys4hlnQUEvXrOgoloQ==",
        "title": "federicobolelli.it"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3X-maPucHwxCdY7Z66ioIOfRSsmqdE3Lufqyn2aU_5k-1zMHTpwCZ0BkbDtBvRA8-57Y17HEgPtpkRd97giNA-KPG0fh3o1kAG1fdYPS8kTXfuG27sKmAcUPQcrX4LJu-xl-UccOL",
        "title": "codabench.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKM9LZg1lLFDaCNTpUQzICptVmMc0yKhql7FsDQ0VVeCq2_hLKUhhvrJBx0m0Rtd0XAQC0xd9PqLgEj1RfWe8tXFiGQkGwHub5tXatDd74GonzZEBt71ZtG9fiAPWYhAIo4S6R-8jJQpTY3u1EeRsCTb4s07sQDB2ROvJCv93uyOD58CiIB7ULI1GZoK0WHFRhgPpuDGQkE7sxTP9zq6EAYNzn7XMpCCdxPkAGNCYCr7EjbjfKlQc7AXp-Is43HkgQDgWKt04aC2qqa-YqOp5vz6mNNAJVLIX3RIYAXw==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "MOFO (Multi-Organ Foundation) Database",
    "paperLink": "https://ieeexplore.ieee.org/document/10552270",
    "description": "A multi-organ ultrasound database established to evaluate the Multi-Organ Foundation (MOFO) model. It focuses on exploring correlations between multiple organs to improve segmentation robustness.",
    "authors": [
      "Hui Chen",
      "Yong Cai",
      "Cheng Wang",
      "Ling Chen",
      "Biting Zhang",
      "Hong Han",
      "Yulan Guo",
      "Hui Ding",
      "Qing Zhang"
    ],
    "githubLink": "http://code.sonography.ai",
    "itemCount": "7,039 images",
    "source": "Scholar",
    "specs": "Ultrasound images; 10 organs across various anatomical regions; Segmentation tasks",
    "year": "2024",
    "id": "saved-1769659052783-2abx0",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHE4SyDAB718NdTNiQvGQWkzgEkKWaN9sQFAUNFz1GefRY8wPdY3U2YBXk2F3RluxSOVhPIxLVKL47DC5zMx3ctbDfAOVzOByY8LHtgSEVeBPrycVRoPuksG67iB-bE",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEL1C04ywOxSwppeBTO8-O3k99a5D3dyBqPCAIoY0PMJARxZ4Qs_kVKqhIoDEXT8iyoutp_jhamxLBAOTXWRCr8zATdrTNyVdpNEjHut53zcVnagp41nmVkgPC1NEI_vuP2MwCA5WHTgtOcRxv2z2OWgrCOMNRMrtXfRgu8prEvGPdU8m1FQZuq2GGO9mwf9V3G2P-tpq3EyI38VgdXz3P9m4F12YzalJFbcGWhS63bNIatrEBeTpuXXnaEexacU0-r6bBkvSSSqPB6QLw8kQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFTU9jfJBO8zCGZqZ-PTjf0oU4c-lTDRR2VFLwwzwda5xTsw-sb1U5SyS4uFthDelltGvM-mvV4Z3TKacz-mrP0tKhrtSGR7PNX1ZLKrGBUY0X_i-RvVvc2LLRmAB2Ztjmh",
        "title": "sciprofiles.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2JLIj8IUjZUwFbSw_Oyewv5Pth7yWxRzJ60dYJy4bnyPXC6l8gc2DL7-vMsIHIkFDgDfdXTb7e42lRhtj3SBqbKpQn8X_ydPCosWqVtv1b8Eucho6ADWrz9NqkHeI",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFVedMln7X_mCR_9nxJbXqHFq4kOqbd-MBX3G3VwzB-Z3BeZZn_QdILwg3xrdB3wUZseZxOlQqKJphb9YEXlZjbBMuh80eFv94sYIrS_gh7qrpk6X-0FcM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEC9OVbQcG07azhZjlT-JAxK_8LFQPDDrQkKYSgeKVtY9n4oOEI5SVXtbyH497psu6I_GHJkAUF1AHQ1_Z-bnZP633L2me1TlhIC11PFd_BGZXLU5gcc9mWys4hlnQUEvXrOgoloQ==",
        "title": "federicobolelli.it"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3X-maPucHwxCdY7Z66ioIOfRSsmqdE3Lufqyn2aU_5k-1zMHTpwCZ0BkbDtBvRA8-57Y17HEgPtpkRd97giNA-KPG0fh3o1kAG1fdYPS8kTXfuG27sKmAcUPQcrX4LJu-xl-UccOL",
        "title": "codabench.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKM9LZg1lLFDaCNTpUQzICptVmMc0yKhql7FsDQ0VVeCq2_hLKUhhvrJBx0m0Rtd0XAQC0xd9PqLgEj1RfWe8tXFiGQkGwHub5tXatDd74GonzZEBt71ZtG9fiAPWYhAIo4S6R-8jJQpTY3u1EeRsCTb4s07sQDB2ROvJCv93uyOD58CiIB7ULI1GZoK0WHFRhgPpuDGQkE7sxTP9zq6EAYNzn7XMpCCdxPkAGNCYCr7EjbjfKlQc7AXp-Is43HkgQDgWKt04aC2qqa-YqOp5vz6mNNAJVLIX3RIYAXw==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "M3D-Data",
    "paperLink": "https://arxiv.org/abs/2404.00578",
    "description": "A large-scale 3D multi-modal medical dataset designed for various vision-language tasks. It includes M3D-Cap (image-text pairs), M3D-VQA (instruction-response pairs), and M3D-Seg (segmentation). It serves as a foundation for training 3D medical VLMs like M3D-LaMed.",
    "authors": [
      "Fan Bai",
      "Yuxin Du",
      "Tiejun Huang",
      "Max Q-H Meng",
      "Bo Zhao"
    ],
    "githubLink": "https://github.com/BAAI-DCAI/M3D",
    "itemCount": "120,000+ image-text pairs; 662,000+ instruction-response pairs",
    "source": "arXiv",
    "specs": "3D CT/MRI volumes, Text Reports, Instruction Data, Segmentation Masks",
    "year": "2024",
    "id": "saved-1769659127699-3hse8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETtOtKJ1k1xOpn1RGhI_nFL-8KTrfpl4eRhZ8tuEpyicqczlaXYJf3wDO4CRfVdMUklFr1MAw94jvIt9QOdA4L-ew5_FPSrn-khm4cUHIqhWMPbHoWTSvRd9I0rV0UCIu7eTtVqU78FF4AyqLZHThDi4zhD-etj47tMRMu1czdXTI4",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn1dEgd4ATbM2i1AJdXE3GT66WwEmDM7MZP-5uAsiBxJGKyEh8nV450Uo2BZRgWKEA9s1GAOH_JlBsrV5JSFglzRxoMXMyOkQLgwK1Fj4xJ06p6hM0GcVhG035",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFixmmSudjAuNH1mYwn7IO4pJ-MuLXoDfPq3h9yBdW5-US9o_nxjPhwZrCiKnMrDwgV1jpa__IoAr2n3SvW0hujC-7oKB0byaakdbmgoDPFZWJvBxz64b6l-dRUNrT2W2zhCsjZOroTpD7Hs7RKPaKjakhM54fwbw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuMb4yR-8k6NvpYgFRLwA6qKoQsMEiQPEcOM37ytNIrtEC2N2eKMeAj0h_PXJJJEws7e7C-olhEJZ3R61Coz73DmHCMhp5fqSAE_vOWFxBsX3YGPBIAdIiSCCvvTN5cr4NU1AVRzx0eDP6jsGBoelSbyDrQR0iKqjmjyK_jlwsEsG_J_6IKzIJxfpTWWJJEJK9pgwW1hsCuPLIESQosrjYlEufb1hR5E-5av8SlAORAWefiFWcNvTQaj3z79SopZp7e-GusfhFOdmSI7Qj_2gWK9t_RF5NQBiE_P8m",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtwDVHpd_YPG6slObaRodCdei4A7GWps2kX_2O4_F-doWG3Mmjyo2c1EDiVAjj-bI2bWgN0fQEKoEbhzumuww0nkKAEar7DhfpjujlgjFtDA3XYD6iXyg0jaQVf-ck",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMG6MRUZxRP_1XfbmT2WLFH93WclISS8fbI6YnARw5Nyil5576lAY_RgOg8dXra_Ct9olDm5PSLAJS-B8OGcGtmGxK2DvBs1SREeWp2e14wXfUjrI8hilzcrruDsdN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsH9d810Lm8xqsY94eI9iie0Ti_-Imsk8wz0nQO9cEWBMj4-c_RIYgkZsd4Zj9nncy0REgf0EfAT_8KPAP7lyQVjPw_NSdDdSCkMVJHZIKQFPjJXFa3ov0c7Zs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3i6vUHUwEAI5drX0faONaoht774Z8EppN4DqpduSeV-AYh-jpv_MJk5zN8w3xwKg28h2Fl0vGjjvfFmM2HmQ_V7VhcSYHxNaYr4YeHAlBp7F73F1kdVOSx77A_7WoATdFINM1mWbauUxJB4aD",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEepUfspOCTIbtuRNxLg204BmGLHy-AHbHDbkGZt5nS9ZUXa71YDxdkidaGaFhm8E3Fp7nCKSx5Py6P1MbeS2N4foCuh9AhrI2Ul5LBKjvnzDk_akT9_4yJdl_T0hd8",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtLUJ9_xaf6Kbn5artnwCxKwbepLlwwzW8fDgWjEFnEHpatS6D_aYQ_LRjh0ADO7ArHjYRpZfbLLHWk-Q4CCfA4BuBm8bLgOdUs0krS021fcYgRgbm_IHjXbtKWJhiVDN5X1K9_4iAgQ1BflW83r42LeE1fa9grOiVFfUtkHYWWQfwMSy7_se72nZfJxxGUe9bmdQVHKuRYV7hTGXLYGenvW_cr_pAC5h6PL4-vvyUEgNeqG9PR_9pR0EDGH97ofBkA98L",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9TURkWJN6QFuNnxALzorY5BPNq-VHMK40EqXjAISIimV29R0C-g9TvfL1xhMGhqZsSS-iByrWAoVFrJ7rsEChwxPg_EzZM3ZZ3UEYDxhloWecviUeoY6cbAV26ROwTxrjGaRnMOWt7FZk",
        "title": "ismrm.org"
      }
    ]
  },
  {
    "title": "BIMCV-R",
    "paperLink": "https://arxiv.org/abs/2403.15992",
    "description": "A landmark dataset specifically curated for 3D CT text-image retrieval. It contains thousands of 3D CT volumes paired with radiological reports, enabling the evaluation of retrieval systems (e.g., MedFinder) in 3D medical imaging.",
    "authors": [
      "Yinda Chen",
      "Che Liu",
      "Xiaoyu Liu",
      "Rossella Arcucci",
      "Zhiwei Xiong"
    ],
    "githubLink": "https://github.com/M3DV/BIMCV-R",
    "itemCount": "8,069 3D CT volumes (>2 million slices)",
    "source": "arXiv",
    "specs": "3D CT volumes, Radiology Reports",
    "year": "2024",
    "id": "saved-1769659127699-61gfw",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETtOtKJ1k1xOpn1RGhI_nFL-8KTrfpl4eRhZ8tuEpyicqczlaXYJf3wDO4CRfVdMUklFr1MAw94jvIt9QOdA4L-ew5_FPSrn-khm4cUHIqhWMPbHoWTSvRd9I0rV0UCIu7eTtVqU78FF4AyqLZHThDi4zhD-etj47tMRMu1czdXTI4",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn1dEgd4ATbM2i1AJdXE3GT66WwEmDM7MZP-5uAsiBxJGKyEh8nV450Uo2BZRgWKEA9s1GAOH_JlBsrV5JSFglzRxoMXMyOkQLgwK1Fj4xJ06p6hM0GcVhG035",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFixmmSudjAuNH1mYwn7IO4pJ-MuLXoDfPq3h9yBdW5-US9o_nxjPhwZrCiKnMrDwgV1jpa__IoAr2n3SvW0hujC-7oKB0byaakdbmgoDPFZWJvBxz64b6l-dRUNrT2W2zhCsjZOroTpD7Hs7RKPaKjakhM54fwbw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuMb4yR-8k6NvpYgFRLwA6qKoQsMEiQPEcOM37ytNIrtEC2N2eKMeAj0h_PXJJJEws7e7C-olhEJZ3R61Coz73DmHCMhp5fqSAE_vOWFxBsX3YGPBIAdIiSCCvvTN5cr4NU1AVRzx0eDP6jsGBoelSbyDrQR0iKqjmjyK_jlwsEsG_J_6IKzIJxfpTWWJJEJK9pgwW1hsCuPLIESQosrjYlEufb1hR5E-5av8SlAORAWefiFWcNvTQaj3z79SopZp7e-GusfhFOdmSI7Qj_2gWK9t_RF5NQBiE_P8m",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtwDVHpd_YPG6slObaRodCdei4A7GWps2kX_2O4_F-doWG3Mmjyo2c1EDiVAjj-bI2bWgN0fQEKoEbhzumuww0nkKAEar7DhfpjujlgjFtDA3XYD6iXyg0jaQVf-ck",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMG6MRUZxRP_1XfbmT2WLFH93WclISS8fbI6YnARw5Nyil5576lAY_RgOg8dXra_Ct9olDm5PSLAJS-B8OGcGtmGxK2DvBs1SREeWp2e14wXfUjrI8hilzcrruDsdN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsH9d810Lm8xqsY94eI9iie0Ti_-Imsk8wz0nQO9cEWBMj4-c_RIYgkZsd4Zj9nncy0REgf0EfAT_8KPAP7lyQVjPw_NSdDdSCkMVJHZIKQFPjJXFa3ov0c7Zs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3i6vUHUwEAI5drX0faONaoht774Z8EppN4DqpduSeV-AYh-jpv_MJk5zN8w3xwKg28h2Fl0vGjjvfFmM2HmQ_V7VhcSYHxNaYr4YeHAlBp7F73F1kdVOSx77A_7WoATdFINM1mWbauUxJB4aD",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEepUfspOCTIbtuRNxLg204BmGLHy-AHbHDbkGZt5nS9ZUXa71YDxdkidaGaFhm8E3Fp7nCKSx5Py6P1MbeS2N4foCuh9AhrI2Ul5LBKjvnzDk_akT9_4yJdl_T0hd8",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtLUJ9_xaf6Kbn5artnwCxKwbepLlwwzW8fDgWjEFnEHpatS6D_aYQ_LRjh0ADO7ArHjYRpZfbLLHWk-Q4CCfA4BuBm8bLgOdUs0krS021fcYgRgbm_IHjXbtKWJhiVDN5X1K9_4iAgQ1BflW83r42LeE1fa9grOiVFfUtkHYWWQfwMSy7_se72nZfJxxGUe9bmdQVHKuRYV7hTGXLYGenvW_cr_pAC5h6PL4-vvyUEgNeqG9PR_9pR0EDGH97ofBkA98L",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9TURkWJN6QFuNnxALzorY5BPNq-VHMK40EqXjAISIimV29R0C-g9TvfL1xhMGhqZsSS-iByrWAoVFrJ7rsEChwxPg_EzZM3ZZ3UEYDxhloWecviUeoY6cbAV26ROwTxrjGaRnMOWt7FZk",
        "title": "ismrm.org"
      }
    ]
  },
  {
    "title": "RadGenome-Brain MRI (AutoRG-Brain)",
    "paperLink": "https://arxiv.org/abs/2407.03321",
    "description": "A grounded report generation benchmark for Brain MRI. It aggregates data from multiple public datasets and adds reference reports and segmentation masks to enable pixel-level grounded visual clues for report generation systems.",
    "authors": [
      "Jiayu Lei",
      "Xiaoman Zhang",
      "Chaoyi Wu",
      "Yanfeng Wang",
      "Weidi Xie",
      "et al."
    ],
    "githubLink": "https://github.com/Deep-Metrics-Lab/AutoRG-Brain",
    "itemCount": "Comprehensive aggregation of 5 public datasets",
    "source": "arXiv",
    "specs": "3D Brain MRI, Text Reports, Segmentation Masks",
    "year": "2024",
    "id": "saved-1769659127699-2jtcm",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETtOtKJ1k1xOpn1RGhI_nFL-8KTrfpl4eRhZ8tuEpyicqczlaXYJf3wDO4CRfVdMUklFr1MAw94jvIt9QOdA4L-ew5_FPSrn-khm4cUHIqhWMPbHoWTSvRd9I0rV0UCIu7eTtVqU78FF4AyqLZHThDi4zhD-etj47tMRMu1czdXTI4",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn1dEgd4ATbM2i1AJdXE3GT66WwEmDM7MZP-5uAsiBxJGKyEh8nV450Uo2BZRgWKEA9s1GAOH_JlBsrV5JSFglzRxoMXMyOkQLgwK1Fj4xJ06p6hM0GcVhG035",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFixmmSudjAuNH1mYwn7IO4pJ-MuLXoDfPq3h9yBdW5-US9o_nxjPhwZrCiKnMrDwgV1jpa__IoAr2n3SvW0hujC-7oKB0byaakdbmgoDPFZWJvBxz64b6l-dRUNrT2W2zhCsjZOroTpD7Hs7RKPaKjakhM54fwbw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuMb4yR-8k6NvpYgFRLwA6qKoQsMEiQPEcOM37ytNIrtEC2N2eKMeAj0h_PXJJJEws7e7C-olhEJZ3R61Coz73DmHCMhp5fqSAE_vOWFxBsX3YGPBIAdIiSCCvvTN5cr4NU1AVRzx0eDP6jsGBoelSbyDrQR0iKqjmjyK_jlwsEsG_J_6IKzIJxfpTWWJJEJK9pgwW1hsCuPLIESQosrjYlEufb1hR5E-5av8SlAORAWefiFWcNvTQaj3z79SopZp7e-GusfhFOdmSI7Qj_2gWK9t_RF5NQBiE_P8m",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtwDVHpd_YPG6slObaRodCdei4A7GWps2kX_2O4_F-doWG3Mmjyo2c1EDiVAjj-bI2bWgN0fQEKoEbhzumuww0nkKAEar7DhfpjujlgjFtDA3XYD6iXyg0jaQVf-ck",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMG6MRUZxRP_1XfbmT2WLFH93WclISS8fbI6YnARw5Nyil5576lAY_RgOg8dXra_Ct9olDm5PSLAJS-B8OGcGtmGxK2DvBs1SREeWp2e14wXfUjrI8hilzcrruDsdN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsH9d810Lm8xqsY94eI9iie0Ti_-Imsk8wz0nQO9cEWBMj4-c_RIYgkZsd4Zj9nncy0REgf0EfAT_8KPAP7lyQVjPw_NSdDdSCkMVJHZIKQFPjJXFa3ov0c7Zs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3i6vUHUwEAI5drX0faONaoht774Z8EppN4DqpduSeV-AYh-jpv_MJk5zN8w3xwKg28h2Fl0vGjjvfFmM2HmQ_V7VhcSYHxNaYr4YeHAlBp7F73F1kdVOSx77A_7WoATdFINM1mWbauUxJB4aD",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEepUfspOCTIbtuRNxLg204BmGLHy-AHbHDbkGZt5nS9ZUXa71YDxdkidaGaFhm8E3Fp7nCKSx5Py6P1MbeS2N4foCuh9AhrI2Ul5LBKjvnzDk_akT9_4yJdl_T0hd8",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtLUJ9_xaf6Kbn5artnwCxKwbepLlwwzW8fDgWjEFnEHpatS6D_aYQ_LRjh0ADO7ArHjYRpZfbLLHWk-Q4CCfA4BuBm8bLgOdUs0krS021fcYgRgbm_IHjXbtKWJhiVDN5X1K9_4iAgQ1BflW83r42LeE1fa9grOiVFfUtkHYWWQfwMSy7_se72nZfJxxGUe9bmdQVHKuRYV7hTGXLYGenvW_cr_pAC5h6PL4-vvyUEgNeqG9PR_9pR0EDGH97ofBkA98L",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9TURkWJN6QFuNnxALzorY5BPNq-VHMK40EqXjAISIimV29R0C-g9TvfL1xhMGhqZsSS-iByrWAoVFrJ7rsEChwxPg_EzZM3ZZ3UEYDxhloWecviUeoY6cbAV26ROwTxrjGaRnMOWt7FZk",
        "title": "ismrm.org"
      }
    ]
  },
  {
    "title": "INSPECT",
    "paperLink": "https://arxiv.org/abs/2311.10726",
    "description": "A large-scale multimodal dataset for Pulmonary Embolism diagnosis and prognosis. It integrates 3D CT Pulmonary Angiography (CTPA) scans with radiology report impressions and structured Electronic Health Records (EHR) from a large patient cohort.",
    "authors": [
      "Adriel Saporta",
      "Alexandre Guihry",
      "Pranav Rajpurkar",
      "et al."
    ],
    "githubLink": "https://stanfordaimi.azurewebsites.net/datasets/inspect",
    "itemCount": "20,078 CT scans; 19,438 patients",
    "source": "Google Scholar",
    "specs": "3D CTPA, Radiology Reports (Impressions), EHR Data",
    "year": "2023",
    "id": "saved-1769659127699-pmag8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETtOtKJ1k1xOpn1RGhI_nFL-8KTrfpl4eRhZ8tuEpyicqczlaXYJf3wDO4CRfVdMUklFr1MAw94jvIt9QOdA4L-ew5_FPSrn-khm4cUHIqhWMPbHoWTSvRd9I0rV0UCIu7eTtVqU78FF4AyqLZHThDi4zhD-etj47tMRMu1czdXTI4",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn1dEgd4ATbM2i1AJdXE3GT66WwEmDM7MZP-5uAsiBxJGKyEh8nV450Uo2BZRgWKEA9s1GAOH_JlBsrV5JSFglzRxoMXMyOkQLgwK1Fj4xJ06p6hM0GcVhG035",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFixmmSudjAuNH1mYwn7IO4pJ-MuLXoDfPq3h9yBdW5-US9o_nxjPhwZrCiKnMrDwgV1jpa__IoAr2n3SvW0hujC-7oKB0byaakdbmgoDPFZWJvBxz64b6l-dRUNrT2W2zhCsjZOroTpD7Hs7RKPaKjakhM54fwbw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuMb4yR-8k6NvpYgFRLwA6qKoQsMEiQPEcOM37ytNIrtEC2N2eKMeAj0h_PXJJJEws7e7C-olhEJZ3R61Coz73DmHCMhp5fqSAE_vOWFxBsX3YGPBIAdIiSCCvvTN5cr4NU1AVRzx0eDP6jsGBoelSbyDrQR0iKqjmjyK_jlwsEsG_J_6IKzIJxfpTWWJJEJK9pgwW1hsCuPLIESQosrjYlEufb1hR5E-5av8SlAORAWefiFWcNvTQaj3z79SopZp7e-GusfhFOdmSI7Qj_2gWK9t_RF5NQBiE_P8m",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtwDVHpd_YPG6slObaRodCdei4A7GWps2kX_2O4_F-doWG3Mmjyo2c1EDiVAjj-bI2bWgN0fQEKoEbhzumuww0nkKAEar7DhfpjujlgjFtDA3XYD6iXyg0jaQVf-ck",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHMG6MRUZxRP_1XfbmT2WLFH93WclISS8fbI6YnARw5Nyil5576lAY_RgOg8dXra_Ct9olDm5PSLAJS-B8OGcGtmGxK2DvBs1SREeWp2e14wXfUjrI8hilzcrruDsdN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEsH9d810Lm8xqsY94eI9iie0Ti_-Imsk8wz0nQO9cEWBMj4-c_RIYgkZsd4Zj9nncy0REgf0EfAT_8KPAP7lyQVjPw_NSdDdSCkMVJHZIKQFPjJXFa3ov0c7Zs",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3i6vUHUwEAI5drX0faONaoht774Z8EppN4DqpduSeV-AYh-jpv_MJk5zN8w3xwKg28h2Fl0vGjjvfFmM2HmQ_V7VhcSYHxNaYr4YeHAlBp7F73F1kdVOSx77A_7WoATdFINM1mWbauUxJB4aD",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEepUfspOCTIbtuRNxLg204BmGLHy-AHbHDbkGZt5nS9ZUXa71YDxdkidaGaFhm8E3Fp7nCKSx5Py6P1MbeS2N4foCuh9AhrI2Ul5LBKjvnzDk_akT9_4yJdl_T0hd8",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEtLUJ9_xaf6Kbn5artnwCxKwbepLlwwzW8fDgWjEFnEHpatS6D_aYQ_LRjh0ADO7ArHjYRpZfbLLHWk-Q4CCfA4BuBm8bLgOdUs0krS021fcYgRgbm_IHjXbtKWJhiVDN5X1K9_4iAgQ1BflW83r42LeE1fa9grOiVFfUtkHYWWQfwMSy7_se72nZfJxxGUe9bmdQVHKuRYV7hTGXLYGenvW_cr_pAC5h6PL4-vvyUEgNeqG9PR_9pR0EDGH97ofBkA98L",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9TURkWJN6QFuNnxALzorY5BPNq-VHMK40EqXjAISIimV29R0C-g9TvfL1xhMGhqZsSS-iByrWAoVFrJ7rsEChwxPg_EzZM3ZZ3UEYDxhloWecviUeoY6cbAV26ROwTxrjGaRnMOWt7FZk",
        "title": "ismrm.org"
      }
    ]
  },
  {
    "title": "HMM-RT (Heterogenous Multi-Cohort Radiotherapy Planning)",
    "paperLink": "https://huggingface.co/datasets/Jungle15/Radiotherapy_HaN_Lung_AIRTP",
    "description": "A dataset introduced for the GDP-HMM dose prediction challenge. It contains heterogeneous multi-site radiotherapy planning data including CT, RTSTRUCT, and dose information.",
    "authors": [
      "Riqiang Gao",
      "et al."
    ],
    "githubLink": "https://huggingface.co/datasets/Jungle15/Radiotherapy_HaN_Lung_AIRTP",
    "itemCount": "Multi-cohort (Scale not fully specified)",
    "source": "Hugging Face",
    "specs": "DICOM (CT, RTSTRUCT, RTPLAN, RTDOSE)",
    "year": "2025",
    "id": "saved-1769659275115-i7nmf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQPPGOIZHTUYPGV-TDqWhvxqNi0OIGs3RLp0xXo7TPQ7O4rLqQV1v8x5TeLpb53bLqGFPYGF5HrIiZ9KGL_ZZX0XAKK1p2zADqHrJN9wiFdAVyCehq-v_Y4MP9PthHZEg1",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "MAMA-MIA Breast DCE-MRI Dataset",
    "paperLink": "https://doi.org/10.1038/s41597-025-04423-3",
    "description": "A large-scale multi-center benchmark for breast cancer segmentation and treatment response prediction (pCR) using DCE-MRI, relevant for radiation therapy planning and neoadjuvant therapy monitoring.",
    "authors": [
      "Lidia Garrucho",
      "et al."
    ],
    "githubLink": "https://zenodo.org/records/10636267",
    "itemCount": "1506 cases",
    "source": "Semantic Scholar",
    "specs": "DCE-MRI, Segmentation Masks, Clinical Metadata",
    "year": "2025",
    "id": "saved-1769659275115-gox4m",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQPPGOIZHTUYPGV-TDqWhvxqNi0OIGs3RLp0xXo7TPQ7O4rLqQV1v8x5TeLpb53bLqGFPYGF5HrIiZ9KGL_ZZX0XAKK1p2zADqHrJN9wiFdAVyCehq-v_Y4MP9PthHZEg1",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "HNTS-MRG 2024 Dataset",
    "paperLink": "https://arxiv.org/abs/2412.19119",
    "description": "Benchmark dataset for the Head and Neck Tumor Segmentation for MR-Guided Applications challenge. It focuses on segmenting primary tumors and lymph nodes in pre- and mid-radiotherapy MRI scans to support adaptive radiotherapy.",
    "authors": [
      "Clifton D. Fuller",
      "Mohamed A. Naser",
      "Kareem A. Wahid",
      "et al."
    ],
    "githubLink": "https://hntsmrg24.grand-challenge.org/",
    "itemCount": "150 training cases",
    "source": "Grand Challenge",
    "specs": "3D T2-weighted MRI (Pre-RT and Mid-RT), Segmentation Masks",
    "year": "2024",
    "id": "saved-1769659275116-lzfpc",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQPPGOIZHTUYPGV-TDqWhvxqNi0OIGs3RLp0xXo7TPQ7O4rLqQV1v8x5TeLpb53bLqGFPYGF5HrIiZ9KGL_ZZX0XAKK1p2zADqHrJN9wiFdAVyCehq-v_Y4MP9PthHZEg1",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "SynthRAD2023 Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2406.01568",
    "description": "A large multi-center dataset for benchmarking synthetic CT (sCT) generation methods from MRI and CBCT. Crucial for MRI-only radiotherapy planning workflows.",
    "authors": [
      "Christopher Thummerer",
      "et al."
    ],
    "githubLink": "https://synthrad2023.grand-challenge.org/",
    "itemCount": "1080 patients",
    "source": "arXiv",
    "specs": "Paired MRI, CBCT, and CT images (Brain and Pelvis)",
    "year": "2024",
    "id": "saved-1769659275116-3pmwi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQPPGOIZHTUYPGV-TDqWhvxqNi0OIGs3RLp0xXo7TPQ7O4rLqQV1v8x5TeLpb53bLqGFPYGF5HrIiZ9KGL_ZZX0XAKK1p2zADqHrJN9wiFdAVyCehq-v_Y4MP9PthHZEg1",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "Weekly Intra-Treatment DWI Dataset (Head and Neck)",
    "paperLink": "https://doi.org/10.1038/s41597-024-03217-z",
    "description": "A longitudinal dataset containing weekly intra-treatment diffusion-weighted imaging (DWI) scans for head and neck cancer patients undergoing MR-Linac treatment, enabling analysis of treatment response.",
    "authors": [
      "Dina M. El-Habashy",
      "Kareem A. Wahid",
      "et al."
    ],
    "githubLink": "https://doi.org/10.57967/hf/1234 (Data available via TCIA/Zenodo)",
    "itemCount": "Variable (longitudinal data for H&N patients)",
    "source": "Nature Scientific Data",
    "specs": "Weekly DWI MRI, ADC maps, Clinical outcomes",
    "year": "2024",
    "id": "saved-1769659275116-7kohi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQPPGOIZHTUYPGV-TDqWhvxqNi0OIGs3RLp0xXo7TPQ7O4rLqQV1v8x5TeLpb53bLqGFPYGF5HrIiZ9KGL_ZZX0XAKK1p2zADqHrJN9wiFdAVyCehq-v_Y4MP9PthHZEg1",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "Gold Atlas Project (Male Pelvis)",
    "paperLink": "https://doi.org/10.1002/mp.12748",
    "description": "A standard reference dataset for training and validation of segmentation algorithms and synthetic CT generation in MRI-guided radiotherapy for the male pelvis.",
    "authors": [
      "Tufve Nyholm",
      "Stina Svensson",
      "et al."
    ],
    "githubLink": "https://zenodo.org/record/583096",
    "itemCount": "19 patients",
    "source": "Semantic Scholar",
    "specs": "T1/T2 MRI, CT, expert consensus delineations",
    "year": "2018",
    "id": "saved-1769659275116-ej2qg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQPPGOIZHTUYPGV-TDqWhvxqNi0OIGs3RLp0xXo7TPQ7O4rLqQV1v8x5TeLpb53bLqGFPYGF5HrIiZ9KGL_ZZX0XAKK1p2zADqHrJN9wiFdAVyCehq-v_Y4MP9PthHZEg1",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "SMILE-UHURA Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2411.09593",
    "description": "Focused on small vessel segmentation at a mesoscopic scale using ultra-high resolution 7T Magnetic Resonance Angiograms (MRA). The dataset addresses the lack of annotated data for high-field MRI vessel segmentation, featuring automated pre-segmentation with extensive manual refinement.",
    "authors": [
      "Soumick Chatterjee",
      "Hendrik Mattern",
      "Andreas Nürnberger",
      "Oliver Speck"
    ],
    "githubLink": "https://github.com/soumickmj/DS6",
    "itemCount": "14 training volumes (plus secret test sets)",
    "source": "arXiv",
    "specs": "7T TOF-MRA images; Voxel-level segmentation masks; High resolution (mesoscopic scale)",
    "year": "2024",
    "id": "saved-1769659338636-y3d07",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEereGk6cUszXeLkH9hOhyJFhuQ2DLaa5xFY5bHCBVIo0K7hRAgy32xBjvIawJoWR1A2UmvWsau588gycdPirPQSPdJAxlEYDaZxUF1ZLCVPHVav7MouJL7YvpuNKk=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGffT-QV8kCAkAz7bb9drAhS2OYhSltIksXze1Z3HC5Lh-Kshn5NaglRpkky-9zXeAO9laJy2__PAAKSQXHdLRzWmFwkeaAjGXlOptngLBWs2RlaY42mN9CGuGHfOIOmMPPp2mIkC_33WrqgWl9WS55HOdUgx_MpiekqnDHh-JRYIWheg2bNpuDh9toOo60zRUngkGBVOEGs-Oqn2vXhc_z60rlyPev4KLUUp0oxEbjI6KPg7d206R_VP0U-mI5ThDbeFJTjrQBSXJeAUFBEXA=",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHt5sgUNtkV0YsKO4MPG8ae6RhdMA5FaGTIsYn9nfh11jeih9ct7d6AXpag9QMByGBfcXS9KmMvIFefVqUr_alJeQcCe39VkC5mQjtxm0lgmX2qQUEdJ0FbpE8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_Kqwp13GqO7ccil-shnVmhS5cE5edQZ_hDxbapiO7N-44v-tPlPB5JWrcOmngFa-vJtFxIsrLqnC4UGhpzq25eJXYsRezbwiyGEzASVHI-BcHofOLRb1cZTETkRqiUG2Atpw3yBff37RQAjJHvNuGJCR7xIopQrOiVwDAYI0=",
        "title": "biorxiv.org"
      }
    ]
  },
  {
    "title": "TopCoW / TopBrain Challenge Datasets",
    "paperLink": "https://arxiv.org/abs/2312.17670",
    "description": "A benchmark for topology-aware anatomical segmentation of the Circle of Willis (TopCoW) and whole-brain vessel anatomy (TopBrain). TopCoW focuses on 13 vessel components, while TopBrain extends this to over 40 landmark arteries and veins. The dataset pairs MRA and CTA scans from the same patients, providing voxel-level multi-class annotations.",
    "authors": [
      "Kaiyuan Yang",
      "Fabio Musio",
      "Yihui Ma",
      "Norman Juchler",
      "Johannes C. Paetzold",
      "Bjoern Menze"
    ],
    "githubLink": "https://github.com/topcow/TopCoW_Challenge_2023",
    "itemCount": "200 paired MRA/CTA scans (TopCoW); TopBrain adds more training batches",
    "source": "Grand Challenge / Zenodo",
    "specs": "Paired MRA and CTA scans; Voxel-level multiclass segmentation masks; 13 classes (TopCoW) to 40+ classes (TopBrain)",
    "year": "2023",
    "id": "saved-1769659338636-rx0zj",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEereGk6cUszXeLkH9hOhyJFhuQ2DLaa5xFY5bHCBVIo0K7hRAgy32xBjvIawJoWR1A2UmvWsau588gycdPirPQSPdJAxlEYDaZxUF1ZLCVPHVav7MouJL7YvpuNKk=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGffT-QV8kCAkAz7bb9drAhS2OYhSltIksXze1Z3HC5Lh-Kshn5NaglRpkky-9zXeAO9laJy2__PAAKSQXHdLRzWmFwkeaAjGXlOptngLBWs2RlaY42mN9CGuGHfOIOmMPPp2mIkC_33WrqgWl9WS55HOdUgx_MpiekqnDHh-JRYIWheg2bNpuDh9toOo60zRUngkGBVOEGs-Oqn2vXhc_z60rlyPev4KLUUp0oxEbjI6KPg7d206R_VP0U-mI5ThDbeFJTjrQBSXJeAUFBEXA=",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHt5sgUNtkV0YsKO4MPG8ae6RhdMA5FaGTIsYn9nfh11jeih9ct7d6AXpag9QMByGBfcXS9KmMvIFefVqUr_alJeQcCe39VkC5mQjtxm0lgmX2qQUEdJ0FbpE8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_Kqwp13GqO7ccil-shnVmhS5cE5edQZ_hDxbapiO7N-44v-tPlPB5JWrcOmngFa-vJtFxIsrLqnC4UGhpzq25eJXYsRezbwiyGEzASVHI-BcHofOLRb1cZTETkRqiUG2Atpw3yBff37RQAjJHvNuGJCR7xIopQrOiVwDAYI0=",
        "title": "biorxiv.org"
      }
    ]
  },
  {
    "title": "COSTA (IXI-Vessel Ground Truth)",
    "paperLink": "https://doi.org/10.5281/zenodo.11025761",
    "description": "A derived dataset enabling the use of the popular IXI dataset for vessel segmentation. COSTA provides 'silver standard' vessel segmentation and topology ground truth for hundreds of subjects from the IXI dataset (acquired at Guy's, Hammersmith, and IOP), generated using a consensus of multi-scale filtering and deep learning methods.",
    "authors": [
      "Yue Zhang",
      "Minyan Zeng",
      "Nian Wang"
    ],
    "githubLink": "https://github.com/jsh0551/COSTA",
    "itemCount": "423 TOF-MRA images (IXI subset)",
    "source": "Zenodo",
    "specs": "TOF-MRA (1.5T and 3T); Binary vessel segmentation masks; Skeleton/Topology maps",
    "year": "2023",
    "id": "saved-1769659338636-dnviu",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEereGk6cUszXeLkH9hOhyJFhuQ2DLaa5xFY5bHCBVIo0K7hRAgy32xBjvIawJoWR1A2UmvWsau588gycdPirPQSPdJAxlEYDaZxUF1ZLCVPHVav7MouJL7YvpuNKk=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGffT-QV8kCAkAz7bb9drAhS2OYhSltIksXze1Z3HC5Lh-Kshn5NaglRpkky-9zXeAO9laJy2__PAAKSQXHdLRzWmFwkeaAjGXlOptngLBWs2RlaY42mN9CGuGHfOIOmMPPp2mIkC_33WrqgWl9WS55HOdUgx_MpiekqnDHh-JRYIWheg2bNpuDh9toOo60zRUngkGBVOEGs-Oqn2vXhc_z60rlyPev4KLUUp0oxEbjI6KPg7d206R_VP0U-mI5ThDbeFJTjrQBSXJeAUFBEXA=",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHt5sgUNtkV0YsKO4MPG8ae6RhdMA5FaGTIsYn9nfh11jeih9ct7d6AXpag9QMByGBfcXS9KmMvIFefVqUr_alJeQcCe39VkC5mQjtxm0lgmX2qQUEdJ0FbpE8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_Kqwp13GqO7ccil-shnVmhS5cE5edQZ_hDxbapiO7N-44v-tPlPB5JWrcOmngFa-vJtFxIsrLqnC4UGhpzq25eJXYsRezbwiyGEzASVHI-BcHofOLRb1cZTETkRqiUG2Atpw3yBff37RQAjJHvNuGJCR7xIopQrOiVwDAYI0=",
        "title": "biorxiv.org"
      }
    ]
  },
  {
    "title": "VesselGraph (Murine Whole Brain)",
    "paperLink": "https://arxiv.org/abs/2108.13233",
    "description": "A benchmark for graph learning on whole-brain vessel graphs. While primarily based on murine (mouse) brain data derived from tissue clearing and light-sheet microscopy, it is a key benchmark for 'whole brain' vessel topology analysis, providing large-scale spatial graphs with millions of nodes.",
    "authors": [
      "Johannes C. Paetzold",
      "Julian McGinnis",
      "Suprosanna Shit",
      "Bjoern Menze"
    ],
    "githubLink": "https://github.com/jocpae/VesselGraph",
    "itemCount": "Whole-brain vessel graphs (millions of edges)",
    "source": "arXiv",
    "specs": "Graph data (nodes/edges) compatible with OGB and PyTorch Geometric; Derived from fluorescence microscopy",
    "year": "2021",
    "id": "saved-1769659338636-mz43m",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEereGk6cUszXeLkH9hOhyJFhuQ2DLaa5xFY5bHCBVIo0K7hRAgy32xBjvIawJoWR1A2UmvWsau588gycdPirPQSPdJAxlEYDaZxUF1ZLCVPHVav7MouJL7YvpuNKk=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGffT-QV8kCAkAz7bb9drAhS2OYhSltIksXze1Z3HC5Lh-Kshn5NaglRpkky-9zXeAO9laJy2__PAAKSQXHdLRzWmFwkeaAjGXlOptngLBWs2RlaY42mN9CGuGHfOIOmMPPp2mIkC_33WrqgWl9WS55HOdUgx_MpiekqnDHh-JRYIWheg2bNpuDh9toOo60zRUngkGBVOEGs-Oqn2vXhc_z60rlyPev4KLUUp0oxEbjI6KPg7d206R_VP0U-mI5ThDbeFJTjrQBSXJeAUFBEXA=",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHt5sgUNtkV0YsKO4MPG8ae6RhdMA5FaGTIsYn9nfh11jeih9ct7d6AXpag9QMByGBfcXS9KmMvIFefVqUr_alJeQcCe39VkC5mQjtxm0lgmX2qQUEdJ0FbpE8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_Kqwp13GqO7ccil-shnVmhS5cE5edQZ_hDxbapiO7N-44v-tPlPB5JWrcOmngFa-vJtFxIsrLqnC4UGhpzq25eJXYsRezbwiyGEzASVHI-BcHofOLRb1cZTETkRqiUG2Atpw3yBff37RQAjJHvNuGJCR7xIopQrOiVwDAYI0=",
        "title": "biorxiv.org"
      }
    ]
  },
  {
    "title": "TubeTK (Tubular Object Extraction Toolkit) Dataset",
    "paperLink": "https://public.kitware.com/Wiki/TubeTK/Data",
    "description": "A large collection of Magnetic Resonance Angiography (MRA) images from healthy subjects, designed to test tubular object extraction algorithms. It includes a subset with expert-annotated vessel centerlines and radii (often referred to in the context of the Bullitt et al. studies).",
    "authors": [
      "Stephen R. Aylward",
      "Elizabeth Bullitt",
      "Julien Jomier",
      "D. Pace"
    ],
    "githubLink": "https://github.com/InsightSoftwareConsortium/ITKTubeTK",
    "itemCount": "109 MRA scans (healthy volunteers); 42 with detailed vascular network annotations",
    "source": "Kitware / MIDAS",
    "specs": "MRA images; Spatial graphs (.tre files) containing vessel centerlines and radii",
    "year": "2013",
    "id": "saved-1769659338636-1ugo4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEereGk6cUszXeLkH9hOhyJFhuQ2DLaa5xFY5bHCBVIo0K7hRAgy32xBjvIawJoWR1A2UmvWsau588gycdPirPQSPdJAxlEYDaZxUF1ZLCVPHVav7MouJL7YvpuNKk=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGffT-QV8kCAkAz7bb9drAhS2OYhSltIksXze1Z3HC5Lh-Kshn5NaglRpkky-9zXeAO9laJy2__PAAKSQXHdLRzWmFwkeaAjGXlOptngLBWs2RlaY42mN9CGuGHfOIOmMPPp2mIkC_33WrqgWl9WS55HOdUgx_MpiekqnDHh-JRYIWheg2bNpuDh9toOo60zRUngkGBVOEGs-Oqn2vXhc_z60rlyPev4KLUUp0oxEbjI6KPg7d206R_VP0U-mI5ThDbeFJTjrQBSXJeAUFBEXA=",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHt5sgUNtkV0YsKO4MPG8ae6RhdMA5FaGTIsYn9nfh11jeih9ct7d6AXpag9QMByGBfcXS9KmMvIFefVqUr_alJeQcCe39VkC5mQjtxm0lgmX2qQUEdJ0FbpE8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_Kqwp13GqO7ccil-shnVmhS5cE5edQZ_hDxbapiO7N-44v-tPlPB5JWrcOmngFa-vJtFxIsrLqnC4UGhpzq25eJXYsRezbwiyGEzASVHI-BcHofOLRb1cZTETkRqiUG2Atpw3yBff37RQAjJHvNuGJCR7xIopQrOiVwDAYI0=",
        "title": "biorxiv.org"
      }
    ]
  },
  {
    "title": "BraVa / Bravissima",
    "paperLink": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3732313/",
    "description": "A database of digital reconstructions of the human brain arterial arborizations from healthy adult subjects. The 'Bravissima' project later converted these spatial graph reconstructions into standard NIfTI formats for broader usage in neuroimaging research.",
    "authors": [
      "L. Wright",
      "J. Mut",
      "A. L. Silva",
      "G. Ascoli"
    ],
    "githubLink": "http://cng.gmu.edu/brava",
    "itemCount": "61 subjects",
    "source": "NITRC / Scholar",
    "specs": "Digital morphological reconstructions (SWC/Graph format); Converted NIfTI density maps",
    "year": "2013",
    "id": "saved-1769659338636-sxdav",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEereGk6cUszXeLkH9hOhyJFhuQ2DLaa5xFY5bHCBVIo0K7hRAgy32xBjvIawJoWR1A2UmvWsau588gycdPirPQSPdJAxlEYDaZxUF1ZLCVPHVav7MouJL7YvpuNKk=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGffT-QV8kCAkAz7bb9drAhS2OYhSltIksXze1Z3HC5Lh-Kshn5NaglRpkky-9zXeAO9laJy2__PAAKSQXHdLRzWmFwkeaAjGXlOptngLBWs2RlaY42mN9CGuGHfOIOmMPPp2mIkC_33WrqgWl9WS55HOdUgx_MpiekqnDHh-JRYIWheg2bNpuDh9toOo60zRUngkGBVOEGs-Oqn2vXhc_z60rlyPev4KLUUp0oxEbjI6KPg7d206R_VP0U-mI5ThDbeFJTjrQBSXJeAUFBEXA=",
        "title": "semanticscholar.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHt5sgUNtkV0YsKO4MPG8ae6RhdMA5FaGTIsYn9nfh11jeih9ct7d6AXpag9QMByGBfcXS9KmMvIFefVqUr_alJeQcCe39VkC5mQjtxm0lgmX2qQUEdJ0FbpE8=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_Kqwp13GqO7ccil-shnVmhS5cE5edQZ_hDxbapiO7N-44v-tPlPB5JWrcOmngFa-vJtFxIsrLqnC4UGhpzq25eJXYsRezbwiyGEzASVHI-BcHofOLRb1cZTETkRqiUG2Atpw3yBff37RQAjJHvNuGJCR7xIopQrOiVwDAYI0=",
        "title": "biorxiv.org"
      }
    ]
  },
  {
    "title": "SynthRAD2025 Grand Challenge Dataset",
    "paperLink": "https://doi.org/10.5281/zenodo.14918089",
    "description": "An extension of the SynthRAD benchmark featuring a larger and more diverse cohort for advanced synthetic CT generation. It includes 2362 cases (890 MRI-CT and 1472 CBCT-CT pairs) covering Head & Neck, Thorax, and Abdomen regions, collected from five European university medical centers. This dataset supports research into MRI-only and adaptive radiotherapy workflows.",
    "authors": [
      "Adrian Thummerer",
      "Matteo Maspero",
      "SynthRAD Consortium"
    ],
    "githubLink": "https://github.com/SynthRAD2025/preprocessing",
    "itemCount": "2362 image pairs",
    "source": "Hugging Face",
    "specs": "MetaImage (.mha) format; MRI-CT and CBCT-CT pairs; Head & Neck, Thorax, Abdomen",
    "year": "2025",
    "id": "saved-1769659402160-vsgv4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBtijk5YK3Q1EyBicX4R6Otj3LAZDsTxsEoaRTB9EnhGujoZP8hWnvSFoP64tncfa9Z9M2_5gO1omhpv6ecpF627OAFBD0ADBGn6TKJJvRgAGtkrlKnqqbNm0O_BQ486KLkgPS1dvyRFfiBvPw",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpUID7ji17wVTa1qhnvqpl_sdBbm9XWh4BGevb5oC0vcT3GUtldYhRt2K-6g7oCKNRMOCPprY9usSOOYrT17qCbjcjmNii2TavwwiqwjhSNY6lidROmbulnpC1sCa19FmQlQc-1FaGH9oYvbBxCK-rJEU0gdPPlKB9OsULiVrGRfkF0HeDefeqrExwpwmX99RizJXVLv_iSCrfLGdH5H6W9AmVWkQxxQMTL8fZ4CBlCZa7mkSJcK4XJ6uNyCU=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEg6loW_QW65_YywPGTgPwwtH8E1dt5Z6HFd5pSTauxvys_h8y8Cur1opGwVjC-L7zCiVePN4dLTxshWI7Ccx4lKxbHk7cqibfHXUmy39Hts1IbbOnO9vI8jQCyuLj8oHjmhMgn",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9HuEz3Ztv1b2cc1LYwQ7tc1S-6OWpnYTvXkZX-F-L8nia3_EmHL3eINmijVHW46CkERlJA-l4ygShBQn3qXUJSpHgE3M5rpYTp2UfiJ97vrLyGk5UinKaOodG_pQB6GjilkflkIsJ6gGX8pHpJwWtUuohf3Wv",
        "title": "scilifelab.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRPX0AXO16l8lzLATDsfMN5iCboH3WxxcPSxajaI1JMBQ0UUMhI36tmhX4hblRU9IGYG4M0P06h-BeyzpfIumpWnPX7dXPSE8l9WzyJtrVvX3PFYnLCR-WdOLo7vJEO_ceAOUnA_VdjTw_iGj-ULkonKQHu3qIcVaC33EkZoZXLyWVu5B8EA8-oNcpUVPYJLOLsSd7OAzNEK6_Rrt4peVF1-RX5hXj5X9xAHmnHhSDpGDTaXn-MzeQb8ZeBhTY9OM=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnshluRE7k3aO1DbFfDrQ6UHEUEISq89tXdVNrzAIFzqhz2hXdSS_fiaPDyE6w7s_ypgVisjdHhknS6n5PNPXCdQ4d7eBaus3OMxGDFrXutrFVOrRWiKg7f7ZxOwxRLPU",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZeLQOcK9RECLvF7drdnbDrOExHrA7dN6FmKS-Y82y7mAt5yhNblfBqoyNKDHPut9farTktiIOxmdtlbD57Nta-BlfmC_mH4bUqSpXYY2tp73_xgOG5ek4aP0xQXSL4MrVrrDp-dUZi-9IyVlwvpQ3nhUNVsVt8MzMafAPNiMFd4dUIa4=",
        "title": "researchdata.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGH8my8vk6lwFdyvujxmGYK30UYu3eW-3kmR_-yyXdw0Ay3Lz0Y0H7hYTpKSAJwkKZrJBaBCQGPBO83N37XRHxHSq0wvVfGsznD2jxTm73onTX5oYqqxjwXKKR13zvjRiisxnOOpy-CIOMxUMQw-tI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3bL-Il_5xuQmdAaaMT2bfMTnRY0jxRYl3LNm8SiogaOu-6X5pBDiBWxMF-wO1WH6hYJxlP9nFO3SE5Pae1X3kkfQ7YFr1HuzXdlOrB7rXYFoNzb-OVG2ddgAkx6efWVanc_6omTw-0PT0",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHd4k8maY_JJVDmJQyTlYdF6Mw4_PaKh-MqS8v3EXtUv6kdYUPdN52Ix8MfkXWppmtX_YraRvMsJ-dTjJsqe7Gsf6b3ksdEhPBxgQopYuHrLPyBW_PfOk2EvloJJw==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMwstA-DMu-ofLFWLBqsxRjxOPvtffk4lkj3F89ut96YkPawhnmAqdB2f8I1AdyqQJri3M-wcdjvVBLoStgRfu8lhIn0zEwwhXLy9AOOwQC4R5ODooZBzaoSJxWFxFnyWQqgIBqIrFXCEOaOAXHxm-PgGf3rC_G7G94dcEfXeAtV-z-QfDFN6uK0o36A==",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHEnOsQZLYKJESQax6071jf_Xq65U_zsdmeQENE5pJwlMbsqC3Uq-PmpmjCcIDJgQDSebCx3dCd4fv3dYwOdVm5KNr1vi8JKcEz4dKM8ArmVWQ3ETj6O0m6uOl6GZObRZmBPvVuYYMZ9KO6MB1F",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8yiueEYAQbXrmzo6R6B8No75JvHVD9lcdobw_QeZ8h2OiIiDajYfVSTKQ2qjKp7Xh0ZKWVxC7DaKDTYRbag7gyv1AnGkb2YC9h49B578QlzqyrYV_xA6LR2CiUM4E_3PuR7vsV8EXYrADE7k=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "LUND-PROBE (LUND Prostate Radiotherapy Open Benchmarking and Evaluation)",
    "paperLink": "https://www.nature.com/articles/s41597-025-04954-5",
    "description": "A comprehensive clinical dataset for MRI-only radiotherapy research. It includes MRI images, synthetic CT (sCT) images, target and organ-at-risk (OAR) segmentations, and radiotherapy dose distributions for 432 prostate cancer patients. An extended subset includes deep learning-generated segmentations and uncertainty maps.",
    "authors": [
      "Viktor Rogowski",
      "Lars E. Olsson",
      "Jonas Scherman",
      "Emilia Persson",
      "Mustafa Kadhim",
      "Christian Jamtheim Gustafsson"
    ],
    "githubLink": "https://github.com/jamtheim/LUND-PROBE",
    "itemCount": "432 patients",
    "source": "Scholar",
    "specs": "NIfTI format; MRI, sCT, Dose, Structures; Prostate anatomy",
    "year": "2025",
    "id": "saved-1769659402160-kg3oj",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBtijk5YK3Q1EyBicX4R6Otj3LAZDsTxsEoaRTB9EnhGujoZP8hWnvSFoP64tncfa9Z9M2_5gO1omhpv6ecpF627OAFBD0ADBGn6TKJJvRgAGtkrlKnqqbNm0O_BQ486KLkgPS1dvyRFfiBvPw",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpUID7ji17wVTa1qhnvqpl_sdBbm9XWh4BGevb5oC0vcT3GUtldYhRt2K-6g7oCKNRMOCPprY9usSOOYrT17qCbjcjmNii2TavwwiqwjhSNY6lidROmbulnpC1sCa19FmQlQc-1FaGH9oYvbBxCK-rJEU0gdPPlKB9OsULiVrGRfkF0HeDefeqrExwpwmX99RizJXVLv_iSCrfLGdH5H6W9AmVWkQxxQMTL8fZ4CBlCZa7mkSJcK4XJ6uNyCU=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEg6loW_QW65_YywPGTgPwwtH8E1dt5Z6HFd5pSTauxvys_h8y8Cur1opGwVjC-L7zCiVePN4dLTxshWI7Ccx4lKxbHk7cqibfHXUmy39Hts1IbbOnO9vI8jQCyuLj8oHjmhMgn",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9HuEz3Ztv1b2cc1LYwQ7tc1S-6OWpnYTvXkZX-F-L8nia3_EmHL3eINmijVHW46CkERlJA-l4ygShBQn3qXUJSpHgE3M5rpYTp2UfiJ97vrLyGk5UinKaOodG_pQB6GjilkflkIsJ6gGX8pHpJwWtUuohf3Wv",
        "title": "scilifelab.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRPX0AXO16l8lzLATDsfMN5iCboH3WxxcPSxajaI1JMBQ0UUMhI36tmhX4hblRU9IGYG4M0P06h-BeyzpfIumpWnPX7dXPSE8l9WzyJtrVvX3PFYnLCR-WdOLo7vJEO_ceAOUnA_VdjTw_iGj-ULkonKQHu3qIcVaC33EkZoZXLyWVu5B8EA8-oNcpUVPYJLOLsSd7OAzNEK6_Rrt4peVF1-RX5hXj5X9xAHmnHhSDpGDTaXn-MzeQb8ZeBhTY9OM=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnshluRE7k3aO1DbFfDrQ6UHEUEISq89tXdVNrzAIFzqhz2hXdSS_fiaPDyE6w7s_ypgVisjdHhknS6n5PNPXCdQ4d7eBaus3OMxGDFrXutrFVOrRWiKg7f7ZxOwxRLPU",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZeLQOcK9RECLvF7drdnbDrOExHrA7dN6FmKS-Y82y7mAt5yhNblfBqoyNKDHPut9farTktiIOxmdtlbD57Nta-BlfmC_mH4bUqSpXYY2tp73_xgOG5ek4aP0xQXSL4MrVrrDp-dUZi-9IyVlwvpQ3nhUNVsVt8MzMafAPNiMFd4dUIa4=",
        "title": "researchdata.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGH8my8vk6lwFdyvujxmGYK30UYu3eW-3kmR_-yyXdw0Ay3Lz0Y0H7hYTpKSAJwkKZrJBaBCQGPBO83N37XRHxHSq0wvVfGsznD2jxTm73onTX5oYqqxjwXKKR13zvjRiisxnOOpy-CIOMxUMQw-tI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3bL-Il_5xuQmdAaaMT2bfMTnRY0jxRYl3LNm8SiogaOu-6X5pBDiBWxMF-wO1WH6hYJxlP9nFO3SE5Pae1X3kkfQ7YFr1HuzXdlOrB7rXYFoNzb-OVG2ddgAkx6efWVanc_6omTw-0PT0",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHd4k8maY_JJVDmJQyTlYdF6Mw4_PaKh-MqS8v3EXtUv6kdYUPdN52Ix8MfkXWppmtX_YraRvMsJ-dTjJsqe7Gsf6b3ksdEhPBxgQopYuHrLPyBW_PfOk2EvloJJw==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMwstA-DMu-ofLFWLBqsxRjxOPvtffk4lkj3F89ut96YkPawhnmAqdB2f8I1AdyqQJri3M-wcdjvVBLoStgRfu8lhIn0zEwwhXLy9AOOwQC4R5ODooZBzaoSJxWFxFnyWQqgIBqIrFXCEOaOAXHxm-PgGf3rC_G7G94dcEfXeAtV-z-QfDFN6uK0o36A==",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHEnOsQZLYKJESQax6071jf_Xq65U_zsdmeQENE5pJwlMbsqC3Uq-PmpmjCcIDJgQDSebCx3dCd4fv3dYwOdVm5KNr1vi8JKcEz4dKM8ArmVWQ3ETj6O0m6uOl6GZObRZmBPvVuYYMZ9KO6MB1F",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8yiueEYAQbXrmzo6R6B8No75JvHVD9lcdobw_QeZ8h2OiIiDajYfVSTKQ2qjKp7Xh0ZKWVxC7DaKDTYRbag7gyv1AnGkb2YC9h49B578QlzqyrYV_xA6LR2CiUM4E_3PuR7vsV8EXYrADE7k=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "SynthRAD2023 Grand Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2303.16912",
    "description": "A large-scale multi-center dataset designed for the SynthRAD2023 challenge to benchmark MRI-to-CT and CBCT-to-CT synthesis algorithms. It contains data from 1080 patients across three Dutch university medical centers, covering brain and pelvis anatomies. The dataset is specifically curated to facilitate the development of synthetic CT (sCT) generation for radiotherapy planning.",
    "authors": [
      "Adrian Thummerer",
      "Matteo Maspero",
      "E. Huijben",
      "M. Terpstra",
      "O. Gurney-Champion",
      "M. Afonso",
      "S. Pai",
      "P. Koopmans",
      "M. van Eijnatten",
      "Z. Perko"
    ],
    "githubLink": "https://github.com/SynthRAD2023/preprocessing",
    "itemCount": "1080 image pairs",
    "source": "Zenodo",
    "specs": "NIfTI format; MRI-CT and CBCT-CT pairs; Brain and Pelvis anatomies",
    "year": "2023",
    "id": "saved-1769659402160-6ymll",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBtijk5YK3Q1EyBicX4R6Otj3LAZDsTxsEoaRTB9EnhGujoZP8hWnvSFoP64tncfa9Z9M2_5gO1omhpv6ecpF627OAFBD0ADBGn6TKJJvRgAGtkrlKnqqbNm0O_BQ486KLkgPS1dvyRFfiBvPw",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpUID7ji17wVTa1qhnvqpl_sdBbm9XWh4BGevb5oC0vcT3GUtldYhRt2K-6g7oCKNRMOCPprY9usSOOYrT17qCbjcjmNii2TavwwiqwjhSNY6lidROmbulnpC1sCa19FmQlQc-1FaGH9oYvbBxCK-rJEU0gdPPlKB9OsULiVrGRfkF0HeDefeqrExwpwmX99RizJXVLv_iSCrfLGdH5H6W9AmVWkQxxQMTL8fZ4CBlCZa7mkSJcK4XJ6uNyCU=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEg6loW_QW65_YywPGTgPwwtH8E1dt5Z6HFd5pSTauxvys_h8y8Cur1opGwVjC-L7zCiVePN4dLTxshWI7Ccx4lKxbHk7cqibfHXUmy39Hts1IbbOnO9vI8jQCyuLj8oHjmhMgn",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9HuEz3Ztv1b2cc1LYwQ7tc1S-6OWpnYTvXkZX-F-L8nia3_EmHL3eINmijVHW46CkERlJA-l4ygShBQn3qXUJSpHgE3M5rpYTp2UfiJ97vrLyGk5UinKaOodG_pQB6GjilkflkIsJ6gGX8pHpJwWtUuohf3Wv",
        "title": "scilifelab.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRPX0AXO16l8lzLATDsfMN5iCboH3WxxcPSxajaI1JMBQ0UUMhI36tmhX4hblRU9IGYG4M0P06h-BeyzpfIumpWnPX7dXPSE8l9WzyJtrVvX3PFYnLCR-WdOLo7vJEO_ceAOUnA_VdjTw_iGj-ULkonKQHu3qIcVaC33EkZoZXLyWVu5B8EA8-oNcpUVPYJLOLsSd7OAzNEK6_Rrt4peVF1-RX5hXj5X9xAHmnHhSDpGDTaXn-MzeQb8ZeBhTY9OM=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnshluRE7k3aO1DbFfDrQ6UHEUEISq89tXdVNrzAIFzqhz2hXdSS_fiaPDyE6w7s_ypgVisjdHhknS6n5PNPXCdQ4d7eBaus3OMxGDFrXutrFVOrRWiKg7f7ZxOwxRLPU",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZeLQOcK9RECLvF7drdnbDrOExHrA7dN6FmKS-Y82y7mAt5yhNblfBqoyNKDHPut9farTktiIOxmdtlbD57Nta-BlfmC_mH4bUqSpXYY2tp73_xgOG5ek4aP0xQXSL4MrVrrDp-dUZi-9IyVlwvpQ3nhUNVsVt8MzMafAPNiMFd4dUIa4=",
        "title": "researchdata.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGH8my8vk6lwFdyvujxmGYK30UYu3eW-3kmR_-yyXdw0Ay3Lz0Y0H7hYTpKSAJwkKZrJBaBCQGPBO83N37XRHxHSq0wvVfGsznD2jxTm73onTX5oYqqxjwXKKR13zvjRiisxnOOpy-CIOMxUMQw-tI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3bL-Il_5xuQmdAaaMT2bfMTnRY0jxRYl3LNm8SiogaOu-6X5pBDiBWxMF-wO1WH6hYJxlP9nFO3SE5Pae1X3kkfQ7YFr1HuzXdlOrB7rXYFoNzb-OVG2ddgAkx6efWVanc_6omTw-0PT0",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHd4k8maY_JJVDmJQyTlYdF6Mw4_PaKh-MqS8v3EXtUv6kdYUPdN52Ix8MfkXWppmtX_YraRvMsJ-dTjJsqe7Gsf6b3ksdEhPBxgQopYuHrLPyBW_PfOk2EvloJJw==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMwstA-DMu-ofLFWLBqsxRjxOPvtffk4lkj3F89ut96YkPawhnmAqdB2f8I1AdyqQJri3M-wcdjvVBLoStgRfu8lhIn0zEwwhXLy9AOOwQC4R5ODooZBzaoSJxWFxFnyWQqgIBqIrFXCEOaOAXHxm-PgGf3rC_G7G94dcEfXeAtV-z-QfDFN6uK0o36A==",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHEnOsQZLYKJESQax6071jf_Xq65U_zsdmeQENE5pJwlMbsqC3Uq-PmpmjCcIDJgQDSebCx3dCd4fv3dYwOdVm5KNr1vi8JKcEz4dKM8ArmVWQ3ETj6O0m6uOl6GZObRZmBPvVuYYMZ9KO6MB1F",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8yiueEYAQbXrmzo6R6B8No75JvHVD9lcdobw_QeZ8h2OiIiDajYfVSTKQ2qjKp7Xh0ZKWVxC7DaKDTYRbag7gyv1AnGkb2YC9h49B578QlzqyrYV_xA6LR2CiUM4E_3PuR7vsV8EXYrADE7k=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Pancreatic-CT-CBCT-SEG",
    "paperLink": "https://doi.org/10.7937/TCIA.2020.6K65-RW71",
    "description": "A dataset containing breath-hold CT and Cone-Beam CT (CBCT) images for 40 patients with locally advanced pancreatic cancer. While primarily designed for segmentation and registration, the paired CT-CBCT data is valuable for benchmarking CBCT-to-CT synthesis methods in adaptive radiotherapy.",
    "authors": [
      "Y. Xu",
      "K.M. Langen",
      "J.R. Mechalakos",
      "C.H. Crane"
    ],
    "githubLink": "https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=61080890",
    "itemCount": "40 patients",
    "source": "The Cancer Imaging Archive (TCIA)",
    "specs": "DICOM format; CT and CBCT pairs; Pancreas/Abdomen",
    "year": "2020",
    "id": "saved-1769659402160-43588",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBtijk5YK3Q1EyBicX4R6Otj3LAZDsTxsEoaRTB9EnhGujoZP8hWnvSFoP64tncfa9Z9M2_5gO1omhpv6ecpF627OAFBD0ADBGn6TKJJvRgAGtkrlKnqqbNm0O_BQ486KLkgPS1dvyRFfiBvPw",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpUID7ji17wVTa1qhnvqpl_sdBbm9XWh4BGevb5oC0vcT3GUtldYhRt2K-6g7oCKNRMOCPprY9usSOOYrT17qCbjcjmNii2TavwwiqwjhSNY6lidROmbulnpC1sCa19FmQlQc-1FaGH9oYvbBxCK-rJEU0gdPPlKB9OsULiVrGRfkF0HeDefeqrExwpwmX99RizJXVLv_iSCrfLGdH5H6W9AmVWkQxxQMTL8fZ4CBlCZa7mkSJcK4XJ6uNyCU=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEg6loW_QW65_YywPGTgPwwtH8E1dt5Z6HFd5pSTauxvys_h8y8Cur1opGwVjC-L7zCiVePN4dLTxshWI7Ccx4lKxbHk7cqibfHXUmy39Hts1IbbOnO9vI8jQCyuLj8oHjmhMgn",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9HuEz3Ztv1b2cc1LYwQ7tc1S-6OWpnYTvXkZX-F-L8nia3_EmHL3eINmijVHW46CkERlJA-l4ygShBQn3qXUJSpHgE3M5rpYTp2UfiJ97vrLyGk5UinKaOodG_pQB6GjilkflkIsJ6gGX8pHpJwWtUuohf3Wv",
        "title": "scilifelab.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRPX0AXO16l8lzLATDsfMN5iCboH3WxxcPSxajaI1JMBQ0UUMhI36tmhX4hblRU9IGYG4M0P06h-BeyzpfIumpWnPX7dXPSE8l9WzyJtrVvX3PFYnLCR-WdOLo7vJEO_ceAOUnA_VdjTw_iGj-ULkonKQHu3qIcVaC33EkZoZXLyWVu5B8EA8-oNcpUVPYJLOLsSd7OAzNEK6_Rrt4peVF1-RX5hXj5X9xAHmnHhSDpGDTaXn-MzeQb8ZeBhTY9OM=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEQnshluRE7k3aO1DbFfDrQ6UHEUEISq89tXdVNrzAIFzqhz2hXdSS_fiaPDyE6w7s_ypgVisjdHhknS6n5PNPXCdQ4d7eBaus3OMxGDFrXutrFVOrRWiKg7f7ZxOwxRLPU",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZeLQOcK9RECLvF7drdnbDrOExHrA7dN6FmKS-Y82y7mAt5yhNblfBqoyNKDHPut9farTktiIOxmdtlbD57Nta-BlfmC_mH4bUqSpXYY2tp73_xgOG5ek4aP0xQXSL4MrVrrDp-dUZi-9IyVlwvpQ3nhUNVsVt8MzMafAPNiMFd4dUIa4=",
        "title": "researchdata.se"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGH8my8vk6lwFdyvujxmGYK30UYu3eW-3kmR_-yyXdw0Ay3Lz0Y0H7hYTpKSAJwkKZrJBaBCQGPBO83N37XRHxHSq0wvVfGsznD2jxTm73onTX5oYqqxjwXKKR13zvjRiisxnOOpy-CIOMxUMQw-tI=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3bL-Il_5xuQmdAaaMT2bfMTnRY0jxRYl3LNm8SiogaOu-6X5pBDiBWxMF-wO1WH6hYJxlP9nFO3SE5Pae1X3kkfQ7YFr1HuzXdlOrB7rXYFoNzb-OVG2ddgAkx6efWVanc_6omTw-0PT0",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHd4k8maY_JJVDmJQyTlYdF6Mw4_PaKh-MqS8v3EXtUv6kdYUPdN52Ix8MfkXWppmtX_YraRvMsJ-dTjJsqe7Gsf6b3ksdEhPBxgQopYuHrLPyBW_PfOk2EvloJJw==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMwstA-DMu-ofLFWLBqsxRjxOPvtffk4lkj3F89ut96YkPawhnmAqdB2f8I1AdyqQJri3M-wcdjvVBLoStgRfu8lhIn0zEwwhXLy9AOOwQC4R5ODooZBzaoSJxWFxFnyWQqgIBqIrFXCEOaOAXHxm-PgGf3rC_G7G94dcEfXeAtV-z-QfDFN6uK0o36A==",
        "title": "cancerimagingarchive.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHEnOsQZLYKJESQax6071jf_Xq65U_zsdmeQENE5pJwlMbsqC3Uq-PmpmjCcIDJgQDSebCx3dCd4fv3dYwOdVm5KNr1vi8JKcEz4dKM8ArmVWQ3ETj6O0m6uOl6GZObRZmBPvVuYYMZ9KO6MB1F",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8yiueEYAQbXrmzo6R6B8No75JvHVD9lcdobw_QeZ8h2OiIiDajYfVSTKQ2qjKp7Xh0ZKWVxC7DaKDTYRbag7gyv1AnGkb2YC9h49B578QlzqyrYV_xA6LR2CiUM4E_3PuR7vsV8EXYrADE7k=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "STS-Tooth (Semi-supervised Teeth Segmentation)",
    "paperLink": "https://zenodo.org/records/13861838",
    "description": "A massive multi-modal dataset designed for semi-supervised teeth segmentation. It includes a 2D track (STS-2D) with panoramic X-rays and a 3D track (STS-3D) with CBCT scans, featuring a large number of unlabeled samples to encourage semi-supervised learning research.",
    "authors": [
      "Yaqi Wang",
      "Zhiming Cui",
      "et al."
    ],
    "githubLink": "https://github.com/ricoleehduu/STS-Challenge",
    "itemCount": "STS-2D: 4,000 images; STS-3D: 148,400 scans (8,800 masks)",
    "source": "Semantic Scholar",
    "specs": "2D Panoramic X-rays, 3D CBCT scans",
    "year": "2024",
    "id": "saved-1769659490247-krx2s",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKoNCK1i87h_HKo9UHa9TpCc57XcEo5osSVPh5w4iNSrxEOojDbFcqiXlpUSfYCK5eqVdMGEsfj-NbM-u6XDSmCeMuqtlOrd4Tz9CslamB",
        "title": "osf.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGu2CsEIVSLVt0jJvlDU7D-MR_5aoj-gqPMmeRxvDyhn_Xj3zgAM049iy_uTIVFe_XiBCmUMYp5TwAVUPsJthUt6pJGdUK0GNObm1BQnj54A8ov344Sv-etF0kWnAGFktD1HGCMYw==",
        "title": "synapse.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjpXWvVMkU6lWML6zq_72uZpJsWYdST57Z5KyzMGZqaa-YJ0RVrE3jEI9cSqB34glLEMG_Tm--ofVJmt_ZgkaHlDB8soOXINC4OUO2GE3jwrpUERhSePd-uZ00kWAPFqCjkd1t-0E=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFCSRbID4iP-C02GZsgaRA3Q2TMfF6s7obGj3fGh8Fqc32uspNk5Igh0MiZnaVJA-kQxjw5elnlpfBPr1zB2gZbfcRUO66VJuMtog4QyVSvXHJPHdHrdnt3Gv79QcI=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsUp7rjkHQo_A6nPG1az1ItZ0q6dhl7pSgRa0Jrg3-_ByK8SqNVOc0BgdaiutgL_NTvAkXcpN_I65KAmnnuTYpax5OkKGwL73i5RuWJmWo9URfiuTBQW4UmYmD5IonoWhjR43JbrgAFEOi9Y=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9vu55oifYis8Yw8JmbuX0LY9HH1QnmNLM50c2_819tMwCWPd9jQ9Mm-x94U32VEesltL2V6XzdnSAtBae3LtoVhYeB6Zr4KaTcJy74iv7O4q5cDJGra01gZqCnRgC_4uSIK8LmnXkgodWdQfPr90fiHY=",
        "title": "kaggle.com"
      }
    ]
  },
  {
    "title": "ToothFairy Challenge Dataset",
    "paperLink": "https://ieeexplore.ieee.org/document/10478644",
    "description": "A benchmark dataset from the MICCAI ToothFairy challenges (2023-2024) focused on voxel-level segmentation of anatomical structures in Cone Beam Computed Tomography (CBCT) volumes. Key tasks include segmenting the Inferior Alveolar Canal (IAC), teeth, mandible, and pharynx.",
    "authors": [
      "Federico Bolelli",
      "Luca Lumetti",
      "Shankeeth Vinayahalingam",
      "Mattia Di Bartolomeo",
      "Costantino Grana"
    ],
    "githubLink": "https://toothfairy.grand-challenge.org/",
    "itemCount": "443+ CBCT volumes",
    "source": "Grand Challenge",
    "specs": "3D CBCT volumes (NIfTI), sparse and dense voxel annotations",
    "year": "2023",
    "id": "saved-1769659490247-7ajq4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKoNCK1i87h_HKo9UHa9TpCc57XcEo5osSVPh5w4iNSrxEOojDbFcqiXlpUSfYCK5eqVdMGEsfj-NbM-u6XDSmCeMuqtlOrd4Tz9CslamB",
        "title": "osf.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGu2CsEIVSLVt0jJvlDU7D-MR_5aoj-gqPMmeRxvDyhn_Xj3zgAM049iy_uTIVFe_XiBCmUMYp5TwAVUPsJthUt6pJGdUK0GNObm1BQnj54A8ov344Sv-etF0kWnAGFktD1HGCMYw==",
        "title": "synapse.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjpXWvVMkU6lWML6zq_72uZpJsWYdST57Z5KyzMGZqaa-YJ0RVrE3jEI9cSqB34glLEMG_Tm--ofVJmt_ZgkaHlDB8soOXINC4OUO2GE3jwrpUERhSePd-uZ00kWAPFqCjkd1t-0E=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFCSRbID4iP-C02GZsgaRA3Q2TMfF6s7obGj3fGh8Fqc32uspNk5Igh0MiZnaVJA-kQxjw5elnlpfBPr1zB2gZbfcRUO66VJuMtog4QyVSvXHJPHdHrdnt3Gv79QcI=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsUp7rjkHQo_A6nPG1az1ItZ0q6dhl7pSgRa0Jrg3-_ByK8SqNVOc0BgdaiutgL_NTvAkXcpN_I65KAmnnuTYpax5OkKGwL73i5RuWJmWo9URfiuTBQW4UmYmD5IonoWhjR43JbrgAFEOi9Y=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9vu55oifYis8Yw8JmbuX0LY9HH1QnmNLM50c2_819tMwCWPd9jQ9Mm-x94U32VEesltL2V6XzdnSAtBae3LtoVhYeB6Zr4KaTcJy74iv7O4q5cDJGra01gZqCnRgC_4uSIK8LmnXkgodWdQfPr90fiHY=",
        "title": "kaggle.com"
      }
    ]
  },
  {
    "title": "DENTEX (Dental Enumeration and Diagnosis)",
    "paperLink": "https://arxiv.org/abs/2305.15838",
    "description": "A benchmark for tooth detection, enumeration, and disease diagnosis on panoramic dental X-rays. Released as part of the MICCAI 2023 DENTEX challenge, it provides hierarchically annotated data (quadrants, tooth enumeration, diagnosis).",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Enis Simsar",
      "Ahmed Kasem",
      "Atilla Ozmens",
      "et al."
    ],
    "githubLink": "https://github.com/ibrahimhamamci/DENTEX",
    "itemCount": "1,000+ panoramic X-rays",
    "source": "Hugging Face",
    "specs": "2D Panoramic X-ray images, bounding box and class annotations",
    "year": "2023",
    "id": "saved-1769659490247-638vl",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKoNCK1i87h_HKo9UHa9TpCc57XcEo5osSVPh5w4iNSrxEOojDbFcqiXlpUSfYCK5eqVdMGEsfj-NbM-u6XDSmCeMuqtlOrd4Tz9CslamB",
        "title": "osf.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGu2CsEIVSLVt0jJvlDU7D-MR_5aoj-gqPMmeRxvDyhn_Xj3zgAM049iy_uTIVFe_XiBCmUMYp5TwAVUPsJthUt6pJGdUK0GNObm1BQnj54A8ov344Sv-etF0kWnAGFktD1HGCMYw==",
        "title": "synapse.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjpXWvVMkU6lWML6zq_72uZpJsWYdST57Z5KyzMGZqaa-YJ0RVrE3jEI9cSqB34glLEMG_Tm--ofVJmt_ZgkaHlDB8soOXINC4OUO2GE3jwrpUERhSePd-uZ00kWAPFqCjkd1t-0E=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFCSRbID4iP-C02GZsgaRA3Q2TMfF6s7obGj3fGh8Fqc32uspNk5Igh0MiZnaVJA-kQxjw5elnlpfBPr1zB2gZbfcRUO66VJuMtog4QyVSvXHJPHdHrdnt3Gv79QcI=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsUp7rjkHQo_A6nPG1az1ItZ0q6dhl7pSgRa0Jrg3-_ByK8SqNVOc0BgdaiutgL_NTvAkXcpN_I65KAmnnuTYpax5OkKGwL73i5RuWJmWo9URfiuTBQW4UmYmD5IonoWhjR43JbrgAFEOi9Y=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9vu55oifYis8Yw8JmbuX0LY9HH1QnmNLM50c2_819tMwCWPd9jQ9Mm-x94U32VEesltL2V6XzdnSAtBae3LtoVhYeB6Zr4KaTcJy74iv7O4q5cDJGra01gZqCnRgC_4uSIK8LmnXkgodWdQfPr90fiHY=",
        "title": "kaggle.com"
      }
    ]
  },
  {
    "title": "Teeth3DS+ (3DTeethSeg / 3DTeethLand)",
    "paperLink": "https://arxiv.org/abs/2210.06094",
    "description": "A large-scale benchmark for 3D teeth segmentation, labeling, and landmark detection on intraoral scans. Originally released for the MICCAI 2022 3DTeethSeg challenge and extended for the 3DTeethLand 2024 challenge. It focuses on the analysis of intraoral 3D scans (IOS) for orthodontic treatment planning.",
    "authors": [
      "Achraf Ben-Hamadou",
      "Nour Neifar",
      "Ahmed Rekik",
      "Oussama Smaoui",
      "Firas Bouzguenda",
      "Sergi Pujades",
      "Edmond Boyer",
      "Edouard Ladroit"
    ],
    "githubLink": "https://github.com/benhamadou/Teeth3DS",
    "itemCount": "1,800+ 3D scans (24,000+ teeth)",
    "source": "arXiv",
    "specs": "3D Intraoral Scans (OBJ/PLY), JSON annotations (labels, instances, landmarks)",
    "year": "2022",
    "id": "saved-1769659490247-157dn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKoNCK1i87h_HKo9UHa9TpCc57XcEo5osSVPh5w4iNSrxEOojDbFcqiXlpUSfYCK5eqVdMGEsfj-NbM-u6XDSmCeMuqtlOrd4Tz9CslamB",
        "title": "osf.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGu2CsEIVSLVt0jJvlDU7D-MR_5aoj-gqPMmeRxvDyhn_Xj3zgAM049iy_uTIVFe_XiBCmUMYp5TwAVUPsJthUt6pJGdUK0GNObm1BQnj54A8ov344Sv-etF0kWnAGFktD1HGCMYw==",
        "title": "synapse.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjpXWvVMkU6lWML6zq_72uZpJsWYdST57Z5KyzMGZqaa-YJ0RVrE3jEI9cSqB34glLEMG_Tm--ofVJmt_ZgkaHlDB8soOXINC4OUO2GE3jwrpUERhSePd-uZ00kWAPFqCjkd1t-0E=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFCSRbID4iP-C02GZsgaRA3Q2TMfF6s7obGj3fGh8Fqc32uspNk5Igh0MiZnaVJA-kQxjw5elnlpfBPr1zB2gZbfcRUO66VJuMtog4QyVSvXHJPHdHrdnt3Gv79QcI=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsUp7rjkHQo_A6nPG1az1ItZ0q6dhl7pSgRa0Jrg3-_ByK8SqNVOc0BgdaiutgL_NTvAkXcpN_I65KAmnnuTYpax5OkKGwL73i5RuWJmWo9URfiuTBQW4UmYmD5IonoWhjR43JbrgAFEOi9Y=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9vu55oifYis8Yw8JmbuX0LY9HH1QnmNLM50c2_819tMwCWPd9jQ9Mm-x94U32VEesltL2V6XzdnSAtBae3LtoVhYeB6Zr4KaTcJy74iv7O4q5cDJGra01gZqCnRgC_4uSIK8LmnXkgodWdQfPr90fiHY=",
        "title": "kaggle.com"
      }
    ]
  },
  {
    "title": "CTooth / CTooth+",
    "paperLink": "https://link.springer.com/chapter/10.1007/978-3-031-13825-5_22",
    "description": "A fully annotated 3D dataset for tooth volume segmentation on Cone Beam Computed Tomography (CBCT) images. It provides high-quality voxel-level annotations often used as a gold standard for 3D tooth segmentation tasks.",
    "authors": [
      "Weiwei Cui",
      "Yaqi Wang",
      "Qianni Zhang",
      "Huiyu Zhou",
      "Liaoyuan Zeng"
    ],
    "githubLink": "https://github.com/News-n-neWs/CTooth",
    "itemCount": "22 annotated volumes (CTooth), extended in CTooth+",
    "source": "Google Scholar",
    "specs": "3D CBCT volumes",
    "year": "2022",
    "id": "saved-1769659490248-vkiv1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKoNCK1i87h_HKo9UHa9TpCc57XcEo5osSVPh5w4iNSrxEOojDbFcqiXlpUSfYCK5eqVdMGEsfj-NbM-u6XDSmCeMuqtlOrd4Tz9CslamB",
        "title": "osf.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGu2CsEIVSLVt0jJvlDU7D-MR_5aoj-gqPMmeRxvDyhn_Xj3zgAM049iy_uTIVFe_XiBCmUMYp5TwAVUPsJthUt6pJGdUK0GNObm1BQnj54A8ov344Sv-etF0kWnAGFktD1HGCMYw==",
        "title": "synapse.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjpXWvVMkU6lWML6zq_72uZpJsWYdST57Z5KyzMGZqaa-YJ0RVrE3jEI9cSqB34glLEMG_Tm--ofVJmt_ZgkaHlDB8soOXINC4OUO2GE3jwrpUERhSePd-uZ00kWAPFqCjkd1t-0E=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFCSRbID4iP-C02GZsgaRA3Q2TMfF6s7obGj3fGh8Fqc32uspNk5Igh0MiZnaVJA-kQxjw5elnlpfBPr1zB2gZbfcRUO66VJuMtog4QyVSvXHJPHdHrdnt3Gv79QcI=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsUp7rjkHQo_A6nPG1az1ItZ0q6dhl7pSgRa0Jrg3-_ByK8SqNVOc0BgdaiutgL_NTvAkXcpN_I65KAmnnuTYpax5OkKGwL73i5RuWJmWo9URfiuTBQW4UmYmD5IonoWhjR43JbrgAFEOi9Y=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9vu55oifYis8Yw8JmbuX0LY9HH1QnmNLM50c2_819tMwCWPd9jQ9Mm-x94U32VEesltL2V6XzdnSAtBae3LtoVhYeB6Zr4KaTcJy74iv7O4q5cDJGra01gZqCnRgC_4uSIK8LmnXkgodWdQfPr90fiHY=",
        "title": "kaggle.com"
      }
    ]
  },
  {
    "title": "Tufts Dental Database",
    "paperLink": "https://ieeexplore.ieee.org/document/9558855",
    "description": "A multimodal panoramic X-ray dataset containing expert labeling of abnormalities and teeth. It is designed to benchmark diagnostic systems and includes radiologist eye-tracking data to support explainable AI research in dentistry.",
    "authors": [
      "Karen Panetta",
      "Rahul Rajendran",
      "Sos Agaian"
    ],
    "githubLink": "https://www.kaggle.com/datasets/deepologylab/tufts-dental-database",
    "itemCount": "1,000 panoramic radiographs",
    "source": "IEEE Xplore",
    "specs": "2D Panoramic X-rays, segmentation masks, eye-tracking data",
    "year": "2021",
    "id": "saved-1769659490248-i6g73",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKoNCK1i87h_HKo9UHa9TpCc57XcEo5osSVPh5w4iNSrxEOojDbFcqiXlpUSfYCK5eqVdMGEsfj-NbM-u6XDSmCeMuqtlOrd4Tz9CslamB",
        "title": "osf.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGu2CsEIVSLVt0jJvlDU7D-MR_5aoj-gqPMmeRxvDyhn_Xj3zgAM049iy_uTIVFe_XiBCmUMYp5TwAVUPsJthUt6pJGdUK0GNObm1BQnj54A8ov344Sv-etF0kWnAGFktD1HGCMYw==",
        "title": "synapse.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjpXWvVMkU6lWML6zq_72uZpJsWYdST57Z5KyzMGZqaa-YJ0RVrE3jEI9cSqB34glLEMG_Tm--ofVJmt_ZgkaHlDB8soOXINC4OUO2GE3jwrpUERhSePd-uZ00kWAPFqCjkd1t-0E=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFCSRbID4iP-C02GZsgaRA3Q2TMfF6s7obGj3fGh8Fqc32uspNk5Igh0MiZnaVJA-kQxjw5elnlpfBPr1zB2gZbfcRUO66VJuMtog4QyVSvXHJPHdHrdnt3Gv79QcI=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwsUp7rjkHQo_A6nPG1az1ItZ0q6dhl7pSgRa0Jrg3-_ByK8SqNVOc0BgdaiutgL_NTvAkXcpN_I65KAmnnuTYpax5OkKGwL73i5RuWJmWo9URfiuTBQW4UmYmD5IonoWhjR43JbrgAFEOi9Y=",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG9vu55oifYis8Yw8JmbuX0LY9HH1QnmNLM50c2_819tMwCWPd9jQ9Mm-x94U32VEesltL2V6XzdnSAtBae3LtoVhYeB6Zr4KaTcJy74iv7O4q5cDJGra01gZqCnRgC_4uSIK8LmnXkgodWdQfPr90fiHY=",
        "title": "kaggle.com"
      }
    ]
  },
  {
    "title": "Chronos Benchmark",
    "paperLink": "https://aclanthology.org/2025.clicit-1.42/",
    "description": "A benchmark for Historical Phase Recognition from text. It utilizes the Chronos dataset to classify historical periods (e.g., crisis, growth) based on textual descriptions of events, bridging computational linguistics and historical dynamics.",
    "authors": [
      "Fabio Celli",
      "Marco Rovera"
    ],
    "githubLink": "https://huggingface.co/datasets/facells/chronos-historical-sdt-benchmark",
    "itemCount": "~1400 annotated text rows",
    "source": "Hugging Face",
    "specs": "Text, 5-class or binary phase labels",
    "year": "2025",
    "id": "saved-1769659546027-7yssh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFm8cXwRgNhLqlCUpYzojHVjbENOfn-IIOiBUFdz39f6UTTiNnf_Wn8C2KwW-PnmiaLCb2AW0mDpxGzwfhowDb_6ErA24EQd5kIQYuMPw7edqJQWod4KetIK_frWh_VdVNy-CgFIh5lnZRG3OJhsl0UwkwJ8CBO9qu-TUU94aH6nj4aFf-diN05CPg-GI8ZDBnUp2zey2mOJ4Ti11ft7bzXfw0bKZmdGH-J0D6g4U52KO5C-spwMiHUltzgOri3ZtsWrQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8QORQS-BjvAPkOyg31Ef6QfnzCnorksKZiNxo-RL2Ulf-AW4LDz7feCmD4RQ6dK__-vWEdoXNRaS0N3dAGtQr9inZcG6GSpZQOKPZslNtpLiGHuYLY-wQxvPR4g==",
        "title": "unistra.fr"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDUKW1RLo2AAmMI8lbfqxt1EzgwNqyWCc2ROQxnD0qJNoNlEwbzOelWnD1BBJoS4_C_UtPS5ZLdjolRi3M5SVMpn_PmCuoG6z_2F8GVVz0U_KJp_kkEgh4pIpqyZq0FnrvNg8YEWkD38FNuvNpu2MpcVsq9iYo1LByKYZYwvjuzvdwdJNsC7TjHwfUX60G23JSAwJOWwikvMlaVrSi0h-K0grcRqaB",
        "title": "liner.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9C9qgUAJZvGcBnBfFmJoJ5MWAgRKy6Pqk6_KxXIpcvjF_mGkhDgdkKC70Kb6S771fYKhtmBTi226HzPbFBw_e2LX4tBRtBUNinvdCWuOC9dqRk3yGhTez7os1OUwYvA8rncSDU5VgXrY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgPgZ1zOhJTE284KvFqlWAFK8tfTjCG39MAKRMb9aSHbbtz-1oE5y2dTKj5za5yRoX-QBwyPPF6XxMXHtYmHVw-DZs_DeDkiaJIAjHYzt-SSf65DWMNqskvZ6lyhpfwoTwQwJRutsq30efUrOUi5smt_o6yiQjSVaKITvbdpVJ5bwoz6eUirCX9odX5gx1HqAqjEW-W0esSumdIDflkOaw0pMreq7WG61YZZs=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "SurgMLLMBench",
    "paperLink": "https://arxiv.org/abs/2511.16857",
    "description": "A comprehensive benchmark for multimodal Large Language Models in surgery. It organizes data around four tasks including phase recognition, step classification, and action detection to test surgical scene understanding capabilities.",
    "authors": [
      "H. Lee",
      "et al. (KIST-HARILAB)"
    ],
    "githubLink": "https://huggingface.co/datasets/KIST-HARILAB/SurgMLLMBench",
    "itemCount": "Composite of multiple datasets",
    "source": "Hugging Face",
    "specs": "Multimodal (Video + Text prompts), Phase/Step/Action labels",
    "year": "2025",
    "id": "saved-1769659546027-t8g0a",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFm8cXwRgNhLqlCUpYzojHVjbENOfn-IIOiBUFdz39f6UTTiNnf_Wn8C2KwW-PnmiaLCb2AW0mDpxGzwfhowDb_6ErA24EQd5kIQYuMPw7edqJQWod4KetIK_frWh_VdVNy-CgFIh5lnZRG3OJhsl0UwkwJ8CBO9qu-TUU94aH6nj4aFf-diN05CPg-GI8ZDBnUp2zey2mOJ4Ti11ft7bzXfw0bKZmdGH-J0D6g4U52KO5C-spwMiHUltzgOri3ZtsWrQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8QORQS-BjvAPkOyg31Ef6QfnzCnorksKZiNxo-RL2Ulf-AW4LDz7feCmD4RQ6dK__-vWEdoXNRaS0N3dAGtQr9inZcG6GSpZQOKPZslNtpLiGHuYLY-wQxvPR4g==",
        "title": "unistra.fr"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDUKW1RLo2AAmMI8lbfqxt1EzgwNqyWCc2ROQxnD0qJNoNlEwbzOelWnD1BBJoS4_C_UtPS5ZLdjolRi3M5SVMpn_PmCuoG6z_2F8GVVz0U_KJp_kkEgh4pIpqyZq0FnrvNg8YEWkD38FNuvNpu2MpcVsq9iYo1LByKYZYwvjuzvdwdJNsC7TjHwfUX60G23JSAwJOWwikvMlaVrSi0h-K0grcRqaB",
        "title": "liner.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9C9qgUAJZvGcBnBfFmJoJ5MWAgRKy6Pqk6_KxXIpcvjF_mGkhDgdkKC70Kb6S771fYKhtmBTi226HzPbFBw_e2LX4tBRtBUNinvdCWuOC9dqRk3yGhTez7os1OUwYvA8rncSDU5VgXrY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgPgZ1zOhJTE284KvFqlWAFK8tfTjCG39MAKRMb9aSHbbtz-1oE5y2dTKj5za5yRoX-QBwyPPF6XxMXHtYmHVw-DZs_DeDkiaJIAjHYzt-SSf65DWMNqskvZ6lyhpfwoTwQwJRutsq30efUrOUi5smt_o6yiQjSVaKITvbdpVJ5bwoz6eUirCX9odX5gx1HqAqjEW-W0esSumdIDflkOaw0pMreq7WG61YZZs=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "Cataract-1K",
    "paperLink": "https://arxiv.org/abs/2312.06295",
    "description": "A large-scale dataset for cataract surgery that addresses scene segmentation, phase recognition, and irregularity detection. It specifically provides phase annotations for a subset of regular videos to enable deep learning studies.",
    "authors": [
      "Negin Ghamsarian",
      "Yassin Khalifa",
      "Klaus Schoeffmann",
      "et al."
    ],
    "githubLink": "https://github.com/Negin-Ghamsarian/Cataract-1K",
    "itemCount": "1000 videos total (56 annotated for phases)",
    "source": "arXiv",
    "specs": "Video (25/30 fps), 12 phases, 512x324 to 1024x768 resolution",
    "year": "2023",
    "id": "saved-1769659546027-wcswn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFm8cXwRgNhLqlCUpYzojHVjbENOfn-IIOiBUFdz39f6UTTiNnf_Wn8C2KwW-PnmiaLCb2AW0mDpxGzwfhowDb_6ErA24EQd5kIQYuMPw7edqJQWod4KetIK_frWh_VdVNy-CgFIh5lnZRG3OJhsl0UwkwJ8CBO9qu-TUU94aH6nj4aFf-diN05CPg-GI8ZDBnUp2zey2mOJ4Ti11ft7bzXfw0bKZmdGH-J0D6g4U52KO5C-spwMiHUltzgOri3ZtsWrQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8QORQS-BjvAPkOyg31Ef6QfnzCnorksKZiNxo-RL2Ulf-AW4LDz7feCmD4RQ6dK__-vWEdoXNRaS0N3dAGtQr9inZcG6GSpZQOKPZslNtpLiGHuYLY-wQxvPR4g==",
        "title": "unistra.fr"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDUKW1RLo2AAmMI8lbfqxt1EzgwNqyWCc2ROQxnD0qJNoNlEwbzOelWnD1BBJoS4_C_UtPS5ZLdjolRi3M5SVMpn_PmCuoG6z_2F8GVVz0U_KJp_kkEgh4pIpqyZq0FnrvNg8YEWkD38FNuvNpu2MpcVsq9iYo1LByKYZYwvjuzvdwdJNsC7TjHwfUX60G23JSAwJOWwikvMlaVrSi0h-K0grcRqaB",
        "title": "liner.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9C9qgUAJZvGcBnBfFmJoJ5MWAgRKy6Pqk6_KxXIpcvjF_mGkhDgdkKC70Kb6S771fYKhtmBTi226HzPbFBw_e2LX4tBRtBUNinvdCWuOC9dqRk3yGhTez7os1OUwYvA8rncSDU5VgXrY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgPgZ1zOhJTE284KvFqlWAFK8tfTjCG39MAKRMb9aSHbbtz-1oE5y2dTKj5za5yRoX-QBwyPPF6XxMXHtYmHVw-DZs_DeDkiaJIAjHYzt-SSf65DWMNqskvZ6lyhpfwoTwQwJRutsq30efUrOUi5smt_o6yiQjSVaKITvbdpVJ5bwoz6eUirCX9odX5gx1HqAqjEW-W0esSumdIDflkOaw0pMreq7WG61YZZs=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AutoLaparo",
    "paperLink": "https://autolaparo.github.io/assets/files/2022_MICCAI_AutoLaparo.pdf",
    "description": "A dataset for integrated multi-tasks in image-guided surgical automation, specifically for laparoscopic hysterectomy. It includes annotations for surgical workflow recognition, laparoscope motion prediction, and instrument segmentation.",
    "authors": [
      "Zixu Zhao",
      "Yueming Jin",
      "Pheng-Ann Heng",
      "et al."
    ],
    "githubLink": "https://github.com/AutoLaparo/AutoLaparo",
    "itemCount": "21 full-length videos",
    "source": "Scholar",
    "specs": "Video (25 fps), 7 phases, motion & segmentation masks, 1920x1080",
    "year": "2022",
    "id": "saved-1769659546028-51rkp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFm8cXwRgNhLqlCUpYzojHVjbENOfn-IIOiBUFdz39f6UTTiNnf_Wn8C2KwW-PnmiaLCb2AW0mDpxGzwfhowDb_6ErA24EQd5kIQYuMPw7edqJQWod4KetIK_frWh_VdVNy-CgFIh5lnZRG3OJhsl0UwkwJ8CBO9qu-TUU94aH6nj4aFf-diN05CPg-GI8ZDBnUp2zey2mOJ4Ti11ft7bzXfw0bKZmdGH-J0D6g4U52KO5C-spwMiHUltzgOri3ZtsWrQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8QORQS-BjvAPkOyg31Ef6QfnzCnorksKZiNxo-RL2Ulf-AW4LDz7feCmD4RQ6dK__-vWEdoXNRaS0N3dAGtQr9inZcG6GSpZQOKPZslNtpLiGHuYLY-wQxvPR4g==",
        "title": "unistra.fr"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDUKW1RLo2AAmMI8lbfqxt1EzgwNqyWCc2ROQxnD0qJNoNlEwbzOelWnD1BBJoS4_C_UtPS5ZLdjolRi3M5SVMpn_PmCuoG6z_2F8GVVz0U_KJp_kkEgh4pIpqyZq0FnrvNg8YEWkD38FNuvNpu2MpcVsq9iYo1LByKYZYwvjuzvdwdJNsC7TjHwfUX60G23JSAwJOWwikvMlaVrSi0h-K0grcRqaB",
        "title": "liner.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9C9qgUAJZvGcBnBfFmJoJ5MWAgRKy6Pqk6_KxXIpcvjF_mGkhDgdkKC70Kb6S771fYKhtmBTi226HzPbFBw_e2LX4tBRtBUNinvdCWuOC9dqRk3yGhTez7os1OUwYvA8rncSDU5VgXrY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgPgZ1zOhJTE284KvFqlWAFK8tfTjCG39MAKRMb9aSHbbtz-1oE5y2dTKj5za5yRoX-QBwyPPF6XxMXHtYmHVw-DZs_DeDkiaJIAjHYzt-SSf65DWMNqskvZ6lyhpfwoTwQwJRutsq30efUrOUi5smt_o6yiQjSVaKITvbdpVJ5bwoz6eUirCX9odX5gx1HqAqjEW-W0esSumdIDflkOaw0pMreq7WG61YZZs=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "HeiChole (HeiSurf)",
    "paperLink": "https://arxiv.org/abs/2109.14956",
    "description": "A multi-center benchmark for surgical workflow and skill analysis. It includes laparoscopic cholecystectomy videos with annotations for phases, actions, instruments, and surgical skill assessment, designed to test generalizability across different medical centers.",
    "authors": [
      "Martin Wagner",
      "Beat-Peter Müller-Stich",
      "Sebastian Speidel",
      "et al."
    ],
    "githubLink": "https://www.synapse.org/#!Synapse:syn18824884",
    "itemCount": "33 videos (22 hours)",
    "source": "arXiv",
    "specs": "Video, 7 phases, 4 actions, 21 instruments, skill ratings",
    "year": "2021",
    "id": "saved-1769659546028-u3slf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFm8cXwRgNhLqlCUpYzojHVjbENOfn-IIOiBUFdz39f6UTTiNnf_Wn8C2KwW-PnmiaLCb2AW0mDpxGzwfhowDb_6ErA24EQd5kIQYuMPw7edqJQWod4KetIK_frWh_VdVNy-CgFIh5lnZRG3OJhsl0UwkwJ8CBO9qu-TUU94aH6nj4aFf-diN05CPg-GI8ZDBnUp2zey2mOJ4Ti11ft7bzXfw0bKZmdGH-J0D6g4U52KO5C-spwMiHUltzgOri3ZtsWrQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8QORQS-BjvAPkOyg31Ef6QfnzCnorksKZiNxo-RL2Ulf-AW4LDz7feCmD4RQ6dK__-vWEdoXNRaS0N3dAGtQr9inZcG6GSpZQOKPZslNtpLiGHuYLY-wQxvPR4g==",
        "title": "unistra.fr"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDUKW1RLo2AAmMI8lbfqxt1EzgwNqyWCc2ROQxnD0qJNoNlEwbzOelWnD1BBJoS4_C_UtPS5ZLdjolRi3M5SVMpn_PmCuoG6z_2F8GVVz0U_KJp_kkEgh4pIpqyZq0FnrvNg8YEWkD38FNuvNpu2MpcVsq9iYo1LByKYZYwvjuzvdwdJNsC7TjHwfUX60G23JSAwJOWwikvMlaVrSi0h-K0grcRqaB",
        "title": "liner.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9C9qgUAJZvGcBnBfFmJoJ5MWAgRKy6Pqk6_KxXIpcvjF_mGkhDgdkKC70Kb6S771fYKhtmBTi226HzPbFBw_e2LX4tBRtBUNinvdCWuOC9dqRk3yGhTez7os1OUwYvA8rncSDU5VgXrY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgPgZ1zOhJTE284KvFqlWAFK8tfTjCG39MAKRMb9aSHbbtz-1oE5y2dTKj5za5yRoX-QBwyPPF6XxMXHtYmHVw-DZs_DeDkiaJIAjHYzt-SSf65DWMNqskvZ6lyhpfwoTwQwJRutsq30efUrOUi5smt_o6yiQjSVaKITvbdpVJ5bwoz6eUirCX9odX5gx1HqAqjEW-W0esSumdIDflkOaw0pMreq7WG61YZZs=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "Cholec80",
    "paperLink": "https://arxiv.org/abs/1608.06771",
    "description": "The gold-standard benchmark for surgical phase recognition. It consists of 80 videos of laparoscopic cholecystectomy surgeries performed by 13 surgeons. The dataset captures the procedure at 25 fps and includes annotations for surgical phases and tool presence.",
    "authors": [
      "Andru P. Twinanda",
      "Sherif Shehata",
      "Didier Mutter",
      "Jacques Marescaux",
      "Michel de Mathelin",
      "Nicolas Padoy"
    ],
    "githubLink": "https://github.com/CAMMA-public/cholect45",
    "itemCount": "80 videos (40 train, 40 test)",
    "source": "arXiv",
    "specs": "Video (25 fps), 7 phases, 7 tool classes, 1920x1080 (downsampled often used)",
    "year": "2016",
    "id": "saved-1769659546029-c2lje",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFm8cXwRgNhLqlCUpYzojHVjbENOfn-IIOiBUFdz39f6UTTiNnf_Wn8C2KwW-PnmiaLCb2AW0mDpxGzwfhowDb_6ErA24EQd5kIQYuMPw7edqJQWod4KetIK_frWh_VdVNy-CgFIh5lnZRG3OJhsl0UwkwJ8CBO9qu-TUU94aH6nj4aFf-diN05CPg-GI8ZDBnUp2zey2mOJ4Ti11ft7bzXfw0bKZmdGH-J0D6g4U52KO5C-spwMiHUltzgOri3ZtsWrQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8QORQS-BjvAPkOyg31Ef6QfnzCnorksKZiNxo-RL2Ulf-AW4LDz7feCmD4RQ6dK__-vWEdoXNRaS0N3dAGtQr9inZcG6GSpZQOKPZslNtpLiGHuYLY-wQxvPR4g==",
        "title": "unistra.fr"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDUKW1RLo2AAmMI8lbfqxt1EzgwNqyWCc2ROQxnD0qJNoNlEwbzOelWnD1BBJoS4_C_UtPS5ZLdjolRi3M5SVMpn_PmCuoG6z_2F8GVVz0U_KJp_kkEgh4pIpqyZq0FnrvNg8YEWkD38FNuvNpu2MpcVsq9iYo1LByKYZYwvjuzvdwdJNsC7TjHwfUX60G23JSAwJOWwikvMlaVrSi0h-K0grcRqaB",
        "title": "liner.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9C9qgUAJZvGcBnBfFmJoJ5MWAgRKy6Pqk6_KxXIpcvjF_mGkhDgdkKC70Kb6S771fYKhtmBTi226HzPbFBw_e2LX4tBRtBUNinvdCWuOC9dqRk3yGhTez7os1OUwYvA8rncSDU5VgXrY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgPgZ1zOhJTE284KvFqlWAFK8tfTjCG39MAKRMb9aSHbbtz-1oE5y2dTKj5za5yRoX-QBwyPPF6XxMXHtYmHVw-DZs_DeDkiaJIAjHYzt-SSf65DWMNqskvZ6lyhpfwoTwQwJRutsq30efUrOUi5smt_o6yiQjSVaKITvbdpVJ5bwoz6eUirCX9odX5gx1HqAqjEW-W0esSumdIDflkOaw0pMreq7WG61YZZs=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "m2cai16-workflow",
    "paperLink": "https://arxiv.org/abs/1610.09337",
    "description": "Released as part of the M2CAI 2016 Challenge, this dataset focuses on surgical workflow segmentation. It contains laparoscopic videos annotated with phase labels to benchmark machine learning approaches in surgical data science.",
    "authors": [
      "Ralf Stauder",
      "Daniel Ostler",
      "M. Kranzfelder",
      "S. Koller",
      "Hubertus Feußner",
      "Nassir Navab"
    ],
    "githubLink": "https://camma.unistra.fr/datasets",
    "itemCount": "41 videos (27 train, 14 test)",
    "source": "arXiv",
    "specs": "Video, 8 surgical phases",
    "year": "2016",
    "id": "saved-1769659546029-g87vy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFm8cXwRgNhLqlCUpYzojHVjbENOfn-IIOiBUFdz39f6UTTiNnf_Wn8C2KwW-PnmiaLCb2AW0mDpxGzwfhowDb_6ErA24EQd5kIQYuMPw7edqJQWod4KetIK_frWh_VdVNy-CgFIh5lnZRG3OJhsl0UwkwJ8CBO9qu-TUU94aH6nj4aFf-diN05CPg-GI8ZDBnUp2zey2mOJ4Ti11ft7bzXfw0bKZmdGH-J0D6g4U52KO5C-spwMiHUltzgOri3ZtsWrQ==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8QORQS-BjvAPkOyg31Ef6QfnzCnorksKZiNxo-RL2Ulf-AW4LDz7feCmD4RQ6dK__-vWEdoXNRaS0N3dAGtQr9inZcG6GSpZQOKPZslNtpLiGHuYLY-wQxvPR4g==",
        "title": "unistra.fr"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGDUKW1RLo2AAmMI8lbfqxt1EzgwNqyWCc2ROQxnD0qJNoNlEwbzOelWnD1BBJoS4_C_UtPS5ZLdjolRi3M5SVMpn_PmCuoG6z_2F8GVVz0U_KJp_kkEgh4pIpqyZq0FnrvNg8YEWkD38FNuvNpu2MpcVsq9iYo1LByKYZYwvjuzvdwdJNsC7TjHwfUX60G23JSAwJOWwikvMlaVrSi0h-K0grcRqaB",
        "title": "liner.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH9C9qgUAJZvGcBnBfFmJoJ5MWAgRKy6Pqk6_KxXIpcvjF_mGkhDgdkKC70Kb6S771fYKhtmBTi226HzPbFBw_e2LX4tBRtBUNinvdCWuOC9dqRk3yGhTez7os1OUwYvA8rncSDU5VgXrY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgPgZ1zOhJTE284KvFqlWAFK8tfTjCG39MAKRMb9aSHbbtz-1oE5y2dTKj5za5yRoX-QBwyPPF6XxMXHtYmHVw-DZs_DeDkiaJIAjHYzt-SSf65DWMNqskvZ6lyhpfwoTwQwJRutsq30efUrOUi5smt_o6yiQjSVaKITvbdpVJ5bwoz6eUirCX9odX5gx1HqAqjEW-W0esSumdIDflkOaw0pMreq7WG61YZZs=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "SICS-155 / SICS-105",
    "paperLink": "https://www.nature.com/articles/s41598-025-00303-z",
    "description": "The first public dataset specifically for Manual Small-Incision Cataract Surgery (MSICS). It contains 155 videos (100 training, 15 validation, 40 testing) annotated with 18 distinct surgical phases. It was released in conjunction with a MICCAI 2025 challenge.",
    "authors": [
      "Simon Mueller",
      "Maximilian W. M. Wintergerst",
      "Thomas Schultz",
      "et al."
    ],
    "githubLink": "https://medvisbonn.github.io",
    "itemCount": "155 videos",
    "source": "Scholar",
    "specs": "Video resolution: 960x540, Frame rate: 30 fps, Annotations: Temporal surgical phases (18 classes)",
    "year": "2025",
    "id": "saved-1769659616402-c6r1f",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn2YSq_3yazJ-hcPJsK6MhJPfwwcCikKN1KBGkblTxcvlefe_J8skgK0WTR-a7AHECEM0vh1X_F-jKmIztgsAsEjb0b1_DtJYymXomjKTNxFSOO1dqUUPrK8i5nobG-MK-X8lEEb6awHn1x6S0XCNte7rBEg==",
        "title": "miccai.org"
      }
    ]
  },
  {
    "title": "Cataract-MSICS (Sankara-MSICS)",
    "paperLink": "https://arxiv.org/abs/2411.16794",
    "description": "A comprehensive dataset for Manual Small-Incision Cataract Surgery (MSICS) containing 53 surgical videos. It includes annotations for 18 surgical phases and pixel-level tool segmentation for 3,527 frames covering 13 surgical tools.",
    "authors": [
      "Bhuvan Sachdeva",
      "Naren Akash",
      "Tajamul Ashraf",
      "Simon Muller",
      "et al."
    ],
    "githubLink": "https://github.com/Sri-Kanchi-Kamakoti-Medical-Trust/ToolSeg",
    "itemCount": "53 videos",
    "source": "arXiv",
    "specs": "Video resolution: 1920x1080 (downscaled for processing), Annotations: Phase timestamps, Pixel-level tool segmentation masks",
    "year": "2024",
    "id": "saved-1769659616402-757fl",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn2YSq_3yazJ-hcPJsK6MhJPfwwcCikKN1KBGkblTxcvlefe_J8skgK0WTR-a7AHECEM0vh1X_F-jKmIztgsAsEjb0b1_DtJYymXomjKTNxFSOO1dqUUPrK8i5nobG-MK-X8lEEb6awHn1x6S0XCNte7rBEg==",
        "title": "miccai.org"
      }
    ]
  },
  {
    "title": "Cataract-1K",
    "paperLink": "https://www.nature.com/articles/s41597-024-03193-4",
    "description": "A large-scale dataset of 1000 cataract surgery videos (Phacoemulsification). It provides phase annotations for 56 videos, semantic segmentation for 30 videos, and subsets for irregularity detection (pupil reaction, lens rotation).",
    "authors": [
      "Negin Ghamsarian",
      "Yosuf El-Shabrawi",
      "Sahar Nasirihaghighi",
      "Klaus Schoeffmann",
      "Raphael Sznitman"
    ],
    "githubLink": "https://github.com/Negin-Ghamsarian/Cataract-1K",
    "itemCount": "1000 videos",
    "source": "Scholar",
    "specs": "Video resolution: 512x324, Frame rate: 25 fps, Annotations: Phases, Semantic Segmentation, Irregularity labels",
    "year": "2024",
    "id": "saved-1769659616402-6ar63",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn2YSq_3yazJ-hcPJsK6MhJPfwwcCikKN1KBGkblTxcvlefe_J8skgK0WTR-a7AHECEM0vh1X_F-jKmIztgsAsEjb0b1_DtJYymXomjKTNxFSOO1dqUUPrK8i5nobG-MK-X8lEEb6awHn1x6S0XCNte7rBEg==",
        "title": "miccai.org"
      }
    ]
  },
  {
    "title": "CaDIS (Cataract Dataset for Image Segmentation)",
    "paperLink": "https://arxiv.org/abs/1906.11586",
    "description": "A dataset for semantic segmentation in cataract surgery, derived from the CATARACTS challenge data. It provides high-quality pixel-level annotations for instruments and anatomical structures.",
    "authors": [
      "Maria Grammatikopoulou",
      "Evangello Flouty",
      "Abdolrahim Kadkhodamohammadi",
      "et al."
    ],
    "githubLink": "https://codalab.lisn.upsaclay.fr/competitions/7857",
    "itemCount": "4670 images",
    "source": "arXiv",
    "specs": "Modality: Images (frames), Annotations: Semantic segmentation masks (36 classes)",
    "year": "2019",
    "id": "saved-1769659616402-xxzis",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn2YSq_3yazJ-hcPJsK6MhJPfwwcCikKN1KBGkblTxcvlefe_J8skgK0WTR-a7AHECEM0vh1X_F-jKmIztgsAsEjb0b1_DtJYymXomjKTNxFSOO1dqUUPrK8i5nobG-MK-X8lEEb6awHn1x6S0XCNte7rBEg==",
        "title": "miccai.org"
      }
    ]
  },
  {
    "title": "Cataract-101",
    "paperLink": "https://www.itec.aau.at/bib/files/p421-schoeffmann.pdf",
    "description": "A standard benchmark dataset for cataract surgery (Phacoemulsification) phase recognition. It consists of 101 videos performed by four different surgeons, annotated with 10 surgical phases.",
    "authors": [
      "Klaus Schoeffmann",
      "Marco Taschwer",
      "Stephanie Sarny",
      "et al."
    ],
    "githubLink": "http://ftp.itec.aau.at/datasets/cataract-101/",
    "itemCount": "101 videos",
    "source": "Scholar",
    "specs": "Video resolution: 720x540, Frame rate: 25 fps, Annotations: Temporal surgical phases",
    "year": "2018",
    "id": "saved-1769659616402-2ts5q",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn2YSq_3yazJ-hcPJsK6MhJPfwwcCikKN1KBGkblTxcvlefe_J8skgK0WTR-a7AHECEM0vh1X_F-jKmIztgsAsEjb0b1_DtJYymXomjKTNxFSOO1dqUUPrK8i5nobG-MK-X8lEEb6awHn1x6S0XCNte7rBEg==",
        "title": "miccai.org"
      }
    ]
  },
  {
    "title": "CATARACTS",
    "paperLink": "https://arxiv.org/abs/1710.09117",
    "description": "A benchmark from the MICCAI 2017 challenge focusing on tool presence detection and phase recognition in Phacoemulsification cataract surgery.",
    "authors": [
      "Hasan Kassem",
      "Massimo Al Ganzoury",
      "et al."
    ],
    "githubLink": "https://cataracts.grand-challenge.org/",
    "itemCount": "50 videos",
    "source": "Grand Challenge",
    "specs": "Video resolution: 1920x1080, Frame rate: 30 fps, Annotations: Tool presence (binary), Phase recognition",
    "year": "2017",
    "id": "saved-1769659616402-i3nf7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFn2YSq_3yazJ-hcPJsK6MhJPfwwcCikKN1KBGkblTxcvlefe_J8skgK0WTR-a7AHECEM0vh1X_F-jKmIztgsAsEjb0b1_DtJYymXomjKTNxFSOO1dqUUPrK8i5nobG-MK-X8lEEb6awHn1x6S0XCNte7rBEg==",
        "title": "miccai.org"
      }
    ]
  },
  {
    "title": "SELMA3D (Self-supervised Learning for 3D Light-Sheet Microscopy Image Segmentation)",
    "paperLink": "https://arxiv.org/abs/2501.03880",
    "description": "A large-scale challenge dataset designed for self-supervised learning in 3D light-sheet microscopy. It includes diverse biological structures (vessels, cells, plaques) from cleared mouse and human brains.",
    "authors": [
      "Ying Chen",
      "Grand Challenge Organizers"
    ],
    "githubLink": "https://github.com/YingChen7/SELMA3D_challenge-submission-example",
    "itemCount": "35 large 3D images (>1000^3 voxels each), 315 annotated patches",
    "source": "Grand Challenge / arXiv",
    "specs": "3D Light-sheet microscopy (LSM), cleared tissue, TIF/HDF5 format",
    "year": "2025",
    "id": "saved-1769659671755-kt4nf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5sEV8nWKkPNPXMdEbT4ornEwieDvEoKrUSAUUdFGJfUfwRgfAmn9Zk6xzAXJR1CIoU3q0IhUFgSSwXf1C0YHDps69eX_UnBa-xgqay0AqiQp-hA4r3XYeL34=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-lv0CBwUS8G_I6DxXGGJtR1DqjoMvpIwO7VJYDF7HmfLY-XtUn5gA00OCN2r2tk1AOzt5H92Zh_eMQV1DiiXPwfgK5uF5WdvArhJzwQVCqkKjw_TRxu_r2ty-AWPR",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "CellSeg3D mesoSPIM Dataset",
    "paperLink": "https://doi.org/10.7554/eLife.99848",
    "description": "A fully human-annotated 3D cell segmentation dataset obtained from mesoSPIM scans of cleared mouse brain tissue, specifically designed to benchmark supervised and self-supervised methods.",
    "authors": [
      "Cyril Achard",
      "Timokleia Kousi",
      "Markus Frey",
      "Mackenzie Mathis"
    ],
    "githubLink": "https://github.com/AdaptiveMotorControlLab/CellSeg3D",
    "itemCount": "2632 annotated cells across 6 volumes",
    "source": "GitHub / eLife",
    "specs": "3D mesoSPIM (Light-sheet), fluorescence, TPH2-positive cells",
    "year": "2024",
    "id": "saved-1769659671755-bfxqe",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5sEV8nWKkPNPXMdEbT4ornEwieDvEoKrUSAUUdFGJfUfwRgfAmn9Zk6xzAXJR1CIoU3q0IhUFgSSwXf1C0YHDps69eX_UnBa-xgqay0AqiQp-hA4r3XYeL34=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-lv0CBwUS8G_I6DxXGGJtR1DqjoMvpIwO7VJYDF7HmfLY-XtUn5gA00OCN2r2tk1AOzt5H92Zh_eMQV1DiiXPwfgK5uF5WdvArhJzwQVCqkKjw_TRxu_r2ty-AWPR",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "EmbedSeg 3D Datasets (Platynereis & Mouse Skull)",
    "paperLink": "https://arxiv.org/abs/2101.10033",
    "description": "A collection of 3D microscopy datasets formatted for instance segmentation, including specific light-sheet acquisitions of Platynereis nuclei and mouse skull nuclei.",
    "authors": [
      "Manan Lalit",
      "Pavel Tomancak",
      "Florian Jug"
    ],
    "githubLink": "https://github.com/juglab/EmbedSeg",
    "itemCount": "4 datasets (2 Light-sheet specific)",
    "source": "arXiv / GitHub",
    "specs": "3D Light-sheet microscopy, instance segmentation masks",
    "year": "2021",
    "id": "saved-1769659671756-bl059",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5sEV8nWKkPNPXMdEbT4ornEwieDvEoKrUSAUUdFGJfUfwRgfAmn9Zk6xzAXJR1CIoU3q0IhUFgSSwXf1C0YHDps69eX_UnBa-xgqay0AqiQp-hA4r3XYeL34=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-lv0CBwUS8G_I6DxXGGJtR1DqjoMvpIwO7VJYDF7HmfLY-XtUn5gA00OCN2r2tk1AOzt5H92Zh_eMQV1DiiXPwfgK5uF5WdvArhJzwQVCqkKjw_TRxu_r2ty-AWPR",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "Fluo-N3DL-TRIC (Cell Tracking Challenge)",
    "paperLink": "http://celltrackingchallenge.net",
    "description": "A large-scale 3D+time dataset for nuclei segmentation and tracking in developing Tribolium Castaneum embryos, acquired using a Zeiss LightSheet Z.1 microscope.",
    "authors": [
      "Akanksha Jain",
      "Cell Tracking Challenge Organizers"
    ],
    "githubLink": "http://celltrackingchallenge.net/3d-datasets/",
    "itemCount": "20.6 GB (Training), 19.9 GB (Test)",
    "source": "Cell Tracking Challenge",
    "specs": "3D+Time Light-sheet, cartographic projections",
    "year": "2017",
    "id": "saved-1769659671756-x0jp4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5sEV8nWKkPNPXMdEbT4ornEwieDvEoKrUSAUUdFGJfUfwRgfAmn9Zk6xzAXJR1CIoU3q0IhUFgSSwXf1C0YHDps69eX_UnBa-xgqay0AqiQp-hA4r3XYeL34=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-lv0CBwUS8G_I6DxXGGJtR1DqjoMvpIwO7VJYDF7HmfLY-XtUn5gA00OCN2r2tk1AOzt5H92Zh_eMQV1DiiXPwfgK5uF5WdvArhJzwQVCqkKjw_TRxu_r2ty-AWPR",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "Fluo-N3DL-DRO (Cell Tracking Challenge)",
    "paperLink": "http://www.nature.com/nmeth/journal/v11/n5/full/nmeth.2929.html",
    "description": "A benchmark dataset for cell tracking and segmentation in 3D light-sheet microscopy, featuring developing Drosophila Melanogaster embryos.",
    "authors": [
      "Philipp J. Keller",
      "Cell Tracking Challenge Organizers"
    ],
    "githubLink": "http://celltrackingchallenge.net/3d-datasets/",
    "itemCount": "5.8 GB (Training), 5.9 GB (Test)",
    "source": "Cell Tracking Challenge",
    "specs": "3D+Time Light-sheet (SIMView), 0.406x0.406x2.03 micron voxel size",
    "year": "2014",
    "id": "saved-1769659671756-nvtn1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5sEV8nWKkPNPXMdEbT4ornEwieDvEoKrUSAUUdFGJfUfwRgfAmn9Zk6xzAXJR1CIoU3q0IhUFgSSwXf1C0YHDps69eX_UnBa-xgqay0AqiQp-hA4r3XYeL34=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-lv0CBwUS8G_I6DxXGGJtR1DqjoMvpIwO7VJYDF7HmfLY-XtUn5gA00OCN2r2tk1AOzt5H92Zh_eMQV1DiiXPwfgK5uF5WdvArhJzwQVCqkKjw_TRxu_r2ty-AWPR",
        "title": "grand-challenge.org"
      }
    ]
  },
  {
    "title": "HNTS-MRG 2024",
    "paperLink": "https://hntsmrg24.grand-challenge.org/",
    "description": "Dataset for the Head and Neck Tumor Segmentation for MR-Guided Applications challenge. Focuses on segmenting primary (GTVp) and nodal (GTVn) tumor volumes on pre-radiotherapy and mid-radiotherapy T2-weighted MRI scans.",
    "authors": [
      "Wahid, K.A.",
      "McDonald, B.A.",
      "Fuller, C.D.",
      "Naser, M.A."
    ],
    "githubLink": "https://github.com/brianmanderson/HNTS-MRG-2024",
    "itemCount": "200 cases (150 Training, 50 Test)",
    "source": "Grand Challenge / Zenodo",
    "specs": "MRI (T2-weighted), NIfTI format, GTVp and GTVn masks",
    "year": "2024",
    "id": "saved-1769659743331-u59qh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHTHnVXXXsTvtceb34hEz3YII83DRkpzVzK-viMyl2foNL16-XtydG2fedNUT_828DF5zQOv5UeYnOzzeHSH_PhYjky8iB7erApfiROY5gUVIAFXLnwoTIOcjGB8zzkoWL46R2tqveDRw0=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaaYleWhYkxgIkqiUojbkAWh6YfExLOM6cuPQfwDwINNAZ2QaSOCdCWX3tw9sdxMHJIjdtEZictOEXVUh1Kh_bSVQh8R0NLy_ek0tjsPZkeIZumJmUQVRAav9qPry1UH06DJgdM6GyBruHD__ZgyBhSOT3lLqLveRu3RQ_OG5S6zNV9a8ivCP9_hS2UGg82LNnObDh2VHuPvzH0WQK-YfvqvEFQOQm1QhVmaXvJ7IHrEl0QVPv1wHm91UVfR7yxL1HvdEeU5Hq6vDot1iszE5rA8qnGaIkzg4JZxLg-E_0V38=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcUdZUPRe4fx-JZ_1xCZq4MM-9pnoupE6ELDLfog7fkJwqFVxSUNbh49891-xLX6wCkoqGWyUXJOvGHt1S8pFiumMZURnZ7algiOHMx19m8bWGa3nhjrrWL1RAgYIBaHM=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELByU3Jbg5ZEKh4rDmcnHdX_qI05rbfnyVz4KJ21Oo1BTT-AM3drPHymFoon312mX-oxR7vn4_S5FcffLZzcMHl1Ru67WKnuji0PWO_FC2k_U_T_nE0rufL9lABDGnw35oZE0CKJBgpQ==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtmDcif-NQca2PUAgxyHGAbAAOZAtIh6_jWHp9lv_godrW92RGCv02RhmiUbdJvk9QxhgH8qJkDNNOqn8AhOpp2xTyw_JMdby0EaKmU_fl0ICqKQ-twsp-IkbfV_5O2uISbUtg1SQNquUqBnURsl263sVPbUmP2ggjZCxu-PYvolthaJaog2KDwLBJylhe1MAm",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "SegRap 2023",
    "paperLink": "https://doi.org/10.1016/j.media.2025.103504",
    "description": "A benchmark for the segmentation of Organs-at-Risk (OARs) and Gross Tumor Volumes (GTV) for Nasopharyngeal Carcinoma (NPC) radiotherapy planning. Includes tasks for both primary tumor (GTVp) and lymph node (GTVn) segmentation.",
    "authors": [
      "Luo, X.",
      "Liao, W.",
      "Xiao, H.",
      "Chen, J.",
      "Song, T.",
      "Zhang, S."
    ],
    "githubLink": "https://github.com/HiLab-git/SegRap2023",
    "itemCount": "200 cases (120 Training, 20 Validation, 60 Test)",
    "source": "MICCAI Challenge / Hugging Face",
    "specs": "CT (Contrast & Non-contrast), 47 labels (45 OARs + 2 GTVs)",
    "year": "2023",
    "id": "saved-1769659743331-9t430",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHTHnVXXXsTvtceb34hEz3YII83DRkpzVzK-viMyl2foNL16-XtydG2fedNUT_828DF5zQOv5UeYnOzzeHSH_PhYjky8iB7erApfiROY5gUVIAFXLnwoTIOcjGB8zzkoWL46R2tqveDRw0=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaaYleWhYkxgIkqiUojbkAWh6YfExLOM6cuPQfwDwINNAZ2QaSOCdCWX3tw9sdxMHJIjdtEZictOEXVUh1Kh_bSVQh8R0NLy_ek0tjsPZkeIZumJmUQVRAav9qPry1UH06DJgdM6GyBruHD__ZgyBhSOT3lLqLveRu3RQ_OG5S6zNV9a8ivCP9_hS2UGg82LNnObDh2VHuPvzH0WQK-YfvqvEFQOQm1QhVmaXvJ7IHrEl0QVPv1wHm91UVfR7yxL1HvdEeU5Hq6vDot1iszE5rA8qnGaIkzg4JZxLg-E_0V38=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcUdZUPRe4fx-JZ_1xCZq4MM-9pnoupE6ELDLfog7fkJwqFVxSUNbh49891-xLX6wCkoqGWyUXJOvGHt1S8pFiumMZURnZ7algiOHMx19m8bWGa3nhjrrWL1RAgYIBaHM=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELByU3Jbg5ZEKh4rDmcnHdX_qI05rbfnyVz4KJ21Oo1BTT-AM3drPHymFoon312mX-oxR7vn4_S5FcffLZzcMHl1Ru67WKnuji0PWO_FC2k_U_T_nE0rufL9lABDGnw35oZE0CKJBgpQ==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtmDcif-NQca2PUAgxyHGAbAAOZAtIh6_jWHp9lv_godrW92RGCv02RhmiUbdJvk9QxhgH8qJkDNNOqn8AhOpp2xTyw_JMdby0EaKmU_fl0ICqKQ-twsp-IkbfV_5O2uISbUtg1SQNquUqBnURsl263sVPbUmP2ggjZCxu-PYvolthaJaog2KDwLBJylhe1MAm",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "HECKTOR 2022 (Head and Neck Tumor segmentation and outcome prediction)",
    "paperLink": "https://doi.org/10.1007/978-3-031-27420-6",
    "description": "A large-scale challenge dataset for the automatic segmentation of Head and Neck primary tumors (GTVp) and lymph nodes (GTVn) from FDG-PET/CT images, as well as patient outcome prediction.",
    "authors": [
      "Andrearczyk, V.",
      "Oreiller, V.",
      "Depeursinge, A.",
      "Jreige, M.",
      "Boughdad, S.",
      "Elhalawani, H."
    ],
    "githubLink": "https://github.com/voreille/hecktor",
    "itemCount": "~882 cases (524 Training, 358 Test)",
    "source": "MICCAI Challenge",
    "specs": "PET/CT images, Multi-modal, NIfTI format",
    "year": "2022",
    "id": "saved-1769659743331-hc7qm",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHTHnVXXXsTvtceb34hEz3YII83DRkpzVzK-viMyl2foNL16-XtydG2fedNUT_828DF5zQOv5UeYnOzzeHSH_PhYjky8iB7erApfiROY5gUVIAFXLnwoTIOcjGB8zzkoWL46R2tqveDRw0=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaaYleWhYkxgIkqiUojbkAWh6YfExLOM6cuPQfwDwINNAZ2QaSOCdCWX3tw9sdxMHJIjdtEZictOEXVUh1Kh_bSVQh8R0NLy_ek0tjsPZkeIZumJmUQVRAav9qPry1UH06DJgdM6GyBruHD__ZgyBhSOT3lLqLveRu3RQ_OG5S6zNV9a8ivCP9_hS2UGg82LNnObDh2VHuPvzH0WQK-YfvqvEFQOQm1QhVmaXvJ7IHrEl0QVPv1wHm91UVfR7yxL1HvdEeU5Hq6vDot1iszE5rA8qnGaIkzg4JZxLg-E_0V38=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcUdZUPRe4fx-JZ_1xCZq4MM-9pnoupE6ELDLfog7fkJwqFVxSUNbh49891-xLX6wCkoqGWyUXJOvGHt1S8pFiumMZURnZ7algiOHMx19m8bWGa3nhjrrWL1RAgYIBaHM=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELByU3Jbg5ZEKh4rDmcnHdX_qI05rbfnyVz4KJ21Oo1BTT-AM3drPHymFoon312mX-oxR7vn4_S5FcffLZzcMHl1Ru67WKnuji0PWO_FC2k_U_T_nE0rufL9lABDGnw35oZE0CKJBgpQ==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtmDcif-NQca2PUAgxyHGAbAAOZAtIh6_jWHp9lv_godrW92RGCv02RhmiUbdJvk9QxhgH8qJkDNNOqn8AhOpp2xTyw_JMdby0EaKmU_fl0ICqKQ-twsp-IkbfV_5O2uISbUtg1SQNquUqBnURsl263sVPbUmP2ggjZCxu-PYvolthaJaog2KDwLBJylhe1MAm",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "NSCLC-Radiomics (Lung1)",
    "paperLink": "https://www.nature.com/articles/ncomms5006",
    "description": "A foundational dataset for lung cancer radiomics and Gross Tumor Volume (GTV) segmentation. It contains pretreatment CT scans of non-small cell lung cancer (NSCLC) patients with manual delineations of the 3D GTV by radiation oncologists.",
    "authors": [
      "Aerts, H.J.",
      "Velazquez, E.R.",
      "Leijenaar, R.T.",
      "Parmar, C.",
      "Grossmann, P.",
      "Cavalho, S.",
      "Bussink, J.",
      "Monshouwer, R.",
      "Haibe-Kains, B.",
      "Rietveld, D."
    ],
    "githubLink": "https://github.com/maastro-radiotherapy/radiomics",
    "itemCount": "422 patients",
    "source": "TCIA / Google Scholar",
    "specs": "CT images, DICOM RTSTRUCT (GTV contours), Clinical data",
    "year": "2014",
    "id": "saved-1769659743331-valvy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHTHnVXXXsTvtceb34hEz3YII83DRkpzVzK-viMyl2foNL16-XtydG2fedNUT_828DF5zQOv5UeYnOzzeHSH_PhYjky8iB7erApfiROY5gUVIAFXLnwoTIOcjGB8zzkoWL46R2tqveDRw0=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHaaYleWhYkxgIkqiUojbkAWh6YfExLOM6cuPQfwDwINNAZ2QaSOCdCWX3tw9sdxMHJIjdtEZictOEXVUh1Kh_bSVQh8R0NLy_ek0tjsPZkeIZumJmUQVRAav9qPry1UH06DJgdM6GyBruHD__ZgyBhSOT3lLqLveRu3RQ_OG5S6zNV9a8ivCP9_hS2UGg82LNnObDh2VHuPvzH0WQK-YfvqvEFQOQm1QhVmaXvJ7IHrEl0QVPv1wHm91UVfR7yxL1HvdEeU5Hq6vDot1iszE5rA8qnGaIkzg4JZxLg-E_0V38=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcUdZUPRe4fx-JZ_1xCZq4MM-9pnoupE6ELDLfog7fkJwqFVxSUNbh49891-xLX6wCkoqGWyUXJOvGHt1S8pFiumMZURnZ7algiOHMx19m8bWGa3nhjrrWL1RAgYIBaHM=",
        "title": "grand-challenge.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELByU3Jbg5ZEKh4rDmcnHdX_qI05rbfnyVz4KJ21Oo1BTT-AM3drPHymFoon312mX-oxR7vn4_S5FcffLZzcMHl1Ru67WKnuji0PWO_FC2k_U_T_nE0rufL9lABDGnw35oZE0CKJBgpQ==",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtmDcif-NQca2PUAgxyHGAbAAOZAtIh6_jWHp9lv_godrW92RGCv02RhmiUbdJvk9QxhgH8qJkDNNOqn8AhOpp2xTyw_JMdby0EaKmU_fl0ICqKQ-twsp-IkbfV_5O2uISbUtg1SQNquUqBnURsl263sVPbUmP2ggjZCxu-PYvolthaJaog2KDwLBJylhe1MAm",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "SegRap2025: Segmentation of Gross Tumor Volume and Lymph Node Clinical Target Volume Challenge",
    "paperLink": "https://zenodo.org/records/10829107",
    "description": "An extension of the SegRap2023 benchmark, this challenge focuses specifically on Lymph Node Clinical Target Volume (LN CTV) and Gross Tumor Volume (GTV) segmentation for Nasopharyngeal Carcinoma. It utilizes an expanded version of the NPC-LN-CTV dataset.",
    "authors": [
      "Xiangde Luo",
      "Jia Fu",
      "Shaoting Zhang",
      "et al."
    ],
    "githubLink": "https://github.com/HiLab-git/SegRap2023",
    "itemCount": "402 patients (Training/Validation/Testing)",
    "source": "MICCAI Challenge / Zenodo",
    "specs": "Multi-modal CT scans; Masks for GTVnx, GTVnd, and LN CTVs",
    "year": "2025",
    "id": "saved-1769659834365-h4pzu",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7gWRpmR81TJ705OI0cQG84we39eIwhDsTM-B4MrLaM1THIKKahoMFxBqA2Eaekk-nABgCY862_X6V-eXaXOS4YNYHXbx5JsNPKlyxf44GIgXSvn3oucq0hFCOTfb7RKW1QQ8aIVy_0wCwXvOPD-qksgBvo0XmHjfsvVUvDfvgjD33TNq-v4dmbc3yG-ZDRmQ2L16Pt2gLTY53gtcnERO7ANfTDCaVpWPJUffQZScHE0xWX7Sj-dGSIwlUV9tZy9Wt5PyprU-SwqBZ6aMlFEJZcYXj5PUYV7T5GWutKLEA-0C2PVA4OXekNTmKLS_PxpmAqqeJlMM1quG8cwHzskeXGe23QvvtmNgEfACuYt0D7qDb",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "A multicenter dataset for lymph node clinical target volume delineation of nasopharyngeal carcinoma (NPC-LN-CTV)",
    "paperLink": "https://doi.org/10.1038/s41597-024-03890-0",
    "description": "A comprehensive benchmark dataset specifically designed for the automated delineation of Lymph Node (LN) Clinical Target Volumes (CTV) in Nasopharyngeal Carcinoma (NPC) radiotherapy. It addresses the scarcity of public data for CTV segmentation by providing high-quality, expert-validated manual contours for six specific lymph node levels (Ia, Ib, II, III, IV, V).",
    "authors": [
      "Xiangde Luo",
      "Wenjun Liao",
      "Yue Zhao",
      "Shaoting Zhang",
      "et al."
    ],
    "githubLink": "https://github.com/Luoxd1996/LNCTVSeg",
    "itemCount": "262 patients (440 CT scans)",
    "source": "Scientific Data (Nature)",
    "specs": "CT scans (non-contrast and contrast-enhanced); Pixel-level segmentation masks for 6 LN CTV levels; NIFTI format",
    "year": "2024",
    "id": "saved-1769659834365-bdozu",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7gWRpmR81TJ705OI0cQG84we39eIwhDsTM-B4MrLaM1THIKKahoMFxBqA2Eaekk-nABgCY862_X6V-eXaXOS4YNYHXbx5JsNPKlyxf44GIgXSvn3oucq0hFCOTfb7RKW1QQ8aIVy_0wCwXvOPD-qksgBvo0XmHjfsvVUvDfvgjD33TNq-v4dmbc3yG-ZDRmQ2L16Pt2gLTY53gtcnERO7ANfTDCaVpWPJUffQZScHE0xWX7Sj-dGSIwlUV9tZy9Wt5PyprU-SwqBZ6aMlFEJZcYXj5PUYV7T5GWutKLEA-0C2PVA4OXekNTmKLS_PxpmAqqeJlMM1quG8cwHzskeXGe23QvvtmNgEfACuYt0D7qDb",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AAPM RT-MAC Grand Challenge 2019 (MRI Auto-Contouring)",
    "paperLink": "https://doi.org/10.7937/tcia.2019.bcfjqfqb",
    "description": "A benchmark dataset for the auto-segmentation of head and neck organs at risk and lymph node levels in MRI. While primarily for OARs, it includes manual contours for Lymph Node Levels II and III, serving as a benchmark for nodal volume definition in MRI-guided radiotherapy.",
    "authors": [
      "C. Cardenas",
      "A. Aristophanous",
      "L. Court",
      "et al."
    ],
    "githubLink": "https://github.com/aapm-rt-mac/challenge",
    "itemCount": "55 patients",
    "source": "The Cancer Imaging Archive (TCIA)",
    "specs": "T2-weighted MRI; DICOM RTSTRUCT contours for LN Levels II and III, Parotid, Submandibular glands",
    "year": "2019",
    "id": "saved-1769659834365-eq7eo",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7gWRpmR81TJ705OI0cQG84we39eIwhDsTM-B4MrLaM1THIKKahoMFxBqA2Eaekk-nABgCY862_X6V-eXaXOS4YNYHXbx5JsNPKlyxf44GIgXSvn3oucq0hFCOTfb7RKW1QQ8aIVy_0wCwXvOPD-qksgBvo0XmHjfsvVUvDfvgjD33TNq-v4dmbc3yG-ZDRmQ2L16Pt2gLTY53gtcnERO7ANfTDCaVpWPJUffQZScHE0xWX7Sj-dGSIwlUV9tZy9Wt5PyprU-SwqBZ6aMlFEJZcYXj5PUYV7T5GWutKLEA-0C2PVA4OXekNTmKLS_PxpmAqqeJlMM1quG8cwHzskeXGe23QvvtmNgEfACuYt0D7qDb",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "PathGen-1.6M",
    "paperLink": "https://arxiv.org/abs/2406.11892",
    "description": "A large-scale pathology dataset containing 1.6 million image-caption pairs. It leverages multi-agent collaboration to extract high-quality patches from TCGA Whole Slide Images (WSIs) and generate detailed captions, designed to train pathology-specific multimodal models.",
    "authors": [
      "Yuxuan Sun",
      "Yunlong Zhang",
      "Yixuan Si",
      "Chenglu Zhu",
      "Kai Zhang",
      "Zhongyi Shui",
      "Jingxiong Li",
      "Xingheng Lyu",
      "Tao Lin",
      "Lin Yang"
    ],
    "githubLink": "https://github.com/PathFoundation/PathGen-1.6M",
    "itemCount": "1.6 Million pairs",
    "source": "arXiv",
    "specs": "Image-text pairs (Histopathology patches and generated captions)",
    "year": "2024",
    "id": "saved-1769659920405-an4ep",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLFN_171K0UrlXI4bTRnOS6Z5FU3wALQkb8oroHtADWRHClRGbiVFVxsOZyPmyviQheT_yzn9yxlLfScT2vU-Ocl7wFbS9TRWgnWTMHhBtd7CVmNp7",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE05X-1Pw6SzrLznT1I2-2RrrI4koKa2xNyT1JxbcczrvQRntrLSS7sbg1bZV3G1VrRweh8E0fMpRcwAGCQfNnWa9edEzfXGfWKUtB45raKpvE7sgWGUOV0G3AHDuWL8eMrTK8gGDoymburEeFNMAU6aeIFyhBXVBWZxgQwPqQD28qskNV1YLcRGlBwLWLlw1s9pH4RuKe_VMkKjD1CFy_zZ8MI8OSzFMU3FtEQzncfn3yKlz2A5MWB",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEq98GXiE12W29w4SNWsv9tJr8Baf5XEWEZEvYkVC070bWrLVvG5niAumKyRu1MCcT07gfNyTaJItkD_ipi3JL9PTIAfOTV03SM7lkjz3X57KqPSWJ4X6zpEy2lVm7vipvxzuB9rJ8ME4g5f9WN",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw27D4_gePMCt7SsMvBTjuFntemlOcgc9AoSiX9FcO4kPcxDVOTurQUOAkH1YiW7R6wKPyVipYavZXI8HnDmAcX4duXtWWbpbZekrWNkN4YRz5BkR9zTlzCpOflZcvaJjKkTxlcXzzQrovdBVrgTYEZ3AY",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6anN6T3m-Z6eb4Nc_dqa2ih0HDLAMztNQwHgsm5rbVdGm0_ZIU-FuPLtvcNrTWXwfvd7oXnm9NvstBTedMWVuiqzxGs7vCT9IeOlp1p3qI-GNin8HLqWoTc30pW69kp9Q2JGi8G8IGnTRbrZ5So7y-KjtjydGq0c0iV5nMaibPnAgjFRtVBPgpdvBXMsviKc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeCP8kZUEyVh_ODgpK5ti0Aj-L8mREoh4Mr_gdwiuYnj9BIiz21DJd3joRt7V0yeR_zHes4ihyocPDQbq0OQkhFK_lT5NF6aSR6iVpo7g0vJdKYoxFvi3RocTDRvwymXCl0vudQJFij_GRCA==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF81bz524lswEKocIxQjkiSII7GsydYkyw_V-RM4O7qe4ofaz3MfQeAxCbtodajTonApiC8WCjmSL4u8Ndu6bgOQEu3weYljnXOxRQMulS-N0OC-3RO3O7UKDnN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzKSSH2vMBrVjYNi6YUfJrsKRzLGemULObZQ6HkTM3kIpV_H3qK_sx8OSLLD-S216ZqkO65ToZrNdROw6vINNdKrE3RbKq7-YS4hk_IlGkh3IVodho7U9W5fkD1peraOHmREJvVPCA1lhX",
        "title": "mendeley.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Xmkuh-fnONPoVJBm6PJggq5ZfSTdU4HDQ2qBHXoRjeFdN1_5ZlNrlSZhfkqsAsaMswrxB-W8pqLRB1-iBzPKZIdO5ueWxJErK4tSubSlXGiRT55l6AYHLx7lnixmyQUauiBi3_SJi0w4gHEfVWcMHdFgDzh7N-P_HwWtdKBgbOyPMnSCxofF0sg_Jg_6l_zAxY_QFflFvL_LjZdIortZEyuzl45GNquWIJLlBjqcOiYoJQp53_W_d1sF8G54UylWS2B944BBTTps5bdi-6K4fF-8eCc=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnDdhne4jymrNuplze8v4QOvmF85Hfxzfecix0oM-rYhTpe125bIvLC46-FXWSkeiCP1MQ6BH5fcLUkoMLs0NUh3XuWIyIcNukRVMqwOw2Rtxuvjkkb4pMeGwTUSgK",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6Gnk6ELT8_NSRu-jWL8q0STCjagE_u1Y4JBWlTbxva5S4HWQnshr3AFgwFUWYdtt61ym_64doVth10tYp1TlULsjERyD4dvfXCxcMaTaHXOVM7EaQ8gDcKWLI",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJuBStWT-pixBntP7WtIsjs6bpzm8FXaMNTV7QWT-TMoa-qfWOqQrfr2O3XEazMvgAQ9C00WX2MxOhddWT7VCtuBt0HqGMEZk_ipX0Kgk4nPDaNw9sPg==",
        "title": "cbioportal.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreVKZQ-Ut_TIvpUK6PUZz05O084jcIp3Lgy6omwMJmXcfxS7Q4Ch4Zvmv8v2nSL9DMYEL8qy9pw9LTjcqttoQM6VAMbAY2Gda7nv8JgcUcV_LhD58Cr86ngzAnp4pZLPkRanm2A==",
        "title": "bevelcloud.ai"
      }
    ]
  },
  {
    "title": "PathText (WsiCaption)",
    "paperLink": "https://arxiv.org/abs/2409.15574",
    "description": "A dataset of approximately 10,000 high-quality Whole Slide Image (WSI) and pathology report pairs. It was created by processing and cleaning PDF pathology reports from The Cancer Genome Atlas (TCGA) to facilitate slide-level report generation tasks.",
    "authors": [
      "Pingyi Chen",
      "Honglin Li",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Zhongyi Shui",
      "Lin Yang"
    ],
    "githubLink": "https://github.com/cpystan/Wsi-Caption",
    "itemCount": "10,000 pairs",
    "source": "Scholar",
    "specs": "WSI-text pairs (Whole Slide Images and report text)",
    "year": "2024",
    "id": "saved-1769659920406-ootfi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLFN_171K0UrlXI4bTRnOS6Z5FU3wALQkb8oroHtADWRHClRGbiVFVxsOZyPmyviQheT_yzn9yxlLfScT2vU-Ocl7wFbS9TRWgnWTMHhBtd7CVmNp7",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE05X-1Pw6SzrLznT1I2-2RrrI4koKa2xNyT1JxbcczrvQRntrLSS7sbg1bZV3G1VrRweh8E0fMpRcwAGCQfNnWa9edEzfXGfWKUtB45raKpvE7sgWGUOV0G3AHDuWL8eMrTK8gGDoymburEeFNMAU6aeIFyhBXVBWZxgQwPqQD28qskNV1YLcRGlBwLWLlw1s9pH4RuKe_VMkKjD1CFy_zZ8MI8OSzFMU3FtEQzncfn3yKlz2A5MWB",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEq98GXiE12W29w4SNWsv9tJr8Baf5XEWEZEvYkVC070bWrLVvG5niAumKyRu1MCcT07gfNyTaJItkD_ipi3JL9PTIAfOTV03SM7lkjz3X57KqPSWJ4X6zpEy2lVm7vipvxzuB9rJ8ME4g5f9WN",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw27D4_gePMCt7SsMvBTjuFntemlOcgc9AoSiX9FcO4kPcxDVOTurQUOAkH1YiW7R6wKPyVipYavZXI8HnDmAcX4duXtWWbpbZekrWNkN4YRz5BkR9zTlzCpOflZcvaJjKkTxlcXzzQrovdBVrgTYEZ3AY",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6anN6T3m-Z6eb4Nc_dqa2ih0HDLAMztNQwHgsm5rbVdGm0_ZIU-FuPLtvcNrTWXwfvd7oXnm9NvstBTedMWVuiqzxGs7vCT9IeOlp1p3qI-GNin8HLqWoTc30pW69kp9Q2JGi8G8IGnTRbrZ5So7y-KjtjydGq0c0iV5nMaibPnAgjFRtVBPgpdvBXMsviKc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeCP8kZUEyVh_ODgpK5ti0Aj-L8mREoh4Mr_gdwiuYnj9BIiz21DJd3joRt7V0yeR_zHes4ihyocPDQbq0OQkhFK_lT5NF6aSR6iVpo7g0vJdKYoxFvi3RocTDRvwymXCl0vudQJFij_GRCA==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF81bz524lswEKocIxQjkiSII7GsydYkyw_V-RM4O7qe4ofaz3MfQeAxCbtodajTonApiC8WCjmSL4u8Ndu6bgOQEu3weYljnXOxRQMulS-N0OC-3RO3O7UKDnN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzKSSH2vMBrVjYNi6YUfJrsKRzLGemULObZQ6HkTM3kIpV_H3qK_sx8OSLLD-S216ZqkO65ToZrNdROw6vINNdKrE3RbKq7-YS4hk_IlGkh3IVodho7U9W5fkD1peraOHmREJvVPCA1lhX",
        "title": "mendeley.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Xmkuh-fnONPoVJBm6PJggq5ZfSTdU4HDQ2qBHXoRjeFdN1_5ZlNrlSZhfkqsAsaMswrxB-W8pqLRB1-iBzPKZIdO5ueWxJErK4tSubSlXGiRT55l6AYHLx7lnixmyQUauiBi3_SJi0w4gHEfVWcMHdFgDzh7N-P_HwWtdKBgbOyPMnSCxofF0sg_Jg_6l_zAxY_QFflFvL_LjZdIortZEyuzl45GNquWIJLlBjqcOiYoJQp53_W_d1sF8G54UylWS2B944BBTTps5bdi-6K4fF-8eCc=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnDdhne4jymrNuplze8v4QOvmF85Hfxzfecix0oM-rYhTpe125bIvLC46-FXWSkeiCP1MQ6BH5fcLUkoMLs0NUh3XuWIyIcNukRVMqwOw2Rtxuvjkkb4pMeGwTUSgK",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6Gnk6ELT8_NSRu-jWL8q0STCjagE_u1Y4JBWlTbxva5S4HWQnshr3AFgwFUWYdtt61ym_64doVth10tYp1TlULsjERyD4dvfXCxcMaTaHXOVM7EaQ8gDcKWLI",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJuBStWT-pixBntP7WtIsjs6bpzm8FXaMNTV7QWT-TMoa-qfWOqQrfr2O3XEazMvgAQ9C00WX2MxOhddWT7VCtuBt0HqGMEZk_ipX0Kgk4nPDaNw9sPg==",
        "title": "cbioportal.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreVKZQ-Ut_TIvpUK6PUZz05O084jcIp3Lgy6omwMJmXcfxS7Q4Ch4Zvmv8v2nSL9DMYEL8qy9pw9LTjcqttoQM6VAMbAY2Gda7nv8JgcUcV_LhD58Cr86ngzAnp4pZLPkRanm2A==",
        "title": "bevelcloud.ai"
      }
    ]
  },
  {
    "title": "TCGA-Reports",
    "paperLink": "https://doi.org/10.1016/j.patter.2023.100913",
    "description": "A machine-readable corpus of pathology reports derived from The Cancer Genome Atlas (TCGA). While primarily a text resource, it links to publicly available TCGA images, serving as a benchmark for text-based AI models and report generation when paired with image data.",
    "authors": [
      "Jivan Kefeli",
      "Benjamin S. Glicksberg",
      "Nicholas P. Tatonetti"
    ],
    "githubLink": "https://github.com/tatonetti-lab/tcga-path-reports",
    "itemCount": "9,523 reports",
    "source": "Scholar",
    "specs": "Text (Pathology reports, linkable to TCGA WSIs)",
    "year": "2024",
    "id": "saved-1769659920406-za1qa",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLFN_171K0UrlXI4bTRnOS6Z5FU3wALQkb8oroHtADWRHClRGbiVFVxsOZyPmyviQheT_yzn9yxlLfScT2vU-Ocl7wFbS9TRWgnWTMHhBtd7CVmNp7",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE05X-1Pw6SzrLznT1I2-2RrrI4koKa2xNyT1JxbcczrvQRntrLSS7sbg1bZV3G1VrRweh8E0fMpRcwAGCQfNnWa9edEzfXGfWKUtB45raKpvE7sgWGUOV0G3AHDuWL8eMrTK8gGDoymburEeFNMAU6aeIFyhBXVBWZxgQwPqQD28qskNV1YLcRGlBwLWLlw1s9pH4RuKe_VMkKjD1CFy_zZ8MI8OSzFMU3FtEQzncfn3yKlz2A5MWB",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEq98GXiE12W29w4SNWsv9tJr8Baf5XEWEZEvYkVC070bWrLVvG5niAumKyRu1MCcT07gfNyTaJItkD_ipi3JL9PTIAfOTV03SM7lkjz3X57KqPSWJ4X6zpEy2lVm7vipvxzuB9rJ8ME4g5f9WN",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw27D4_gePMCt7SsMvBTjuFntemlOcgc9AoSiX9FcO4kPcxDVOTurQUOAkH1YiW7R6wKPyVipYavZXI8HnDmAcX4duXtWWbpbZekrWNkN4YRz5BkR9zTlzCpOflZcvaJjKkTxlcXzzQrovdBVrgTYEZ3AY",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6anN6T3m-Z6eb4Nc_dqa2ih0HDLAMztNQwHgsm5rbVdGm0_ZIU-FuPLtvcNrTWXwfvd7oXnm9NvstBTedMWVuiqzxGs7vCT9IeOlp1p3qI-GNin8HLqWoTc30pW69kp9Q2JGi8G8IGnTRbrZ5So7y-KjtjydGq0c0iV5nMaibPnAgjFRtVBPgpdvBXMsviKc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeCP8kZUEyVh_ODgpK5ti0Aj-L8mREoh4Mr_gdwiuYnj9BIiz21DJd3joRt7V0yeR_zHes4ihyocPDQbq0OQkhFK_lT5NF6aSR6iVpo7g0vJdKYoxFvi3RocTDRvwymXCl0vudQJFij_GRCA==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF81bz524lswEKocIxQjkiSII7GsydYkyw_V-RM4O7qe4ofaz3MfQeAxCbtodajTonApiC8WCjmSL4u8Ndu6bgOQEu3weYljnXOxRQMulS-N0OC-3RO3O7UKDnN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzKSSH2vMBrVjYNi6YUfJrsKRzLGemULObZQ6HkTM3kIpV_H3qK_sx8OSLLD-S216ZqkO65ToZrNdROw6vINNdKrE3RbKq7-YS4hk_IlGkh3IVodho7U9W5fkD1peraOHmREJvVPCA1lhX",
        "title": "mendeley.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Xmkuh-fnONPoVJBm6PJggq5ZfSTdU4HDQ2qBHXoRjeFdN1_5ZlNrlSZhfkqsAsaMswrxB-W8pqLRB1-iBzPKZIdO5ueWxJErK4tSubSlXGiRT55l6AYHLx7lnixmyQUauiBi3_SJi0w4gHEfVWcMHdFgDzh7N-P_HwWtdKBgbOyPMnSCxofF0sg_Jg_6l_zAxY_QFflFvL_LjZdIortZEyuzl45GNquWIJLlBjqcOiYoJQp53_W_d1sF8G54UylWS2B944BBTTps5bdi-6K4fF-8eCc=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnDdhne4jymrNuplze8v4QOvmF85Hfxzfecix0oM-rYhTpe125bIvLC46-FXWSkeiCP1MQ6BH5fcLUkoMLs0NUh3XuWIyIcNukRVMqwOw2Rtxuvjkkb4pMeGwTUSgK",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6Gnk6ELT8_NSRu-jWL8q0STCjagE_u1Y4JBWlTbxva5S4HWQnshr3AFgwFUWYdtt61ym_64doVth10tYp1TlULsjERyD4dvfXCxcMaTaHXOVM7EaQ8gDcKWLI",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJuBStWT-pixBntP7WtIsjs6bpzm8FXaMNTV7QWT-TMoa-qfWOqQrfr2O3XEazMvgAQ9C00WX2MxOhddWT7VCtuBt0HqGMEZk_ipX0Kgk4nPDaNw9sPg==",
        "title": "cbioportal.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreVKZQ-Ut_TIvpUK6PUZz05O084jcIp3Lgy6omwMJmXcfxS7Q4Ch4Zvmv8v2nSL9DMYEL8qy9pw9LTjcqttoQM6VAMbAY2Gda7nv8JgcUcV_LhD58Cr86ngzAnp4pZLPkRanm2A==",
        "title": "bevelcloud.ai"
      }
    ]
  },
  {
    "title": "Quilt-1M",
    "paperLink": "https://arxiv.org/abs/2306.11207",
    "description": "A large-scale vision-language dataset curated from educational histopathology videos on YouTube. It uses automatic speech recognition and alignment to create image-text pairs, marking one of the largest public datasets for pathology visual-language learning.",
    "authors": [
      "Wisdom Oluchi Ikezogwo",
      "Mehmet Saygin Seyfioglu",
      "Fatemeh Ghezloo",
      "Dylan Stefan Chan Geva",
      "Fatwir Sheikh Mohammed",
      "Pavan Kumar Anand",
      "Ranjay Krishna",
      "Linda Shapiro"
    ],
    "githubLink": "https://github.com/wisdomikezogwo/quilt1m",
    "itemCount": "1 Million pairs",
    "source": "arXiv",
    "specs": "Image-text pairs (Video frames and aligned speech/text)",
    "year": "2023",
    "id": "saved-1769659920406-1raw7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLFN_171K0UrlXI4bTRnOS6Z5FU3wALQkb8oroHtADWRHClRGbiVFVxsOZyPmyviQheT_yzn9yxlLfScT2vU-Ocl7wFbS9TRWgnWTMHhBtd7CVmNp7",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE05X-1Pw6SzrLznT1I2-2RrrI4koKa2xNyT1JxbcczrvQRntrLSS7sbg1bZV3G1VrRweh8E0fMpRcwAGCQfNnWa9edEzfXGfWKUtB45raKpvE7sgWGUOV0G3AHDuWL8eMrTK8gGDoymburEeFNMAU6aeIFyhBXVBWZxgQwPqQD28qskNV1YLcRGlBwLWLlw1s9pH4RuKe_VMkKjD1CFy_zZ8MI8OSzFMU3FtEQzncfn3yKlz2A5MWB",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEq98GXiE12W29w4SNWsv9tJr8Baf5XEWEZEvYkVC070bWrLVvG5niAumKyRu1MCcT07gfNyTaJItkD_ipi3JL9PTIAfOTV03SM7lkjz3X57KqPSWJ4X6zpEy2lVm7vipvxzuB9rJ8ME4g5f9WN",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw27D4_gePMCt7SsMvBTjuFntemlOcgc9AoSiX9FcO4kPcxDVOTurQUOAkH1YiW7R6wKPyVipYavZXI8HnDmAcX4duXtWWbpbZekrWNkN4YRz5BkR9zTlzCpOflZcvaJjKkTxlcXzzQrovdBVrgTYEZ3AY",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6anN6T3m-Z6eb4Nc_dqa2ih0HDLAMztNQwHgsm5rbVdGm0_ZIU-FuPLtvcNrTWXwfvd7oXnm9NvstBTedMWVuiqzxGs7vCT9IeOlp1p3qI-GNin8HLqWoTc30pW69kp9Q2JGi8G8IGnTRbrZ5So7y-KjtjydGq0c0iV5nMaibPnAgjFRtVBPgpdvBXMsviKc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeCP8kZUEyVh_ODgpK5ti0Aj-L8mREoh4Mr_gdwiuYnj9BIiz21DJd3joRt7V0yeR_zHes4ihyocPDQbq0OQkhFK_lT5NF6aSR6iVpo7g0vJdKYoxFvi3RocTDRvwymXCl0vudQJFij_GRCA==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF81bz524lswEKocIxQjkiSII7GsydYkyw_V-RM4O7qe4ofaz3MfQeAxCbtodajTonApiC8WCjmSL4u8Ndu6bgOQEu3weYljnXOxRQMulS-N0OC-3RO3O7UKDnN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzKSSH2vMBrVjYNi6YUfJrsKRzLGemULObZQ6HkTM3kIpV_H3qK_sx8OSLLD-S216ZqkO65ToZrNdROw6vINNdKrE3RbKq7-YS4hk_IlGkh3IVodho7U9W5fkD1peraOHmREJvVPCA1lhX",
        "title": "mendeley.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Xmkuh-fnONPoVJBm6PJggq5ZfSTdU4HDQ2qBHXoRjeFdN1_5ZlNrlSZhfkqsAsaMswrxB-W8pqLRB1-iBzPKZIdO5ueWxJErK4tSubSlXGiRT55l6AYHLx7lnixmyQUauiBi3_SJi0w4gHEfVWcMHdFgDzh7N-P_HwWtdKBgbOyPMnSCxofF0sg_Jg_6l_zAxY_QFflFvL_LjZdIortZEyuzl45GNquWIJLlBjqcOiYoJQp53_W_d1sF8G54UylWS2B944BBTTps5bdi-6K4fF-8eCc=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnDdhne4jymrNuplze8v4QOvmF85Hfxzfecix0oM-rYhTpe125bIvLC46-FXWSkeiCP1MQ6BH5fcLUkoMLs0NUh3XuWIyIcNukRVMqwOw2Rtxuvjkkb4pMeGwTUSgK",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6Gnk6ELT8_NSRu-jWL8q0STCjagE_u1Y4JBWlTbxva5S4HWQnshr3AFgwFUWYdtt61ym_64doVth10tYp1TlULsjERyD4dvfXCxcMaTaHXOVM7EaQ8gDcKWLI",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJuBStWT-pixBntP7WtIsjs6bpzm8FXaMNTV7QWT-TMoa-qfWOqQrfr2O3XEazMvgAQ9C00WX2MxOhddWT7VCtuBt0HqGMEZk_ipX0Kgk4nPDaNw9sPg==",
        "title": "cbioportal.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreVKZQ-Ut_TIvpUK6PUZz05O084jcIp3Lgy6omwMJmXcfxS7Q4Ch4Zvmv8v2nSL9DMYEL8qy9pw9LTjcqttoQM6VAMbAY2Gda7nv8JgcUcV_LhD58Cr86ngzAnp4pZLPkRanm2A==",
        "title": "bevelcloud.ai"
      }
    ]
  },
  {
    "title": "OpenPath",
    "paperLink": "https://www.nature.com/articles/s41591-023-02504-3",
    "description": "A dataset of pathology images and natural language descriptions collected from public medical discussion forums on Twitter (PathTwitter). It was used to develop the PLIP (Pathology Language-Image Pre-training) model.",
    "authors": [
      "Zhi Huang",
      "Federico Bianchi",
      "Mert Yuksekgonul",
      "Thomas J. Montine",
      "James Zou"
    ],
    "githubLink": "https://github.com/jamesd-hw/PLIP",
    "itemCount": "208,414 pairs",
    "source": "Hugging Face",
    "specs": "Image-text pairs (Social media images and text)",
    "year": "2023",
    "id": "saved-1769659920406-boejg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLFN_171K0UrlXI4bTRnOS6Z5FU3wALQkb8oroHtADWRHClRGbiVFVxsOZyPmyviQheT_yzn9yxlLfScT2vU-Ocl7wFbS9TRWgnWTMHhBtd7CVmNp7",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE05X-1Pw6SzrLznT1I2-2RrrI4koKa2xNyT1JxbcczrvQRntrLSS7sbg1bZV3G1VrRweh8E0fMpRcwAGCQfNnWa9edEzfXGfWKUtB45raKpvE7sgWGUOV0G3AHDuWL8eMrTK8gGDoymburEeFNMAU6aeIFyhBXVBWZxgQwPqQD28qskNV1YLcRGlBwLWLlw1s9pH4RuKe_VMkKjD1CFy_zZ8MI8OSzFMU3FtEQzncfn3yKlz2A5MWB",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEq98GXiE12W29w4SNWsv9tJr8Baf5XEWEZEvYkVC070bWrLVvG5niAumKyRu1MCcT07gfNyTaJItkD_ipi3JL9PTIAfOTV03SM7lkjz3X57KqPSWJ4X6zpEy2lVm7vipvxzuB9rJ8ME4g5f9WN",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw27D4_gePMCt7SsMvBTjuFntemlOcgc9AoSiX9FcO4kPcxDVOTurQUOAkH1YiW7R6wKPyVipYavZXI8HnDmAcX4duXtWWbpbZekrWNkN4YRz5BkR9zTlzCpOflZcvaJjKkTxlcXzzQrovdBVrgTYEZ3AY",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6anN6T3m-Z6eb4Nc_dqa2ih0HDLAMztNQwHgsm5rbVdGm0_ZIU-FuPLtvcNrTWXwfvd7oXnm9NvstBTedMWVuiqzxGs7vCT9IeOlp1p3qI-GNin8HLqWoTc30pW69kp9Q2JGi8G8IGnTRbrZ5So7y-KjtjydGq0c0iV5nMaibPnAgjFRtVBPgpdvBXMsviKc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeCP8kZUEyVh_ODgpK5ti0Aj-L8mREoh4Mr_gdwiuYnj9BIiz21DJd3joRt7V0yeR_zHes4ihyocPDQbq0OQkhFK_lT5NF6aSR6iVpo7g0vJdKYoxFvi3RocTDRvwymXCl0vudQJFij_GRCA==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF81bz524lswEKocIxQjkiSII7GsydYkyw_V-RM4O7qe4ofaz3MfQeAxCbtodajTonApiC8WCjmSL4u8Ndu6bgOQEu3weYljnXOxRQMulS-N0OC-3RO3O7UKDnN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzKSSH2vMBrVjYNi6YUfJrsKRzLGemULObZQ6HkTM3kIpV_H3qK_sx8OSLLD-S216ZqkO65ToZrNdROw6vINNdKrE3RbKq7-YS4hk_IlGkh3IVodho7U9W5fkD1peraOHmREJvVPCA1lhX",
        "title": "mendeley.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Xmkuh-fnONPoVJBm6PJggq5ZfSTdU4HDQ2qBHXoRjeFdN1_5ZlNrlSZhfkqsAsaMswrxB-W8pqLRB1-iBzPKZIdO5ueWxJErK4tSubSlXGiRT55l6AYHLx7lnixmyQUauiBi3_SJi0w4gHEfVWcMHdFgDzh7N-P_HwWtdKBgbOyPMnSCxofF0sg_Jg_6l_zAxY_QFflFvL_LjZdIortZEyuzl45GNquWIJLlBjqcOiYoJQp53_W_d1sF8G54UylWS2B944BBTTps5bdi-6K4fF-8eCc=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnDdhne4jymrNuplze8v4QOvmF85Hfxzfecix0oM-rYhTpe125bIvLC46-FXWSkeiCP1MQ6BH5fcLUkoMLs0NUh3XuWIyIcNukRVMqwOw2Rtxuvjkkb4pMeGwTUSgK",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6Gnk6ELT8_NSRu-jWL8q0STCjagE_u1Y4JBWlTbxva5S4HWQnshr3AFgwFUWYdtt61ym_64doVth10tYp1TlULsjERyD4dvfXCxcMaTaHXOVM7EaQ8gDcKWLI",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJuBStWT-pixBntP7WtIsjs6bpzm8FXaMNTV7QWT-TMoa-qfWOqQrfr2O3XEazMvgAQ9C00WX2MxOhddWT7VCtuBt0HqGMEZk_ipX0Kgk4nPDaNw9sPg==",
        "title": "cbioportal.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreVKZQ-Ut_TIvpUK6PUZz05O084jcIp3Lgy6omwMJmXcfxS7Q4Ch4Zvmv8v2nSL9DMYEL8qy9pw9LTjcqttoQM6VAMbAY2Gda7nv8JgcUcV_LhD58Cr86ngzAnp4pZLPkRanm2A==",
        "title": "bevelcloud.ai"
      }
    ]
  },
  {
    "title": "PathCap (PathAsst)",
    "paperLink": "https://arxiv.org/abs/2305.15072",
    "description": "A curated dataset of pathology image-caption pairs sourced from PubMed and various textbooks, cleaned and optimized for training generative foundation models in pathology.",
    "authors": [
      "Yuxuan Sun",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Kai Zhang",
      "Zhongyi Shui",
      "Xiaoxuan Yu",
      "Yizhi Zhao",
      "Honglin Li",
      "Yunlong Zhang",
      "Ruojia Zhao",
      "Lin Yang"
    ],
    "githubLink": "https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology",
    "itemCount": "207,000 pairs",
    "source": "Hugging Face",
    "specs": "Image-text pairs",
    "year": "2023",
    "id": "saved-1769659920406-oi36r",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLFN_171K0UrlXI4bTRnOS6Z5FU3wALQkb8oroHtADWRHClRGbiVFVxsOZyPmyviQheT_yzn9yxlLfScT2vU-Ocl7wFbS9TRWgnWTMHhBtd7CVmNp7",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE05X-1Pw6SzrLznT1I2-2RrrI4koKa2xNyT1JxbcczrvQRntrLSS7sbg1bZV3G1VrRweh8E0fMpRcwAGCQfNnWa9edEzfXGfWKUtB45raKpvE7sgWGUOV0G3AHDuWL8eMrTK8gGDoymburEeFNMAU6aeIFyhBXVBWZxgQwPqQD28qskNV1YLcRGlBwLWLlw1s9pH4RuKe_VMkKjD1CFy_zZ8MI8OSzFMU3FtEQzncfn3yKlz2A5MWB",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEq98GXiE12W29w4SNWsv9tJr8Baf5XEWEZEvYkVC070bWrLVvG5niAumKyRu1MCcT07gfNyTaJItkD_ipi3JL9PTIAfOTV03SM7lkjz3X57KqPSWJ4X6zpEy2lVm7vipvxzuB9rJ8ME4g5f9WN",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw27D4_gePMCt7SsMvBTjuFntemlOcgc9AoSiX9FcO4kPcxDVOTurQUOAkH1YiW7R6wKPyVipYavZXI8HnDmAcX4duXtWWbpbZekrWNkN4YRz5BkR9zTlzCpOflZcvaJjKkTxlcXzzQrovdBVrgTYEZ3AY",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6anN6T3m-Z6eb4Nc_dqa2ih0HDLAMztNQwHgsm5rbVdGm0_ZIU-FuPLtvcNrTWXwfvd7oXnm9NvstBTedMWVuiqzxGs7vCT9IeOlp1p3qI-GNin8HLqWoTc30pW69kp9Q2JGi8G8IGnTRbrZ5So7y-KjtjydGq0c0iV5nMaibPnAgjFRtVBPgpdvBXMsviKc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeCP8kZUEyVh_ODgpK5ti0Aj-L8mREoh4Mr_gdwiuYnj9BIiz21DJd3joRt7V0yeR_zHes4ihyocPDQbq0OQkhFK_lT5NF6aSR6iVpo7g0vJdKYoxFvi3RocTDRvwymXCl0vudQJFij_GRCA==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF81bz524lswEKocIxQjkiSII7GsydYkyw_V-RM4O7qe4ofaz3MfQeAxCbtodajTonApiC8WCjmSL4u8Ndu6bgOQEu3weYljnXOxRQMulS-N0OC-3RO3O7UKDnN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzKSSH2vMBrVjYNi6YUfJrsKRzLGemULObZQ6HkTM3kIpV_H3qK_sx8OSLLD-S216ZqkO65ToZrNdROw6vINNdKrE3RbKq7-YS4hk_IlGkh3IVodho7U9W5fkD1peraOHmREJvVPCA1lhX",
        "title": "mendeley.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Xmkuh-fnONPoVJBm6PJggq5ZfSTdU4HDQ2qBHXoRjeFdN1_5ZlNrlSZhfkqsAsaMswrxB-W8pqLRB1-iBzPKZIdO5ueWxJErK4tSubSlXGiRT55l6AYHLx7lnixmyQUauiBi3_SJi0w4gHEfVWcMHdFgDzh7N-P_HwWtdKBgbOyPMnSCxofF0sg_Jg_6l_zAxY_QFflFvL_LjZdIortZEyuzl45GNquWIJLlBjqcOiYoJQp53_W_d1sF8G54UylWS2B944BBTTps5bdi-6K4fF-8eCc=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnDdhne4jymrNuplze8v4QOvmF85Hfxzfecix0oM-rYhTpe125bIvLC46-FXWSkeiCP1MQ6BH5fcLUkoMLs0NUh3XuWIyIcNukRVMqwOw2Rtxuvjkkb4pMeGwTUSgK",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6Gnk6ELT8_NSRu-jWL8q0STCjagE_u1Y4JBWlTbxva5S4HWQnshr3AFgwFUWYdtt61ym_64doVth10tYp1TlULsjERyD4dvfXCxcMaTaHXOVM7EaQ8gDcKWLI",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJuBStWT-pixBntP7WtIsjs6bpzm8FXaMNTV7QWT-TMoa-qfWOqQrfr2O3XEazMvgAQ9C00WX2MxOhddWT7VCtuBt0HqGMEZk_ipX0Kgk4nPDaNw9sPg==",
        "title": "cbioportal.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreVKZQ-Ut_TIvpUK6PUZz05O084jcIp3Lgy6omwMJmXcfxS7Q4Ch4Zvmv8v2nSL9DMYEL8qy9pw9LTjcqttoQM6VAMbAY2Gda7nv8JgcUcV_LhD58Cr86ngzAnp4pZLPkRanm2A==",
        "title": "bevelcloud.ai"
      }
    ]
  },
  {
    "title": "ARCH",
    "paperLink": "https://openaccess.thecvf.com/content/CVPR2021/html/Gamper_Multiple_Instance_Captioning_Learning_Representations_From_Histopathology_Textbooks_and_CVPR_2021_paper.html",
    "description": "A Multiple Instance Captioning dataset containing dense diagnostic and morphological descriptions sourced from histopathology textbooks and open-access articles. It is designed to facilitate dense supervision for computational pathology tasks.",
    "authors": [
      "Jevgenij Gamper",
      "Nasir Rajpoot"
    ],
    "githubLink": "https://github.com/maduc7/Histopathology-Datasets",
    "itemCount": "~7,500 pairs",
    "source": "Semantic Scholar",
    "specs": "Image-text pairs (Images and dense captions)",
    "year": "2021",
    "id": "saved-1769659920406-jgawy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHLFN_171K0UrlXI4bTRnOS6Z5FU3wALQkb8oroHtADWRHClRGbiVFVxsOZyPmyviQheT_yzn9yxlLfScT2vU-Ocl7wFbS9TRWgnWTMHhBtd7CVmNp7",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE05X-1Pw6SzrLznT1I2-2RrrI4koKa2xNyT1JxbcczrvQRntrLSS7sbg1bZV3G1VrRweh8E0fMpRcwAGCQfNnWa9edEzfXGfWKUtB45raKpvE7sgWGUOV0G3AHDuWL8eMrTK8gGDoymburEeFNMAU6aeIFyhBXVBWZxgQwPqQD28qskNV1YLcRGlBwLWLlw1s9pH4RuKe_VMkKjD1CFy_zZ8MI8OSzFMU3FtEQzncfn3yKlz2A5MWB",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEq98GXiE12W29w4SNWsv9tJr8Baf5XEWEZEvYkVC070bWrLVvG5niAumKyRu1MCcT07gfNyTaJItkD_ipi3JL9PTIAfOTV03SM7lkjz3X57KqPSWJ4X6zpEy2lVm7vipvxzuB9rJ8ME4g5f9WN",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw27D4_gePMCt7SsMvBTjuFntemlOcgc9AoSiX9FcO4kPcxDVOTurQUOAkH1YiW7R6wKPyVipYavZXI8HnDmAcX4duXtWWbpbZekrWNkN4YRz5BkR9zTlzCpOflZcvaJjKkTxlcXzzQrovdBVrgTYEZ3AY",
        "title": "miccai.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6anN6T3m-Z6eb4Nc_dqa2ih0HDLAMztNQwHgsm5rbVdGm0_ZIU-FuPLtvcNrTWXwfvd7oXnm9NvstBTedMWVuiqzxGs7vCT9IeOlp1p3qI-GNin8HLqWoTc30pW69kp9Q2JGi8G8IGnTRbrZ5So7y-KjtjydGq0c0iV5nMaibPnAgjFRtVBPgpdvBXMsviKc=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGeCP8kZUEyVh_ODgpK5ti0Aj-L8mREoh4Mr_gdwiuYnj9BIiz21DJd3joRt7V0yeR_zHes4ihyocPDQbq0OQkhFK_lT5NF6aSR6iVpo7g0vJdKYoxFvi3RocTDRvwymXCl0vudQJFij_GRCA==",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF81bz524lswEKocIxQjkiSII7GsydYkyw_V-RM4O7qe4ofaz3MfQeAxCbtodajTonApiC8WCjmSL4u8Ndu6bgOQEu3weYljnXOxRQMulS-N0OC-3RO3O7UKDnN",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHzKSSH2vMBrVjYNi6YUfJrsKRzLGemULObZQ6HkTM3kIpV_H3qK_sx8OSLLD-S216ZqkO65ToZrNdROw6vINNdKrE3RbKq7-YS4hk_IlGkh3IVodho7U9W5fkD1peraOHmREJvVPCA1lhX",
        "title": "mendeley.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Xmkuh-fnONPoVJBm6PJggq5ZfSTdU4HDQ2qBHXoRjeFdN1_5ZlNrlSZhfkqsAsaMswrxB-W8pqLRB1-iBzPKZIdO5ueWxJErK4tSubSlXGiRT55l6AYHLx7lnixmyQUauiBi3_SJi0w4gHEfVWcMHdFgDzh7N-P_HwWtdKBgbOyPMnSCxofF0sg_Jg_6l_zAxY_QFflFvL_LjZdIortZEyuzl45GNquWIJLlBjqcOiYoJQp53_W_d1sF8G54UylWS2B944BBTTps5bdi-6K4fF-8eCc=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnDdhne4jymrNuplze8v4QOvmF85Hfxzfecix0oM-rYhTpe125bIvLC46-FXWSkeiCP1MQ6BH5fcLUkoMLs0NUh3XuWIyIcNukRVMqwOw2Rtxuvjkkb4pMeGwTUSgK",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6Gnk6ELT8_NSRu-jWL8q0STCjagE_u1Y4JBWlTbxva5S4HWQnshr3AFgwFUWYdtt61ym_64doVth10tYp1TlULsjERyD4dvfXCxcMaTaHXOVM7EaQ8gDcKWLI",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJuBStWT-pixBntP7WtIsjs6bpzm8FXaMNTV7QWT-TMoa-qfWOqQrfr2O3XEazMvgAQ9C00WX2MxOhddWT7VCtuBt0HqGMEZk_ipX0Kgk4nPDaNw9sPg==",
        "title": "cbioportal.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHreVKZQ-Ut_TIvpUK6PUZz05O084jcIp3Lgy6omwMJmXcfxS7Q4Ch4Zvmv8v2nSL9DMYEL8qy9pw9LTjcqttoQM6VAMbAY2Gda7nv8JgcUcV_LhD58Cr86ngzAnp4pZLPkRanm2A==",
        "title": "bevelcloud.ai"
      }
    ]
  },
  {
    "title": "Foundation-Model-Driven Parkinson's Disease Auto Diagnosis Challenge (Ruijin-PD)",
    "paperLink": "https://doi.org/10.5281/zenodo.15094606",
    "description": "A benchmark dataset designed for the MICCAI 2025 challenge, specifically focusing on using foundation models for Parkinson's Disease diagnosis and Deep Gray Matter (DGM) segmentation. It provides high-quality annotations for anatomical structures often difficult to segment.",
    "authors": [
      "Ruijin Imaging Neuroscience Group"
    ],
    "githubLink": "https://ruijin-pd.github.io/",
    "itemCount": "500 subjects (105,000+ images)",
    "source": "GitHub Pages / Zenodo",
    "specs": "Multi-parametric MRI (T1WI, QSM, NM-MRI) with segmentation masks for 7 anatomical structures",
    "year": "2025",
    "id": "saved-1769659962837-j21lx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGIpYYIsKRwPErdXpq5dTXBrTGVV-qmQ_Jd2nWIPs0DwwOTcl6xAvC6w0wmDmyTg61QcTa2Gs7VhrWfg4I6Jg37i8aMgdZL3e2_Ox7xAd7SSZVRZs1EML6xsSa3H-U=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBbu9lzuW8MxfBcLgLXdv3OhNlHfvT-aFyQJI5pbOsg3GB-pXirFkeT3ZmuYXNOciSNktoxmpfhEIrliTrnt7JrCjnc93oI_4hIDfOigB0QCPuEwcnYoswjy03lR5vjzkq1CLYug==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEVRUPU0dw90fGXFzyt-JwcnGyMdHuoblI9iOFQcha27Vdh1fCoTzcEA6LGlGLnNV7FmRpFvkAYGxvyUt9U02xVRjVVv6wBCRKr7TKSu5kdYrM290lzoN2tBSRXcZZWoGwlRMwdo3ZWbEM=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtNuq9LZboVLk0H1jLyDHRYiITfEueHHnJuJ7qJ09p-aipBlxKAoRfaeXENOVjC3m3NVWaFQJuy_aNMnCGGDM2aOaXeiljmIypGmT3C5EDNjH8gli-Xc5t094b",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSLw7ljE_txBtuhu2YzqBY2tl2HC3MjpUIFVxDfiL7mSXehPMumsfKE7z_CMqRY7wxVxeWkEe-mymIloZXcr341E8bJSLxqDmCFRQ1XbkpaqrIfaK75ru4qpuEU-A=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4_e5S3Gi7C1EbzA81jTMwJ0_az5YI-rxUQ9802Uis1-Gbhx7sCQ-LbRVhEzRZekZN82ESDawNtxtDCn17RcgwDUaex3C2CgTmWGEzqLAzlKA_yqHRBhU0x2tjyjejkIVz8HYAoqY-Ji9e_Q==",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH06yllTzngvgtNczIVe3v3hvUphGQP2McXF-Zwenp4KG9_1Y4pfAcrBaR79jWBrhdhPWjNYT1XSJ7-7L9Dgo7iELpA1pRZ46HSm-7bqjFFzdgZGZes8V_zbnAehdKz3FkH2ro7SMoNEz5HZWLUbi0BJ_8FdTWYsXX4cgYFYei0bb-NWjTIqtODB_29uGjrO8dqifQpwUrjgfrGJnVSZcC78S24s7vmNFKCbKBDXho31vsmVlSE9Mc40SNoaGpdpBnZQbNX8iNRSO-fJ4Vy_i-v2CmfAb86CJm9s21JgPJyUlayRrc_MAv0-46Z7ENQWks77_4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_THfFuzajS067QUAcHchtzDMv0axha6Xqzzd7O8_kphMENybMcAKmSUE1zcd4YZYgqS_EgiimZrD9RS7NEkznPaj106c_rIC9zXtv8t24nZLTq_6rsCxk5siP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSPbyWTGczOtF-meWm88VbH-bjHq_pLi2VvMkLm4nnlBLVa1kxbVfnxDmHjsISvRhXSB6Vh8Fb1bXmfT4tzifCo0pOfcvNRAOByN0y_UVlO5yVTL5BldQdk8rCkMkR",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "CLIMB (Clinical Large-scale Integrative Multi-modal Benchmark)",
    "paperLink": "https://arxiv.org/abs/2503.05678",
    "description": "A massive multimodal clinical benchmark designed to train and evaluate clinical foundation models. It unifies data across imaging, language, and genomics, explicitly including Parkinson's disease data (PPMI) to test model generalization and multitask capabilities.",
    "authors": [
      "Zifeng Wang",
      "Sunan Xu",
      "et al."
    ],
    "githubLink": "https://github.com/DDVD233/climb",
    "itemCount": "4.51 million samples (total benchmark), ~718 PD records (subset)",
    "source": "arXiv",
    "specs": "Multimodal: 2D/3D Imaging (MRI, CT), Time Series (EEG/ECG), Text, Genomics",
    "year": "2025",
    "id": "saved-1769659962837-y701l",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGIpYYIsKRwPErdXpq5dTXBrTGVV-qmQ_Jd2nWIPs0DwwOTcl6xAvC6w0wmDmyTg61QcTa2Gs7VhrWfg4I6Jg37i8aMgdZL3e2_Ox7xAd7SSZVRZs1EML6xsSa3H-U=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBbu9lzuW8MxfBcLgLXdv3OhNlHfvT-aFyQJI5pbOsg3GB-pXirFkeT3ZmuYXNOciSNktoxmpfhEIrliTrnt7JrCjnc93oI_4hIDfOigB0QCPuEwcnYoswjy03lR5vjzkq1CLYug==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEVRUPU0dw90fGXFzyt-JwcnGyMdHuoblI9iOFQcha27Vdh1fCoTzcEA6LGlGLnNV7FmRpFvkAYGxvyUt9U02xVRjVVv6wBCRKr7TKSu5kdYrM290lzoN2tBSRXcZZWoGwlRMwdo3ZWbEM=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtNuq9LZboVLk0H1jLyDHRYiITfEueHHnJuJ7qJ09p-aipBlxKAoRfaeXENOVjC3m3NVWaFQJuy_aNMnCGGDM2aOaXeiljmIypGmT3C5EDNjH8gli-Xc5t094b",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSLw7ljE_txBtuhu2YzqBY2tl2HC3MjpUIFVxDfiL7mSXehPMumsfKE7z_CMqRY7wxVxeWkEe-mymIloZXcr341E8bJSLxqDmCFRQ1XbkpaqrIfaK75ru4qpuEU-A=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4_e5S3Gi7C1EbzA81jTMwJ0_az5YI-rxUQ9802Uis1-Gbhx7sCQ-LbRVhEzRZekZN82ESDawNtxtDCn17RcgwDUaex3C2CgTmWGEzqLAzlKA_yqHRBhU0x2tjyjejkIVz8HYAoqY-Ji9e_Q==",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH06yllTzngvgtNczIVe3v3hvUphGQP2McXF-Zwenp4KG9_1Y4pfAcrBaR79jWBrhdhPWjNYT1XSJ7-7L9Dgo7iELpA1pRZ46HSm-7bqjFFzdgZGZes8V_zbnAehdKz3FkH2ro7SMoNEz5HZWLUbi0BJ_8FdTWYsXX4cgYFYei0bb-NWjTIqtODB_29uGjrO8dqifQpwUrjgfrGJnVSZcC78S24s7vmNFKCbKBDXho31vsmVlSE9Mc40SNoaGpdpBnZQbNX8iNRSO-fJ4Vy_i-v2CmfAb86CJm9s21JgPJyUlayRrc_MAv0-46Z7ENQWks77_4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_THfFuzajS067QUAcHchtzDMv0axha6Xqzzd7O8_kphMENybMcAKmSUE1zcd4YZYgqS_EgiimZrD9RS7NEkznPaj106c_rIC9zXtv8t24nZLTq_6rsCxk5siP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSPbyWTGczOtF-meWm88VbH-bjHq_pLi2VvMkLm4nnlBLVa1kxbVfnxDmHjsISvRhXSB6Vh8Fb1bXmfT4tzifCo0pOfcvNRAOByN0y_UVlO5yVTL5BldQdk8rCkMkR",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "YouTubePD",
    "paperLink": "https://openreview.net/forum?id=14x72Fk8M6",
    "description": "The first public multimodal benchmark for Parkinson's Disease analysis derived from social media content. It is designed to evaluate discriminative and generative tasks using foundation models in 'in-the-wild' settings.",
    "authors": [
      "Sam W. Li",
      "Yuxiong Wang",
      "et al."
    ],
    "githubLink": "https://github.com/samwli/YouTubePD-data",
    "itemCount": "283 videos (16 public figures)",
    "source": "NeurIPS / OpenReview",
    "specs": "Multimodal: Video (facial expressions), Audio, Facial Landmarks",
    "year": "2023",
    "id": "saved-1769659962837-bpdys",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGIpYYIsKRwPErdXpq5dTXBrTGVV-qmQ_Jd2nWIPs0DwwOTcl6xAvC6w0wmDmyTg61QcTa2Gs7VhrWfg4I6Jg37i8aMgdZL3e2_Ox7xAd7SSZVRZs1EML6xsSa3H-U=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBbu9lzuW8MxfBcLgLXdv3OhNlHfvT-aFyQJI5pbOsg3GB-pXirFkeT3ZmuYXNOciSNktoxmpfhEIrliTrnt7JrCjnc93oI_4hIDfOigB0QCPuEwcnYoswjy03lR5vjzkq1CLYug==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEVRUPU0dw90fGXFzyt-JwcnGyMdHuoblI9iOFQcha27Vdh1fCoTzcEA6LGlGLnNV7FmRpFvkAYGxvyUt9U02xVRjVVv6wBCRKr7TKSu5kdYrM290lzoN2tBSRXcZZWoGwlRMwdo3ZWbEM=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtNuq9LZboVLk0H1jLyDHRYiITfEueHHnJuJ7qJ09p-aipBlxKAoRfaeXENOVjC3m3NVWaFQJuy_aNMnCGGDM2aOaXeiljmIypGmT3C5EDNjH8gli-Xc5t094b",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSLw7ljE_txBtuhu2YzqBY2tl2HC3MjpUIFVxDfiL7mSXehPMumsfKE7z_CMqRY7wxVxeWkEe-mymIloZXcr341E8bJSLxqDmCFRQ1XbkpaqrIfaK75ru4qpuEU-A=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4_e5S3Gi7C1EbzA81jTMwJ0_az5YI-rxUQ9802Uis1-Gbhx7sCQ-LbRVhEzRZekZN82ESDawNtxtDCn17RcgwDUaex3C2CgTmWGEzqLAzlKA_yqHRBhU0x2tjyjejkIVz8HYAoqY-Ji9e_Q==",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH06yllTzngvgtNczIVe3v3hvUphGQP2McXF-Zwenp4KG9_1Y4pfAcrBaR79jWBrhdhPWjNYT1XSJ7-7L9Dgo7iELpA1pRZ46HSm-7bqjFFzdgZGZes8V_zbnAehdKz3FkH2ro7SMoNEz5HZWLUbi0BJ_8FdTWYsXX4cgYFYei0bb-NWjTIqtODB_29uGjrO8dqifQpwUrjgfrGJnVSZcC78S24s7vmNFKCbKBDXho31vsmVlSE9Mc40SNoaGpdpBnZQbNX8iNRSO-fJ4Vy_i-v2CmfAb86CJm9s21JgPJyUlayRrc_MAv0-46Z7ENQWks77_4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_THfFuzajS067QUAcHchtzDMv0axha6Xqzzd7O8_kphMENybMcAKmSUE1zcd4YZYgqS_EgiimZrD9RS7NEkznPaj106c_rIC9zXtv8t24nZLTq_6rsCxk5siP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSPbyWTGczOtF-meWm88VbH-bjHq_pLi2VvMkLm4nnlBLVa1kxbVfnxDmHjsISvRhXSB6Vh8Fb1bXmfT4tzifCo0pOfcvNRAOByN0y_UVlO5yVTL5BldQdk8rCkMkR",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PC-GITA (Standard and Extended)",
    "paperLink": "https://arxiv.org/abs/2406.16128",
    "description": "A widely used speech benchmark for Parkinson's detection, recently adopted for evaluating speech foundation models like WavLM and Whisper. The extended version (e-PC-GITA) tests model robustness in real-world acoustic conditions.",
    "authors": [
      "J. R. Orozco-Arroyave",
      "et al."
    ],
    "githubLink": "https://github.com/vic-sim/PC-GITA-dataset",
    "itemCount": "100 subjects (Standard), 40 subjects (Extended)",
    "source": "arXiv / ResearchGate",
    "specs": "Audio (Speech recordings: vowels, words, sentences, monologue)",
    "year": "2014",
    "id": "saved-1769659962837-o8py3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGIpYYIsKRwPErdXpq5dTXBrTGVV-qmQ_Jd2nWIPs0DwwOTcl6xAvC6w0wmDmyTg61QcTa2Gs7VhrWfg4I6Jg37i8aMgdZL3e2_Ox7xAd7SSZVRZs1EML6xsSa3H-U=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBbu9lzuW8MxfBcLgLXdv3OhNlHfvT-aFyQJI5pbOsg3GB-pXirFkeT3ZmuYXNOciSNktoxmpfhEIrliTrnt7JrCjnc93oI_4hIDfOigB0QCPuEwcnYoswjy03lR5vjzkq1CLYug==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEVRUPU0dw90fGXFzyt-JwcnGyMdHuoblI9iOFQcha27Vdh1fCoTzcEA6LGlGLnNV7FmRpFvkAYGxvyUt9U02xVRjVVv6wBCRKr7TKSu5kdYrM290lzoN2tBSRXcZZWoGwlRMwdo3ZWbEM=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtNuq9LZboVLk0H1jLyDHRYiITfEueHHnJuJ7qJ09p-aipBlxKAoRfaeXENOVjC3m3NVWaFQJuy_aNMnCGGDM2aOaXeiljmIypGmT3C5EDNjH8gli-Xc5t094b",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSLw7ljE_txBtuhu2YzqBY2tl2HC3MjpUIFVxDfiL7mSXehPMumsfKE7z_CMqRY7wxVxeWkEe-mymIloZXcr341E8bJSLxqDmCFRQ1XbkpaqrIfaK75ru4qpuEU-A=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4_e5S3Gi7C1EbzA81jTMwJ0_az5YI-rxUQ9802Uis1-Gbhx7sCQ-LbRVhEzRZekZN82ESDawNtxtDCn17RcgwDUaex3C2CgTmWGEzqLAzlKA_yqHRBhU0x2tjyjejkIVz8HYAoqY-Ji9e_Q==",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH06yllTzngvgtNczIVe3v3hvUphGQP2McXF-Zwenp4KG9_1Y4pfAcrBaR79jWBrhdhPWjNYT1XSJ7-7L9Dgo7iELpA1pRZ46HSm-7bqjFFzdgZGZes8V_zbnAehdKz3FkH2ro7SMoNEz5HZWLUbi0BJ_8FdTWYsXX4cgYFYei0bb-NWjTIqtODB_29uGjrO8dqifQpwUrjgfrGJnVSZcC78S24s7vmNFKCbKBDXho31vsmVlSE9Mc40SNoaGpdpBnZQbNX8iNRSO-fJ4Vy_i-v2CmfAb86CJm9s21JgPJyUlayRrc_MAv0-46Z7ENQWks77_4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_THfFuzajS067QUAcHchtzDMv0axha6Xqzzd7O8_kphMENybMcAKmSUE1zcd4YZYgqS_EgiimZrD9RS7NEkznPaj106c_rIC9zXtv8t24nZLTq_6rsCxk5siP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSPbyWTGczOtF-meWm88VbH-bjHq_pLi2VvMkLm4nnlBLVa1kxbVfnxDmHjsISvRhXSB6Vh8Fb1bXmfT4tzifCo0pOfcvNRAOByN0y_UVlO5yVTL5BldQdk8rCkMkR",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "PPMI (Parkinson's Progression Markers Initiative)",
    "paperLink": "https://www.ppmi-info.org/",
    "description": "A comprehensive observational clinical study used as the primary data source for many foundation model papers (e.g., Quest2Dx, medical LLMs). It tracks disease progression using imaging, biologic, and clinical data.",
    "authors": [
      "The Michael J. Fox Foundation"
    ],
    "githubLink": "https://github.com/search?q=PPMI+dataset",
    "itemCount": ">1,400 participants",
    "source": "Scholar / Official Repository",
    "specs": "Multimodal: MRI, SPECT, genetics, CSF/blood biomarkers, clinical questionnaires",
    "year": "2010",
    "id": "saved-1769659962838-fhg4c",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGIpYYIsKRwPErdXpq5dTXBrTGVV-qmQ_Jd2nWIPs0DwwOTcl6xAvC6w0wmDmyTg61QcTa2Gs7VhrWfg4I6Jg37i8aMgdZL3e2_Ox7xAd7SSZVRZs1EML6xsSa3H-U=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBbu9lzuW8MxfBcLgLXdv3OhNlHfvT-aFyQJI5pbOsg3GB-pXirFkeT3ZmuYXNOciSNktoxmpfhEIrliTrnt7JrCjnc93oI_4hIDfOigB0QCPuEwcnYoswjy03lR5vjzkq1CLYug==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEVRUPU0dw90fGXFzyt-JwcnGyMdHuoblI9iOFQcha27Vdh1fCoTzcEA6LGlGLnNV7FmRpFvkAYGxvyUt9U02xVRjVVv6wBCRKr7TKSu5kdYrM290lzoN2tBSRXcZZWoGwlRMwdo3ZWbEM=",
        "title": "emergentmind.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGtNuq9LZboVLk0H1jLyDHRYiITfEueHHnJuJ7qJ09p-aipBlxKAoRfaeXENOVjC3m3NVWaFQJuy_aNMnCGGDM2aOaXeiljmIypGmT3C5EDNjH8gli-Xc5t094b",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSLw7ljE_txBtuhu2YzqBY2tl2HC3MjpUIFVxDfiL7mSXehPMumsfKE7z_CMqRY7wxVxeWkEe-mymIloZXcr341E8bJSLxqDmCFRQ1XbkpaqrIfaK75ru4qpuEU-A=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4_e5S3Gi7C1EbzA81jTMwJ0_az5YI-rxUQ9802Uis1-Gbhx7sCQ-LbRVhEzRZekZN82ESDawNtxtDCn17RcgwDUaex3C2CgTmWGEzqLAzlKA_yqHRBhU0x2tjyjejkIVz8HYAoqY-Ji9e_Q==",
        "title": "icml.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH06yllTzngvgtNczIVe3v3hvUphGQP2McXF-Zwenp4KG9_1Y4pfAcrBaR79jWBrhdhPWjNYT1XSJ7-7L9Dgo7iELpA1pRZ46HSm-7bqjFFzdgZGZes8V_zbnAehdKz3FkH2ro7SMoNEz5HZWLUbi0BJ_8FdTWYsXX4cgYFYei0bb-NWjTIqtODB_29uGjrO8dqifQpwUrjgfrGJnVSZcC78S24s7vmNFKCbKBDXho31vsmVlSE9Mc40SNoaGpdpBnZQbNX8iNRSO-fJ4Vy_i-v2CmfAb86CJm9s21JgPJyUlayRrc_MAv0-46Z7ENQWks77_4=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_THfFuzajS067QUAcHchtzDMv0axha6Xqzzd7O8_kphMENybMcAKmSUE1zcd4YZYgqS_EgiimZrD9RS7NEkznPaj106c_rIC9zXtv8t24nZLTq_6rsCxk5siP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSPbyWTGczOtF-meWm88VbH-bjHq_pLi2VvMkLm4nnlBLVa1kxbVfnxDmHjsISvRhXSB6Vh8Fb1bXmfT4tzifCo0pOfcvNRAOByN0y_UVlO5yVTL5BldQdk8rCkMkR",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "M2Surv",
    "paperLink": "https://arxiv.org/abs/2506.19324",
    "description": "A benchmark framework focusing on incomplete multimodal survival prediction. It introduces a memory-augmented hypergraph learning approach to handle missing modalities and effectively integrate pathology slides with genomic profiles.",
    "authors": [
      "Mingcheng Qu",
      "Guang Yang",
      "et al."
    ],
    "githubLink": "https://github.com/MCPathology/M2Surv",
    "itemCount": "5 cancer datasets from TCGA",
    "source": "arXiv",
    "specs": "Whole Slide Images (Pathology) + Genomic profiles",
    "year": "2025",
    "id": "saved-1769660018802-tssvk",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDO37iXaXiW-d7_EHLxvvlRLntzV1CfWiwgukr7QrGK9FL0cmBs_KwpTIAzSnSECk-MI9VHAOsH7Fs7pLDSUozsAF3YZVdodjEsObx5IPvY26K-kuMSdwTwLWpk_p7_wwgAnqeCgaHr1gPepUc",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFapIz6UwXmvYFBW9d09RYDYYs44M-q2XU2wjUeIFkcTL-4fL1FDoaKLHu1jmeGC7GcUwHUMtoCROaieU-SyAVZbZag0nnWbame5K9TIkm1B5fIgGz9x3DSmFWqdAD4T2dvlzd0zjeaDZwF24zzHOADVVsgLDR2QzrvupmPSmxpyX0=",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "HANCOCK Challenge Dataset",
    "paperLink": "https://zenodo.org/records/13837586",
    "description": "A highly multimodal dataset released for a 2025 challenge on survival and recurrence prediction in head and neck oncology. It distinguishes itself by including unstructured text reports alongside imaging and tabular data.",
    "authors": [
      "Organizers of the HANCOCK Challenge"
    ],
    "githubLink": "https://zenodo.org/records/13837586",
    "itemCount": "763 patients",
    "source": "Zenodo",
    "specs": "6 modalities: Clinical, Pathology Reports (Text), Histopathology Images, TMA, Blood Data, Surgery Reports",
    "year": "2025",
    "id": "saved-1769660018802-f2zgg",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDO37iXaXiW-d7_EHLxvvlRLntzV1CfWiwgukr7QrGK9FL0cmBs_KwpTIAzSnSECk-MI9VHAOsH7Fs7pLDSUozsAF3YZVdodjEsObx5IPvY26K-kuMSdwTwLWpk_p7_wwgAnqeCgaHr1gPepUc",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFapIz6UwXmvYFBW9d09RYDYYs44M-q2XU2wjUeIFkcTL-4fL1FDoaKLHu1jmeGC7GcUwHUMtoCROaieU-SyAVZbZag0nnWbame5K9TIkm1B5fIgGz9x3DSmFWqdAD4T2dvlzd0zjeaDZwF24zzHOADVVsgLDR2QzrvupmPSmxpyX0=",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "SurvBoard",
    "paperLink": "https://academic.oup.com/bib/article/26/1/bbae466/7926861",
    "description": "A comprehensive benchmarking framework designed for the standardized evaluation of multi-omics cancer survival models. It addresses issues like missing modalities and experimental variability by providing preprocessed datasets and evaluation protocols for both single-cancer and pan-cancer settings.",
    "authors": [
      "Boeva Lab",
      "Herrmann et al."
    ],
    "githubLink": "https://github.com/BoevaLab/survboard",
    "itemCount": "28 datasets (spanning TCGA, ICGC, TARGET, METABRIC)",
    "source": "Scholar",
    "specs": "Multi-omics (Gene expression, miRNA, etc.) + Clinical data",
    "year": "2024",
    "id": "saved-1769660018802-u67j8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDO37iXaXiW-d7_EHLxvvlRLntzV1CfWiwgukr7QrGK9FL0cmBs_KwpTIAzSnSECk-MI9VHAOsH7Fs7pLDSUozsAF3YZVdodjEsObx5IPvY26K-kuMSdwTwLWpk_p7_wwgAnqeCgaHr1gPepUc",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFapIz6UwXmvYFBW9d09RYDYYs44M-q2XU2wjUeIFkcTL-4fL1FDoaKLHu1jmeGC7GcUwHUMtoCROaieU-SyAVZbZag0nnWbame5K9TIkm1B5fIgGz9x3DSmFWqdAD4T2dvlzd0zjeaDZwF24zzHOADVVsgLDR2QzrvupmPSmxpyX0=",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "TCGA-SurvMob",
    "paperLink": "https://github.com/bblodfon/tcga-survmob",
    "description": "A benchmarking repository for evaluating survival machine learning models across multimodal TCGA datasets, specifically focusing on Pancreatic (PAAD) and Bladder (BLCA) cancers with Bayesian analysis of results.",
    "authors": [
      "bblodfon (GitHub User)"
    ],
    "githubLink": "https://github.com/bblodfon/tcga-survmob",
    "itemCount": "2 primary cohorts (PAAD, BLCA) + expandable",
    "source": "GitHub",
    "specs": "Multi-omics + Clinical data",
    "year": "2024",
    "id": "saved-1769660018802-j0ctd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDO37iXaXiW-d7_EHLxvvlRLntzV1CfWiwgukr7QrGK9FL0cmBs_KwpTIAzSnSECk-MI9VHAOsH7Fs7pLDSUozsAF3YZVdodjEsObx5IPvY26K-kuMSdwTwLWpk_p7_wwgAnqeCgaHr1gPepUc",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFapIz6UwXmvYFBW9d09RYDYYs44M-q2XU2wjUeIFkcTL-4fL1FDoaKLHu1jmeGC7GcUwHUMtoCROaieU-SyAVZbZag0nnWbame5K9TIkm1B5fIgGz9x3DSmFWqdAD4T2dvlzd0zjeaDZwF24zzHOADVVsgLDR2QzrvupmPSmxpyX0=",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "RADCURE",
    "paperLink": "https://doi.org/10.1002/mp.16104",
    "description": "A large-scale benchmark dataset specifically for head and neck cancer survival and recurrence prediction. It is widely used to evaluate radiomics and deep learning models that combine medical imaging with electronic medical records.",
    "authors": [
      "A. L. Welch",
      "A. R. McIntosh",
      "et al."
    ],
    "githubLink": "https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=127058563",
    "itemCount": "3,346 patients",
    "source": "The Cancer Imaging Archive (TCIA)",
    "specs": "CT Imaging + Clinical data (EHR) + Radiation treatment contours (GTV)",
    "year": "2023",
    "id": "saved-1769660018802-tvrkf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDO37iXaXiW-d7_EHLxvvlRLntzV1CfWiwgukr7QrGK9FL0cmBs_KwpTIAzSnSECk-MI9VHAOsH7Fs7pLDSUozsAF3YZVdodjEsObx5IPvY26K-kuMSdwTwLWpk_p7_wwgAnqeCgaHr1gPepUc",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFapIz6UwXmvYFBW9d09RYDYYs44M-q2XU2wjUeIFkcTL-4fL1FDoaKLHu1jmeGC7GcUwHUMtoCROaieU-SyAVZbZag0nnWbame5K9TIkm1B5fIgGz9x3DSmFWqdAD4T2dvlzd0zjeaDZwF24zzHOADVVsgLDR2QzrvupmPSmxpyX0=",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "MultiSurv",
    "paperLink": "https://doi.org/10.1038/s41598-021-92799-4",
    "description": "A multimodal deep learning benchmark and model for long-term cancer survival prediction. It utilizes a large-scale pan-cancer dataset to integrate diverse data modalities, functioning as a key baseline for multimodal integration strategies.",
    "authors": [
      "Luis Vale Silva",
      "Andre Altmann",
      "et al."
    ],
    "githubLink": "https://github.com/luisvalesilva/multisurv",
    "itemCount": "33 cancer types (~11,000 patients)",
    "source": "arXiv",
    "specs": "6 modalities: Clinical, mRNA, miRNA, DNA methylation, CNV, Whole Slide Images (WSI)",
    "year": "2021",
    "id": "saved-1769660018802-jmxaa",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDO37iXaXiW-d7_EHLxvvlRLntzV1CfWiwgukr7QrGK9FL0cmBs_KwpTIAzSnSECk-MI9VHAOsH7Fs7pLDSUozsAF3YZVdodjEsObx5IPvY26K-kuMSdwTwLWpk_p7_wwgAnqeCgaHr1gPepUc",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFapIz6UwXmvYFBW9d09RYDYYs44M-q2XU2wjUeIFkcTL-4fL1FDoaKLHu1jmeGC7GcUwHUMtoCROaieU-SyAVZbZag0nnWbame5K9TIkm1B5fIgGz9x3DSmFWqdAD4T2dvlzd0zjeaDZwF24zzHOADVVsgLDR2QzrvupmPSmxpyX0=",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "Hemorica",
    "paperLink": "https://arxiv.org/abs/2509.22993",
    "description": "A comprehensive dataset comprising 372 head CT examinations with exhaustive annotations for five ICH subtypes. It includes patient-wise and slice-wise classification labels, subtype-specific bounding boxes, 2D pixel masks, and 3D voxel masks, serving as a unified benchmark for classification, detection, and segmentation.",
    "authors": [
      "Kasra Davoodi",
      "Mohammad Hoseyni",
      "Javad Khoramdel",
      "Reza Barati",
      "Reihaneh Mortazavi",
      "Amirhossein Nikoofard",
      "Mahdi Aliyari-Shoorehdeli",
      "Jaber Hatam Parikhan"
    ],
    "githubLink": "Not yet available (Paper indicates release)",
    "itemCount": "372 head CT examinations",
    "source": "arXiv",
    "specs": "CT scans, 5 Class Annotations (EPH, SDH, SAH, IPH, IVH), 2D/3D masks, bounding boxes",
    "year": "2025",
    "id": "saved-1769660126468-o6vkk",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEmSFv0mvV2eGYSzlrPikRwh9uo-TZ1mbpC_ku40NnkU2LSrXWNrMfzWLpf_I9Sx_e_Rs4hn2Txlf7O2Norvzz4N_gg0ZZQcYzLJlAMFuxsvSuG4zhFAw5khDxNdcmn_Y2Um0GDzUXFXIofPplPMe0ZaegZqBtNhYcE4g==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Psybw4omO1_vidCfyVAeeSjeclUIcbkGlyUCygS_Hu0AnlV9YyUvBIcNxYc3K6mZ2EfgO0t-lqOO57Xj5OxPIhDux4fAQtuN1LGoFD94GxNkZcobU3oiIwM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhQ18t4DysqUApxd-iVRS3O8aTSzeYJUFWTq16ERIB1hnPVPpkpgvQ1xV9HPDVVzBm_11nmdChWEQRbdpPrP3hQ9isvQIQoYVxP4Q3-0a7ABTMFalswBf497vatNcwinzPjUAY3HsK4bgXDmF23bYk0d_O695ClM3rP5O2GCiDwW8sXCpkLUwe-juQPPgw7GUwHSbGTPu-YTQ59Eb6ZU0=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMAxndVdJj7Q2Sk_ru_DNFiDm3pc9HWYYfFIcwPLEFKo1IU82-yOFqTB0my000kPPhIbjTC-2_1qODX1_FZMPC_oypUJf12ZeaQ0_cqnncm6ZPmaYbPwqU9B1pvQ-9INZSB7uMfsrFLWMZQLGAmA3TedH1Tf0rlrctXmoVP7qzn-FcUNnd_TcJBtrzVGPmzKHF3eHwKrfnjY-HEG5uTzreNWHnqbfGdO09m-VQGWcdGeWdK2wpT52u4-D6rjZ1V-H44V4NR1AlHyfbiMkuAB9SuWcSyvbheC56-VQXrwju",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGyXRI1Z6ju-__zn1bkythz5Qn42i_jV0YYdZGS0vka8zbo7sb5A_V5DwLxVQCMLaeSwRHEI7fmGjp9foj-3fJehoYwVHJ7WegRwQTjFsCAN3ZKZwDIsPtGG4c7cfSkyAnwa-wyOeIMsIf-wBE4nQsdxznecRKIEI9hxTlvvZC7M9N0fJV590-GiApfpWO0XyJqzBWPlhBRFljaR82DuJGwQ8_9ba_QN5oMuW7-aOAkkXD9ljTNu1yf7sIB5_ou94dtuVBisoBaQK5L-LogHNY",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "HemSeg-200",
    "paperLink": "https://arxiv.org/abs/2405.14559",
    "description": "A voxel-annotated dataset designed for precise segmentation of Intraparenchymal (IPH) and Intraventricular (IVH) hemorrhages. It consists of 222 CT annotations sourced from the RSNA 2019 challenge and annotated at the voxel level to facilitate quantitative analysis and clinical treatment planning.",
    "authors": [
      "Changwei Song",
      "Qing Zhao",
      "Jianqiang Li",
      "Xin Yue",
      "Ruoyun Gao",
      "Zhaoxuan Wang",
      "An Gao",
      "Guanghui Fu"
    ],
    "githubLink": "https://github.com/songchangwei/3DCT-SD-IVH-ICH",
    "itemCount": "222 CT annotations",
    "source": "arXiv",
    "specs": "Voxel-level annotations, NIfTI format, Focus on IPH and IVH subtypes",
    "year": "2024",
    "id": "saved-1769660126468-pmx7r",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEmSFv0mvV2eGYSzlrPikRwh9uo-TZ1mbpC_ku40NnkU2LSrXWNrMfzWLpf_I9Sx_e_Rs4hn2Txlf7O2Norvzz4N_gg0ZZQcYzLJlAMFuxsvSuG4zhFAw5khDxNdcmn_Y2Um0GDzUXFXIofPplPMe0ZaegZqBtNhYcE4g==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Psybw4omO1_vidCfyVAeeSjeclUIcbkGlyUCygS_Hu0AnlV9YyUvBIcNxYc3K6mZ2EfgO0t-lqOO57Xj5OxPIhDux4fAQtuN1LGoFD94GxNkZcobU3oiIwM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhQ18t4DysqUApxd-iVRS3O8aTSzeYJUFWTq16ERIB1hnPVPpkpgvQ1xV9HPDVVzBm_11nmdChWEQRbdpPrP3hQ9isvQIQoYVxP4Q3-0a7ABTMFalswBf497vatNcwinzPjUAY3HsK4bgXDmF23bYk0d_O695ClM3rP5O2GCiDwW8sXCpkLUwe-juQPPgw7GUwHSbGTPu-YTQ59Eb6ZU0=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMAxndVdJj7Q2Sk_ru_DNFiDm3pc9HWYYfFIcwPLEFKo1IU82-yOFqTB0my000kPPhIbjTC-2_1qODX1_FZMPC_oypUJf12ZeaQ0_cqnncm6ZPmaYbPwqU9B1pvQ-9INZSB7uMfsrFLWMZQLGAmA3TedH1Tf0rlrctXmoVP7qzn-FcUNnd_TcJBtrzVGPmzKHF3eHwKrfnjY-HEG5uTzreNWHnqbfGdO09m-VQGWcdGeWdK2wpT52u4-D6rjZ1V-H44V4NR1AlHyfbiMkuAB9SuWcSyvbheC56-VQXrwju",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGyXRI1Z6ju-__zn1bkythz5Qn42i_jV0YYdZGS0vka8zbo7sb5A_V5DwLxVQCMLaeSwRHEI7fmGjp9foj-3fJehoYwVHJ7WegRwQTjFsCAN3ZKZwDIsPtGG4c7cfSkyAnwa-wyOeIMsIf-wBE4nQsdxznecRKIEI9hxTlvvZC7M9N0fJV590-GiApfpWO0XyJqzBWPlhBRFljaR82DuJGwQ8_9ba_QN5oMuW7-aOAkkXD9ljTNu1yf7sIB5_ou94dtuVBisoBaQK5L-LogHNY",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "BHSD (Brain Hemorrhage Segmentation Dataset)",
    "paperLink": "https://arxiv.org/abs/2308.11298",
    "description": "The first public 3D multi-class Intracranial Hemorrhage (ICH) segmentation dataset, addressing the lack of multi-class segmentation benchmarks. It provides pixel-level annotations for 192 volumes and slice-level annotations for 2200 volumes across five ICH categories (Epidural, Intraparenchymal, Intraventricular, Subarachnoid, Subdural).",
    "authors": [
      "Biao Wu",
      "Yutong Xie",
      "Zeyu Zhang",
      "Jinchao Ge",
      "Kaspar Yaxley",
      "Suzan Bahadir",
      "Qi Wu",
      "Yifan Liu",
      "Minh-Son To"
    ],
    "githubLink": "https://github.com/White65534/BHSD",
    "itemCount": "192 labeled 3D volumes (pixel-level); 2200 volumes (slice-level/unlabeled)",
    "source": "arXiv",
    "specs": "3D CT volumes, NIfTI format, 5 Class Annotations (EDH, IPH, IVH, SAH, SDH)",
    "year": "2023",
    "id": "saved-1769660126468-5q45x",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEmSFv0mvV2eGYSzlrPikRwh9uo-TZ1mbpC_ku40NnkU2LSrXWNrMfzWLpf_I9Sx_e_Rs4hn2Txlf7O2Norvzz4N_gg0ZZQcYzLJlAMFuxsvSuG4zhFAw5khDxNdcmn_Y2Um0GDzUXFXIofPplPMe0ZaegZqBtNhYcE4g==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Psybw4omO1_vidCfyVAeeSjeclUIcbkGlyUCygS_Hu0AnlV9YyUvBIcNxYc3K6mZ2EfgO0t-lqOO57Xj5OxPIhDux4fAQtuN1LGoFD94GxNkZcobU3oiIwM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhQ18t4DysqUApxd-iVRS3O8aTSzeYJUFWTq16ERIB1hnPVPpkpgvQ1xV9HPDVVzBm_11nmdChWEQRbdpPrP3hQ9isvQIQoYVxP4Q3-0a7ABTMFalswBf497vatNcwinzPjUAY3HsK4bgXDmF23bYk0d_O695ClM3rP5O2GCiDwW8sXCpkLUwe-juQPPgw7GUwHSbGTPu-YTQ59Eb6ZU0=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMAxndVdJj7Q2Sk_ru_DNFiDm3pc9HWYYfFIcwPLEFKo1IU82-yOFqTB0my000kPPhIbjTC-2_1qODX1_FZMPC_oypUJf12ZeaQ0_cqnncm6ZPmaYbPwqU9B1pvQ-9INZSB7uMfsrFLWMZQLGAmA3TedH1Tf0rlrctXmoVP7qzn-FcUNnd_TcJBtrzVGPmzKHF3eHwKrfnjY-HEG5uTzreNWHnqbfGdO09m-VQGWcdGeWdK2wpT52u4-D6rjZ1V-H44V4NR1AlHyfbiMkuAB9SuWcSyvbheC56-VQXrwju",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGyXRI1Z6ju-__zn1bkythz5Qn42i_jV0YYdZGS0vka8zbo7sb5A_V5DwLxVQCMLaeSwRHEI7fmGjp9foj-3fJehoYwVHJ7WegRwQTjFsCAN3ZKZwDIsPtGG4c7cfSkyAnwa-wyOeIMsIf-wBE4nQsdxznecRKIEI9hxTlvvZC7M9N0fJV590-GiApfpWO0XyJqzBWPlhBRFljaR82DuJGwQ8_9ba_QN5oMuW7-aOAkkXD9ljTNu1yf7sIB5_ou94dtuVBisoBaQK5L-LogHNY",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "INSTANCE 2022 Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2301.03281",
    "description": "The first challenge dataset dedicated to resolving the anisotropic problem in 3D intracranial hemorrhage segmentation. It includes 200 3D volumes with refined labeling, serving as a solid benchmark for segmentation tasks, though primarily focused on the single-class (hemorrhage region) segmentation task.",
    "authors": [
      "Xiangyu Li",
      "Gongning Luo",
      "Wei Wang",
      "Kuanquan Wang",
      "Yue Gao",
      "Xavier Lladó",
      "et al."
    ],
    "githubLink": "https://github.com/PerceptionComputingLab/INSTANCE2022",
    "itemCount": "200 3D volumes (100 train, 30 val, 70 test)",
    "source": "Grand Challenge",
    "specs": "3D Non-contrast Head CT, NIfTI format, Voxel-level annotations",
    "year": "2023",
    "id": "saved-1769660126468-h6iv3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEmSFv0mvV2eGYSzlrPikRwh9uo-TZ1mbpC_ku40NnkU2LSrXWNrMfzWLpf_I9Sx_e_Rs4hn2Txlf7O2Norvzz4N_gg0ZZQcYzLJlAMFuxsvSuG4zhFAw5khDxNdcmn_Y2Um0GDzUXFXIofPplPMe0ZaegZqBtNhYcE4g==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Psybw4omO1_vidCfyVAeeSjeclUIcbkGlyUCygS_Hu0AnlV9YyUvBIcNxYc3K6mZ2EfgO0t-lqOO57Xj5OxPIhDux4fAQtuN1LGoFD94GxNkZcobU3oiIwM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhQ18t4DysqUApxd-iVRS3O8aTSzeYJUFWTq16ERIB1hnPVPpkpgvQ1xV9HPDVVzBm_11nmdChWEQRbdpPrP3hQ9isvQIQoYVxP4Q3-0a7ABTMFalswBf497vatNcwinzPjUAY3HsK4bgXDmF23bYk0d_O695ClM3rP5O2GCiDwW8sXCpkLUwe-juQPPgw7GUwHSbGTPu-YTQ59Eb6ZU0=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMAxndVdJj7Q2Sk_ru_DNFiDm3pc9HWYYfFIcwPLEFKo1IU82-yOFqTB0my000kPPhIbjTC-2_1qODX1_FZMPC_oypUJf12ZeaQ0_cqnncm6ZPmaYbPwqU9B1pvQ-9INZSB7uMfsrFLWMZQLGAmA3TedH1Tf0rlrctXmoVP7qzn-FcUNnd_TcJBtrzVGPmzKHF3eHwKrfnjY-HEG5uTzreNWHnqbfGdO09m-VQGWcdGeWdK2wpT52u4-D6rjZ1V-H44V4NR1AlHyfbiMkuAB9SuWcSyvbheC56-VQXrwju",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGyXRI1Z6ju-__zn1bkythz5Qn42i_jV0YYdZGS0vka8zbo7sb5A_V5DwLxVQCMLaeSwRHEI7fmGjp9foj-3fJehoYwVHJ7WegRwQTjFsCAN3ZKZwDIsPtGG4c7cfSkyAnwa-wyOeIMsIf-wBE4nQsdxznecRKIEI9hxTlvvZC7M9N0fJV590-GiApfpWO0XyJqzBWPlhBRFljaR82DuJGwQ8_9ba_QN5oMuW7-aOAkkXD9ljTNu1yf7sIB5_ou94dtuVBisoBaQK5L-LogHNY",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "PhysioNet CT-ICH",
    "paperLink": "https://physionet.org/content/ct-ich/",
    "description": "A dataset of non-contrast head CT scans with slice-wise annotations for intracranial hemorrhage segmentation and detection of skull fractures. Each slice is annotated for the presence of different ICH subtypes and fracture, with ICH regions delineated.",
    "authors": [
      "Haghbayan, M. H.",
      "et al."
    ],
    "githubLink": "https://physionet.org/content/ct-ich/",
    "itemCount": "82 CT scans (75 subjects public)",
    "source": "PhysioNet",
    "specs": "CT scans, Slice-wise segmentation masks, 5 subtypes (IVH, IPH, SAH, EDH, SDH)",
    "year": "2020",
    "id": "saved-1769660126468-q7vbn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEmSFv0mvV2eGYSzlrPikRwh9uo-TZ1mbpC_ku40NnkU2LSrXWNrMfzWLpf_I9Sx_e_Rs4hn2Txlf7O2Norvzz4N_gg0ZZQcYzLJlAMFuxsvSuG4zhFAw5khDxNdcmn_Y2Um0GDzUXFXIofPplPMe0ZaegZqBtNhYcE4g==",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2Psybw4omO1_vidCfyVAeeSjeclUIcbkGlyUCygS_Hu0AnlV9YyUvBIcNxYc3K6mZ2EfgO0t-lqOO57Xj5OxPIhDux4fAQtuN1LGoFD94GxNkZcobU3oiIwM=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhQ18t4DysqUApxd-iVRS3O8aTSzeYJUFWTq16ERIB1hnPVPpkpgvQ1xV9HPDVVzBm_11nmdChWEQRbdpPrP3hQ9isvQIQoYVxP4Q3-0a7ABTMFalswBf497vatNcwinzPjUAY3HsK4bgXDmF23bYk0d_O695ClM3rP5O2GCiDwW8sXCpkLUwe-juQPPgw7GUwHSbGTPu-YTQ59Eb6ZU0=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMAxndVdJj7Q2Sk_ru_DNFiDm3pc9HWYYfFIcwPLEFKo1IU82-yOFqTB0my000kPPhIbjTC-2_1qODX1_FZMPC_oypUJf12ZeaQ0_cqnncm6ZPmaYbPwqU9B1pvQ-9INZSB7uMfsrFLWMZQLGAmA3TedH1Tf0rlrctXmoVP7qzn-FcUNnd_TcJBtrzVGPmzKHF3eHwKrfnjY-HEG5uTzreNWHnqbfGdO09m-VQGWcdGeWdK2wpT52u4-D6rjZ1V-H44V4NR1AlHyfbiMkuAB9SuWcSyvbheC56-VQXrwju",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHGyXRI1Z6ju-__zn1bkythz5Qn42i_jV0YYdZGS0vka8zbo7sb5A_V5DwLxVQCMLaeSwRHEI7fmGjp9foj-3fJehoYwVHJ7WegRwQTjFsCAN3ZKZwDIsPtGG4c7cfSkyAnwa-wyOeIMsIf-wBE4nQsdxznecRKIEI9hxTlvvZC7M9N0fJV590-GiApfpWO0XyJqzBWPlhBRFljaR82DuJGwQ8_9ba_QN5oMuW7-aOAkkXD9ljTNu1yf7sIB5_ou94dtuVBisoBaQK5L-LogHNY",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "MS-Multi-Spine Challenge (MICCAI 2025)",
    "paperLink": "https://zenodo.org/records/14051167",
    "description": "A benchmark challenge for detecting Multiple Sclerosis lesions in the spinal cord using multi-sequence MRI. It addresses the 'missing modality' problem by providing various combinations of sequences (T2, STIR, PSIR, MP2RAGE) for training and testing.",
    "authors": [
      "Romain Casey",
      "Benoit Combès",
      "François Cotton",
      "Michel Dojat",
      "Michael Kain",
      "Anne Kerbrat",
      "Sorina Pop"
    ],
    "githubLink": "https://ms-multispine.sciencesconf.org/",
    "itemCount": "200 subjects (100 Training pairs/triplets, 100 Testing sets)",
    "source": "Zenodo",
    "specs": "Multi-sequence MRI (Sagittal T2, STIR, PSIR, MP2RAGE)",
    "year": "2025",
    "id": "saved-1769660213926-t86tt",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGg_961QNW6W6S2dPkVWMyY_sKGkwlXHHSf7hjp3FOGR0FK6UIxENhsL1ZZb81fjbs1H9FWtuy52ybVpkS-QXgKx83wKGY_nxwb7RW_IjgVkeRMFaW5KlYISHuSDu_8ns6prpcaszKf9QTI_P9xvHiH2SrEARNAGZldyHHG4LeNO98=",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2fiWNIy0fwnWLVhMzVSsFLbSLckFbwu8gTbft8FTp2qi4-yY4tRF3ZktNO58ClVIKQ-jx04aFM7KTsKSTDN0_asCPOiNYjqUJnQk6teZ1Zmn9N52iO95C3NKWDAuPPz3p9_rmrDLfLW7vbWzjy1Mq5WgHZ22QpjjjrSCO5D7mExD4fOdPKd4Ep71b9hgdux0wPfu4pw1H9eV9xjSKCkZxrVFiIqjs6uX5t7siNhqlNptujXZ6YnZRKFx4N_8QaH5aKanj1Ufshbk=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEijIdPuXBE0NofmxxA2nS850AQ1BqWY7cK-pW7h0wCOjh6FIaAUpH7n2lqE68olkGMQt3glS0SSq8DqQTC3E3Me3ma8GIOsiFHVD1RFQE2IIKXHERVjrECnWaXIQnzkma9-PGj_O3W3HtyRvA6",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "TUM Axial T2w Whole Spine MS Dataset",
    "paperLink": "https://www.medrxiv.org/content/10.1101/2025.01.22.25320959v1",
    "description": "A large-scale, multi-center dataset designed for robust segmentation of MS lesions in axial T2-weighted scans covering the entire spinal cord (cervical, thoracic, lumbar). It integrates data from four major centers (TUM, NYU, UCSF, BWH).",
    "authors": [
      "Enamundram Naga Karthik",
      "Julian McGinnis",
      "Ricarda Wurm",
      "Sebastian Ruehling",
      "Robert Graf",
      "Mark Muehlau",
      "Julien Cohen-Adad"
    ],
    "githubLink": "https://github.com/ivadomed/model-seg-ms-axial-t2w",
    "itemCount": "582 patients, 2167 scans",
    "source": "medRxiv",
    "specs": "Axial T2-weighted MRI",
    "year": "2025",
    "id": "saved-1769660213926-ur83e",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGg_961QNW6W6S2dPkVWMyY_sKGkwlXHHSf7hjp3FOGR0FK6UIxENhsL1ZZb81fjbs1H9FWtuy52ybVpkS-QXgKx83wKGY_nxwb7RW_IjgVkeRMFaW5KlYISHuSDu_8ns6prpcaszKf9QTI_P9xvHiH2SrEARNAGZldyHHG4LeNO98=",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2fiWNIy0fwnWLVhMzVSsFLbSLckFbwu8gTbft8FTp2qi4-yY4tRF3ZktNO58ClVIKQ-jx04aFM7KTsKSTDN0_asCPOiNYjqUJnQk6teZ1Zmn9N52iO95C3NKWDAuPPz3p9_rmrDLfLW7vbWzjy1Mq5WgHZ22QpjjjrSCO5D7mExD4fOdPKd4Ep71b9hgdux0wPfu4pw1H9eV9xjSKCkZxrVFiIqjs6uX5t7siNhqlNptujXZ6YnZRKFx4N_8QaH5aKanj1Ufshbk=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEijIdPuXBE0NofmxxA2nS850AQ1BqWY7cK-pW7h0wCOjh6FIaAUpH7n2lqE68olkGMQt3glS0SSq8DqQTC3E3Me3ma8GIOsiFHVD1RFQE2IIKXHERVjrECnWaXIQnzkma9-PGj_O3W3HtyRvA6",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "MSLesSeg",
    "paperLink": "https://www.nature.com/articles/s41597-025-05250-y",
    "description": "A publicly accessible MRI dataset with expert-validated annotations for MS lesion segmentation. While covering various brain regions, it explicitly includes spinal cord areas, making it a relevant benchmark for whole-CNS lesion detection.",
    "authors": [
      "Francesco Guarnera",
      "Alessia Rondinella",
      "Sebastiano Battiato",
      "et al."
    ],
    "githubLink": "https://www.researchgate.net/publication/383184511_MSLesSeg_baseline_and_benchmarking_of_a_new_Multiple_Sclerosis_Lesion_Segmentation_dataset",
    "itemCount": "115 scans (75 patients)",
    "source": "Scientific Data",
    "specs": "T1, T2, FLAIR",
    "year": "2025",
    "id": "saved-1769660213926-qety7",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGg_961QNW6W6S2dPkVWMyY_sKGkwlXHHSf7hjp3FOGR0FK6UIxENhsL1ZZb81fjbs1H9FWtuy52ybVpkS-QXgKx83wKGY_nxwb7RW_IjgVkeRMFaW5KlYISHuSDu_8ns6prpcaszKf9QTI_P9xvHiH2SrEARNAGZldyHHG4LeNO98=",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2fiWNIy0fwnWLVhMzVSsFLbSLckFbwu8gTbft8FTp2qi4-yY4tRF3ZktNO58ClVIKQ-jx04aFM7KTsKSTDN0_asCPOiNYjqUJnQk6teZ1Zmn9N52iO95C3NKWDAuPPz3p9_rmrDLfLW7vbWzjy1Mq5WgHZ22QpjjjrSCO5D7mExD4fOdPKd4Ep71b9hgdux0wPfu4pw1H9eV9xjSKCkZxrVFiIqjs6uX5t7siNhqlNptujXZ6YnZRKFx4N_8QaH5aKanj1Ufshbk=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEijIdPuXBE0NofmxxA2nS850AQ1BqWY7cK-pW7h0wCOjh6FIaAUpH7n2lqE68olkGMQt3glS0SSq8DqQTC3E3Me3ma8GIOsiFHVD1RFQE2IIKXHERVjrECnWaXIQnzkma9-PGj_O3W3HtyRvA6",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "Cervical Spinal Cord and MS Lesions Dataset",
    "paperLink": "https://ieeexplore.ieee.org/document/10633215",
    "description": "A public dataset specifically for the segmentation of the cervical spinal cord and MS lesions. It consists of T2-weighted axial slices collected from volunteer patients at Akdeniz University Hospital.",
    "authors": [
      "Rukiye Polattimur",
      "Mehmet Süleyman Yıldırım"
    ],
    "githubLink": "https://data.mendeley.com/datasets/ydkrtmygjp/1",
    "itemCount": "87 patients, 231 slices",
    "source": "Mendeley Data",
    "specs": "Axial T2-weighted MRI (2D slices, PNG/NIfTI)",
    "year": "2024",
    "id": "saved-1769660213926-mvowd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGg_961QNW6W6S2dPkVWMyY_sKGkwlXHHSf7hjp3FOGR0FK6UIxENhsL1ZZb81fjbs1H9FWtuy52ybVpkS-QXgKx83wKGY_nxwb7RW_IjgVkeRMFaW5KlYISHuSDu_8ns6prpcaszKf9QTI_P9xvHiH2SrEARNAGZldyHHG4LeNO98=",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2fiWNIy0fwnWLVhMzVSsFLbSLckFbwu8gTbft8FTp2qi4-yY4tRF3ZktNO58ClVIKQ-jx04aFM7KTsKSTDN0_asCPOiNYjqUJnQk6teZ1Zmn9N52iO95C3NKWDAuPPz3p9_rmrDLfLW7vbWzjy1Mq5WgHZ22QpjjjrSCO5D7mExD4fOdPKd4Ep71b9hgdux0wPfu4pw1H9eV9xjSKCkZxrVFiIqjs6uX5t7siNhqlNptujXZ6YnZRKFx4N_8QaH5aKanj1Ufshbk=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEijIdPuXBE0NofmxxA2nS850AQ1BqWY7cK-pW7h0wCOjh6FIaAUpH7n2lqE68olkGMQt3glS0SSq8DqQTC3E3Me3ma8GIOsiFHVD1RFQE2IIKXHERVjrECnWaXIQnzkma9-PGj_O3W3HtyRvA6",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "SCT Multi-Center MS Lesion Dataset",
    "paperLink": "https://doi.org/10.1016/j.neuroimage.2019.01.043",
    "description": "A widely used benchmark dataset for spinal cord lesion segmentation, originally used to train the 'sct_deepseg_lesion' model in the Spinal Cord Toolbox. It contains data from five different centers with manual annotations.",
    "authors": [
      "Charley Gros",
      "Benjamin De Leener",
      "Hassan Badji",
      "Josefina Maranzano",
      "Dominique Eden",
      "Julien Cohen-Adad"
    ],
    "githubLink": "https://github.com/neuropoly/spinalcordtoolbox",
    "itemCount": "265 patients",
    "source": "NeuroImage",
    "specs": "Sagittal T2-weighted MRI",
    "year": "2019",
    "id": "saved-1769660213926-bhrj9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGg_961QNW6W6S2dPkVWMyY_sKGkwlXHHSf7hjp3FOGR0FK6UIxENhsL1ZZb81fjbs1H9FWtuy52ybVpkS-QXgKx83wKGY_nxwb7RW_IjgVkeRMFaW5KlYISHuSDu_8ns6prpcaszKf9QTI_P9xvHiH2SrEARNAGZldyHHG4LeNO98=",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE2fiWNIy0fwnWLVhMzVSsFLbSLckFbwu8gTbft8FTp2qi4-yY4tRF3ZktNO58ClVIKQ-jx04aFM7KTsKSTDN0_asCPOiNYjqUJnQk6teZ1Zmn9N52iO95C3NKWDAuPPz3p9_rmrDLfLW7vbWzjy1Mq5WgHZ22QpjjjrSCO5D7mExD4fOdPKd4Ep71b9hgdux0wPfu4pw1H9eV9xjSKCkZxrVFiIqjs6uX5t7siNhqlNptujXZ6YnZRKFx4N_8QaH5aKanj1Ufshbk=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEijIdPuXBE0NofmxxA2nS850AQ1BqWY7cK-pW7h0wCOjh6FIaAUpH7n2lqE68olkGMQt3glS0SSq8DqQTC3E3Me3ma8GIOsiFHVD1RFQE2IIKXHERVjrECnWaXIQnzkma9-PGj_O3W3HtyRvA6",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "HECKTOR 2025 (Head and Neck Tumor) Dataset",
    "paperLink": "https://arxiv.org/abs/2509.00367",
    "description": "A large-scale, multi-centric dataset comprising 1,123 annotated PET/CT studies from patients with histologically confirmed head and neck cancer. It is designed for three tasks: tumor/lymph node segmentation, recurrence-free survival prediction, and HPV status classification.",
    "authors": [
      "Numan Saeed",
      "Salma Hassan",
      "Shahad Hardan",
      "et al."
    ],
    "githubLink": "https://github.com/BioMedIA-MBZUAI/HECKTOR2025",
    "itemCount": "1,123 patients",
    "source": "arXiv",
    "specs": "PET/CT images (NIfTI), GTVp and GTVn masks, Clinical metadata (TNM, HPV, outcomes)",
    "year": "2025",
    "id": "saved-1769660286162-e0f71",
    "groundingSources": []
  },
  {
    "title": "HNTS-MRG 2024 (Head and Neck Tumor Segmentation for MR-Guided Applications)",
    "paperLink": "https://arxiv.org/abs/2411.18585",
    "description": "A benchmark challenge dataset focusing on the segmentation of primary tumors (GTVp) and metastatic lymph nodes (GTVn) in MRI scans for MR-guided radiation therapy. It explores the use of multi-timepoint data (Pre-RT and Mid-RT).",
    "authors": [
      "Carlos E. Cardenas",
      "Mohamed A. Naser",
      "Kareem A. Wahid",
      "et al."
    ],
    "githubLink": "https://grand-challenge.org/challenges/hnts-mrg-2024/",
    "itemCount": "200 cases (150 training, 50 testing)",
    "source": "arXiv",
    "specs": "MRI (T2-weighted), Pre-RT and Mid-RT scans, GTVp and GTVn segmentation masks",
    "year": "2024",
    "id": "saved-1769660286162-9c5dg",
    "groundingSources": []
  },
  {
    "title": "HaN-Seg (Head and Neck Organ-at-Risk Segmentation)",
    "paperLink": "https://doi.org/10.1002/mp.16197",
    "description": "A dataset containing paired CT and MR images for head and neck cancer patients, with manual segmentations for 30 Organs-at-Risk (OARs). It supports research in multimodal segmentation and radiotherapy planning.",
    "authors": [
      "Gašper Podobnik",
      "Primož Strojan",
      "Primož Peterlin",
      "Bulat Ibragimov",
      "Tomaž Vrtovec"
    ],
    "githubLink": "https://huggingface.co/datasets/gasperp/HaN-Seg",
    "itemCount": "42 patients",
    "source": "Hugging Face",
    "specs": "CT, T1-weighted MRI, 30 OAR binary masks, NRRD format",
    "year": "2023",
    "id": "saved-1769660286163-zhmzc",
    "groundingSources": []
  },
  {
    "title": "HECKTOR 2022 Challenge Dataset",
    "paperLink": "https://link.springer.com/chapter/10.1007/978-3-031-27420-6_1",
    "description": "The dataset used for the 3rd edition of the HECKTOR challenge. It significantly expanded on previous years to include 883 cases from 9 centers, focusing on automatic segmentation of tumors and lymph nodes, as well as recurrence-free survival prediction.",
    "authors": [
      "Vincent Andrearczyk",
      "Valentin Oreiller",
      "Sarah Boughdad",
      "et al."
    ],
    "githubLink": "https://github.com/xmuyzz/HECKTOR2022",
    "itemCount": "883 cases (524 training, 359 testing)",
    "source": "Scholar",
    "specs": "FDG-PET/CT images, GTVp and GTVn masks, Clinical data",
    "year": "2022",
    "id": "saved-1769660286163-7938c",
    "groundingSources": []
  },
  {
    "title": "HECKTOR 2021 Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2201.04138",
    "description": "The dataset for the 2nd HECKTOR challenge, focusing on primary tumor segmentation and progression-free survival (PFS) prediction in PET/CT images.",
    "authors": [
      "Vincent Andrearczyk",
      "Valentin Oreiller",
      "Martin Vallières",
      "et al."
    ],
    "githubLink": "https://github.com/voreille/hecktor",
    "itemCount": "325 cases (224 training, 101 testing)",
    "source": "arXiv",
    "specs": "FDG-PET/CT images, GTVp masks, Clinical/Survival data",
    "year": "2021",
    "id": "saved-1769660286163-w1mm4",
    "groundingSources": []
  },
  {
    "title": "MedAgentBench",
    "paperLink": "https://arxiv.org/abs/2501.14654",
    "description": "A realistic virtual EHR environment designed to benchmark medical Large Language Model (LLM) agents. It evaluates agents on 100 patient-specific clinically-derived tasks including information retrieval, diagnosis, and treatment planning within a FHIR-compliant interactive environment.",
    "authors": [
      "Yixing Jiang",
      "Kameron C. Black",
      "Gloria Geng",
      "Danny Park",
      "James Zou",
      "Andrew Y. Ng",
      "Jonathan H. Chen"
    ],
    "githubLink": "https://github.com/stanfordmlgroup/MedAgentBench",
    "itemCount": "100 tasks, 100 patient profiles (700,000+ data elements)",
    "source": "arXiv",
    "specs": "Text (EHR data, clinical notes), FHIR-compliant environment interactions",
    "year": "2025",
    "id": "saved-1769660386135-xvorh",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_DOXwarOEgUDeqOYDLiAmaa8I5j9pXNxnlBIW5jDy7rKQUVQzwKO9jPoUSWEG023Cri-90uZU0wLFzZateeuEc0JvRtmhMi7OOooSagVFGz14zM3eP8NiVp9_OTteIEU20Tbpu7rRBRItFr8=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3YZxJM95R1kJooQlaFJEjtUJHwlr7lys-QfTd1bYwYE1xOvCcbiFCZVhvQhl3Ddw0rja-ui-T_n3DeASiiiYRLPK2SxWtG2rLVNIxzeXLvxnwbFmJfcXKoiVXv2kgWXpE7ixwFw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHfnteFpGjA5pd6X23xFbVYA27R2CPjCwNQ4GGtEFKDCWNKXgdzvom1zBy4RTzuwEmg7wxdXTsj5ga7o8Ipw7sobZHEf1bQk8MURMUfZ3N6YEryxhwCCZA4nmR9LCB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnJTswTpYRoryJLO3US_UIT9r69vhqMzjyPOnhMQjZtsaTGjCy5E-6WffiUtE3d8SLUDHsWyvuAelR0ufefnlUrLEJvkyp1EC626EusLmB3dRJadlC2prmXu-f6ohprSu2kaN0BPqMmDPIHL0_VK-I53yfTZuL7Q0DQaaK7A==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKjR5-6n7V9yjXA_ULBvr4NHCehsGbCEX0KHFoQeHMoySq87Yb86_iEQMhzaLi40zvaFlQs2Y3BiRCotajc0FkDriatH930X9tkahO1DNxPzDXQoUT0ZJpKAQ7voVA9oO8rAC1exY2TSFPG5_bleyFHPp4fJRSsdBNXJPhQg==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhti3pn4CfOLfrk69s3fG_8NW6CkLob8svRlp_KecShZUcLObHpQKO58ONkYRvweTLfIRwhr4KoCMIXgHkChFXq4kcV1KIW0E44bkzqb_tCo4ZTWqmMWrQ50CHlZF5",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "3MDBench",
    "paperLink": "https://arxiv.org/abs/2504.13861",
    "description": "Medical Multimodal Multi-agent Dialogue Benchmark (3MDBench) is a framework for simulating and evaluating LLM-driven telemedical consultations. It simulates patient variability and evaluates diagnostic accuracy and dialogue quality using multi-agent interactions.",
    "authors": [
      "Ivan Sviridov",
      "Other Authors"
    ],
    "githubLink": "https://github.com/univanxx/3mdbench",
    "itemCount": "2,996 cases",
    "source": "arXiv",
    "specs": "Multimodal (Text + Images), Multi-agent dialogue format",
    "year": "2025",
    "id": "saved-1769660386135-d4aa0",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_DOXwarOEgUDeqOYDLiAmaa8I5j9pXNxnlBIW5jDy7rKQUVQzwKO9jPoUSWEG023Cri-90uZU0wLFzZateeuEc0JvRtmhMi7OOooSagVFGz14zM3eP8NiVp9_OTteIEU20Tbpu7rRBRItFr8=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3YZxJM95R1kJooQlaFJEjtUJHwlr7lys-QfTd1bYwYE1xOvCcbiFCZVhvQhl3Ddw0rja-ui-T_n3DeASiiiYRLPK2SxWtG2rLVNIxzeXLvxnwbFmJfcXKoiVXv2kgWXpE7ixwFw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHfnteFpGjA5pd6X23xFbVYA27R2CPjCwNQ4GGtEFKDCWNKXgdzvom1zBy4RTzuwEmg7wxdXTsj5ga7o8Ipw7sobZHEf1bQk8MURMUfZ3N6YEryxhwCCZA4nmR9LCB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnJTswTpYRoryJLO3US_UIT9r69vhqMzjyPOnhMQjZtsaTGjCy5E-6WffiUtE3d8SLUDHsWyvuAelR0ufefnlUrLEJvkyp1EC626EusLmB3dRJadlC2prmXu-f6ohprSu2kaN0BPqMmDPIHL0_VK-I53yfTZuL7Q0DQaaK7A==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKjR5-6n7V9yjXA_ULBvr4NHCehsGbCEX0KHFoQeHMoySq87Yb86_iEQMhzaLi40zvaFlQs2Y3BiRCotajc0FkDriatH930X9tkahO1DNxPzDXQoUT0ZJpKAQ7voVA9oO8rAC1exY2TSFPG5_bleyFHPp4fJRSsdBNXJPhQg==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhti3pn4CfOLfrk69s3fG_8NW6CkLob8svRlp_KecShZUcLObHpQKO58ONkYRvweTLfIRwhr4KoCMIXgHkChFXq4kcV1KIW0E44bkzqb_tCo4ZTWqmMWrQ50CHlZF5",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MSDiagnosis",
    "paperLink": "https://arxiv.org/abs/2408.10039",
    "description": "A benchmark for evaluating Large Language Models in multi-step clinical diagnosis. It covers the full diagnostic process including primary diagnosis, differential diagnosis, and final diagnosis, requiring models to self-evaluate and refine their results.",
    "authors": [
      "Ruihui Hou",
      "Shencheng Chen",
      "Yongqi Fan",
      "Guangya Yu",
      "Lifeng Zhu",
      "Jing Sun",
      "Jingping Liu",
      "Tong Ruan"
    ],
    "githubLink": "Not Available",
    "itemCount": "2,225 clinical cases",
    "source": "arXiv",
    "specs": "Text (Medical records, diagnostic questions)",
    "year": "2024",
    "id": "saved-1769660386135-cgfoo",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_DOXwarOEgUDeqOYDLiAmaa8I5j9pXNxnlBIW5jDy7rKQUVQzwKO9jPoUSWEG023Cri-90uZU0wLFzZateeuEc0JvRtmhMi7OOooSagVFGz14zM3eP8NiVp9_OTteIEU20Tbpu7rRBRItFr8=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3YZxJM95R1kJooQlaFJEjtUJHwlr7lys-QfTd1bYwYE1xOvCcbiFCZVhvQhl3Ddw0rja-ui-T_n3DeASiiiYRLPK2SxWtG2rLVNIxzeXLvxnwbFmJfcXKoiVXv2kgWXpE7ixwFw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHfnteFpGjA5pd6X23xFbVYA27R2CPjCwNQ4GGtEFKDCWNKXgdzvom1zBy4RTzuwEmg7wxdXTsj5ga7o8Ipw7sobZHEf1bQk8MURMUfZ3N6YEryxhwCCZA4nmR9LCB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnJTswTpYRoryJLO3US_UIT9r69vhqMzjyPOnhMQjZtsaTGjCy5E-6WffiUtE3d8SLUDHsWyvuAelR0ufefnlUrLEJvkyp1EC626EusLmB3dRJadlC2prmXu-f6ohprSu2kaN0BPqMmDPIHL0_VK-I53yfTZuL7Q0DQaaK7A==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKjR5-6n7V9yjXA_ULBvr4NHCehsGbCEX0KHFoQeHMoySq87Yb86_iEQMhzaLi40zvaFlQs2Y3BiRCotajc0FkDriatH930X9tkahO1DNxPzDXQoUT0ZJpKAQ7voVA9oO8rAC1exY2TSFPG5_bleyFHPp4fJRSsdBNXJPhQg==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhti3pn4CfOLfrk69s3fG_8NW6CkLob8svRlp_KecShZUcLObHpQKO58ONkYRvweTLfIRwhr4KoCMIXgHkChFXq4kcV1KIW0E44bkzqb_tCo4ZTWqmMWrQ50CHlZF5",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MedCalc-Bench",
    "paperLink": "https://arxiv.org/abs/2406.12036",
    "description": "A dataset designed to evaluate the medical calculation capabilities of LLMs. It includes rule-based and equation-based calculation tasks derived from real-world clinical calculators, requiring models to extract attributes from patient notes and perform arithmetic.",
    "authors": [
      "Nikhil Khandekar",
      "Qiao Jin",
      "Guangzhi Xiong",
      "Soren Dunn",
      "Serina S. Applebaum",
      "Zain Anwar",
      "Maame Sarfo-Gyamfi",
      "Conrad W. Safranek",
      "Abid A. Anwar",
      "Andrew Zhang",
      "Aidan Gilson",
      "Maxwell B. Singer",
      "Amisha Dave",
      "Andrew Taylor",
      "Aidong Zhang",
      "Qingyu Chen",
      "Zhiyong Lu"
    ],
    "githubLink": "https://github.com/ncbi-nlp/MedCalc-Bench",
    "itemCount": "1,047 instances (55 calculation tasks)",
    "source": "arXiv",
    "specs": "Text/Structured (Patient notes, questions, calculation logic)",
    "year": "2024",
    "id": "saved-1769660386135-e1aav",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_DOXwarOEgUDeqOYDLiAmaa8I5j9pXNxnlBIW5jDy7rKQUVQzwKO9jPoUSWEG023Cri-90uZU0wLFzZateeuEc0JvRtmhMi7OOooSagVFGz14zM3eP8NiVp9_OTteIEU20Tbpu7rRBRItFr8=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3YZxJM95R1kJooQlaFJEjtUJHwlr7lys-QfTd1bYwYE1xOvCcbiFCZVhvQhl3Ddw0rja-ui-T_n3DeASiiiYRLPK2SxWtG2rLVNIxzeXLvxnwbFmJfcXKoiVXv2kgWXpE7ixwFw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHfnteFpGjA5pd6X23xFbVYA27R2CPjCwNQ4GGtEFKDCWNKXgdzvom1zBy4RTzuwEmg7wxdXTsj5ga7o8Ipw7sobZHEf1bQk8MURMUfZ3N6YEryxhwCCZA4nmR9LCB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnJTswTpYRoryJLO3US_UIT9r69vhqMzjyPOnhMQjZtsaTGjCy5E-6WffiUtE3d8SLUDHsWyvuAelR0ufefnlUrLEJvkyp1EC626EusLmB3dRJadlC2prmXu-f6ohprSu2kaN0BPqMmDPIHL0_VK-I53yfTZuL7Q0DQaaK7A==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKjR5-6n7V9yjXA_ULBvr4NHCehsGbCEX0KHFoQeHMoySq87Yb86_iEQMhzaLi40zvaFlQs2Y3BiRCotajc0FkDriatH930X9tkahO1DNxPzDXQoUT0ZJpKAQ7voVA9oO8rAC1exY2TSFPG5_bleyFHPp4fJRSsdBNXJPhQg==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhti3pn4CfOLfrk69s3fG_8NW6CkLob8svRlp_KecShZUcLObHpQKO58ONkYRvweTLfIRwhr4KoCMIXgHkChFXq4kcV1KIW0E44bkzqb_tCo4ZTWqmMWrQ50CHlZF5",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "MIMIC-CDM",
    "paperLink": "https://www.medrxiv.org/content/10.1101/2024.01.26.24301824v1",
    "description": "A clinical decision-making dataset derived from MIMIC-IV, focusing on the diagnosis of four acute abdominal pathologies (appendicitis, cholecystitis, diverticulitis, pancreatitis). It is designed to evaluate LLMs on gathering information and formulating diagnosis/treatment plans.",
    "authors": [
      "Paul Hager",
      "Jung E. Park",
      "Sai M. Arava",
      "Jennifer J. Yland",
      "Dennis Wei",
      "Karthikeyan Natesan Ramamurthy",
      "A.J. Klompas",
      "Isaiah Onwuanumkpe",
      "Yikuan Li",
      "Kunal H. Chandnerkar",
      "Jia Li",
      "Cosmin A. Bejan",
      "E.K. Oermann"
    ],
    "githubLink": "https://github.com/paulhager/MIMIC-Clinical-Decision-Making-Dataset",
    "itemCount": "2,400 patients",
    "source": "Scholar",
    "specs": "Text (EHR history, lab results, radiology reports)",
    "year": "2024",
    "id": "saved-1769660386135-dtgct",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_DOXwarOEgUDeqOYDLiAmaa8I5j9pXNxnlBIW5jDy7rKQUVQzwKO9jPoUSWEG023Cri-90uZU0wLFzZateeuEc0JvRtmhMi7OOooSagVFGz14zM3eP8NiVp9_OTteIEU20Tbpu7rRBRItFr8=",
        "title": "aclanthology.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3YZxJM95R1kJooQlaFJEjtUJHwlr7lys-QfTd1bYwYE1xOvCcbiFCZVhvQhl3Ddw0rja-ui-T_n3DeASiiiYRLPK2SxWtG2rLVNIxzeXLvxnwbFmJfcXKoiVXv2kgWXpE7ixwFw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEHfnteFpGjA5pd6X23xFbVYA27R2CPjCwNQ4GGtEFKDCWNKXgdzvom1zBy4RTzuwEmg7wxdXTsj5ga7o8Ipw7sobZHEf1bQk8MURMUfZ3N6YEryxhwCCZA4nmR9LCB",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnJTswTpYRoryJLO3US_UIT9r69vhqMzjyPOnhMQjZtsaTGjCy5E-6WffiUtE3d8SLUDHsWyvuAelR0ufefnlUrLEJvkyp1EC626EusLmB3dRJadlC2prmXu-f6ohprSu2kaN0BPqMmDPIHL0_VK-I53yfTZuL7Q0DQaaK7A==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKjR5-6n7V9yjXA_ULBvr4NHCehsGbCEX0KHFoQeHMoySq87Yb86_iEQMhzaLi40zvaFlQs2Y3BiRCotajc0FkDriatH930X9tkahO1DNxPzDXQoUT0ZJpKAQ7voVA9oO8rAC1exY2TSFPG5_bleyFHPp4fJRSsdBNXJPhQg==",
        "title": "medrxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhti3pn4CfOLfrk69s3fG_8NW6CkLob8svRlp_KecShZUcLObHpQKO58ONkYRvweTLfIRwhr4KoCMIXgHkChFXq4kcV1KIW0E44bkzqb_tCo4ZTWqmMWrQ50CHlZF5",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "CMRxRecon (CMRxRecon2024 / CMRxRecon2023)",
    "paperLink": "https://arxiv.org/abs/2406.19043",
    "description": "The largest and most diverse publicly available cardiac MRI k-space dataset, specifically designed to facilitate the development of universal 'foundation models' for MRI reconstruction. It covers multi-modality (Cine, Mapping, Tagging, Phase-contrast, Dark-blood), multi-view (Long-axis, Short-axis), and multi-coil data. The 2024 version expands on the 2023 challenge dataset to boost universal machine learning.",
    "authors": [
      "Chengyan Wang",
      "Jun Lyu",
      "Shuo Wang",
      "Fanwen Wang",
      "Zi Wang",
      "Xiaobo Qu"
    ],
    "githubLink": "https://github.com/CmrxRecon/CMRxRecon-SciData",
    "itemCount": "330 subjects (2024 release); 300 subjects (2023 release)",
    "source": "Scientific Data / arXiv",
    "specs": "Raw multi-coil k-space data, MATLAB format. Includes Cine, T1/T2 Mapping, Tagging, Flow, Dark-blood modalities.",
    "year": "2024",
    "id": "saved-1769660459960-hhtvq",
    "groundingSources": []
  },
  {
    "title": "Automated Cardiac Diagnosis Challenge (ACDC)",
    "paperLink": "https://ieeexplore.ieee.org/document/8360453",
    "description": "While primarily a segmentation challenge, ACDC is frequently used as a benchmark for image-domain reconstruction and super-resolution tasks (downstream evaluation of foundation models). It contains cine MRI images with expert annotations.",
    "authors": [
      "O. Bernard",
      "A. Lalande",
      "C. Zotti",
      "F. Cervenansky"
    ],
    "githubLink": "https://www.creatis.insa-lyon.fr/Challenge/acdc/",
    "itemCount": "100 patients (with ground truth)",
    "source": "IEEE Transactions on Medical Imaging",
    "specs": "Image-domain data (NIfTI format). Short-axis Cine MRI.",
    "year": "2018",
    "id": "saved-1769660459960-2dfev",
    "groundingSources": []
  },
  {
    "title": "Dehazing Echocardiography Challenge 2025 (DehazingEcho2025)",
    "paperLink": "https://arxiv.org/abs/2508.17326",
    "description": "A benchmark dataset designed for the MICCAI 2025 challenge focused on dehazing and enhancing echocardiography images, particularly for 'difficult-to-image' patients (e.g., those with obesity). The dataset pairs high-quality images from easy-to-image patients with lower-quality, hazy images to train and evaluate dehazing algorithms.",
    "authors": [
      "Tristan S.W. Stevens",
      "Faik C. Meral",
      "Jason Yu",
      "Iason Z. Apostolakis",
      "Jean-Luc Robert",
      "Ruud J.G. van Sloun"
    ],
    "githubLink": "https://github.com/tristan-deep/semantic-diffusion-echo-dehazing",
    "itemCount": "approx. 6,700 frames",
    "source": "arXiv",
    "specs": "Four-chamber view echocardiography images; Clean set (easy-to-image) and Noisy set (difficult-to-image).",
    "year": "2025",
    "id": "saved-1769660515080-dcukb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEnr5FC5ysq42QRCfgW6pdXuxHQAxuXBvf3KlmQzg1Oj6FfMpKTNuXXAPVIwMu8n_DKMG4GzFTS1VIjDcg_UtBD2eOjrbb_MQ1RxV5BsMLjU8fP9KpRnThoexggJMXp2AX25Fb5JPoiYumErMGbzsHCQ0ImtslvcFnfOsvzWHBX4iVWu_dd8ciPjwNJrTcAZpT9SaUCIVKwv50kyabDNHkEhe3AxOlYQ8hcfFWugPUm1JSZDhU=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2k9UDQ7vgvD0He6j0iGayqAQpGuVSK2qH4yrw0EDjXQuy0fG0KUCC8laDR6GL6z9bfUf-SxqmyHEB0NzNnnqaZn2CS0IgIQ_tnmj1ClvnRAyCgnB4kzDDAHB3h-9xfjj8ArASdkElAKjyYOdURS7NqOvP6aEuTGXHDOq8WAOri37urbOlWcA=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn3PKtN927RGBggJEx8ngToJGZ9LO1VN4jQVuExR2RblNdRg5GxoS7JI_I2hjerebhvggeNwHZsQwp7GogDsiK1YziHDIRP2bA8gnPMKvzvrrhfwR9QWTzOlWJ5wrGpXENPNeLWE3ZrwfuYX1inwehFevBLgY7Ip_q7p_4LobZo-Ct3Ns6Sb0YRjOUFd1oUc1z80k0BPtp82R4Hjtn-itE7WynVZUAvjdRiQev_TOyA9-urAu7zv4m4ByE6o7b7QuqaogTswGr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEOeAmmq22Jk05MXWD9GMIz5nzG--1LHQZOcVaX0rPUz20qA7c9_n-gv74zyBqw8ZEVwYnARzQq1hBazz3ZgYCAx8ReON1xO2FKkpXHRzO0CLX_tOD2z-IyITv1dAXzsgbCk812QUtG6XJkcVJ-gKbzMpAmJzBu1ZCOrY9MoskdSG9X4N9Regjv6Q6iOB7pGaB8r0l8Ic-qg==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "EchoNet-Dynamic (Used for Clean Priors)",
    "paperLink": "https://doi.org/10.1038/s41586-020-2145-8",
    "description": "While primarily a cardiac function assessment dataset, EchoNet-Dynamic is frequently used in dehazing research (e.g., by Stevens et al.) to train generative priors for clean ultrasound images. It contains annotated echocardiogram videos used as the 'ground truth' or baseline distribution for developing dehazing diffusion models.",
    "authors": [
      "David Ouyang",
      "Bryan He",
      "Amirata Ghorbani",
      "Neal Yuan",
      "Joseph Ebinger",
      "Curt P. Langlotz",
      "Paul A. Heidenreich",
      "Robert A. Harrington",
      "David H. Liang",
      "Euan A. Ashley",
      "James Y. Zou"
    ],
    "githubLink": "https://github.com/echonet/dynamic",
    "itemCount": "10,030 videos",
    "source": "Nature",
    "specs": "Apical-4-chamber echocardiography videos; 112 x 112 pixel resolution.",
    "year": "2020",
    "id": "saved-1769660515080-sinh6",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEnr5FC5ysq42QRCfgW6pdXuxHQAxuXBvf3KlmQzg1Oj6FfMpKTNuXXAPVIwMu8n_DKMG4GzFTS1VIjDcg_UtBD2eOjrbb_MQ1RxV5BsMLjU8fP9KpRnThoexggJMXp2AX25Fb5JPoiYumErMGbzsHCQ0ImtslvcFnfOsvzWHBX4iVWu_dd8ciPjwNJrTcAZpT9SaUCIVKwv50kyabDNHkEhe3AxOlYQ8hcfFWugPUm1JSZDhU=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2k9UDQ7vgvD0He6j0iGayqAQpGuVSK2qH4yrw0EDjXQuy0fG0KUCC8laDR6GL6z9bfUf-SxqmyHEB0NzNnnqaZn2CS0IgIQ_tnmj1ClvnRAyCgnB4kzDDAHB3h-9xfjj8ArASdkElAKjyYOdURS7NqOvP6aEuTGXHDOq8WAOri37urbOlWcA=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHn3PKtN927RGBggJEx8ngToJGZ9LO1VN4jQVuExR2RblNdRg5GxoS7JI_I2hjerebhvggeNwHZsQwp7GogDsiK1YziHDIRP2bA8gnPMKvzvrrhfwR9QWTzOlWJ5wrGpXENPNeLWE3ZrwfuYX1inwehFevBLgY7Ip_q7p_4LobZo-Ct3Ns6Sb0YRjOUFd1oUc1z80k0BPtp82R4Hjtn-itE7WynVZUAvjdRiQev_TOyA9-urAu7zv4m4ByE6o7b7QuqaogTswGr",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEEOeAmmq22Jk05MXWD9GMIz5nzG--1LHQZOcVaX0rPUz20qA7c9_n-gv74zyBqw8ZEVwYnARzQq1hBazz3ZgYCAx8ReON1xO2FKkpXHRzO0CLX_tOD2z-IyITv1dAXzsgbCk812QUtG6XJkcVJ-gKbzMpAmJzBu1ZCOrY9MoskdSG9X4N9Regjv6Q6iOB7pGaB8r0l8Ic-qg==",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "TBI-IT (Traumatic Brain Injury Image/Text)",
    "paperLink": "https://arxiv.org/abs/2403.09062",
    "description": "A multimodal dataset designed to enhance AI accuracy in TBI diagnosis. It incorporates electronic medical records (EMRs) and head CT images with specific annotations for brain midline, hematoma, and fractures.",
    "authors": [
      "Jie Li",
      "Jiaying Wen",
      "et al."
    ],
    "githubLink": "https://arxiv.org/abs/2403.09062",
    "itemCount": "Not specified (Multimodal EMR + CT)",
    "source": "arXiv",
    "specs": "CT Images, Text (EMR)",
    "year": "2024",
    "id": "saved-1769660574246-sl0v8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcZQqkkskT_Kl4owJfdwAXFd558sVz3hf2eixzCaRxsXwPJL7lCBBa5v9y8AmkyCxqx_S_whNDMBYyein6TEq8mAELAWjI2pxI1B-ZY4z64RP04QrcKiBEwmIWIPDCUIuQRE6psiQToA==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "ISLES 2022 (Ischemic Stroke Lesion Segmentation)",
    "paperLink": "https://www.nature.com/articles/s41597-022-01875-5",
    "description": "A multi-center MRI dataset for the segmentation of acute to sub-acute ischemic stroke lesions. It includes multi-modal imaging (DWI, ADC, FLAIR) to benchmark segmentation models.",
    "authors": [
      "Moritz R. Hernandez Petzsche",
      "Ezequiel de la Rosa",
      "Uta Hanning",
      "et al."
    ],
    "githubLink": "https://github.com/ezequieldlrosa/isles22",
    "itemCount": "400 cases (250 training, 150 test)",
    "source": "Google Scholar",
    "specs": "MRI (DWI, ADC, FLAIR), NIfTI format",
    "year": "2022",
    "id": "saved-1769660574246-74taf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcZQqkkskT_Kl4owJfdwAXFd558sVz3hf2eixzCaRxsXwPJL7lCBBa5v9y8AmkyCxqx_S_whNDMBYyein6TEq8mAELAWjI2pxI1B-ZY4z64RP04QrcKiBEwmIWIPDCUIuQRE6psiQToA==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "RSNA Intracranial Hemorrhage Detection",
    "paperLink": "https://pubs.rsna.org/doi/10.1148/ryai.2020190211",
    "description": "A massive dataset comprised of de-identified head CT scans, created for the 2019 RSNA AI Challenge. It serves as a benchmark for detecting and classifying five subtypes of intracranial hemorrhage: intraparenchymal, intraventricular, subarachnoid, subdural, and epidural.",
    "authors": [
      "Adam E. Flanders",
      "Luciano M. Prevedello",
      "George Shih",
      "et al."
    ],
    "githubLink": "https://github.com/RSNA/AI-Challenge-Data",
    "itemCount": "25,000+ exams (874,035 images)",
    "source": "Kaggle",
    "specs": "CT images (DICOM), CSV labels",
    "year": "2020",
    "id": "saved-1769660574247-yuiqz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcZQqkkskT_Kl4owJfdwAXFd558sVz3hf2eixzCaRxsXwPJL7lCBBa5v9y8AmkyCxqx_S_whNDMBYyein6TEq8mAELAWjI2pxI1B-ZY4z64RP04QrcKiBEwmIWIPDCUIuQRE6psiQToA==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "PhysioNet CT-ICH (Computed Tomography Images for Intracranial Hemorrhage)",
    "paperLink": "https://physionet.org/content/ct-ich/1.3.1/",
    "description": "A dataset containing head CT scans of patients with traumatic brain injury (TBI). It provides slice-level annotations for intracranial hemorrhage regions and skull fractures, segmented by radiologists.",
    "authors": [
      "Murtadha Hssayeni"
    ],
    "githubLink": "https://github.com/Murtadha44/-Intracranial-Hemorrhage-Segmentation-Using-Deep-Convolutional-Model-U-Net",
    "itemCount": "82 CT scans",
    "source": "PhysioNet",
    "specs": "CT scans (NIfTI/JPG), Segmentation Masks",
    "year": "2020",
    "id": "saved-1769660574247-9qua4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcZQqkkskT_Kl4owJfdwAXFd558sVz3hf2eixzCaRxsXwPJL7lCBBa5v9y8AmkyCxqx_S_whNDMBYyein6TEq8mAELAWjI2pxI1B-ZY4z64RP04QrcKiBEwmIWIPDCUIuQRE6psiQToA==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "CQ500",
    "paperLink": "https://arxiv.org/abs/1803.05854",
    "description": "A dataset collected from multiple radiology centers containing head CT scans and their associated clinical reports. It is widely used for validating deep learning algorithms for detecting critical findings like intracranial hemorrhage, midline shift, and mass effect.",
    "authors": [
      "Sasank Chilamkurthy",
      "Rohit Ghosh",
      "Suresh Tanamala",
      "et al."
    ],
    "githubLink": "https://github.com/muschellij2/cq500_code",
    "itemCount": "491 scans",
    "source": "arXiv",
    "specs": "CT scans (DICOM), Clinical Reports",
    "year": "2018",
    "id": "saved-1769660574247-8w6uc",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcZQqkkskT_Kl4owJfdwAXFd558sVz3hf2eixzCaRxsXwPJL7lCBBa5v9y8AmkyCxqx_S_whNDMBYyein6TEq8mAELAWjI2pxI1B-ZY4z64RP04QrcKiBEwmIWIPDCUIuQRE6psiQToA==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "CHIMERA (Combining HIstology, Medical Imaging, and molEcular Data)",
    "paperLink": "https://zenodo.org/records/13981215",
    "description": "A multimodal benchmark aimed at advancing precision medicine in cancer care by integrating transcriptomics (molecular), histopathology (histology), and radiology (MRI) data. It focuses on predicting recurrence in prostate cancer and non-invasive bladder cancer (NIBC).",
    "authors": [
      "CHIMERA Challenge Organizers",
      "MICCAI 2025"
    ],
    "githubLink": "https://chimera.grand-challenge.org/",
    "itemCount": "Not fully specified in snippets (multi-center data for Prostate and Bladder cancer)",
    "source": "Grand Challenge / Zenodo",
    "specs": "Modalities: Histopathology (WSI), Radiology (MRI), Genomics (Transcriptomics)",
    "year": "2025",
    "id": "saved-1769660633013-g65mk",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH9xnZ0NTSPBfYEPU_rLuDmRfPRQAcxk2PzMyxPD921CqlZXpRb-2nIKVEHRrPUOzsoamQUcaBOxpKEaIUzKGbYjYhnerFcSQyOgifk4td2-stSuHIb5p0H9V4aGtk",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKzgFth7JbgSSF0IxGykeQgXfZwOfjHGKVdVFDsUX9bp4lyr9Kdlvv09JWMAFlQGXZRPY4VldeyY3cvoaypASmmKovlBY2ua3BcHy6JKjMNcvj2ES5JQf_lnxT9RvUnJgve4DOBKXUPJ2PFhoN-pogL56_evg=",
        "title": "cancerimagingarchive.net"
      }
    ]
  },
  {
    "title": "MultiMed",
    "paperLink": "https://arxiv.org/abs/2408.12682",
    "description": "A massively multimodal benchmark designed to evaluate large-scale learning across a wide spectrum of medical modalities. It covers tasks like disease prognosis and protein structure prediction.",
    "authors": [
      "Emergent Mind",
      "et al."
    ],
    "githubLink": "https://github.com/MultiMed-Bench/MultiMed",
    "itemCount": "2.56 million samples",
    "source": "arXiv",
    "specs": "Modalities: 10 types including Text, Radiology (OCT, X-ray, CT, MRI), Pathology, Genomics, Protein, EEG",
    "year": "2024",
    "id": "saved-1769660633013-j6pg6",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH9xnZ0NTSPBfYEPU_rLuDmRfPRQAcxk2PzMyxPD921CqlZXpRb-2nIKVEHRrPUOzsoamQUcaBOxpKEaIUzKGbYjYhnerFcSQyOgifk4td2-stSuHIb5p0H9V4aGtk",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKzgFth7JbgSSF0IxGykeQgXfZwOfjHGKVdVFDsUX9bp4lyr9Kdlvv09JWMAFlQGXZRPY4VldeyY3cvoaypASmmKovlBY2ua3BcHy6JKjMNcvj2ES5JQf_lnxT9RvUnJgve4DOBKXUPJ2PFhoN-pogL56_evg=",
        "title": "cancerimagingarchive.net"
      }
    ]
  },
  {
    "title": "NSCLC-Radiogenomics",
    "paperLink": "https://doi.org/10.1038/sdata.2018.202",
    "description": "A dataset created to facilitate the discovery of relationships between genomic and medical image features in Non-Small Cell Lung Cancer. It links CT/PET-CT imaging data with RNA sequencing and gene mutation data.",
    "authors": [
      "Bakr, S.",
      "Gevaert, O.",
      "Echegaray, S.",
      "et al."
    ],
    "githubLink": "https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiogenomics",
    "itemCount": "211 subjects (116 with paired genomics)",
    "source": "TCIA / TCGA",
    "specs": "Modalities: Radiology (CT, PET/CT), Pathology (Linked TCGA Tissue), Molecular (RNA-Seq, Gene Mutation)",
    "year": "2018",
    "id": "saved-1769660633013-nvj0g",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH9xnZ0NTSPBfYEPU_rLuDmRfPRQAcxk2PzMyxPD921CqlZXpRb-2nIKVEHRrPUOzsoamQUcaBOxpKEaIUzKGbYjYhnerFcSQyOgifk4td2-stSuHIb5p0H9V4aGtk",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKzgFth7JbgSSF0IxGykeQgXfZwOfjHGKVdVFDsUX9bp4lyr9Kdlvv09JWMAFlQGXZRPY4VldeyY3cvoaypASmmKovlBY2ua3BcHy6JKjMNcvj2ES5JQf_lnxT9RvUnJgve4DOBKXUPJ2PFhoN-pogL56_evg=",
        "title": "cancerimagingarchive.net"
      }
    ]
  },
  {
    "title": "TCGA-LGG Radiogenomics",
    "paperLink": "https://doi.org/10.7937/K9/TCIA.2016.L4LTD3TK",
    "description": "A collection connecting clinical, genomic, and imaging data for Lower Grade Glioma (LGG) patients. It allows researchers to correlate tissue genotypes with radiological phenotypes (MRI) and patient outcomes.",
    "authors": [
      "Pedano, N.",
      "Flanders, A.",
      "Scarpace, L.",
      "et al."
    ],
    "githubLink": "https://wiki.cancerimagingarchive.net/display/Public/TCGA-LGG",
    "itemCount": "199 subjects",
    "source": "TCIA / TCGA",
    "specs": "Modalities: Radiology (MRI), Pathology (Digital Slides via TCGA), Molecular (Genomic clusters, 1p/19q status)",
    "year": "2016",
    "id": "saved-1769660633013-ud1d4",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH9xnZ0NTSPBfYEPU_rLuDmRfPRQAcxk2PzMyxPD921CqlZXpRb-2nIKVEHRrPUOzsoamQUcaBOxpKEaIUzKGbYjYhnerFcSQyOgifk4td2-stSuHIb5p0H9V4aGtk",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKzgFth7JbgSSF0IxGykeQgXfZwOfjHGKVdVFDsUX9bp4lyr9Kdlvv09JWMAFlQGXZRPY4VldeyY3cvoaypASmmKovlBY2ua3BcHy6JKjMNcvj2ES5JQf_lnxT9RvUnJgve4DOBKXUPJ2PFhoN-pogL56_evg=",
        "title": "cancerimagingarchive.net"
      }
    ]
  },
  {
    "title": "EndoBench",
    "paperLink": "https://huggingface.co/datasets/Saint-lsy/EndoBench",
    "description": "A comprehensive benchmark for evaluating Multi-Modal Large Language Models (MLLMs) on endoscopy tasks. It includes a curated collection of Visual Question Answering (VQA) pairs derived from multiple public endoscopy datasets.",
    "authors": [
      "Shuaiyi Liu",
      "et al."
    ],
    "githubLink": "https://huggingface.co/datasets/Saint-lsy/EndoBench",
    "itemCount": "6,832 VQA pairs",
    "source": "Hugging Face",
    "specs": "Visual Question Answering (VQA) pairs, Multi-modal",
    "year": "2025",
    "id": "saved-1769660714904-lwzkn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "SUN-SEG",
    "paperLink": "https://arxiv.org/abs/2208.03159",
    "description": "A large-scale video polyp segmentation dataset derived from the SUN database. It provides high-quality frame-by-frame annotations including object masks, boundaries, scribbles, and polygons for colonoscopy videos.",
    "authors": [
      "Ge-Peng Ji",
      "Guolei Sun",
      "Yu-Cheng Chou",
      "Runmin Wu",
      "Yi-Wei Chen",
      "Fan Zhou",
      "Deng-Ping Fan"
    ],
    "githubLink": "https://github.com/GewelsJI/VPS",
    "itemCount": "158,690 frames",
    "source": "arXiv",
    "specs": "Video frames, Segmentation masks, Attribute labels, Boundaries",
    "year": "2022",
    "id": "saved-1769660714904-8uokb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Hyper-Kvasir",
    "paperLink": "https://www.nature.com/articles/s41597-020-00622-y",
    "description": "A large-scale comprehensive dataset for gastrointestinal endoscopy. It includes images and videos covering anatomical landmarks, pathological findings, and normal findings, designed for classification, segmentation, and detection tasks.",
    "authors": [
      "Hanna Borgli",
      "Vajira Thambawita",
      "Pia H. Smedsrud",
      "Steven Hicks",
      "Debesh Jha",
      "Eskil K. Stensland",
      "Kristine R. Berg",
      "Pål Halvorsen",
      "Michael A. Riegler"
    ],
    "githubLink": "https://github.com/simula/hyper-kvasir",
    "itemCount": "110,079 images, 373 videos",
    "source": "Scholar",
    "specs": "Images (JPEG), Videos, Segmentation masks (1,000 images), Class labels (23 classes)",
    "year": "2020",
    "id": "saved-1769660714904-s0ni1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Kvasir-SEG",
    "paperLink": "https://arxiv.org/abs/1911.07069",
    "description": "A focused dataset for polyp segmentation in gastrointestinal endoscopy. It provides pixel-level ground truth masks for polyp regions, aiding in the development of automatic polyp detection and segmentation algorithms.",
    "authors": [
      "Debesh Jha",
      "Pia H. Smedsrud",
      "Michael A. Riegler",
      "Pål Halvorsen",
      "Thomas de Lange",
      "Dag Johansen",
      "Håvard D. Johansen"
    ],
    "githubLink": "https://github.com/DebeshJha/Kvasir-SEG",
    "itemCount": "1,000 images",
    "source": "arXiv",
    "specs": "Images, Binary segmentation masks, Bounding boxes (JSON)",
    "year": "2020",
    "id": "saved-1769660714904-wayoy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "EndoVis 2018 Robotic Scene Segmentation",
    "paperLink": "https://arxiv.org/abs/2001.11190",
    "description": "A dataset from the MICCAI 2018 challenge focused on semantic segmentation of surgical scenes. It includes classes for surgical instruments as well as anatomical structures like the kidney and large intestine.",
    "authors": [
      "Max Allan",
      "Satoshi Kondo",
      "Sebastian Bodenstedt",
      "Stefan Leger",
      "Rahim Kadkhodamohammadi",
      "et al."
    ],
    "githubLink": "https://endovissub2018-roboticscenesegmentation.grand-challenge.org/",
    "itemCount": "19 sequences (approx. 2,800 frames)",
    "source": "arXiv",
    "specs": "Video frames, Semantic segmentation masks (multiple classes)",
    "year": "2020",
    "id": "saved-1769660714904-yff22",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "CholecSeg8k",
    "paperLink": "https://arxiv.org/abs/2012.12453",
    "description": "A semantic segmentation dataset derived from the Cholec80 dataset. It contains annotated frames for 13 distinct classes (e.g., liver, fat, grasper, hook) found in laparoscopic cholecystectomy.",
    "authors": [
      "W. Hong",
      "C. L. Liu",
      "B. Luo",
      "et al."
    ],
    "githubLink": "https://github.com/CAMMA-public/CholecSeg8k",
    "itemCount": "8,080 frames",
    "source": "arXiv",
    "specs": "Images, Pixel-level semantic segmentation masks (13 classes)",
    "year": "2020",
    "id": "saved-1769660714905-4g0mp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "EndoVis 2017 Robotic Instrument Segmentation",
    "paperLink": "https://arxiv.org/abs/1902.06426",
    "description": "A benchmark dataset from the MICCAI 2017 Endoscopic Vision Challenge for segmenting articulated robotic instruments. It involves binary, part-based, and type-based segmentation tasks in laparoscopic video.",
    "authors": [
      "Max Allan",
      "Alex Shvets",
      "Thomas Kurmann",
      "Zichen Zhang",
      "Rahul Duggal",
      "et al."
    ],
    "githubLink": "https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/",
    "itemCount": "10 sequences (approx. 3,000 frames)",
    "source": "arXiv",
    "specs": "Video frames, Stereo camera data, Pixel-wise segmentation masks",
    "year": "2019",
    "id": "saved-1769660714905-7fwks",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Cholec80",
    "paperLink": "https://arxiv.org/abs/1608.08223",
    "description": "A dataset for surgical phase recognition and tool presence detection in laparoscopic cholecystectomy procedures. It contains 80 videos of surgeries performed by 13 surgeons, with frame-by-frame annotations for phases and tool usage.",
    "authors": [
      "Andru P. Twinanda",
      "Sherif Shehata",
      "Didier Mutter",
      "Jacques Marescaux",
      "Michel de Mathelin",
      "Nicolas Padoy"
    ],
    "githubLink": "https://github.com/CAMMA-public/TF-Cholec80",
    "itemCount": "80 videos (approx. 370,000 frames)",
    "source": "arXiv",
    "specs": "Video (25 fps), Phase annotations (7 phases), Tool presence labels (7 tools)",
    "year": "2016",
    "id": "saved-1769660714905-pka34",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFKEWJIn7JVJTsWo9I6fJIVwmjXdZshzy9nJVejhSQZkqCyVZLxA8A892ey6wKa0voJMqOI2uPGru3Af8xY9wGc-hGCsBVO8l-Z0UkI-GI0gTsKBScS-UNmfv97fXAYoB1jMVS1qOf0Qn_AH59Kqc14",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFU8EI-Yrjt9QB_KG3ayBS6ZJdj5hpGM3IOV_De9tHFH70QB0A3NUMSBi1hs3kdEkVxntetDMAthbvRhGgu_InCIsWZ_DBXL9MYictKC5AMOkNdWmfe_4_KmvwQ05e4fmj6gR2tSeq-QX64qr8d6ByD3krIY9rUWePYOa-X7UjEEevRWGnTcwEld2qo_2FJFlltb0dMZcmD",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFlzRX6sQxk_W3KvEV_uNIYRG8bqnHz0JcH7dZ3jk8Plvg8WTbbboHAG5rwXC9mP1RfysSagTtxOsQLPj-K_XAz5FCE3Gt85i6PWENbxSnywwe8Ussj7hhiRovel53f40ZDnNSfBxDftLvZ3limMQ==",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEMTUylisBggNvGcvql_9hY202v5c3m6CC347PYkDZQ1-4qefe7f0_MgAYQ0kgqQ23oJIi9kLEZkUmB6OSej2oSCrmfKkE6XRjzT1lgWczuNJ8BIkl6krbceY3jGwhHpYQtbNhM",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4souBYefQnx4mw0-xwnYOkEsacO93LzhZ8B1VyKtJMe_rtPCR1UWNjsHgqC3J4_GIso7uzu84JkDLbNgQUzEWXSPYfpvIhuh5tq54POOKIlSlWYLz2QuO-Qxr",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEcMx371twGA9D7sWYe8wG3YtSE5SKYDYgoFbni4uqceV__8i9tljB3TFq6Kk4MremuQ0A6h2meZV5wno4F6b3W6_jdhwHqm6OAjyr4LNNB6r4ssgYMSWL2au_e",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGbT3Z8mzQA5WImeNZ1QAtwuj0R9HP11Fd3nkynpjN9RfSaekWw3qM1ZH3l1m25B4XWZMSH3CHd4tuVeTA_kdPgt8ZSPLFBrUuusOBc_D6mLnJ8eggzgS9iDK0p",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGV2lwl4Hs_cufbLYNdss7ekUR7B4w3dVI_4vYR0LuCZ407idcAinvj7gIxlhJqMvYxSIIsylwHOF1VHLSDFRu8w01os1bcX0ec6EolGG_uhAljcJfZEUrU1c2IHW-lDKpLbh18JIq5KKorbLpWIk9piNCUibFT_329",
        "title": "ieee.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzkH1_4PDa11x6cCZLd00rWNThEgVbkRapJFnPYdhRenVLR7SUxGvslrZaEcuLrgrWtxHl-rQ1t1KSx9TjThdt11JnnCKzOACp7quQYVLNuKH3jdcxm0PFCtQ8HH-ceiU6IsW3gKa_QAXD5x8=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "RVD (Retinal Video Dataset)",
    "paperLink": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3b2f8c5b08c908272504268153493822-Abstract-Datasets_and_Benchmarks.html",
    "description": "The first video-based retinal vessel dataset captured using handheld devices (smartphones). It includes temporal annotations for analyzing vessel pulsation and hemodynamic fluctuations.",
    "authors": [
      "Md Wahiduzzaman Khan",
      "Hongwei Sheng",
      "Hu Zhang",
      "Heming Du",
      "Sen Wang",
      "Minas Coroneo",
      "Farshid Hajati",
      "Sahar Shariflou",
      "Michael Kalloniatis",
      "Jack Phu",
      "Ashish Agar",
      "Zi Huang",
      "S. Mojtaba Golzan",
      "Xin Yu"
    ],
    "githubLink": "https://uq-cvlab.github.io/Retinal-Video-Dataset/",
    "itemCount": "635 videos",
    "source": "NeurIPS",
    "specs": "Smartphone-based fundus videos, spatial and temporal annotations",
    "year": "2023",
    "id": "saved-1769660786774-8sew2",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "FIVES (Fundus Image Dataset for AI-based Vessel Segmentation)",
    "paperLink": "https://www.nature.com/articles/s41597-022-01564-3",
    "description": "A large-scale dataset specifically designed for AI-based vessel segmentation. It addresses the lack of large, high-quality, multi-disease public datasets and includes pixel-wise manual annotations standardized by experts.",
    "authors": [
      "Kai Jin",
      "Xingru Huang",
      "Jingxing Zhou",
      "Yunxiang Li",
      "Yan Yan",
      "Yibao Sun",
      "Qianni Zhang",
      "Yaqi Wang",
      "Juan Ye"
    ],
    "githubLink": "https://github.com/Isengard-U/FIVES",
    "itemCount": "800 images (600 training, 200 testing)",
    "source": "arXiv",
    "specs": "High-resolution color fundus images (2048x2048), pixel-wise manual annotation",
    "year": "2022",
    "id": "saved-1769660786775-in0o9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "IOSTAR",
    "paperLink": "https://ieeexplore.ieee.org/document/7514995",
    "description": "A dataset acquired using Scanning Laser Ophthalmoscopy (SLO), differing from the standard fundus camera images. It is used to test segmentation robustness on different imaging modalities.",
    "authors": [
      "Jiong Zhang",
      "B. Dashtbozorg",
      "E. Bekkers",
      "J.P.W. Pluim",
      "R. Duits",
      "B.M. ter Haar Romeny"
    ],
    "githubLink": "https://www.idiap.ch/software/bob/docs/bob/bob.db.iostar/stable/",
    "itemCount": "30 images",
    "source": "Scholar",
    "specs": "SLO images, 1024x1024 pixels, binary vessel masks, optic disc masks",
    "year": "2016",
    "id": "saved-1769660786775-4advs",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "HRF (High-Resolution Fundus)",
    "paperLink": "https://www5.cs.fau.de/research/data/fundus-images/",
    "description": "Designed to support comparative studies on vessel segmentation algorithms using high-resolution images. It includes healthy eyes as well as those with diabetic retinopathy and glaucoma.",
    "authors": [
      "A. Budai",
      "R. Bock",
      "A. Maier",
      "J. Hornegger",
      "G. Michelson"
    ],
    "githubLink": "https://paperswithcode.com/dataset/hrf",
    "itemCount": "45 images (15 healthy, 15 DR, 15 glaucoma)",
    "source": "Scholar",
    "specs": "High-resolution color fundus images (3504x2336), binary masks",
    "year": "2013",
    "id": "saved-1769660786775-ks5ee",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "RITE (Retinal Images for Vessel Extraction)",
    "paperLink": "https://pubmed.ncbi.nlm.nih.gov/24579172/",
    "description": "A dataset derived from DRIVE, featuring improved segmentation ground truth and specific annotations for arteries and veins (A/V classification).",
    "authors": [
      "Qiao Hu",
      "Michael D. Abràmoff",
      "Mona K. Garvin"
    ],
    "githubLink": "https://github.com/ShengzhouYe/RITE-dataset-mirror",
    "itemCount": "40 images (derived from DRIVE)",
    "source": "Scholar",
    "specs": "Color fundus images, A/V classification masks, vessel tree extraction",
    "year": "2013",
    "id": "saved-1769660786775-e7m00",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "CHASE_DB1",
    "paperLink": "https://ieeexplore.ieee.org/document/6173822",
    "description": "A subset of the Child Heart and Health Study in England (CHASE) dataset. It contains retinal images of children and is used to study the relationship between retinal vessel tortuosity and cardiovascular risk factors.",
    "authors": [
      "M.M. Fraz",
      "P. Remagnino",
      "A. Hoppe",
      "B. Uyyanonvara",
      "A.R. Rudnicka",
      "C.G. Owen",
      "S.A. Barman"
    ],
    "githubLink": "https://paperswithcode.com/dataset/chase-db1",
    "itemCount": "28 images (from 14 subjects)",
    "source": "Scholar",
    "specs": "Color fundus images, 999x960 pixels, binary masks",
    "year": "2012",
    "id": "saved-1769660786775-5ot7c",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "DRIVE (Digital Retinal Images for Vessel Extraction)",
    "paperLink": "https://pubmed.ncbi.nlm.nih.gov/15539105/",
    "description": "One of the most established benchmarks for retinal vessel segmentation. The images were acquired from a diabetic retinopathy screening program in the Netherlands. It contains 40 images divided into training and test sets.",
    "authors": [
      "J.J. Staal",
      "M.D. Abramoff",
      "M. Niemeijer",
      "M.A. Viergever",
      "B. van Ginneken"
    ],
    "githubLink": "https://github.com/topics/drive-dataset",
    "itemCount": "40 images (20 training, 20 testing)",
    "source": "Scholar",
    "specs": "Color fundus images, 565x584 pixels, binary vessel masks",
    "year": "2004",
    "id": "saved-1769660786775-3y3vz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "STARE (Structured Analysis of the Retina)",
    "paperLink": "https://ieeexplore.ieee.org/document/841147",
    "description": "A classic dataset for vessel segmentation containing images with various pathologies. It was originally collected for a project on automated diagnosis of diseases of the human eye.",
    "authors": [
      "A. Hoover",
      "V. Kouznetsova",
      "M. Goldbaum"
    ],
    "githubLink": "https://paperswithcode.com/dataset/stare",
    "itemCount": "20 images",
    "source": "Scholar",
    "specs": "Color fundus images, 700x605 pixels, manual segmentations",
    "year": "2000",
    "id": "saved-1769660786775-t1mlf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWOjiS98yHn_5sxQxwlMCGzEyWpKIvjMMDK2ubs66jggPIc48jBHQFU8rhjNI73Ed_--mCvXJRf1F2BYbTtcA0Wm_hh1vFy7sL975Kri1LNj1zcnQX9YRgb37Yzdx9lrFjiwzC8wuISe-iqdFp45uAH8Mbx-7dM6PJeBbn2kM1Actl9EoXMFtCrs7CrA==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGRJbjtE0_LVyJpt4LgseoAF1urQlQwLdv6qMT3DUvz2HS0kNVUBoUChFIHFR-4TxeamFMAYusfPfrZtu_fHGDkX9K6RAF0UU0Ys9QUwp4m_owg6YwjA5V_2Bk-UFxiLbWwT6sMiyXs_TAfL3pOjAtS4OULMi5uX2XvjOZaSUcHsUOLMZtcjEcgKfVWSRY=",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDuGTVD2TY5v5t7GYUBx5yCuoZssbyrwHyZiw6RCzHHYlBmJioAjRJouuSXifOhmRgqlMthw2RPf1rf8Ej4RZXZjAIJPXrAAACGJa7auNaGuYSZawKMmMTp6M7NFrlPzpJ9QevxVHZZegRDRv0db9-heTnxws3WBlAZeLHNi7d-fCavLcTSjM7lrD0bPZ7ERR4ZZZejS1P",
        "title": "kaggle.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwPDVAQ7QSrQ4PgfCHjIJJiY31aZaTi6weAnyniLaQV8MrRR7nVeuVXVQPs-60M5Zi_kkuZvXJubUL2AVm194btyqF0Ilo4PabmM9xvaqBNcE4lqi7aZzMhyb2PMXON7KsbAiwTs6RFuoi1hrRDNeWpQ2i8t4HRemRkVt_jT_AW79jyEvu8qB8l4kDHSjmAWdWuDHeYbaX9qKQDWNMC35M4lxYZOOapL6jdsVQnLPPQCHL9Bo-FsT5QCFhgto=",
        "title": "neurips.cc"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiqDkziI09WurMnTb1aa_HvNdt_PcinlxyGUo_qojIlOvN2wySoF87_DkSEOjVAAOQTa161V3KxPveLjXgUwEwNL64ZpQYwbO9DkHEIkVgbCNlivG1P5NDAvPbOCBCEx2wZiUv",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQELKxWXuSmknrN2XXz_ddg01FNUpi9gGIDWth1pOaA-NaehBifzHJQdIXwyVMAHA8kM0_2TeAKhVhlGk5vaA5AZJjOKsL3QMBzepXCyBezXD3MgtRt-f_1JL5fpvdGt",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGKybnfT70pn2GbY2GkvvQp1elfN-T1TbaKnVOfbq-yg32TUwImY-6c8XS-9LuO2_aytUxeViy55qaT2-pJ5R1zLGyWawYW8rWIwNLnwJWBv1H5pII8A2Aa-qbsOz0=",
        "title": "zenodo.org"
      }
    ]
  },
  {
    "title": "Google Landmarks Dataset v2 (GLDv2)",
    "paperLink": "https://arxiv.org/abs/2004.01804",
    "description": "A massive dataset for large-scale, fine-grained instance recognition and image retrieval of human-made and natural landmarks. It features an extremely long-tailed class distribution and large intra-class variability.",
    "authors": [
      "Tobias Weyand",
      "Andre Araujo",
      "Bingyi Cao",
      "Jack Sim"
    ],
    "githubLink": "https://github.com/cvdfoundation/google-landmark",
    "itemCount": "5 million+ images (200k+ classes)",
    "source": "arXiv",
    "specs": "Image Retrieval/Recognition labels; Images",
    "year": "2020",
    "id": "saved-1769660856937-jkhe1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "DeepFashion2",
    "paperLink": "https://arxiv.org/abs/1901.07973",
    "description": "A versatile benchmark for fashion image analysis, including clothes detection, pose estimation, and segmentation. It significantly improves upon the original DeepFashion by providing dense landmark annotations for clothing items.",
    "authors": [
      "Yuying Ge",
      "Ruimao Zhang",
      "Lingyun Wu",
      "Xiaogang Wang",
      "Xiaoou Tang",
      "Ping Luo"
    ],
    "githubLink": "https://github.com/switchablenorms/DeepFashion2",
    "itemCount": "491,000 images (801,000 clothing items)",
    "source": "arXiv",
    "specs": "Dense landmarks (varies by category, e.g., 39 points); Images",
    "year": "2019",
    "id": "saved-1769660856938-nxk0q",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "JD-Landmark (106-Point Facial Landmark Localization)",
    "paperLink": "https://ieeexplore.ieee.org/document/8784799",
    "description": "Introduced for the Grand Challenge of 106-Point Facial Landmark Localization, this dataset covers large variations in pose, expression, and occlusion. It defines a dense 106-point annotation scheme covering the face contour and facial components.",
    "authors": [
      "Yinglu Liu",
      "Hao Shen",
      "Yue Si",
      "Xiaobo Wang",
      "Xiangyu Zhu",
      "Hailin Shi"
    ],
    "githubLink": "https://github.com/jd-opensource/lapa-dataset",
    "itemCount": "13,393 images (11,393 training, 2,000 testing)",
    "source": "Scholar",
    "specs": "106 facial landmarks; Images",
    "year": "2019",
    "id": "saved-1769660856938-alko8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "Wider Facial Landmarks in-the-Wild (WFLW)",
    "paperLink": "https://arxiv.org/abs/1805.10483",
    "description": "A challenging dataset for facial landmark detection that introduces large variations in pose, expression, and occlusion. It includes rich attribute annotations (e.g., make-up, blur, illumination) to allow for comprehensive analysis of algorithm robustness.",
    "authors": [
      "Wayne Wu",
      "Chen Qian",
      "Shuo Yang",
      "Quan Wang",
      "Yici Cai",
      "Qiang Zhou"
    ],
    "githubLink": "https://github.com/wywu/LAB",
    "itemCount": "10,000 faces (7,500 training, 2,500 testing)",
    "source": "arXiv",
    "specs": "98 fully manual annotated landmarks; Rich attributes; Images",
    "year": "2018",
    "id": "saved-1769660856938-6hra1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "MPII Human Pose Dataset",
    "paperLink": "https://openaccess.thecvf.com/content_cvpr_2014/papers/Andriluka_2D_Human_Pose_2014_CVPR_paper.pdf",
    "description": "A state-of-the-art benchmark for evaluation of articulated human pose estimation. It covers a wide range of human activities and provides 2D body joint annotations (landmarks) for people in real-world images.",
    "authors": [
      "Mykhaylo Andriluka",
      "Leonid Pishchulin",
      "Peter Gehler",
      "Bernt Schiele"
    ],
    "githubLink": "http://human-pose.mpi-inf.mpg.de/",
    "itemCount": "25,000 images (40,000 annotated people)",
    "source": "Scholar",
    "specs": "16 body joints (landmarks); Images",
    "year": "2014",
    "id": "saved-1769660856938-7xnjf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "300 Faces In-the-Wild (300W)",
    "paperLink": "https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf",
    "description": "A standard benchmark for facial landmark localization created by combining multiple datasets (LFPW, AFW, HELEN, XM2VTS). It tests the ability to handle unseen subjects, pose, expression, and illumination variations. The dataset is split into Common and Challenging subsets.",
    "authors": [
      "Christos Sagonas",
      "Epameinondas Antonakos",
      "Georgios Tzimiropoulos",
      "Stefanos Zafeiriou",
      "Maja Pantic"
    ],
    "githubLink": "https://github.com/mrgloom/Face-landmarks-detection-benchmark",
    "itemCount": "3,837 images (3,148 training, 600 test, plus others)",
    "source": "Scholar",
    "specs": "68 facial landmarks; Images",
    "year": "2013",
    "id": "saved-1769660856938-biaif",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "Caltech Occluded Faces in the Wild (COFW)",
    "paperLink": "https://openaccess.thecvf.com/content_iccv_2013/papers/Burgos-Artizzu_Robust_Face_Landmark_2013_ICCV_paper.pdf",
    "description": "Designed to test landmark estimation under heavy occlusion. The dataset explicitly annotates occlusions for each landmark, presenting faces in real-world conditions with accessories and object interactions.",
    "authors": [
      "Xavier P. Burgos-Artizzu",
      "Pietro Perona",
      "Piotr Dollar"
    ],
    "githubLink": "https://github.com/mrgloom/Face-landmarks-detection-benchmark",
    "itemCount": "1,852 images (1,345 training, 507 testing)",
    "source": "Scholar",
    "specs": "29 landmarks with occlusion labels; Images",
    "year": "2013",
    "id": "saved-1769660856938-9iqkm",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "Annotated Facial Landmarks in the Wild (AFLW)",
    "paperLink": "https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw",
    "description": "A large-scale database for facial landmark localization featuring real-world images with a wide range of natural face poses. It provides annotations for visible landmarks, making it suitable for assessing performance on non-frontal faces.",
    "authors": [
      "Martin Koestinger",
      "Paul Wohlhart",
      "Peter M. Roth",
      "Horst Bischof"
    ],
    "githubLink": "https://github.com/mrgloom/Face-landmarks-detection-benchmark",
    "itemCount": "25,993 faces (from Flickr)",
    "source": "Scholar",
    "specs": "21 landmarks (annotated upon visibility); Images",
    "year": "2011",
    "id": "saved-1769660856938-4p998",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHevldmr6wskVjUxRgSlyiKYqOR_Ic5mynxhH5ndq1eld1EWH2tmlNHVB_97PXl6qZ99MPcbIlTClz-UjG0uv2rmLn7dyy0rpjL0RclQPdad47bCN6lhCh8xOnil0SO-H4VIYm78ZryAWGn9eJwqJkfGw==",
        "title": "scribd.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3G3c2rBe19Z5vKv4wsnAJZx6elj2egagPPn5zT5pNg7MghYWycw-U2or16TM8qkV-QG2gXNnbzCDyqPy4dwZuquSLIY0U3S-v1_97qkMsh4gp7Ya2fqEZBdiA",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOWv2f32PuukCjgejbOghxPj1XXXybUr-P6LW5CzYCPeyAe_MPhI0VH7vPfM2OJEaIQZ_h9rlCTJ47bOO-NYp3EWStzxE1LIIc0I9Uk-CgFFVHJkPwLk_i0BG1lDzK5s7KC2chc2tpm5CrOjKtosc5701EWkBKexZPQ4RWbNK4qF9qL_VTnrJAYa2gZ8SQKQwgcsKSt3oQ12-f_bDF-23us4mHOvd-fQbsTmtVpJHvDVwMdNyUAvZAZCuWvok73pqGIVC2i3umQdoMKLrAwp0gaLairKfJiNmZtIM=",
        "title": "thecvf.com"
      }
    ]
  },
  {
    "title": "A Low-Field MRI Dataset For Spatiotemporal Analysis of Developing Brain",
    "paperLink": "https://doi.org/10.1038/s41597-025-04450-w",
    "description": "A dataset of 100 healthy infants (aged 1-70 days) scanned on a 0.35T low-field MRI system. The dataset is designed to facilitate the development of routine low-field MRI imaging pipelines and the analysis of brain structural changes during early life. It includes manual brain masks and atlas-based whole-brain segmentations.",
    "authors": [
      "Zhexian Sun",
      "Jian Huang",
      "Xiaohui Ma",
      "Gang Yu"
    ],
    "githubLink": "https://www.scidb.cn/en/detail?dataSetId=724054406268846080",
    "itemCount": "100 subjects (100 T2-weighted images)",
    "source": "Scientific Data (Nature) / Science Data Bank",
    "specs": "0.35T T2-weighted MRI; in-plane resolution ~0.85mm, slice thickness ~6mm; BIDS format.",
    "year": "2025",
    "id": "saved-1769660916360-f0jjq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGcQ0EmuuoaJ9qrn1yLvuUAwSrVMIFBKkLYBDih8q2V4UZbrGkiWXw0GHNBsANbdgdMvD-2krnyfSXKlsRLXVwLPL69YezFlyslht5b8YI4keJJocDAjE5umtR7BMM=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3ngXpU3x_JsXlwDfQ4iyeEB8a3_x7zroJebXwaAZbHpUUMFUMIp-2jhtYM96TouM-kzIHQmJ7uPhYR5ZMG5s_dMFFuQRUaxdPjdLPzRCfgRYjsG8-_6ypIlSuGLw5zsLE1EjyuO2mDcgCRQJjh1GBGnZLyXWADPFxuRAN9MpF414YieTt4Q3pM0MiSQQ4nr-OnIxQCv4GbaxKZVn4sK4WPmdnCrBqdOTtIDD7GumugQ3AQIM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "LISA (Low-field Pediatric Brain Magnetic Resonance Image Segmentation and Quality Assurance) Dataset",
    "paperLink": "https://arxiv.org/abs/2502.14088",
    "description": "A benchmark dataset from the MICCAI 2024/2025 Challenge designed for segmenting pediatric brain structures and assessing image quality in ultra-low-field MRI. It addresses the lack of labeled pediatric data for low-field scanners (0.064T) in low-resource settings. The dataset includes paired high-field (3T) and low-field (0.064T) scans for ground truth generation.",
    "authors": [
      "Natasha Lepore",
      "Marius George Linguraru",
      "UNITY Consortium"
    ],
    "githubLink": "https://github.com/LISA-Challenge/LISA2024",
    "itemCount": "474 images from 158 subjects (Task 2a/2b)",
    "source": "MICCAI Challenge / Zenodo",
    "specs": "0.064T (Hyperfine Swoop) T2-weighted MRI, paired with 3T references; includes segmentation masks for hippocampi and basal ganglia, and QA scores.",
    "year": "2024",
    "id": "saved-1769660916360-wh8r9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGcQ0EmuuoaJ9qrn1yLvuUAwSrVMIFBKkLYBDih8q2V4UZbrGkiWXw0GHNBsANbdgdMvD-2krnyfSXKlsRLXVwLPL69YezFlyslht5b8YI4keJJocDAjE5umtR7BMM=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG3ngXpU3x_JsXlwDfQ4iyeEB8a3_x7zroJebXwaAZbHpUUMFUMIp-2jhtYM96TouM-kzIHQmJ7uPhYR5ZMG5s_dMFFuQRUaxdPjdLPzRCfgRYjsG8-_6ypIlSuGLw5zsLE1EjyuO2mDcgCRQJjh1GBGnZLyXWADPFxuRAN9MpF414YieTt4Q3pM0MiSQQ4nr-OnIxQCv4GbaxKZVn4sK4WPmdnCrBqdOTtIDD7GumugQ3AQIM=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "PET-Bench",
    "paperLink": "https://arxiv.org/abs/2601.02737",
    "description": "The first large-scale benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in functional imaging. It includes hierarchical QA pairs to test functional perception without morphological priors.",
    "authors": [
      "Zanting Ye",
      "Xiaolong Niu",
      "Xuanbin Wu",
      "Xu Han",
      "Shengyuan Liu",
      "Jing Hao",
      "Zhihao Peng",
      "et al."
    ],
    "githubLink": "https://huggingface.co/datasets/TZT21999/PET-Bench",
    "itemCount": "52,308 QA pairs from 9,732 studies",
    "source": "Hugging Face / arXiv",
    "specs": "QA pairs, PET volumes (Multi-site, Multi-tracer)",
    "year": "2026",
    "id": "saved-1769660991463-d3x16",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUq2G1DLRDcvpIRbH9VnfVCnXpf-sH7lqGAtv43nDFLZIYLvdgF1FTU4HV5ZhM9wkr5AnYok0Tq6-abej3L4cYkysNKBGxfxb7SMnvSzGe_jkv4w68ru4MbKZXwK_VTFwOWCNBQe83TsAP9vA=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETiVvzFcT0GLIYqt47IoP4Gw6IOmH2-uzekdJvBm4LRF9_jwsuxb4m4TG-woKMJSVJKVzT_ks7qSYvn5LIKLb4tzPR00xhc7t_WfL38evOHEu5prEfy0OrlGiXkpkQRbYvtrXBZUnabV5iBBF4BGnEalm8Y5jnSxnOwrF8dKctfdYxn-rmKTRbELk=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "ENHANCE.PET 1.6k",
    "paperLink": "https://doi.org/10.21203/rs.3.rs-7169062/v1",
    "description": "A whole-body FDG-PET/CT dataset focusing on healthy tissue segmentation. It provides manual verification for CT-derived segmentations of 130 non-pathological tissues.",
    "authors": [
      "Daria Ferrara",
      "Manuel Pires",
      "Sebastian Gutschmayer",
      "Josef Yu",
      "Yasser G. Abdelhafez",
      "Lalith Kumar Shiyam Sundar",
      "Thomas Beyer",
      "et al."
    ],
    "githubLink": "https://enhance-pet.s3.eu-central-1.amazonaws.com/index.html",
    "itemCount": "1,597 whole-body/total-body PET/CT scans",
    "source": "Research Square / Scientific Data",
    "specs": "Whole-body FDG-PET/CT, NIfTI, Multi-organ segmentation",
    "year": "2025",
    "id": "saved-1769660991463-tz8y9",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUq2G1DLRDcvpIRbH9VnfVCnXpf-sH7lqGAtv43nDFLZIYLvdgF1FTU4HV5ZhM9wkr5AnYok0Tq6-abej3L4cYkysNKBGxfxb7SMnvSzGe_jkv4w68ru4MbKZXwK_VTFwOWCNBQe83TsAP9vA=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETiVvzFcT0GLIYqt47IoP4Gw6IOmH2-uzekdJvBm4LRF9_jwsuxb4m4TG-woKMJSVJKVzT_ks7qSYvn5LIKLb4tzPR00xhc7t_WfL38evOHEu5prEfy0OrlGiXkpkQRbYvtrXBZUnabV5iBBF4BGnEalm8Y5jnSxnOwrF8dKctfdYxn-rmKTRbELk=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "CT to PET Translation (CPDM Dataset)",
    "paperLink": "https://arxiv.org/abs/2410.22230",
    "description": "A massive dataset curated to support CT-to-PET image translation tasks using diffusion models. It facilitates the generation of synthetic PET images from widely available CT scans to reduce costs and radiation exposure.",
    "authors": [
      "Dac Thai Nguyen",
      "Trung Thanh Nguyen",
      "Huu Tien Nguyen",
      "Thanh Trung Nguyen",
      "Huy Hieu Pham",
      "Thanh Hung Nguyen",
      "et al."
    ],
    "githubLink": "https://github.com/thanhhff/CPDM",
    "itemCount": "2,028,628 paired CT-PET slices (3,454 patients)",
    "source": "arXiv",
    "specs": "Paired CT and PET slices, DICOM/NIfTI",
    "year": "2024",
    "id": "saved-1769660991463-3a966",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUq2G1DLRDcvpIRbH9VnfVCnXpf-sH7lqGAtv43nDFLZIYLvdgF1FTU4HV5ZhM9wkr5AnYok0Tq6-abej3L4cYkysNKBGxfxb7SMnvSzGe_jkv4w68ru4MbKZXwK_VTFwOWCNBQe83TsAP9vA=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETiVvzFcT0GLIYqt47IoP4Gw6IOmH2-uzekdJvBm4LRF9_jwsuxb4m4TG-woKMJSVJKVzT_ks7qSYvn5LIKLb4tzPR00xhc7t_WfL38evOHEu5prEfy0OrlGiXkpkQRbYvtrXBZUnabV5iBBF4BGnEalm8Y5jnSxnOwrF8dKctfdYxn-rmKTRbELk=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AutoPET (Automated Lesion Segmentation in Whole-Body PET/CT)",
    "paperLink": "https://www.nature.com/articles/s41597-022-01718-3",
    "description": "A large-scale, expertly annotated benchmark for automated lesion segmentation in whole-body FDG-PET/CT. It covers identifying malignant lesions in lymphoma, melanoma, and lung cancer patients.",
    "authors": [
      "Sergios Gatidis",
      "Tobias Hepp",
      "Marcel Früh",
      "Christian La Fougère",
      "Konstantin Nikolaou",
      "Christina Pfannenberg",
      "Bernhard Schölkopf",
      "Thomas Küstner",
      "et al."
    ],
    "githubLink": "https://github.com/autoPET-challenge",
    "itemCount": "1,014 FDG-PET/CT studies (AutoPET III)",
    "source": "TCIA / Grand Challenge",
    "specs": "Whole-body FDG-PET/CT volumes, 3D segmentation masks (NIfTI/DICOM)",
    "year": "2022",
    "id": "saved-1769660991463-3yfn0",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUq2G1DLRDcvpIRbH9VnfVCnXpf-sH7lqGAtv43nDFLZIYLvdgF1FTU4HV5ZhM9wkr5AnYok0Tq6-abej3L4cYkysNKBGxfxb7SMnvSzGe_jkv4w68ru4MbKZXwK_VTFwOWCNBQe83TsAP9vA=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETiVvzFcT0GLIYqt47IoP4Gw6IOmH2-uzekdJvBm4LRF9_jwsuxb4m4TG-woKMJSVJKVzT_ks7qSYvn5LIKLb4tzPR00xhc7t_WfL38evOHEu5prEfy0OrlGiXkpkQRbYvtrXBZUnabV5iBBF4BGnEalm8Y5jnSxnOwrF8dKctfdYxn-rmKTRbELk=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "LUNG-PET-CT-DX",
    "paperLink": "https://doi.org/10.7937/TCIA.2020.NNC2-0461",
    "description": "A large-scale dataset for lung cancer diagnosis containing CT and PET/CT images with XML annotations of tumor bounding boxes. Subjects are classified into four major histopathological subtypes.",
    "authors": [
      "The Cancer Imaging Archive (TCIA)",
      "Submitters from Harbin Medical University"
    ],
    "githubLink": "https://wiki.cancerimagingarchive.net/display/Public/LUNG-PET-CT-Dx",
    "itemCount": "355 lung cancer subjects",
    "source": "TCIA",
    "specs": "CT, PET/CT sequences, XML bounding box annotations",
    "year": "2020",
    "id": "saved-1769660991463-fdmkw",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUq2G1DLRDcvpIRbH9VnfVCnXpf-sH7lqGAtv43nDFLZIYLvdgF1FTU4HV5ZhM9wkr5AnYok0Tq6-abej3L4cYkysNKBGxfxb7SMnvSzGe_jkv4w68ru4MbKZXwK_VTFwOWCNBQe83TsAP9vA=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETiVvzFcT0GLIYqt47IoP4Gw6IOmH2-uzekdJvBm4LRF9_jwsuxb4m4TG-woKMJSVJKVzT_ks7qSYvn5LIKLb4tzPR00xhc7t_WfL38evOHEu5prEfy0OrlGiXkpkQRbYvtrXBZUnabV5iBBF4BGnEalm8Y5jnSxnOwrF8dKctfdYxn-rmKTRbELk=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "Soft-Tissue-Sarcoma (TCIA)",
    "paperLink": "https://doi.org/10.1088/0031-9155/60/14/5471",
    "description": "A collection containing FDG-PET/CT and anatomical MR imaging data for patients with soft-tissue sarcomas of the extremities, used for predicting lung metastases via radiomics.",
    "authors": [
      "Martin Vallières",
      "Carolyn R. Freeman",
      "Sonia R. Skamene",
      "Issam El Naqa"
    ],
    "githubLink": "https://github.com/mvallieres/radiomics-sarcoma",
    "itemCount": "51 patients",
    "source": "TCIA",
    "specs": "FDG-PET/CT, MRI (T1/T2), RTSTRUCT, Clinical Data",
    "year": "2015",
    "id": "saved-1769660991464-t2b9k",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFUq2G1DLRDcvpIRbH9VnfVCnXpf-sH7lqGAtv43nDFLZIYLvdgF1FTU4HV5ZhM9wkr5AnYok0Tq6-abej3L4cYkysNKBGxfxb7SMnvSzGe_jkv4w68ru4MbKZXwK_VTFwOWCNBQe83TsAP9vA=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETiVvzFcT0GLIYqt47IoP4Gw6IOmH2-uzekdJvBm4LRF9_jwsuxb4m4TG-woKMJSVJKVzT_ks7qSYvn5LIKLb4tzPR00xhc7t_WfL38evOHEu5prEfy0OrlGiXkpkQRbYvtrXBZUnabV5iBBF4BGnEalm8Y5jnSxnOwrF8dKctfdYxn-rmKTRbELk=",
        "title": "researchgate.net"
      }
    ]
  },
  {
    "title": "AutoPET Challenge Dataset (Automated Lesion Segmentation in Whole-Body PET/CT)",
    "paperLink": "https://doi.org/10.1038/s42256-022-00501-3",
    "description": "A comprehensive benchmark dataset for automated lesion segmentation in whole-body FDG-PET/CT. It addresses the challenge of distinguishing metabolic activity of tumors from physiological background uptake.",
    "authors": [
      "Sergios Gatidis",
      "Tobias Hepp",
      "Marcel Früh",
      "Claus La Fougère",
      "Konstantin Nikolaou",
      "Christian Pfleger",
      "Bernhard Schölkopf",
      "Thomas Küstner"
    ],
    "githubLink": "https://github.com/autoPET-challenge/autoPET",
    "itemCount": "1,014 PET/CT studies from 900 patients",
    "source": "Grand Challenge / TCIA",
    "specs": "Whole-body FDG-PET/CT volumes (DICOM/NIfTI), Segmentation masks",
    "year": "2022",
    "id": "saved-1769661035781-xd7cw",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQElmfkiOasJe-e1ClzCZHRDxtdJl499oTTuV7eunTlz-pXqPeCTtO7dU_Dy-h8hfr1-gqP3HduNY-kqRAM-lW6yYOl8-B_jHqXaUx6TQNNyhlzkBSqEj-WW26au",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "HECKTOR Challenge Dataset (Head and Neck Tumor segmentation in PET/CT)",
    "paperLink": "https://arxiv.org/abs/2202.05656",
    "description": "A large-scale benchmark dataset for the automatic segmentation of head and neck tumors (GTVt) and lymph nodes (GTVn) in PET/CT images. Originally launched as a MICCAI challenge, it is a primary benchmark for 'PEC/CT' (PET/CT) analysis in oncology.",
    "authors": [
      "Vincent Andrearczyk",
      "Valentin Oreiller",
      "Sarah Boughdad",
      "Catherine Cheze Le Rest",
      "Hesham Elhalawani",
      "Mario Jreige",
      "John O. Prior",
      "Martin Vallières",
      "Dimitris Visvikis",
      "Mathieu Hatt",
      "Adrien Depeursinge"
    ],
    "githubLink": "https://github.com/voreille/hecktor",
    "itemCount": "Over 882 cases (varies by challenge edition)",
    "source": "Grand Challenge / MICCAI",
    "specs": "Multimodal PET and CT images (NIfTI format), Clinical data (CSV)",
    "year": "2020",
    "id": "saved-1769661035781-e8jju",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQElmfkiOasJe-e1ClzCZHRDxtdJl499oTTuV7eunTlz-pXqPeCTtO7dU_Dy-h8hfr1-gqP3HduNY-kqRAM-lW6yYOl8-B_jHqXaUx6TQNNyhlzkBSqEj-WW26au",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Lung-PET-CT-Dx (Large-Scale CT and PET/CT Dataset for Lung Cancer Diagnosis)",
    "paperLink": "https://doi.org/10.1038/s41597-023-01976-9",
    "description": "A large-scale dataset of CT and PET/CT images for lung cancer diagnosis, covering five histological classifications. It is designed to support computer-aided diagnosis systems for lung cancer.",
    "authors": [
      "Zhihao Xu",
      "Qing Zhang",
      "Qianjin Feng",
      "Wufan Chen"
    ],
    "githubLink": "https://github.com/Micklexqg/Lung-PET-CT-Dx",
    "itemCount": "355 patients, 25,000+ images",
    "source": "The Cancer Imaging Archive (TCIA)",
    "specs": "CT and PET/CT images (DICOM), Annotation files (XML)",
    "year": "2020",
    "id": "saved-1769661035781-i96m8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQElmfkiOasJe-e1ClzCZHRDxtdJl499oTTuV7eunTlz-pXqPeCTtO7dU_Dy-h8hfr1-gqP3HduNY-kqRAM-lW6yYOl8-B_jHqXaUx6TQNNyhlzkBSqEj-WW26au",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Soft-tissue-Sarcoma (FDG-PET/CT and MR Imaging of Soft Tissue Sarcoma)",
    "paperLink": "https://doi.org/10.7937/K9/TCIA.2015.7GO2GSKS",
    "description": "A dataset containing FDG-PET/CT and MR imaging data for patients with histologically proven soft tissue sarcomas of the extremities. Used for investigating the correlation between imaging features and histopathology.",
    "authors": [
      "F. Vallières",
      "C.R. Freeman",
      "S.R. Skamene",
      "H. Elhalawani"
    ],
    "githubLink": "https://github.com/mvallieres/radiomics",
    "itemCount": "51 patients",
    "source": "The Cancer Imaging Archive (TCIA)",
    "specs": "FDG-PET/CT, MRI (T1, T2, STIR), Clinical data",
    "year": "2015",
    "id": "saved-1769661035781-fyivb",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQElmfkiOasJe-e1ClzCZHRDxtdJl499oTTuV7eunTlz-pXqPeCTtO7dU_Dy-h8hfr1-gqP3HduNY-kqRAM-lW6yYOl8-B_jHqXaUx6TQNNyhlzkBSqEj-WW26au",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "LNMBench: Benchmarking Real-World Medical Image Classification with Noisy Labels",
    "paperLink": "https://arxiv.org/abs/2512.09315",
    "description": "A comprehensive benchmark specifically targeted at the challenge of 'Noisy Labels' in medical imaging. It evaluates the robustness of classification models against both synthetic and real-world label noise.",
    "authors": [
      "Yuan Ma",
      "Junlin Hou",
      "Chao Zhang",
      "Lie Ju",
      "et al."
    ],
    "githubLink": "https://github.com/myyy777/LNMBench",
    "itemCount": "7 datasets",
    "source": "arXiv",
    "specs": "6 imaging modalities, 3 noise patterns (including real-world noise)",
    "year": "2025",
    "id": "saved-1769661086272-csnxa",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2Dw-5FK1FLjksxo4uhC6QswXK5QBlnE3h71qL_WsUr_UyY60k12IGTY04LaYwq4rH_In3eSDM9oUK16DEcdS_l_h0ARHtGUKqyHN9x85fuJESHEvqc9KTfNf2",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSqrPzWmpIn8lcw077FHUc5UOF66-uHZm1e0uCEr-qFRXkCHsXcKTgIFkUOWOlkN55VD3PmDnzeE-WgtwMhgotf7Gp5tA-YIWPfNhyTe4pkRHkm0DPEGAh_HuBNRVn8cKMpuE=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8LT0vLE25r8C-NyB9hwBfMwheir0qt-sgJIueU1tVUO4c8sJv1TiJCVoTwweeCDMU6dIpSwIrOqoLmrQcY-zE0Wa_dsTzCsFvYUnASu_PlxXGL2dK2YcFxAj9ttItjhxAxcLO5yprOg==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4nLPS6NkdN_cc-Gpi0NKxT9AQXHKLOVK3x4TSDGe3HiJWFQhnUNYUU-EtCTpM8Xp77oYq9pur7MRUrPX5e9CgxG8QV8EPylJz3n4WSoihL76w7vIXLM6CujzYqYs00VrwE9UxK9HqH4AP-2H0clNEdR3mM5ICgfq-GKTXWjcFgboB-BPqAzkRtwW4_l0pXEvnpCGi_zA0wcDxF9il8IIPtKfjVlYRkF-YxDZ_LL41f1NncHoApA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFqmExA0ETNezpFt1tnJuBndYCpT1sLDCXfFyKgipqPwhQ1KUfZ80Z69dNelNWfOmE9ITGUE2i2RMTAECf4j5_I0SCmBPdMO7kzXz3wKHGHtkANwhH_lsamIXwwAQlHiHrRNcI8kmcAgTYGAJkbsowUWPK_JLXJWWpD2Nxsb-hYZmxyZVRYJZoVBx-hkEYPtLPNEBS02EINJCJVafNtYwBSgPwn9qI86VeFpT1rsYyBJkLq",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuC9MfKwXNP4PLDOm8-uNj7fJJtoduYzaDFHdR6Z73hkW3MnJKqbHUg2WM5ZXU_Dyy1Uda7FkAzgFTJqctlSiIsx1Dv-38bh0JbPU6S7-XPHLFJoGV1LX3YjnvL4bDtLbxSV5vKhNfNVU9w0xnxIK_QRkyt6YzpHKOWNKm4VJkyjJGrip5sf9Zc7wwZwmSw5goe0XuEZp7L-75k5tAII0=",
        "title": "slideslive.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8e6E1SPRqgegjuJYfo37rXBeAGoGC3yFwx809hcclAxrsd55YgbIXJHCw483V_7i1tSMLCpQtMnPR4v0ExLQtLzuQ_e9qvR1XxX14ouqIUW0_5SREvl3PKAAC82MpjMf9V8VdJpfCXA==",
        "title": "catalyzex.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZSIHNxavzd8x9NA_8pO3EIkMV-PV_F_irUNIS5voGjwF3y7SFKo7YD9IVBEpOVUCx9VKfeYdHcpNhT4rRnGC141AS__P2ZvP2xJEOZPf55_V5UlIpeZiS5nXobyWwPY7vbsLhSzFqnYop_YjvzkR1Nwg48PSqIInvx0-jjSkt_QDkqzHaS6c653XG31htCyQt6cvPSFiWJ0TwzXvMVloFE9jLUgR3z07prod551Xzv3PNzh_ro9VppVl18-c7ukbmO6hS2KB7QHXwsHUnIDXUYjzAvl0svVo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGcnSWWURoNIy4Lu-ncZvddZfRBShUaBmeV6sZe9I3cyVFals42ft_3zlDztiGABPGx2W1AnJg4HFmfUDOjRCfmoTO4TK_20SeNMmn22j2gbVHqqFB-ucGJ5hriFKtnTIDkVDcO45V15FDhz0kh7gD318oRLMOqxyivQy6p32VjQ==",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "MedSegBench: A Comprehensive Benchmark for Medical Image Segmentation",
    "paperLink": "https://www.nature.com/articles/s41597-024-04108-6",
    "description": "A standardized benchmark combining diverse medical segmentation datasets to facilitate fair comparison of deep learning models. It unifies preprocessing and splits across widely different modalities to ensure consistent evaluation.",
    "authors": [
      "Musa Aydin",
      "Zeki Kus"
    ],
    "githubLink": "https://github.com/zekikus/MedSegBench",
    "itemCount": "Over 60,000 images",
    "source": "Scholar",
    "specs": "35 datasets, 19 classes, modalities include Ultrasound, MRI, X-ray",
    "year": "2024",
    "id": "saved-1769661086272-e5nku",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2Dw-5FK1FLjksxo4uhC6QswXK5QBlnE3h71qL_WsUr_UyY60k12IGTY04LaYwq4rH_In3eSDM9oUK16DEcdS_l_h0ARHtGUKqyHN9x85fuJESHEvqc9KTfNf2",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSqrPzWmpIn8lcw077FHUc5UOF66-uHZm1e0uCEr-qFRXkCHsXcKTgIFkUOWOlkN55VD3PmDnzeE-WgtwMhgotf7Gp5tA-YIWPfNhyTe4pkRHkm0DPEGAh_HuBNRVn8cKMpuE=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8LT0vLE25r8C-NyB9hwBfMwheir0qt-sgJIueU1tVUO4c8sJv1TiJCVoTwweeCDMU6dIpSwIrOqoLmrQcY-zE0Wa_dsTzCsFvYUnASu_PlxXGL2dK2YcFxAj9ttItjhxAxcLO5yprOg==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4nLPS6NkdN_cc-Gpi0NKxT9AQXHKLOVK3x4TSDGe3HiJWFQhnUNYUU-EtCTpM8Xp77oYq9pur7MRUrPX5e9CgxG8QV8EPylJz3n4WSoihL76w7vIXLM6CujzYqYs00VrwE9UxK9HqH4AP-2H0clNEdR3mM5ICgfq-GKTXWjcFgboB-BPqAzkRtwW4_l0pXEvnpCGi_zA0wcDxF9il8IIPtKfjVlYRkF-YxDZ_LL41f1NncHoApA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFqmExA0ETNezpFt1tnJuBndYCpT1sLDCXfFyKgipqPwhQ1KUfZ80Z69dNelNWfOmE9ITGUE2i2RMTAECf4j5_I0SCmBPdMO7kzXz3wKHGHtkANwhH_lsamIXwwAQlHiHrRNcI8kmcAgTYGAJkbsowUWPK_JLXJWWpD2Nxsb-hYZmxyZVRYJZoVBx-hkEYPtLPNEBS02EINJCJVafNtYwBSgPwn9qI86VeFpT1rsYyBJkLq",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuC9MfKwXNP4PLDOm8-uNj7fJJtoduYzaDFHdR6Z73hkW3MnJKqbHUg2WM5ZXU_Dyy1Uda7FkAzgFTJqctlSiIsx1Dv-38bh0JbPU6S7-XPHLFJoGV1LX3YjnvL4bDtLbxSV5vKhNfNVU9w0xnxIK_QRkyt6YzpHKOWNKm4VJkyjJGrip5sf9Zc7wwZwmSw5goe0XuEZp7L-75k5tAII0=",
        "title": "slideslive.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8e6E1SPRqgegjuJYfo37rXBeAGoGC3yFwx809hcclAxrsd55YgbIXJHCw483V_7i1tSMLCpQtMnPR4v0ExLQtLzuQ_e9qvR1XxX14ouqIUW0_5SREvl3PKAAC82MpjMf9V8VdJpfCXA==",
        "title": "catalyzex.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZSIHNxavzd8x9NA_8pO3EIkMV-PV_F_irUNIS5voGjwF3y7SFKo7YD9IVBEpOVUCx9VKfeYdHcpNhT4rRnGC141AS__P2ZvP2xJEOZPf55_V5UlIpeZiS5nXobyWwPY7vbsLhSzFqnYop_YjvzkR1Nwg48PSqIInvx0-jjSkt_QDkqzHaS6c653XG31htCyQt6cvPSFiWJ0TwzXvMVloFE9jLUgR3z07prod551Xzv3PNzh_ro9VppVl18-c7ukbmO6hS2KB7QHXwsHUnIDXUYjzAvl0svVo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGcnSWWURoNIy4Lu-ncZvddZfRBShUaBmeV6sZe9I3cyVFals42ft_3zlDztiGABPGx2W1AnJg4HFmfUDOjRCfmoTO4TK_20SeNMmn22j2gbVHqqFB-ucGJ5hriFKtnTIDkVDcO45V15FDhz0kh7gD318oRLMOqxyivQy6p32VjQ==",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models",
    "paperLink": "https://arxiv.org/abs/2407.00983",
    "description": "A benchmark dedicated to evaluating the fairness of foundation models in medical imaging. It integrates multiple datasets and models to assess bias and fairness-utility trade-offs in classification and segmentation tasks.",
    "authors": [
      "Ruinan Jin",
      "Zikang Xu",
      "Yuan Zhong",
      "Qi Dou",
      "Xiaoxiao Li",
      "et al."
    ],
    "githubLink": "https://github.com/FairMedFM/FairMedFM",
    "itemCount": "17 datasets",
    "source": "arXiv",
    "specs": "Evaluates 20 Foundation Models across Classification and Segmentation tasks",
    "year": "2024",
    "id": "saved-1769661086273-gok9p",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2Dw-5FK1FLjksxo4uhC6QswXK5QBlnE3h71qL_WsUr_UyY60k12IGTY04LaYwq4rH_In3eSDM9oUK16DEcdS_l_h0ARHtGUKqyHN9x85fuJESHEvqc9KTfNf2",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSqrPzWmpIn8lcw077FHUc5UOF66-uHZm1e0uCEr-qFRXkCHsXcKTgIFkUOWOlkN55VD3PmDnzeE-WgtwMhgotf7Gp5tA-YIWPfNhyTe4pkRHkm0DPEGAh_HuBNRVn8cKMpuE=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8LT0vLE25r8C-NyB9hwBfMwheir0qt-sgJIueU1tVUO4c8sJv1TiJCVoTwweeCDMU6dIpSwIrOqoLmrQcY-zE0Wa_dsTzCsFvYUnASu_PlxXGL2dK2YcFxAj9ttItjhxAxcLO5yprOg==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4nLPS6NkdN_cc-Gpi0NKxT9AQXHKLOVK3x4TSDGe3HiJWFQhnUNYUU-EtCTpM8Xp77oYq9pur7MRUrPX5e9CgxG8QV8EPylJz3n4WSoihL76w7vIXLM6CujzYqYs00VrwE9UxK9HqH4AP-2H0clNEdR3mM5ICgfq-GKTXWjcFgboB-BPqAzkRtwW4_l0pXEvnpCGi_zA0wcDxF9il8IIPtKfjVlYRkF-YxDZ_LL41f1NncHoApA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFqmExA0ETNezpFt1tnJuBndYCpT1sLDCXfFyKgipqPwhQ1KUfZ80Z69dNelNWfOmE9ITGUE2i2RMTAECf4j5_I0SCmBPdMO7kzXz3wKHGHtkANwhH_lsamIXwwAQlHiHrRNcI8kmcAgTYGAJkbsowUWPK_JLXJWWpD2Nxsb-hYZmxyZVRYJZoVBx-hkEYPtLPNEBS02EINJCJVafNtYwBSgPwn9qI86VeFpT1rsYyBJkLq",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuC9MfKwXNP4PLDOm8-uNj7fJJtoduYzaDFHdR6Z73hkW3MnJKqbHUg2WM5ZXU_Dyy1Uda7FkAzgFTJqctlSiIsx1Dv-38bh0JbPU6S7-XPHLFJoGV1LX3YjnvL4bDtLbxSV5vKhNfNVU9w0xnxIK_QRkyt6YzpHKOWNKm4VJkyjJGrip5sf9Zc7wwZwmSw5goe0XuEZp7L-75k5tAII0=",
        "title": "slideslive.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8e6E1SPRqgegjuJYfo37rXBeAGoGC3yFwx809hcclAxrsd55YgbIXJHCw483V_7i1tSMLCpQtMnPR4v0ExLQtLzuQ_e9qvR1XxX14ouqIUW0_5SREvl3PKAAC82MpjMf9V8VdJpfCXA==",
        "title": "catalyzex.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZSIHNxavzd8x9NA_8pO3EIkMV-PV_F_irUNIS5voGjwF3y7SFKo7YD9IVBEpOVUCx9VKfeYdHcpNhT4rRnGC141AS__P2ZvP2xJEOZPf55_V5UlIpeZiS5nXobyWwPY7vbsLhSzFqnYop_YjvzkR1Nwg48PSqIInvx0-jjSkt_QDkqzHaS6c653XG31htCyQt6cvPSFiWJ0TwzXvMVloFE9jLUgR3z07prod551Xzv3PNzh_ro9VppVl18-c7ukbmO6hS2KB7QHXwsHUnIDXUYjzAvl0svVo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGcnSWWURoNIy4Lu-ncZvddZfRBShUaBmeV6sZe9I3cyVFals42ft_3zlDztiGABPGx2W1AnJg4HFmfUDOjRCfmoTO4TK_20SeNMmn22j2gbVHqqFB-ucGJ5hriFKtnTIDkVDcO45V15FDhz0kh7gD318oRLMOqxyivQy6p32VjQ==",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "MedFMC: Real-world Dataset and Benchmark for Foundation Model Adaptation",
    "paperLink": "https://arxiv.org/abs/2306.09579",
    "description": "A real-world dataset and benchmark specifically designed to evaluate the adaptation of foundation models to downstream medical image classification tasks, focusing on few-shot learning and transfer capabilities across diverse clinical scenarios.",
    "authors": [
      "Dequan Wang",
      "Xiaosong Wang",
      "Lilong Wang",
      "Mengzhang Li",
      "Shaoting Zhang",
      "et al."
    ],
    "githubLink": "https://github.com/openmedlab/MedFM",
    "itemCount": "22,349 images",
    "source": "arXiv",
    "specs": "5 datasets (ChestDR, ColonPath, Endo, NeoJaundice, Retino); X-ray, Pathology, Endoscopy, Dermatology, Fundus",
    "year": "2023",
    "id": "saved-1769661086273-ss52b",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2Dw-5FK1FLjksxo4uhC6QswXK5QBlnE3h71qL_WsUr_UyY60k12IGTY04LaYwq4rH_In3eSDM9oUK16DEcdS_l_h0ARHtGUKqyHN9x85fuJESHEvqc9KTfNf2",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSqrPzWmpIn8lcw077FHUc5UOF66-uHZm1e0uCEr-qFRXkCHsXcKTgIFkUOWOlkN55VD3PmDnzeE-WgtwMhgotf7Gp5tA-YIWPfNhyTe4pkRHkm0DPEGAh_HuBNRVn8cKMpuE=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8LT0vLE25r8C-NyB9hwBfMwheir0qt-sgJIueU1tVUO4c8sJv1TiJCVoTwweeCDMU6dIpSwIrOqoLmrQcY-zE0Wa_dsTzCsFvYUnASu_PlxXGL2dK2YcFxAj9ttItjhxAxcLO5yprOg==",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH4nLPS6NkdN_cc-Gpi0NKxT9AQXHKLOVK3x4TSDGe3HiJWFQhnUNYUU-EtCTpM8Xp77oYq9pur7MRUrPX5e9CgxG8QV8EPylJz3n4WSoihL76w7vIXLM6CujzYqYs00VrwE9UxK9HqH4AP-2H0clNEdR3mM5ICgfq-GKTXWjcFgboB-BPqAzkRtwW4_l0pXEvnpCGi_zA0wcDxF9il8IIPtKfjVlYRkF-YxDZ_LL41f1NncHoApA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFqmExA0ETNezpFt1tnJuBndYCpT1sLDCXfFyKgipqPwhQ1KUfZ80Z69dNelNWfOmE9ITGUE2i2RMTAECf4j5_I0SCmBPdMO7kzXz3wKHGHtkANwhH_lsamIXwwAQlHiHrRNcI8kmcAgTYGAJkbsowUWPK_JLXJWWpD2Nxsb-hYZmxyZVRYJZoVBx-hkEYPtLPNEBS02EINJCJVafNtYwBSgPwn9qI86VeFpT1rsYyBJkLq",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuC9MfKwXNP4PLDOm8-uNj7fJJtoduYzaDFHdR6Z73hkW3MnJKqbHUg2WM5ZXU_Dyy1Uda7FkAzgFTJqctlSiIsx1Dv-38bh0JbPU6S7-XPHLFJoGV1LX3YjnvL4bDtLbxSV5vKhNfNVU9w0xnxIK_QRkyt6YzpHKOWNKm4VJkyjJGrip5sf9Zc7wwZwmSw5goe0XuEZp7L-75k5tAII0=",
        "title": "slideslive.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG8e6E1SPRqgegjuJYfo37rXBeAGoGC3yFwx809hcclAxrsd55YgbIXJHCw483V_7i1tSMLCpQtMnPR4v0ExLQtLzuQ_e9qvR1XxX14ouqIUW0_5SREvl3PKAAC82MpjMf9V8VdJpfCXA==",
        "title": "catalyzex.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZSIHNxavzd8x9NA_8pO3EIkMV-PV_F_irUNIS5voGjwF3y7SFKo7YD9IVBEpOVUCx9VKfeYdHcpNhT4rRnGC141AS__P2ZvP2xJEOZPf55_V5UlIpeZiS5nXobyWwPY7vbsLhSzFqnYop_YjvzkR1Nwg48PSqIInvx0-jjSkt_QDkqzHaS6c653XG31htCyQt6cvPSFiWJ0TwzXvMVloFE9jLUgR3z07prod551Xzv3PNzh_ro9VppVl18-c7ukbmO6hS2KB7QHXwsHUnIDXUYjzAvl0svVo=",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGcnSWWURoNIy4Lu-ncZvddZfRBShUaBmeV6sZe9I3cyVFals42ft_3zlDztiGABPGx2W1AnJg4HFmfUDOjRCfmoTO4TK_20SeNMmn22j2gbVHqqFB-ucGJ5hriFKtnTIDkVDcO45V15FDhz0kh7gD318oRLMOqxyivQy6p32VjQ==",
        "title": "medrxiv.org"
      }
    ]
  },
  {
    "title": "M&Ms-2 (Multi-Disease, Multi-View & Multi-Center Right Ventricular Segmentation)",
    "paperLink": "https://ieeexplore.ieee.org/document/10109121",
    "description": "A follow-up to the M&Ms challenge, M&Ms-2 focuses on segmenting the Right Ventricle (RV) in cardiac MRI. It addresses the complexity of RV shape and pathology by including data from 360 patients across 3 centers and 3 vendors (Siemens, GE, Philips). The dataset includes short-axis and long-axis views and covers multiple diseases affecting the RV.",
    "authors": [
      "Carlos Martín-Isla",
      "Víctor M. Campello",
      "Karim Lekadir"
    ],
    "githubLink": "https://www.ub.edu/mnms-2/",
    "itemCount": "360 patients",
    "source": "IEEE Journal of Biomedical and Health Informatics",
    "specs": "Cardiac MRI (Short-axis, Long-axis), Multi-vendor (Siemens, GE, Philips), Multi-center, Multi-disease (RV pathologies)",
    "year": "2023",
    "id": "saved-1769661105942-uxva1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFOZNjHFLYdQIM5jyEUquKw1dcYEbl-vxFjRF0_RAa2XmfJzs-30Fb_bj1LG97tPt-ux7TSErkw0nZ3aj_v_Fk7ijovOQVKUqVcKx09r77ZxOhcPLBRmYPHKf_M66O976JsrfRPByDuKebY1bXwZusDD7NAKvqMzO5v_ABjNYpX66D3BFOpRcctZ8nSUK2oZPig2t9is3axjPQrg6d60t7wR3b2iEWcZW7_HOpphK424amOz7ySmy3K-Yaf03u_BXSMiw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE9zr_litsoeAYo_O42oOAJI_FOHnAI-2RVzIN15PhQ1WU7gbQVW7JOvNQIjqpmDWVMuu2QrK6LpdDPijfXvUhcYM06NPS9OLH_byMg9ZoqAYEPn5CfBRV4vl9Blw5Cf3VtwYC1VSvKWD9TA_F05wuXryfvqVJOQjbtG-mFWkviWRnfXxfS1oXeZTKgCayGo78iQqWchrzzwtPoIGPSXZaeTsXl56zug0CuZwRV8OFPxAi72ztAhlH30dq4Tjh7332o",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXLfeaRpfSPQ4u10_weznKkOVzRXt6uPzFPeoie3yBkwIMtuL1djsBaB8ECtouo3gzz1H6DAzyM9Tw4-B9-HRJxw3uvkvc6wZqvr7pKOlS3hEnllJDRVMmvvZnpUXGfqB_g4yTAhGIMrT69Z8kawWj2ABwakUISMlAAWZ0QF9UgyC2msygMqOcMHpg6Sv9meOKSVo5zDkwpjpTEsSRSHDtQTU=",
        "title": "springermedizin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwkOpifbmTzC-pkBhjRg587rvhRt2dvze69s_-uQSTsMSUazLhIElO1R-H9Po0kbjWP8pAG35xPR4ijZeloybYliqiHx1NwFZSbvS0YhYq5POnEfshs73-n1X9XnWG6dMkui621B9JZXBwvIpa58IdRxBL6gbIiuPI63SljZqI_Dm4fsfmPnPlXtNsd4__rIGgNG8WKhjq7B805HsSZp_1eUndlOp2XY9HXUWOELpDJVX-KJYOFunP6HPJktlvjblt-XKmcYa1RoXEkw_ZfIQya5-ZN2AyFNU6gjgYNMU=",
        "title": "opengtn.eu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF95j9ofWlnoh-ounn6MktpacaihjAN-X5VyjauXBxxPuGFFu75yZYgpilmoykdNehF219ehzpNoTTYdg7wtKG75_T6DJgBy7A8-xe_eLV-de0t1GgmrclNrXXnVHAze3aTzdulsEgUvnWEuaPYcb2u1b4MlHoB9x1gP5_XPg-sVgfrUoIUytH5pI_CXeUPLN4-_4bYXWoqjRoG9GCEO8s7Z5epW93N0fwJr8OBH0HrCiRKnLYSAvAlg9AyoL9_kcEi_ARzXL5MaLg_FsRHm6yWaRFTUZsThyxKPtqTCJ8NT0B0JtA7hub0XqLnYDTTQm6gv0bDPj_YMI54x110yg==",
        "title": "auanews.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmUGN7lFQUBnKSJ68nlbynAHu1HHBSGKhd3_euU6ZkJSKS9ecxpdeZUbdXho_BF6UCDtvE-HDlugwxOGljoFUWwSLQEKRylLx5GnLUfa_K0_YoVKj7ZrZM-KQxLRZLGxp5yvyh",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEte-B9EAkeWquR2vPNRVxGUJjKmNg_vI5Nd4eCNdefSD2FNyYB26KK5Ikak0_tsIlDqL7XBoKG4tgus62Mlu6pP4cmrhVtG-D8UFKII1dr1YgL5YsP1vRRJwnYKzl0rW_n1_HAcCyqFCuBPAhnB8fmwoE-LE_X-QjOYzI-MawfnlLIE1w3q6_z28sS8BhJKqjg4Xc=",
        "title": "cam.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtr9MGX8AUij-BqE1-CWcox1LrGZ8451gbUFHp5v7OHAEWMUQ5VFt2_VzsqywMsVXTG2LJL0ciZEsZ9K90UB_ImnXFXSMXU5Gav62v76-nT0hs1i_FOe1F0TZNp9hGQblq6INPywWeG_WdZXE=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "PI-CAI (Prostate Imaging: Cancer AI) Challenge",
    "paperLink": "https://pubmed.ncbi.nlm.nih.gov/38677274/",
    "description": "The PI-CAI challenge provides a large-scale, multi-center, multi-vendor benchmark for the detection of clinically significant prostate cancer in MRI. It features over 1,500 training cases and 1,000 testing cases derived from disparate centers using Siemens, Philips, and GE scanners, addressing the domain shift issue in prostate MRI analysis.",
    "authors": [
      "Anindo Saha",
      "Joeran Bosma",
      "Jasper Twilt",
      "Henkjan Huisman"
    ],
    "githubLink": "https://pi-cai.grand-challenge.org/",
    "itemCount": "1,500 training cases (public), 1,000 testing",
    "source": "Medical Image Analysis",
    "specs": "Prostate MRI (bpMRI), Multi-vendor (Siemens, Philips, GE), Multi-center",
    "year": "2022",
    "id": "saved-1769661105942-l7xlr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFOZNjHFLYdQIM5jyEUquKw1dcYEbl-vxFjRF0_RAa2XmfJzs-30Fb_bj1LG97tPt-ux7TSErkw0nZ3aj_v_Fk7ijovOQVKUqVcKx09r77ZxOhcPLBRmYPHKf_M66O976JsrfRPByDuKebY1bXwZusDD7NAKvqMzO5v_ABjNYpX66D3BFOpRcctZ8nSUK2oZPig2t9is3axjPQrg6d60t7wR3b2iEWcZW7_HOpphK424amOz7ySmy3K-Yaf03u_BXSMiw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE9zr_litsoeAYo_O42oOAJI_FOHnAI-2RVzIN15PhQ1WU7gbQVW7JOvNQIjqpmDWVMuu2QrK6LpdDPijfXvUhcYM06NPS9OLH_byMg9ZoqAYEPn5CfBRV4vl9Blw5Cf3VtwYC1VSvKWD9TA_F05wuXryfvqVJOQjbtG-mFWkviWRnfXxfS1oXeZTKgCayGo78iQqWchrzzwtPoIGPSXZaeTsXl56zug0CuZwRV8OFPxAi72ztAhlH30dq4Tjh7332o",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXLfeaRpfSPQ4u10_weznKkOVzRXt6uPzFPeoie3yBkwIMtuL1djsBaB8ECtouo3gzz1H6DAzyM9Tw4-B9-HRJxw3uvkvc6wZqvr7pKOlS3hEnllJDRVMmvvZnpUXGfqB_g4yTAhGIMrT69Z8kawWj2ABwakUISMlAAWZ0QF9UgyC2msygMqOcMHpg6Sv9meOKSVo5zDkwpjpTEsSRSHDtQTU=",
        "title": "springermedizin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwkOpifbmTzC-pkBhjRg587rvhRt2dvze69s_-uQSTsMSUazLhIElO1R-H9Po0kbjWP8pAG35xPR4ijZeloybYliqiHx1NwFZSbvS0YhYq5POnEfshs73-n1X9XnWG6dMkui621B9JZXBwvIpa58IdRxBL6gbIiuPI63SljZqI_Dm4fsfmPnPlXtNsd4__rIGgNG8WKhjq7B805HsSZp_1eUndlOp2XY9HXUWOELpDJVX-KJYOFunP6HPJktlvjblt-XKmcYa1RoXEkw_ZfIQya5-ZN2AyFNU6gjgYNMU=",
        "title": "opengtn.eu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF95j9ofWlnoh-ounn6MktpacaihjAN-X5VyjauXBxxPuGFFu75yZYgpilmoykdNehF219ehzpNoTTYdg7wtKG75_T6DJgBy7A8-xe_eLV-de0t1GgmrclNrXXnVHAze3aTzdulsEgUvnWEuaPYcb2u1b4MlHoB9x1gP5_XPg-sVgfrUoIUytH5pI_CXeUPLN4-_4bYXWoqjRoG9GCEO8s7Z5epW93N0fwJr8OBH0HrCiRKnLYSAvAlg9AyoL9_kcEi_ARzXL5MaLg_FsRHm6yWaRFTUZsThyxKPtqTCJ8NT0B0JtA7hub0XqLnYDTTQm6gv0bDPj_YMI54x110yg==",
        "title": "auanews.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmUGN7lFQUBnKSJ68nlbynAHu1HHBSGKhd3_euU6ZkJSKS9ecxpdeZUbdXho_BF6UCDtvE-HDlugwxOGljoFUWwSLQEKRylLx5GnLUfa_K0_YoVKj7ZrZM-KQxLRZLGxp5yvyh",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEte-B9EAkeWquR2vPNRVxGUJjKmNg_vI5Nd4eCNdefSD2FNyYB26KK5Ikak0_tsIlDqL7XBoKG4tgus62Mlu6pP4cmrhVtG-D8UFKII1dr1YgL5YsP1vRRJwnYKzl0rW_n1_HAcCyqFCuBPAhnB8fmwoE-LE_X-QjOYzI-MawfnlLIE1w3q6_z28sS8BhJKqjg4Xc=",
        "title": "cam.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtr9MGX8AUij-BqE1-CWcox1LrGZ8451gbUFHp5v7OHAEWMUQ5VFt2_VzsqywMsVXTG2LJL0ciZEsZ9K90UB_ImnXFXSMXU5Gav62v76-nT0hs1i_FOe1F0TZNp9hGQblq6INPywWeG_WdZXE=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "M&Ms (Multi-Centre, Multi-Vendor & Multi-Disease Cardiac Image Segmentation Challenge)",
    "paperLink": "https://ieeexplore.ieee.org/document/9446927",
    "description": "The M&Ms dataset is designed to benchmark the generalizability of cardiac segmentation models across different clinical centers, MRI vendors, and cardiac diseases. It contains 375 patients (175 training, 40 validation, 160 testing) scanned at 6 different clinical centers using 4 different magnetic resonance manufacturers (Siemens, Philips, GE, and Canon). The dataset includes patients with hypertrophic and dilated cardiomyopathies as well as healthy subjects.",
    "authors": [
      "Víctor M. Campello",
      "Polyxeni Gkontra",
      "Carlo Izquierdo",
      "Kushibar K",
      "Karim Lekadir"
    ],
    "githubLink": "https://github.com/MarioProjects/MnMsCardiac",
    "itemCount": "375 patients (3D CMR volumes)",
    "source": "IEEE Transactions on Medical Imaging",
    "specs": "Cardiac MRI (CMR), 3D volumes, Multi-vendor (Siemens, Philips, GE, Canon), Multi-center (6 centers), Multi-disease",
    "year": "2021",
    "id": "saved-1769661105942-z2oqn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFOZNjHFLYdQIM5jyEUquKw1dcYEbl-vxFjRF0_RAa2XmfJzs-30Fb_bj1LG97tPt-ux7TSErkw0nZ3aj_v_Fk7ijovOQVKUqVcKx09r77ZxOhcPLBRmYPHKf_M66O976JsrfRPByDuKebY1bXwZusDD7NAKvqMzO5v_ABjNYpX66D3BFOpRcctZ8nSUK2oZPig2t9is3axjPQrg6d60t7wR3b2iEWcZW7_HOpphK424amOz7ySmy3K-Yaf03u_BXSMiw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE9zr_litsoeAYo_O42oOAJI_FOHnAI-2RVzIN15PhQ1WU7gbQVW7JOvNQIjqpmDWVMuu2QrK6LpdDPijfXvUhcYM06NPS9OLH_byMg9ZoqAYEPn5CfBRV4vl9Blw5Cf3VtwYC1VSvKWD9TA_F05wuXryfvqVJOQjbtG-mFWkviWRnfXxfS1oXeZTKgCayGo78iQqWchrzzwtPoIGPSXZaeTsXl56zug0CuZwRV8OFPxAi72ztAhlH30dq4Tjh7332o",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXLfeaRpfSPQ4u10_weznKkOVzRXt6uPzFPeoie3yBkwIMtuL1djsBaB8ECtouo3gzz1H6DAzyM9Tw4-B9-HRJxw3uvkvc6wZqvr7pKOlS3hEnllJDRVMmvvZnpUXGfqB_g4yTAhGIMrT69Z8kawWj2ABwakUISMlAAWZ0QF9UgyC2msygMqOcMHpg6Sv9meOKSVo5zDkwpjpTEsSRSHDtQTU=",
        "title": "springermedizin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwkOpifbmTzC-pkBhjRg587rvhRt2dvze69s_-uQSTsMSUazLhIElO1R-H9Po0kbjWP8pAG35xPR4ijZeloybYliqiHx1NwFZSbvS0YhYq5POnEfshs73-n1X9XnWG6dMkui621B9JZXBwvIpa58IdRxBL6gbIiuPI63SljZqI_Dm4fsfmPnPlXtNsd4__rIGgNG8WKhjq7B805HsSZp_1eUndlOp2XY9HXUWOELpDJVX-KJYOFunP6HPJktlvjblt-XKmcYa1RoXEkw_ZfIQya5-ZN2AyFNU6gjgYNMU=",
        "title": "opengtn.eu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF95j9ofWlnoh-ounn6MktpacaihjAN-X5VyjauXBxxPuGFFu75yZYgpilmoykdNehF219ehzpNoTTYdg7wtKG75_T6DJgBy7A8-xe_eLV-de0t1GgmrclNrXXnVHAze3aTzdulsEgUvnWEuaPYcb2u1b4MlHoB9x1gP5_XPg-sVgfrUoIUytH5pI_CXeUPLN4-_4bYXWoqjRoG9GCEO8s7Z5epW93N0fwJr8OBH0HrCiRKnLYSAvAlg9AyoL9_kcEi_ARzXL5MaLg_FsRHm6yWaRFTUZsThyxKPtqTCJ8NT0B0JtA7hub0XqLnYDTTQm6gv0bDPj_YMI54x110yg==",
        "title": "auanews.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmUGN7lFQUBnKSJ68nlbynAHu1HHBSGKhd3_euU6ZkJSKS9ecxpdeZUbdXho_BF6UCDtvE-HDlugwxOGljoFUWwSLQEKRylLx5GnLUfa_K0_YoVKj7ZrZM-KQxLRZLGxp5yvyh",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEte-B9EAkeWquR2vPNRVxGUJjKmNg_vI5Nd4eCNdefSD2FNyYB26KK5Ikak0_tsIlDqL7XBoKG4tgus62Mlu6pP4cmrhVtG-D8UFKII1dr1YgL5YsP1vRRJwnYKzl0rW_n1_HAcCyqFCuBPAhnB8fmwoE-LE_X-QjOYzI-MawfnlLIE1w3q6_z28sS8BhJKqjg4Xc=",
        "title": "cam.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtr9MGX8AUij-BqE1-CWcox1LrGZ8451gbUFHp5v7OHAEWMUQ5VFt2_VzsqywMsVXTG2LJL0ciZEsZ9K90UB_ImnXFXSMXU5Gav62v76-nT0hs1i_FOe1F0TZNp9hGQblq6INPywWeG_WdZXE=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "FeTS (Federated Tumor Segmentation) Challenge Dataset",
    "paperLink": "https://arxiv.org/abs/2105.05874",
    "description": "The FeTS dataset is constructed to benchmark federated learning algorithms for brain tumor segmentation. It aggregates MRI scans from multiple institutions globally, representing a highly heterogeneous data distribution in terms of scanners, protocols, and patient demographics, making it a key benchmark for multi-center generalization.",
    "authors": [
      "Sarthak Pati",
      "Ujjwal Baid",
      "Brandon Edwards",
      "Spyridon Bakas"
    ],
    "githubLink": "https://www.synapse.org/#!Synapse:syn28546456",
    "itemCount": "~2,000+ cases (aggregated)",
    "source": "arXiv / Nature Methods",
    "specs": "Brain MRI, Multi-center (global), Multi-vendor, Tumor segmentation",
    "year": "2021",
    "id": "saved-1769661105943-4k74o",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFOZNjHFLYdQIM5jyEUquKw1dcYEbl-vxFjRF0_RAa2XmfJzs-30Fb_bj1LG97tPt-ux7TSErkw0nZ3aj_v_Fk7ijovOQVKUqVcKx09r77ZxOhcPLBRmYPHKf_M66O976JsrfRPByDuKebY1bXwZusDD7NAKvqMzO5v_ABjNYpX66D3BFOpRcctZ8nSUK2oZPig2t9is3axjPQrg6d60t7wR3b2iEWcZW7_HOpphK424amOz7ySmy3K-Yaf03u_BXSMiw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE9zr_litsoeAYo_O42oOAJI_FOHnAI-2RVzIN15PhQ1WU7gbQVW7JOvNQIjqpmDWVMuu2QrK6LpdDPijfXvUhcYM06NPS9OLH_byMg9ZoqAYEPn5CfBRV4vl9Blw5Cf3VtwYC1VSvKWD9TA_F05wuXryfvqVJOQjbtG-mFWkviWRnfXxfS1oXeZTKgCayGo78iQqWchrzzwtPoIGPSXZaeTsXl56zug0CuZwRV8OFPxAi72ztAhlH30dq4Tjh7332o",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXLfeaRpfSPQ4u10_weznKkOVzRXt6uPzFPeoie3yBkwIMtuL1djsBaB8ECtouo3gzz1H6DAzyM9Tw4-B9-HRJxw3uvkvc6wZqvr7pKOlS3hEnllJDRVMmvvZnpUXGfqB_g4yTAhGIMrT69Z8kawWj2ABwakUISMlAAWZ0QF9UgyC2msygMqOcMHpg6Sv9meOKSVo5zDkwpjpTEsSRSHDtQTU=",
        "title": "springermedizin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwkOpifbmTzC-pkBhjRg587rvhRt2dvze69s_-uQSTsMSUazLhIElO1R-H9Po0kbjWP8pAG35xPR4ijZeloybYliqiHx1NwFZSbvS0YhYq5POnEfshs73-n1X9XnWG6dMkui621B9JZXBwvIpa58IdRxBL6gbIiuPI63SljZqI_Dm4fsfmPnPlXtNsd4__rIGgNG8WKhjq7B805HsSZp_1eUndlOp2XY9HXUWOELpDJVX-KJYOFunP6HPJktlvjblt-XKmcYa1RoXEkw_ZfIQya5-ZN2AyFNU6gjgYNMU=",
        "title": "opengtn.eu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF95j9ofWlnoh-ounn6MktpacaihjAN-X5VyjauXBxxPuGFFu75yZYgpilmoykdNehF219ehzpNoTTYdg7wtKG75_T6DJgBy7A8-xe_eLV-de0t1GgmrclNrXXnVHAze3aTzdulsEgUvnWEuaPYcb2u1b4MlHoB9x1gP5_XPg-sVgfrUoIUytH5pI_CXeUPLN4-_4bYXWoqjRoG9GCEO8s7Z5epW93N0fwJr8OBH0HrCiRKnLYSAvAlg9AyoL9_kcEi_ARzXL5MaLg_FsRHm6yWaRFTUZsThyxKPtqTCJ8NT0B0JtA7hub0XqLnYDTTQm6gv0bDPj_YMI54x110yg==",
        "title": "auanews.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmUGN7lFQUBnKSJ68nlbynAHu1HHBSGKhd3_euU6ZkJSKS9ecxpdeZUbdXho_BF6UCDtvE-HDlugwxOGljoFUWwSLQEKRylLx5GnLUfa_K0_YoVKj7ZrZM-KQxLRZLGxp5yvyh",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEte-B9EAkeWquR2vPNRVxGUJjKmNg_vI5Nd4eCNdefSD2FNyYB26KK5Ikak0_tsIlDqL7XBoKG4tgus62Mlu6pP4cmrhVtG-D8UFKII1dr1YgL5YsP1vRRJwnYKzl0rW_n1_HAcCyqFCuBPAhnB8fmwoE-LE_X-QjOYzI-MawfnlLIE1w3q6_z28sS8BhJKqjg4Xc=",
        "title": "cam.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtr9MGX8AUij-BqE1-CWcox1LrGZ8451gbUFHp5v7OHAEWMUQ5VFt2_VzsqywMsVXTG2LJL0ciZEsZ9K90UB_ImnXFXSMXU5Gav62v76-nT0hs1i_FOe1F0TZNp9hGQblq6INPywWeG_WdZXE=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "CC-CCII (China Consortium of Chest CT Image Investigation)",
    "paperLink": "https://www.cell.com/cell/fulltext/S0092-8674(20)30551-1",
    "description": "A large-scale, multi-center, and multi-vendor chest CT dataset established to aid in the diagnosis and prognosis of COVID-19. It contains data from 6 hospitals and includes cases of COVID-19, common pneumonia, and normal controls. It is one of the largest public resources for developing generalizable AI models for lung disease screening.",
    "authors": [
      "Kang Zhang",
      "Xiaohong Liu",
      "Jun Shen",
      "Jianxing He"
    ],
    "githubLink": "http://ncov-ai.big.ac.cn/download",
    "itemCount": "~4,000+ patients (varies by split/version)",
    "source": "Cell / Nature Scientific Data",
    "specs": "Chest CT, Multi-center (6 hospitals), Multi-vendor, Multi-disease (COVID-19, CP, Normal)",
    "year": "2020",
    "id": "saved-1769661105943-68gwp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFOZNjHFLYdQIM5jyEUquKw1dcYEbl-vxFjRF0_RAa2XmfJzs-30Fb_bj1LG97tPt-ux7TSErkw0nZ3aj_v_Fk7ijovOQVKUqVcKx09r77ZxOhcPLBRmYPHKf_M66O976JsrfRPByDuKebY1bXwZusDD7NAKvqMzO5v_ABjNYpX66D3BFOpRcctZ8nSUK2oZPig2t9is3axjPQrg6d60t7wR3b2iEWcZW7_HOpphK424amOz7ySmy3K-Yaf03u_BXSMiw==",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE9zr_litsoeAYo_O42oOAJI_FOHnAI-2RVzIN15PhQ1WU7gbQVW7JOvNQIjqpmDWVMuu2QrK6LpdDPijfXvUhcYM06NPS9OLH_byMg9ZoqAYEPn5CfBRV4vl9Blw5Cf3VtwYC1VSvKWD9TA_F05wuXryfvqVJOQjbtG-mFWkviWRnfXxfS1oXeZTKgCayGo78iQqWchrzzwtPoIGPSXZaeTsXl56zug0CuZwRV8OFPxAi72ztAhlH30dq4Tjh7332o",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXLfeaRpfSPQ4u10_weznKkOVzRXt6uPzFPeoie3yBkwIMtuL1djsBaB8ECtouo3gzz1H6DAzyM9Tw4-B9-HRJxw3uvkvc6wZqvr7pKOlS3hEnllJDRVMmvvZnpUXGfqB_g4yTAhGIMrT69Z8kawWj2ABwakUISMlAAWZ0QF9UgyC2msygMqOcMHpg6Sv9meOKSVo5zDkwpjpTEsSRSHDtQTU=",
        "title": "springermedizin.de"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwkOpifbmTzC-pkBhjRg587rvhRt2dvze69s_-uQSTsMSUazLhIElO1R-H9Po0kbjWP8pAG35xPR4ijZeloybYliqiHx1NwFZSbvS0YhYq5POnEfshs73-n1X9XnWG6dMkui621B9JZXBwvIpa58IdRxBL6gbIiuPI63SljZqI_Dm4fsfmPnPlXtNsd4__rIGgNG8WKhjq7B805HsSZp_1eUndlOp2XY9HXUWOELpDJVX-KJYOFunP6HPJktlvjblt-XKmcYa1RoXEkw_ZfIQya5-ZN2AyFNU6gjgYNMU=",
        "title": "opengtn.eu"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF95j9ofWlnoh-ounn6MktpacaihjAN-X5VyjauXBxxPuGFFu75yZYgpilmoykdNehF219ehzpNoTTYdg7wtKG75_T6DJgBy7A8-xe_eLV-de0t1GgmrclNrXXnVHAze3aTzdulsEgUvnWEuaPYcb2u1b4MlHoB9x1gP5_XPg-sVgfrUoIUytH5pI_CXeUPLN4-_4bYXWoqjRoG9GCEO8s7Z5epW93N0fwJr8OBH0HrCiRKnLYSAvAlg9AyoL9_kcEi_ARzXL5MaLg_FsRHm6yWaRFTUZsThyxKPtqTCJ8NT0B0JtA7hub0XqLnYDTTQm6gv0bDPj_YMI54x110yg==",
        "title": "auanews.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmUGN7lFQUBnKSJ68nlbynAHu1HHBSGKhd3_euU6ZkJSKS9ecxpdeZUbdXho_BF6UCDtvE-HDlugwxOGljoFUWwSLQEKRylLx5GnLUfa_K0_YoVKj7ZrZM-KQxLRZLGxp5yvyh",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEte-B9EAkeWquR2vPNRVxGUJjKmNg_vI5Nd4eCNdefSD2FNyYB26KK5Ikak0_tsIlDqL7XBoKG4tgus62Mlu6pP4cmrhVtG-D8UFKII1dr1YgL5YsP1vRRJwnYKzl0rW_n1_HAcCyqFCuBPAhnB8fmwoE-LE_X-QjOYzI-MawfnlLIE1w3q6_z28sS8BhJKqjg4Xc=",
        "title": "cam.ac.uk"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtr9MGX8AUij-BqE1-CWcox1LrGZ8451gbUFHp5v7OHAEWMUQ5VFt2_VzsqywMsVXTG2LJL0ciZEsZ9K90UB_ImnXFXSMXU5Gav62v76-nT0hs1i_FOe1F0TZNp9hGQblq6INPywWeG_WdZXE=",
        "title": "nih.gov"
      }
    ]
  },
  {
    "title": "OpenMIBOOD (Open Medical Imaging Benchmarks for Out-Of-Distribution Detection)",
    "paperLink": "https://arxiv.org/abs/2503.02587",
    "description": "A comprehensive framework for evaluating post-hoc OOD detection methods in medical imaging. It organizes 14 datasets into three benchmarks (MIDOG, PhaKIR, OASIS-3) covering covariate shifts, near-OOD, and far-OOD scenarios to ensure reliability in clinical deployments.",
    "authors": [
      "Max Gutbrod",
      "David Rauber",
      "Danilo Weber Nunes",
      "Christoph Palm"
    ],
    "githubLink": "https://github.com/remic-othr/OpenMIBOOD",
    "itemCount": "14 datasets",
    "source": "arXiv",
    "specs": "2D Images (Histopathology, MRI, Endoscopy)",
    "year": "2025",
    "id": "saved-1769661153405-h7uxy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFPbI3BKbO5Dn_oOMUFmB7me32F39BjEVJyTwGKi6PXN0FFbnNuxB3GldOM3bm_wL1K4oqekxouaO5Hr9E3nGRpFnvWfObYjbkyNwBeal89dq4G4ABxBtaKUNJP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1s_k3M4lG8F4dPgYQiFpPaiBz3LU5aAAXTz8OIR-mR_kiBeTZqVknFzcIQifqj11XkWIvbeNyq7IrGj0Oq1y98w1m084YYJvz_nWa4UH9Mp3XTCEbdIkvH6Z_",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgborbaGmFtWamnk2qV7OVN_ebVyyrAff0ZXHpR2xdttfFdYMOcVhqXuwiWUy5k8usOchdokJFjz23ilWxQKa04kN05dfdGlxShQxJ5yBUhNuzz4q7STrMWF2CjUH5gn9VsQ==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "MedOOD",
    "paperLink": "https://arxiv.org/abs/2502.11638",
    "description": "A curated collection of medical datasets designed to simulate clinically relevant distributional shifts, including transformations, modality changes, and organ variations. It serves as a benchmark for evaluating post-hoc OOD detection methods like normalizing flows.",
    "authors": [
      "Dariush Lotfi",
      "et al."
    ],
    "githubLink": "https://github.com/dlotfi/MedOODFlow",
    "itemCount": "Curated subsets from multiple public datasets",
    "source": "arXiv",
    "specs": "2D Images (X-ray, Fundus, Histology)",
    "year": "2025",
    "id": "saved-1769661153405-97btx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFPbI3BKbO5Dn_oOMUFmB7me32F39BjEVJyTwGKi6PXN0FFbnNuxB3GldOM3bm_wL1K4oqekxouaO5Hr9E3nGRpFnvWfObYjbkyNwBeal89dq4G4ABxBtaKUNJP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1s_k3M4lG8F4dPgYQiFpPaiBz3LU5aAAXTz8OIR-mR_kiBeTZqVknFzcIQifqj11XkWIvbeNyq7IrGj0Oq1y98w1m084YYJvz_nWa4UH9Mp3XTCEbdIkvH6Z_",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgborbaGmFtWamnk2qV7OVN_ebVyyrAff0ZXHpR2xdttfFdYMOcVhqXuwiWUy5k8usOchdokJFjz23ilWxQKa04kN05dfdGlxShQxJ5yBUhNuzz4q7STrMWF2CjUH5gn9VsQ==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "BenchMD",
    "paperLink": "https://arxiv.org/abs/2304.08486",
    "description": "A benchmark for modality-agnostic learning on medical data. It evaluates models on their ability to handle distribution shifts across hospitals and domains using 19 real-world datasets across 7 modalities, including both images and sensors.",
    "authors": [
      "Pranav Rajpurkar",
      "et al."
    ],
    "githubLink": "https://github.com/rajpurkarlab/BenchMD",
    "itemCount": "19 datasets",
    "source": "arXiv",
    "specs": "2D Images (CXR, Derm, Fundus, Mammo) and 1D Sensors (ECG, EEG)",
    "year": "2023",
    "id": "saved-1769661153405-ojpfn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFPbI3BKbO5Dn_oOMUFmB7me32F39BjEVJyTwGKi6PXN0FFbnNuxB3GldOM3bm_wL1K4oqekxouaO5Hr9E3nGRpFnvWfObYjbkyNwBeal89dq4G4ABxBtaKUNJP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1s_k3M4lG8F4dPgYQiFpPaiBz3LU5aAAXTz8OIR-mR_kiBeTZqVknFzcIQifqj11XkWIvbeNyq7IrGj0Oq1y98w1m084YYJvz_nWa4UH9Mp3XTCEbdIkvH6Z_",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgborbaGmFtWamnk2qV7OVN_ebVyyrAff0ZXHpR2xdttfFdYMOcVhqXuwiWUy5k8usOchdokJFjz23ilWxQKa04kN05dfdGlxShQxJ5yBUhNuzz4q7STrMWF2CjUH5gn9VsQ==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "WILDS (Medical Subsets: Camelyon17-wilds, RxRx1-wilds)",
    "paperLink": "https://arxiv.org/abs/2012.07421",
    "description": "A benchmark of in-the-wild distribution shifts. The medical subsets (Camelyon17 and RxRx1) focus on tumor identification across different hospitals and genetic perturbation classification across experimental batches, respectively.",
    "authors": [
      "Pang Wei Koh",
      "Shiori Sagawa",
      "Henrik Marklund",
      "Sang Michael Xie",
      "Marvin Zhang",
      "Akshay Balsubramani",
      "Weihua Hu",
      "Michihiro Yasunaga",
      "Richard Lanas Phillips",
      "Irena Gao",
      "Tony Lee",
      "Etienne David",
      "Ian Stavness",
      "Wei Guo",
      "Berton A. Earnshaw",
      "Imran S. Haque",
      "Sara Beery",
      "Jure Leskovec",
      "Anshul Kundaje",
      "Emma Pierson",
      "Sergey Levine",
      "Chelsea Finn",
      "Percy Liang"
    ],
    "githubLink": "https://github.com/p-lambda/wilds",
    "itemCount": "450,000+ patches (Camelyon17), 125,000+ images (RxRx1)",
    "source": "arXiv",
    "specs": "2D Images (Histopathology, Fluorescence Microscopy)",
    "year": "2021",
    "id": "saved-1769661153405-9s7cy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFPbI3BKbO5Dn_oOMUFmB7me32F39BjEVJyTwGKi6PXN0FFbnNuxB3GldOM3bm_wL1K4oqekxouaO5Hr9E3nGRpFnvWfObYjbkyNwBeal89dq4G4ABxBtaKUNJP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1s_k3M4lG8F4dPgYQiFpPaiBz3LU5aAAXTz8OIR-mR_kiBeTZqVknFzcIQifqj11XkWIvbeNyq7IrGj0Oq1y98w1m084YYJvz_nWa4UH9Mp3XTCEbdIkvH6Z_",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgborbaGmFtWamnk2qV7OVN_ebVyyrAff0ZXHpR2xdttfFdYMOcVhqXuwiWUy5k8usOchdokJFjz23ilWxQKa04kN05dfdGlxShQxJ5yBUhNuzz4q7STrMWF2CjUH5gn9VsQ==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Medical Out-of-Distribution Analysis Challenge (MOOD)",
    "paperLink": "https://arxiv.org/abs/2011.01897",
    "description": "A challenge and benchmark focused on detecting anomalies in medical scans. It provides a standardized testbed with sample-level and voxel-level tasks, using brain MRI and abdominal CT scans with synthetic and simulated anomalies.",
    "authors": [
      "David Zimmerer",
      "Fabian Isensee",
      "Jens Petersen",
      "Simon Kohl",
      "Klaus Maier-Hein"
    ],
    "githubLink": "https://github.com/MIC-DKFZ/mood",
    "itemCount": "~800 training scans (Brain MRI & Abdominal CT)",
    "source": "arXiv",
    "specs": "3D Volumes (NIfTI format)",
    "year": "2020",
    "id": "saved-1769661153405-71zlt",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFPbI3BKbO5Dn_oOMUFmB7me32F39BjEVJyTwGKi6PXN0FFbnNuxB3GldOM3bm_wL1K4oqekxouaO5Hr9E3nGRpFnvWfObYjbkyNwBeal89dq4G4ABxBtaKUNJP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1s_k3M4lG8F4dPgYQiFpPaiBz3LU5aAAXTz8OIR-mR_kiBeTZqVknFzcIQifqj11XkWIvbeNyq7IrGj0Oq1y98w1m084YYJvz_nWa4UH9Mp3XTCEbdIkvH6Z_",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgborbaGmFtWamnk2qV7OVN_ebVyyrAff0ZXHpR2xdttfFdYMOcVhqXuwiWUy5k8usOchdokJFjz23ilWxQKa04kN05dfdGlxShQxJ5yBUhNuzz4q7STrMWF2CjUH5gn9VsQ==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Benchmark of Medical Out-of-Distribution Detection (Cao et al.)",
    "paperLink": "https://arxiv.org/abs/2007.04250",
    "description": "One of the first dedicated benchmarks for medical OOD detection. It establishes baselines for detecting out-of-distribution samples across three primary medical domains: Chest X-rays, Fundus imaging, and Histology slides.",
    "authors": [
      "Tianshi Cao",
      "Chin-Wei Huang",
      "David Yu-Tung Hui",
      "Joseph Paul Cohen"
    ],
    "githubLink": "https://github.com/caotians1/OD-test-master",
    "itemCount": "Aggregated from NIH CXR, PCam, etc.",
    "source": "arXiv",
    "specs": "2D Images",
    "year": "2020",
    "id": "saved-1769661153405-r37z1",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFPbI3BKbO5Dn_oOMUFmB7me32F39BjEVJyTwGKi6PXN0FFbnNuxB3GldOM3bm_wL1K4oqekxouaO5Hr9E3nGRpFnvWfObYjbkyNwBeal89dq4G4ABxBtaKUNJP",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF1s_k3M4lG8F4dPgYQiFpPaiBz3LU5aAAXTz8OIR-mR_kiBeTZqVknFzcIQifqj11XkWIvbeNyq7IrGj0Oq1y98w1m084YYJvz_nWa4UH9Mp3XTCEbdIkvH6Z_",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHgborbaGmFtWamnk2qV7OVN_ebVyyrAff0ZXHpR2xdttfFdYMOcVhqXuwiWUy5k8usOchdokJFjz23ilWxQKa04kN05dfdGlxShQxJ5yBUhNuzz4q7STrMWF2CjUH5gn9VsQ==",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Multi Camera Robust Diagnosis of Fundus Diseases (MuCaRD)",
    "paperLink": "https://zenodo.org/records/13981204",
    "description": "A benchmark challenge designed to evaluate the robustness of AI models for diagnosing fundus diseases (Glaucoma and Diabetic Retinopathy) across unseen camera systems. It emphasizes generalization from a source camera (Topcon NW400) to target cameras (Optomed, Canon, etc.) and few-shot adaptation.",
    "authors": [
      "Mediwhale",
      "MICCAI Society"
    ],
    "githubLink": "https://codabench.org/competitions/53",
    "itemCount": "~2,400 images (400 training, 1,000 validation, 1,000 test)",
    "source": "MICCAI 2025 Challenge / Zenodo",
    "specs": "Fundus retinal images from 5 different camera manufacturers (Topcon, Optomed, Mediworks, Optos, Canon). Tasks include zero-shot robustness and few-shot adaptation.",
    "year": "2025",
    "id": "saved-1769661171159-addwx",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcqdOFaxnCCZp7_F4zg1Pa5zGuZ-2OkjTFwZke3RK2hBU3gDIHIwSGGKGw76wuJYAqL9EYp0lhrCUE165ju04XkqAH_8sh5iXgVa0TzfZ7PI-t4ZmXkx1C50aqaj7lXgTA5Q3Ernied0t-pNjwLK1WvtbkFDS91O1xhQYOpvDGmNiNlNtTSlkGX2eo69ySEAIhibyULLpcwJxc8w1mPHP6cVBwuKIJdb68VP8gTVAs5X4qKeWFT6B3uPlqRF0=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHu7eiJz1Rjhvbzluuif9MqkzePZyISxuCaH7EDxS-cwtlOfZvOLcESRLsP43jBQhx8QiH5jfh2J9xAD4v1jcE3-XQn0KAEf_tvTL2dV2CYPj8MB3eGeiZruvLHLPpuxJbPk96rAdaGmvuR9HU=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVnFWNV45Diw7no6NedVQ1FdT7WMoivu_syUvZAZagrmZA-BTOh8hV_dZz3pv452g32GN3j2Lhf_aQrOTPqp-dKrlj1ss7tmyWCKerVYl2BN_OeNktAzwiJin3WATHwhnGywM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGs_DjGUofUqEQysu39KZLTuIS6fqjJaLX0hwqL4xvo7J0MSZ9Lvt9ycZlytXnH0xz_MovMTmR-AOyAwEtXxYD9n0kitPy4895NBRaUcSVeknXRleB9tDCebypGr6UKEAW0RrWtiM489fAsBK5FDZanLahU325UNCnKR83UQn3yUyzsPmM7edvdIgEsnfGzGBYqGB4ek4gv_ds4SDD_O8sBRoW3_j5oD75gyY0GtfeN5bc_jdNpMYs7nGyxbC4pnNK3g8BsgGTpHWbMUqC5qVssuMqBxITBoiU",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiYgv4l8iV9I2MKn1uVQJapwLOYH8GDZABKHzKz_JBBiyoVWDOT5L0DAuz2tMJm5N5IWgeOMOz0_Cu8KvrzBiTq2RCNJe40bwKStb3Dspzy3dZK-lE-bQozbET-P5H8BIX-wabN7b4yLeD4kdM2p0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcQh0v5gvRtuLKEmAlvLAoYnxjv00ZBTQX1O09xKGMh01Dh00v8iyAz4APU83pr11wqeQeZYOS_9EMBTd2zTJ-RATlYABGLND2sjeSpgo5SGJ35F3CKy35tkO46d9TSP0B_303Bw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGn2IpHC9C4GV2URI7SEWpQaA6RW-ZepfG0vnaPUBnzcqEc_RUVklRyy1UkjR_HTaOdXIK_3WEPhe0cpAakPzkiT6EVGT3pwLRbkIv5zHCnd9EEDUlOy0EmjmI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark",
    "paperLink": "https://arxiv.org/abs/2406.18339",
    "description": "A benchmark for 3D anomaly detection (diagnosis of defects) in industrial parts, utilizing multi-view and multimodal data. It specifically addresses the challenge of training on single instances (synthetic or real) and generalizing to real-world multi-view setups.",
    "authors": [
      "Alex Costanzino",
      "Pierluigi Zama Ramirez",
      "Luigi Lella",
      "Matteo Ragaglia",
      "Alessandro Oliva",
      "Giuseppe Lisanti",
      "Luigi Di Stefano"
    ],
    "githubLink": "https://alex-costanzino.github.io/radar/",
    "itemCount": "333 instances across 8 object types",
    "source": "ICCV 2025 / arXiv",
    "specs": "Multimodal (RGB + Point Cloud), Multi-view (High-res 12MP images), 3D segmentation ground truth. Includes CAD models.",
    "year": "2025",
    "id": "saved-1769661171159-0t8zn",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcqdOFaxnCCZp7_F4zg1Pa5zGuZ-2OkjTFwZke3RK2hBU3gDIHIwSGGKGw76wuJYAqL9EYp0lhrCUE165ju04XkqAH_8sh5iXgVa0TzfZ7PI-t4ZmXkx1C50aqaj7lXgTA5Q3Ernied0t-pNjwLK1WvtbkFDS91O1xhQYOpvDGmNiNlNtTSlkGX2eo69ySEAIhibyULLpcwJxc8w1mPHP6cVBwuKIJdb68VP8gTVAs5X4qKeWFT6B3uPlqRF0=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHu7eiJz1Rjhvbzluuif9MqkzePZyISxuCaH7EDxS-cwtlOfZvOLcESRLsP43jBQhx8QiH5jfh2J9xAD4v1jcE3-XQn0KAEf_tvTL2dV2CYPj8MB3eGeiZruvLHLPpuxJbPk96rAdaGmvuR9HU=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVnFWNV45Diw7no6NedVQ1FdT7WMoivu_syUvZAZagrmZA-BTOh8hV_dZz3pv452g32GN3j2Lhf_aQrOTPqp-dKrlj1ss7tmyWCKerVYl2BN_OeNktAzwiJin3WATHwhnGywM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGs_DjGUofUqEQysu39KZLTuIS6fqjJaLX0hwqL4xvo7J0MSZ9Lvt9ycZlytXnH0xz_MovMTmR-AOyAwEtXxYD9n0kitPy4895NBRaUcSVeknXRleB9tDCebypGr6UKEAW0RrWtiM489fAsBK5FDZanLahU325UNCnKR83UQn3yUyzsPmM7edvdIgEsnfGzGBYqGB4ek4gv_ds4SDD_O8sBRoW3_j5oD75gyY0GtfeN5bc_jdNpMYs7nGyxbC4pnNK3g8BsgGTpHWbMUqC5qVssuMqBxITBoiU",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiYgv4l8iV9I2MKn1uVQJapwLOYH8GDZABKHzKz_JBBiyoVWDOT5L0DAuz2tMJm5N5IWgeOMOz0_Cu8KvrzBiTq2RCNJe40bwKStb3Dspzy3dZK-lE-bQozbET-P5H8BIX-wabN7b4yLeD4kdM2p0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcQh0v5gvRtuLKEmAlvLAoYnxjv00ZBTQX1O09xKGMh01Dh00v8iyAz4APU83pr11wqeQeZYOS_9EMBTd2zTJ-RATlYABGLND2sjeSpgo5SGJ35F3CKy35tkO46d9TSP0B_303Bw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGn2IpHC9C4GV2URI7SEWpQaA6RW-ZepfG0vnaPUBnzcqEc_RUVklRyy1UkjR_HTaOdXIK_3WEPhe0cpAakPzkiT6EVGT3pwLRbkIv5zHCnd9EEDUlOy0EmjmI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection",
    "paperLink": "https://arxiv.org/abs/2403.11867",
    "description": "A large-scale, multi-view industrial anomaly detection dataset designed to overcome the saturation of existing benchmarks. It supports multi-view anomaly detection and diagnosis in realistic manufacturing settings with diverse defect types and material properties.",
    "authors": [
      "Chengjie Wang",
      "Wenbing Zhu",
      "Bin-Bin Gao",
      "Zhenye Gan",
      "Jiangning Zhang",
      "Zhihao Gu",
      "Shuguang Qian",
      "Mingang Chen",
      "Lizhuang Ma"
    ],
    "githubLink": "https://github.com/realiad4ad/Real-IAD",
    "itemCount": "150,000 high-resolution images",
    "source": "CVPR 2024 / Hugging Face",
    "specs": "Multi-view RGB images (and updated multimodal D³ version with Point Cloud/Pseudo-3D), 30 object categories, 1,110 defect types. High resolution (2K-5K).",
    "year": "2024",
    "id": "saved-1769661171159-wdxyf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcqdOFaxnCCZp7_F4zg1Pa5zGuZ-2OkjTFwZke3RK2hBU3gDIHIwSGGKGw76wuJYAqL9EYp0lhrCUE165ju04XkqAH_8sh5iXgVa0TzfZ7PI-t4ZmXkx1C50aqaj7lXgTA5Q3Ernied0t-pNjwLK1WvtbkFDS91O1xhQYOpvDGmNiNlNtTSlkGX2eo69ySEAIhibyULLpcwJxc8w1mPHP6cVBwuKIJdb68VP8gTVAs5X4qKeWFT6B3uPlqRF0=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHu7eiJz1Rjhvbzluuif9MqkzePZyISxuCaH7EDxS-cwtlOfZvOLcESRLsP43jBQhx8QiH5jfh2J9xAD4v1jcE3-XQn0KAEf_tvTL2dV2CYPj8MB3eGeiZruvLHLPpuxJbPk96rAdaGmvuR9HU=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVnFWNV45Diw7no6NedVQ1FdT7WMoivu_syUvZAZagrmZA-BTOh8hV_dZz3pv452g32GN3j2Lhf_aQrOTPqp-dKrlj1ss7tmyWCKerVYl2BN_OeNktAzwiJin3WATHwhnGywM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGs_DjGUofUqEQysu39KZLTuIS6fqjJaLX0hwqL4xvo7J0MSZ9Lvt9ycZlytXnH0xz_MovMTmR-AOyAwEtXxYD9n0kitPy4895NBRaUcSVeknXRleB9tDCebypGr6UKEAW0RrWtiM489fAsBK5FDZanLahU325UNCnKR83UQn3yUyzsPmM7edvdIgEsnfGzGBYqGB4ek4gv_ds4SDD_O8sBRoW3_j5oD75gyY0GtfeN5bc_jdNpMYs7nGyxbC4pnNK3g8BsgGTpHWbMUqC5qVssuMqBxITBoiU",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiYgv4l8iV9I2MKn1uVQJapwLOYH8GDZABKHzKz_JBBiyoVWDOT5L0DAuz2tMJm5N5IWgeOMOz0_Cu8KvrzBiTq2RCNJe40bwKStb3Dspzy3dZK-lE-bQozbET-P5H8BIX-wabN7b4yLeD4kdM2p0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcQh0v5gvRtuLKEmAlvLAoYnxjv00ZBTQX1O09xKGMh01Dh00v8iyAz4APU83pr11wqeQeZYOS_9EMBTd2zTJ-RATlYABGLND2sjeSpgo5SGJ35F3CKy35tkO46d9TSP0B_303Bw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGn2IpHC9C4GV2URI7SEWpQaA6RW-ZepfG0vnaPUBnzcqEc_RUVklRyy1UkjR_HTaOdXIK_3WEPhe0cpAakPzkiT6EVGT3pwLRbkIv5zHCnd9EEDUlOy0EmjmI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "TULIP: Multi-camera 3D Precision Assessment of Parkinson's Disease",
    "paperLink": "https://openaccess.thecvf.com/content/CVPR2024/html/Kim_TULIP_Multi-camera_3D_Precision_Assessment_of_Parkinsons_Disease_CVPR_2024_paper.html",
    "description": "A medical diagnosis benchmark focused on the precision assessment of Parkinson's Disease motor symptoms. It uses multi-camera recordings to reconstruct 3D body movements for automated UPDRS scoring, offering a robust alternative to subjective clinical ratings.",
    "authors": [
      "Kyungdo Kim",
      "Sihan Lyu",
      "Sneha Mantri",
      "Timothy W. Dunn"
    ],
    "githubLink": "https://zenodo.org/records/10684260",
    "itemCount": "Recordings from 6 cameras for various subjects (Parkinson's patients and controls)",
    "source": "CVPR 2024 / Zenodo",
    "specs": "Synchronized RGB videos from 6 views, 3D pose reconstructions, clinical expert ratings (UPDRS scores) for 25 motor exam components.",
    "year": "2024",
    "id": "saved-1769661171159-nmpl6",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcqdOFaxnCCZp7_F4zg1Pa5zGuZ-2OkjTFwZke3RK2hBU3gDIHIwSGGKGw76wuJYAqL9EYp0lhrCUE165ju04XkqAH_8sh5iXgVa0TzfZ7PI-t4ZmXkx1C50aqaj7lXgTA5Q3Ernied0t-pNjwLK1WvtbkFDS91O1xhQYOpvDGmNiNlNtTSlkGX2eo69ySEAIhibyULLpcwJxc8w1mPHP6cVBwuKIJdb68VP8gTVAs5X4qKeWFT6B3uPlqRF0=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHu7eiJz1Rjhvbzluuif9MqkzePZyISxuCaH7EDxS-cwtlOfZvOLcESRLsP43jBQhx8QiH5jfh2J9xAD4v1jcE3-XQn0KAEf_tvTL2dV2CYPj8MB3eGeiZruvLHLPpuxJbPk96rAdaGmvuR9HU=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVnFWNV45Diw7no6NedVQ1FdT7WMoivu_syUvZAZagrmZA-BTOh8hV_dZz3pv452g32GN3j2Lhf_aQrOTPqp-dKrlj1ss7tmyWCKerVYl2BN_OeNktAzwiJin3WATHwhnGywM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGs_DjGUofUqEQysu39KZLTuIS6fqjJaLX0hwqL4xvo7J0MSZ9Lvt9ycZlytXnH0xz_MovMTmR-AOyAwEtXxYD9n0kitPy4895NBRaUcSVeknXRleB9tDCebypGr6UKEAW0RrWtiM489fAsBK5FDZanLahU325UNCnKR83UQn3yUyzsPmM7edvdIgEsnfGzGBYqGB4ek4gv_ds4SDD_O8sBRoW3_j5oD75gyY0GtfeN5bc_jdNpMYs7nGyxbC4pnNK3g8BsgGTpHWbMUqC5qVssuMqBxITBoiU",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiYgv4l8iV9I2MKn1uVQJapwLOYH8GDZABKHzKz_JBBiyoVWDOT5L0DAuz2tMJm5N5IWgeOMOz0_Cu8KvrzBiTq2RCNJe40bwKStb3Dspzy3dZK-lE-bQozbET-P5H8BIX-wabN7b4yLeD4kdM2p0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcQh0v5gvRtuLKEmAlvLAoYnxjv00ZBTQX1O09xKGMh01Dh00v8iyAz4APU83pr11wqeQeZYOS_9EMBTd2zTJ-RATlYABGLND2sjeSpgo5SGJ35F3CKy35tkO46d9TSP0B_303Bw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGn2IpHC9C4GV2URI7SEWpQaA6RW-ZepfG0vnaPUBnzcqEc_RUVklRyy1UkjR_HTaOdXIK_3WEPhe0cpAakPzkiT6EVGT3pwLRbkIv5zHCnd9EEDUlOy0EmjmI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "MSAD: Multi-Scenario Anomaly Detection Dataset",
    "paperLink": "https://openreview.net/forum?id=Qh0v5gvRtu",
    "description": "A diverse video anomaly detection benchmark capturing 14 distinct surveillance scenarios with various camera views, lighting, and weather conditions. It serves as a robust diagnostic tool for security anomalies in dynamic environments.",
    "authors": [
      "Liyun Zhu",
      "Lei Wang",
      "Arjun Raj",
      "Tom Gedeon",
      "Chen Chen"
    ],
    "githubLink": "https://github.com/Tom-roujiang/MSAD",
    "itemCount": "720 videos",
    "source": "NeurIPS 2024",
    "specs": "RGB Video, 14 scenarios, multiple camera views, diverse anomaly types (human and non-human related).",
    "year": "2024",
    "id": "saved-1769661171159-rg851",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcqdOFaxnCCZp7_F4zg1Pa5zGuZ-2OkjTFwZke3RK2hBU3gDIHIwSGGKGw76wuJYAqL9EYp0lhrCUE165ju04XkqAH_8sh5iXgVa0TzfZ7PI-t4ZmXkx1C50aqaj7lXgTA5Q3Ernied0t-pNjwLK1WvtbkFDS91O1xhQYOpvDGmNiNlNtTSlkGX2eo69ySEAIhibyULLpcwJxc8w1mPHP6cVBwuKIJdb68VP8gTVAs5X4qKeWFT6B3uPlqRF0=",
        "title": "zenodo.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHu7eiJz1Rjhvbzluuif9MqkzePZyISxuCaH7EDxS-cwtlOfZvOLcESRLsP43jBQhx8QiH5jfh2J9xAD4v1jcE3-XQn0KAEf_tvTL2dV2CYPj8MB3eGeiZruvLHLPpuxJbPk96rAdaGmvuR9HU=",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVnFWNV45Diw7no6NedVQ1FdT7WMoivu_syUvZAZagrmZA-BTOh8hV_dZz3pv452g32GN3j2Lhf_aQrOTPqp-dKrlj1ss7tmyWCKerVYl2BN_OeNktAzwiJin3WATHwhnGywM=",
        "title": "github.io"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGs_DjGUofUqEQysu39KZLTuIS6fqjJaLX0hwqL4xvo7J0MSZ9Lvt9ycZlytXnH0xz_MovMTmR-AOyAwEtXxYD9n0kitPy4895NBRaUcSVeknXRleB9tDCebypGr6UKEAW0RrWtiM489fAsBK5FDZanLahU325UNCnKR83UQn3yUyzsPmM7edvdIgEsnfGzGBYqGB4ek4gv_ds4SDD_O8sBRoW3_j5oD75gyY0GtfeN5bc_jdNpMYs7nGyxbC4pnNK3g8BsgGTpHWbMUqC5qVssuMqBxITBoiU",
        "title": "thecvf.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHiYgv4l8iV9I2MKn1uVQJapwLOYH8GDZABKHzKz_JBBiyoVWDOT5L0DAuz2tMJm5N5IWgeOMOz0_Cu8KvrzBiTq2RCNJe40bwKStb3Dspzy3dZK-lE-bQozbET-P5H8BIX-wabN7b4yLeD4kdM2p0=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcQh0v5gvRtuLKEmAlvLAoYnxjv00ZBTQX1O09xKGMh01Dh00v8iyAz4APU83pr11wqeQeZYOS_9EMBTd2zTJ-RATlYABGLND2sjeSpgo5SGJ35F3CKy35tkO46d9TSP0B_303Bw==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGn2IpHC9C4GV2URI7SEWpQaA6RW-ZepfG0vnaPUBnzcqEc_RUVklRyy1UkjR_HTaOdXIK_3WEPhe0cpAakPzkiT6EVGT3pwLRbkIv5zHCnd9EEDUlOy0EmjmI=",
        "title": "github.io"
      }
    ]
  },
  {
    "title": "MultiEYE",
    "paperLink": "https://arxiv.org/abs/2412.09402",
    "description": "A newly introduced large-scale multi-modal benchmark dataset for eye disease diagnosis. It combines fundus images with OCT (Optical Coherence Tomography) data to enhance disease recognition through cross-modal knowledge distillation.",
    "authors": [
      "Wei Li",
      "Zhuoheng Li",
      "Weisen Zhao",
      "Jia Kwok",
      "Yijie Huang",
      "Haoqian Wang"
    ],
    "githubLink": "https://github.com/Liare/MultiEYE",
    "itemCount": "Not specified in snippet (Large scale)",
    "source": "arXiv",
    "specs": "Multi-modal (Fundus Images + OCT), Multi-class classification",
    "year": "2024",
    "id": "saved-1769661214149-2u9c6",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6pdmGtz9CPnB3FnFJwDQlVEcjql_99lrNQwLAB1NqWLRSFagWri_S5ibWQr69uUKgAPrzz-gdyfk9ubIeUbEi3nAdoQKsNJJSWj25a92SROL9v2NYs5ytrpM1TsE3j4j-Nwj0tILK9PZXa6vdL9NCONBZH8VJUn-fbN85gZwOe34H1dVG7NZJBX-A-uRhog==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSSQbyDdgpc1RkuwQQf_Jz5sLmYV6zEeXwgtY57ztCBRn_jLfcK2VS98bYdBR7xqZpWucbULMfjinEnFrwEOEXeZru70BWrH_DYzNrw_YRezEA14cQS7YW15Y285yMc2GsDt0gDxCsscUQf4eT",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJe4TjULWHaNuOM_d17G3NkOM2hbE50vZ-nLuL5aL6fgukDX9JBM-GL6ruoXnBaGo4fUZ1BU9ix_oBZunzwYlU0b-rRmLDorc7GjYupZvWGv_Vc3OPInAHNu5ZxCjG50c8OzkVuglEMqlWtw7Y5kpbQ3qDFyrm9-XCzzyTjgIOZXfNHtVMP7Pjt0n5k8s1yvDiZ10yeMuIjDhVMx6Hv_MoF5jcy-wJFErzv6FHzdAyUJnbKp91AORJyaL98X0tlANTRv7j8fO7Bg==",
        "title": "fortuneonline.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE78l3U4ONXAkFaMMJPe3kqv-9VQ-3Pd0GIGxk6rQoICt5miN89h7HUcXJvJcxuT879DBQxzSoxCqD-kLNbrGUKldCv-Fo8QLW2ssGcKvVMOT6uzeVSz64hHfUm8oniE_nyV_jOXAcaYB5O",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBotW3R-8wPyIXfWbBmWsDgamzPyPeM9ub2ypJ7_zDAzN_i9Z3JwkwYDTulO75azNp0B-6SvCnZgNf0OcC0KQ1Ap2ICoIdQiGyP4SRYnm3Y_o0zBapIcZByUMnbVjr7ZixmYPwbGHYL_Tg",
        "title": "adcis.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8cklrXbsGXXwL8KhIBDNXf0LDbzbFBitMKuuBaTpvLQ7DfrEqKsDcUNhwd9bcZOkD7K5j_lOBXf6hwI5UwYSLZTVhvvjTZG7qwxB_t6UPruQhvFj0_oa_siozQ8bnS9DYJo-b613bWplF4m6JvOew8KfYGOEx5rHWBy7HUsq82fVPMOVbe1tRakkhBYM=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "RFMiD (Retinal Fundus Multi-Disease Image Dataset)",
    "paperLink": "https://doi.org/10.3390/data6020014",
    "description": "A publicly available dataset designed for the development of multi-disease detection models. It contains 3,200 retinal fundus images annotated with 46 different conditions, making it one of the most diverse datasets for rare and frequent retinal pathologies.",
    "authors": [
      "Samiksha Pachade",
      "Prasanna Porwal",
      "Dhanshree Thulkar",
      "Manesh Kokare",
      "Girish Deshmukh",
      "Vivek Sahasrabuddhe",
      "Fabrice Mériaudeau"
    ],
    "githubLink": "https://github.com/tkclimb/RFMiD",
    "itemCount": "3,200 images",
    "source": "Scholar",
    "specs": "Color Fundus Images, 46 Disease Classes (Multi-label)",
    "year": "2021",
    "id": "saved-1769661214150-k8wgq",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6pdmGtz9CPnB3FnFJwDQlVEcjql_99lrNQwLAB1NqWLRSFagWri_S5ibWQr69uUKgAPrzz-gdyfk9ubIeUbEi3nAdoQKsNJJSWj25a92SROL9v2NYs5ytrpM1TsE3j4j-Nwj0tILK9PZXa6vdL9NCONBZH8VJUn-fbN85gZwOe34H1dVG7NZJBX-A-uRhog==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSSQbyDdgpc1RkuwQQf_Jz5sLmYV6zEeXwgtY57ztCBRn_jLfcK2VS98bYdBR7xqZpWucbULMfjinEnFrwEOEXeZru70BWrH_DYzNrw_YRezEA14cQS7YW15Y285yMc2GsDt0gDxCsscUQf4eT",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJe4TjULWHaNuOM_d17G3NkOM2hbE50vZ-nLuL5aL6fgukDX9JBM-GL6ruoXnBaGo4fUZ1BU9ix_oBZunzwYlU0b-rRmLDorc7GjYupZvWGv_Vc3OPInAHNu5ZxCjG50c8OzkVuglEMqlWtw7Y5kpbQ3qDFyrm9-XCzzyTjgIOZXfNHtVMP7Pjt0n5k8s1yvDiZ10yeMuIjDhVMx6Hv_MoF5jcy-wJFErzv6FHzdAyUJnbKp91AORJyaL98X0tlANTRv7j8fO7Bg==",
        "title": "fortuneonline.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE78l3U4ONXAkFaMMJPe3kqv-9VQ-3Pd0GIGxk6rQoICt5miN89h7HUcXJvJcxuT879DBQxzSoxCqD-kLNbrGUKldCv-Fo8QLW2ssGcKvVMOT6uzeVSz64hHfUm8oniE_nyV_jOXAcaYB5O",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBotW3R-8wPyIXfWbBmWsDgamzPyPeM9ub2ypJ7_zDAzN_i9Z3JwkwYDTulO75azNp0B-6SvCnZgNf0OcC0KQ1Ap2ICoIdQiGyP4SRYnm3Y_o0zBapIcZByUMnbVjr7ZixmYPwbGHYL_Tg",
        "title": "adcis.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8cklrXbsGXXwL8KhIBDNXf0LDbzbFBitMKuuBaTpvLQ7DfrEqKsDcUNhwd9bcZOkD7K5j_lOBXf6hwI5UwYSLZTVhvvjTZG7qwxB_t6UPruQhvFj0_oa_siozQ8bnS9DYJo-b613bWplF4m6JvOew8KfYGOEx5rHWBy7HUsq82fVPMOVbe1tRakkhBYM=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "ODIR-5K (Ocular Disease Intelligent Recognition)",
    "paperLink": "https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k",
    "description": "A large-scale multi-label classification dataset comprising 5,000 paired left and right eye fundus images (10,000 images total) from 5,000 patients. It covers 8 categories of ocular conditions including Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, and Normal. The dataset focuses on real-world clinical settings with 'structured text' patient information.",
    "authors": [
      "Peking University",
      "Shanggong Medical Technology"
    ],
    "githubLink": "https://github.com/ckx0806/ODIR-5K",
    "itemCount": "10,000 images",
    "source": "Kaggle",
    "specs": "Color Fundus Images (JPG), Structured Text (Age, Gender, Diagnostic Keywords), 8 Classes",
    "year": "2019",
    "id": "saved-1769661214150-tvjwf",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6pdmGtz9CPnB3FnFJwDQlVEcjql_99lrNQwLAB1NqWLRSFagWri_S5ibWQr69uUKgAPrzz-gdyfk9ubIeUbEi3nAdoQKsNJJSWj25a92SROL9v2NYs5ytrpM1TsE3j4j-Nwj0tILK9PZXa6vdL9NCONBZH8VJUn-fbN85gZwOe34H1dVG7NZJBX-A-uRhog==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSSQbyDdgpc1RkuwQQf_Jz5sLmYV6zEeXwgtY57ztCBRn_jLfcK2VS98bYdBR7xqZpWucbULMfjinEnFrwEOEXeZru70BWrH_DYzNrw_YRezEA14cQS7YW15Y285yMc2GsDt0gDxCsscUQf4eT",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJe4TjULWHaNuOM_d17G3NkOM2hbE50vZ-nLuL5aL6fgukDX9JBM-GL6ruoXnBaGo4fUZ1BU9ix_oBZunzwYlU0b-rRmLDorc7GjYupZvWGv_Vc3OPInAHNu5ZxCjG50c8OzkVuglEMqlWtw7Y5kpbQ3qDFyrm9-XCzzyTjgIOZXfNHtVMP7Pjt0n5k8s1yvDiZ10yeMuIjDhVMx6Hv_MoF5jcy-wJFErzv6FHzdAyUJnbKp91AORJyaL98X0tlANTRv7j8fO7Bg==",
        "title": "fortuneonline.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE78l3U4ONXAkFaMMJPe3kqv-9VQ-3Pd0GIGxk6rQoICt5miN89h7HUcXJvJcxuT879DBQxzSoxCqD-kLNbrGUKldCv-Fo8QLW2ssGcKvVMOT6uzeVSz64hHfUm8oniE_nyV_jOXAcaYB5O",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBotW3R-8wPyIXfWbBmWsDgamzPyPeM9ub2ypJ7_zDAzN_i9Z3JwkwYDTulO75azNp0B-6SvCnZgNf0OcC0KQ1Ap2ICoIdQiGyP4SRYnm3Y_o0zBapIcZByUMnbVjr7ZixmYPwbGHYL_Tg",
        "title": "adcis.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8cklrXbsGXXwL8KhIBDNXf0LDbzbFBitMKuuBaTpvLQ7DfrEqKsDcUNhwd9bcZOkD7K5j_lOBXf6hwI5UwYSLZTVhvvjTZG7qwxB_t6UPruQhvFj0_oa_siozQ8bnS9DYJo-b613bWplF4m6JvOew8KfYGOEx5rHWBy7HUsq82fVPMOVbe1tRakkhBYM=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "IDRiD (Indian Diabetic Retinopathy Image Dataset)",
    "paperLink": "https://doi.org/10.1109/ACCESS.2018.2839060",
    "description": "A dataset focused on diabetic retinopathy (DR) and diabetic macular edema (DME). It provides high-quality pixel-level annotations for typical diabetic retinal lesions (microaneurysms, hemorrhages, hard exudates, soft exudates) and optic disc/fovea center coordinates, making it a benchmark for segmentation tasks.",
    "authors": [
      "Prasanna Porwal",
      "Samiksha Pachade",
      "Ravi Kamble",
      "Manesh Kokare",
      "Girish Deshmukh",
      "Vivek Sahasrabuddhe",
      "Fabrice Mériaudeau"
    ],
    "githubLink": "https://github.com/aditya-kamath/IDRiD-Segmentation",
    "itemCount": "516 images",
    "source": "Scholar",
    "specs": "Color Fundus Images, Pixel-level Segmentation Masks, Disease Grading",
    "year": "2018",
    "id": "saved-1769661214150-yzrrr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6pdmGtz9CPnB3FnFJwDQlVEcjql_99lrNQwLAB1NqWLRSFagWri_S5ibWQr69uUKgAPrzz-gdyfk9ubIeUbEi3nAdoQKsNJJSWj25a92SROL9v2NYs5ytrpM1TsE3j4j-Nwj0tILK9PZXa6vdL9NCONBZH8VJUn-fbN85gZwOe34H1dVG7NZJBX-A-uRhog==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSSQbyDdgpc1RkuwQQf_Jz5sLmYV6zEeXwgtY57ztCBRn_jLfcK2VS98bYdBR7xqZpWucbULMfjinEnFrwEOEXeZru70BWrH_DYzNrw_YRezEA14cQS7YW15Y285yMc2GsDt0gDxCsscUQf4eT",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJe4TjULWHaNuOM_d17G3NkOM2hbE50vZ-nLuL5aL6fgukDX9JBM-GL6ruoXnBaGo4fUZ1BU9ix_oBZunzwYlU0b-rRmLDorc7GjYupZvWGv_Vc3OPInAHNu5ZxCjG50c8OzkVuglEMqlWtw7Y5kpbQ3qDFyrm9-XCzzyTjgIOZXfNHtVMP7Pjt0n5k8s1yvDiZ10yeMuIjDhVMx6Hv_MoF5jcy-wJFErzv6FHzdAyUJnbKp91AORJyaL98X0tlANTRv7j8fO7Bg==",
        "title": "fortuneonline.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE78l3U4ONXAkFaMMJPe3kqv-9VQ-3Pd0GIGxk6rQoICt5miN89h7HUcXJvJcxuT879DBQxzSoxCqD-kLNbrGUKldCv-Fo8QLW2ssGcKvVMOT6uzeVSz64hHfUm8oniE_nyV_jOXAcaYB5O",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBotW3R-8wPyIXfWbBmWsDgamzPyPeM9ub2ypJ7_zDAzN_i9Z3JwkwYDTulO75azNp0B-6SvCnZgNf0OcC0KQ1Ap2ICoIdQiGyP4SRYnm3Y_o0zBapIcZByUMnbVjr7ZixmYPwbGHYL_Tg",
        "title": "adcis.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8cklrXbsGXXwL8KhIBDNXf0LDbzbFBitMKuuBaTpvLQ7DfrEqKsDcUNhwd9bcZOkD7K5j_lOBXf6hwI5UwYSLZTVhvvjTZG7qwxB_t6UPruQhvFj0_oa_siozQ8bnS9DYJo-b613bWplF4m6JvOew8KfYGOEx5rHWBy7HUsq82fVPMOVbe1tRakkhBYM=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Messidor-2",
    "paperLink": "http://www.adcis.net/en/third-party/messidor2/",
    "description": "An extension of the original Messidor dataset, consisting of paired macula-centered eye fundus images. It is widely used for benchmarking diabetic retinopathy grading and macular edema detection algorithms.",
    "authors": [
      "Messidor Program Partners",
      "Brest University Hospital"
    ],
    "githubLink": "https://www.kaggle.com/datasets/google-brain/messidor2-dr-grades",
    "itemCount": "1,748 images",
    "source": "Scholar",
    "specs": "Color Fundus Images (JPG/PNG), DR and DME Grades",
    "year": "2014",
    "id": "saved-1769661214150-0fkpl",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6pdmGtz9CPnB3FnFJwDQlVEcjql_99lrNQwLAB1NqWLRSFagWri_S5ibWQr69uUKgAPrzz-gdyfk9ubIeUbEi3nAdoQKsNJJSWj25a92SROL9v2NYs5ytrpM1TsE3j4j-Nwj0tILK9PZXa6vdL9NCONBZH8VJUn-fbN85gZwOe34H1dVG7NZJBX-A-uRhog==",
        "title": "github.com"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSSQbyDdgpc1RkuwQQf_Jz5sLmYV6zEeXwgtY57ztCBRn_jLfcK2VS98bYdBR7xqZpWucbULMfjinEnFrwEOEXeZru70BWrH_DYzNrw_YRezEA14cQS7YW15Y285yMc2GsDt0gDxCsscUQf4eT",
        "title": "nih.gov"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJe4TjULWHaNuOM_d17G3NkOM2hbE50vZ-nLuL5aL6fgukDX9JBM-GL6ruoXnBaGo4fUZ1BU9ix_oBZunzwYlU0b-rRmLDorc7GjYupZvWGv_Vc3OPInAHNu5ZxCjG50c8OzkVuglEMqlWtw7Y5kpbQ3qDFyrm9-XCzzyTjgIOZXfNHtVMP7Pjt0n5k8s1yvDiZ10yeMuIjDhVMx6Hv_MoF5jcy-wJFErzv6FHzdAyUJnbKp91AORJyaL98X0tlANTRv7j8fO7Bg==",
        "title": "fortuneonline.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE78l3U4ONXAkFaMMJPe3kqv-9VQ-3Pd0GIGxk6rQoICt5miN89h7HUcXJvJcxuT879DBQxzSoxCqD-kLNbrGUKldCv-Fo8QLW2ssGcKvVMOT6uzeVSz64hHfUm8oniE_nyV_jOXAcaYB5O",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBotW3R-8wPyIXfWbBmWsDgamzPyPeM9ub2ypJ7_zDAzN_i9Z3JwkwYDTulO75azNp0B-6SvCnZgNf0OcC0KQ1Ap2ICoIdQiGyP4SRYnm3Y_o0zBapIcZByUMnbVjr7ZixmYPwbGHYL_Tg",
        "title": "adcis.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8cklrXbsGXXwL8KhIBDNXf0LDbzbFBitMKuuBaTpvLQ7DfrEqKsDcUNhwd9bcZOkD7K5j_lOBXf6hwI5UwYSLZTVhvvjTZG7qwxB_t6UPruQhvFj0_oa_siozQ8bnS9DYJo-b613bWplF4m6JvOew8KfYGOEx5rHWBy7HUsq82fVPMOVbe1tRakkhBYM=",
        "title": "github.com"
      }
    ]
  },
  {
    "title": "Unsupervised People's Speech Dataset",
    "paperLink": "https://aclanthology.org/2025.loresmt-1.22/",
    "description": "A massive dataset of unsupervised speech extracted from Archive.org, designed to train self-supervised speech models, particularly for low-resource languages.",
    "authors": [
      "Sarah Luger",
      "Rafael Mosquera-Gómez",
      "Alex Miłowski",
      "Thom Vaughan",
      "Sara Hincapie-Monsalve",
      "Pedro Ortiz Suarez",
      "Kurt Bollacker"
    ],
    "githubLink": "https://upschallenge.org/",
    "itemCount": ">821,000 hours",
    "source": "Scholar",
    "specs": "Audio (speech), 89+ languages, Unsupervised, CC-BY/CC-BY-SA",
    "year": "2025",
    "id": "saved-1769661346478-lit51",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfbDvcVFzFe-ZkVLpWStSmWWzptN8EImFKMLxFQE9JDj8JJnYYq7vaQLNrAuffNBv6XYSqatJboXdQF8UQ9ZCdi1VdSt3fxQrMZG-accfhXCg1cFqtLwphUWHo0rJTjINbLa4ArA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE148WIAug-UdwHcrGvIPNFfwIDuE_x4e-yo-zH-PeyT9T4HfVCBwU17ziQ568TGyd6dfPqdaE7pXdD9BqVG9pCTFFxwyooUwaqFvGxHdmTkUqo3o_EpxzB8wS4zbZeuY5qD4F8TNAlOxNPgxUa2MuyqJuTjHre3_X5tvIlFjqb5K30kUjSHlfbidh-Dw==",
        "title": "dmlr.ai"
      }
    ]
  },
  {
    "title": "Speech Wikimedia Dataset",
    "paperLink": "https://arxiv.org/abs/2308.15710",
    "description": "A multilingual speech dataset compiled from Wikimedia Commons, featuring audio files with transcriptions in 77 languages. It supports speech recognition and translation tasks.",
    "authors": [
      "Rafael Mosquera Gómez",
      "Julian Eusse",
      "Juan Ciro",
      "Daniel Galvez",
      "Ryan Hileman",
      "Kurt Bollacker",
      "David Kanter"
    ],
    "githubLink": "https://huggingface.co/datasets/MLCommons/speech-wikimedia",
    "itemCount": "~1,780 hours",
    "source": "arXiv",
    "specs": "Audio, Transcriptions, 77 languages, CC-BY-SA/Public Domain",
    "year": "2023",
    "id": "saved-1769661346478-18541",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfbDvcVFzFe-ZkVLpWStSmWWzptN8EImFKMLxFQE9JDj8JJnYYq7vaQLNrAuffNBv6XYSqatJboXdQF8UQ9ZCdi1VdSt3fxQrMZG-accfhXCg1cFqtLwphUWHo0rJTjINbLa4ArA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE148WIAug-UdwHcrGvIPNFfwIDuE_x4e-yo-zH-PeyT9T4HfVCBwU17ziQ568TGyd6dfPqdaE7pXdD9BqVG9pCTFFxwyooUwaqFvGxHdmTkUqo3o_EpxzB8wS4zbZeuY5qD4F8TNAlOxNPgxUa2MuyqJuTjHre3_X5tvIlFjqb5K30kUjSHlfbidh-Dw==",
        "title": "dmlr.ai"
      }
    ]
  },
  {
    "title": "The Dollar Street Dataset",
    "paperLink": "https://openreview.net/forum?id=qnfYsave0U4",
    "description": "A supervised image dataset of everyday household items from homes around the world, capturing socioeconomic diversity. It includes demographic metadata such as region, country, and monthly income.",
    "authors": [
      "William Gaviria Rojas",
      "Sudnya Diamos",
      "Keertan Kini",
      "David Kanter",
      "Vijay Janapa Reddi",
      "Cody Coleman"
    ],
    "githubLink": "https://github.com/mlcommons/dollar_street",
    "itemCount": "38,479 images",
    "source": "Scholar",
    "specs": "Images, Socioeconomic metadata, 63 countries, CC-BY",
    "year": "2022",
    "id": "saved-1769661346478-ybbze",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfbDvcVFzFe-ZkVLpWStSmWWzptN8EImFKMLxFQE9JDj8JJnYYq7vaQLNrAuffNBv6XYSqatJboXdQF8UQ9ZCdi1VdSt3fxQrMZG-accfhXCg1cFqtLwphUWHo0rJTjINbLa4ArA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE148WIAug-UdwHcrGvIPNFfwIDuE_x4e-yo-zH-PeyT9T4HfVCBwU17ziQ568TGyd6dfPqdaE7pXdD9BqVG9pCTFFxwyooUwaqFvGxHdmTkUqo3o_EpxzB8wS4zbZeuY5qD4F8TNAlOxNPgxUa2MuyqJuTjHre3_X5tvIlFjqb5K30kUjSHlfbidh-Dw==",
        "title": "dmlr.ai"
      }
    ]
  },
  {
    "title": "DataPerf",
    "paperLink": "https://arxiv.org/abs/2207.10062",
    "description": "A benchmark suite for data-centric AI development, evaluating the quality of datasets and data-centric algorithms across vision, speech, and other modalities.",
    "authors": [
      "Mark Mazumder",
      "Colby Banbury",
      "Xiaozhe Yao",
      "Bojan Karlaš",
      "William Gaviria Rojas",
      "Sudnya Diamos",
      "Greg Diamos",
      "Lynn He",
      "Alicia Parrish",
      "Hannah Rose Kirk",
      "Jessica Quaye",
      "Charvi Rastogi",
      "Douwe Kiela",
      "David Jurado",
      "David Kanter",
      "Rafael Mosquera",
      "Juan Ciro",
      "Lora Aroyo",
      "Bilge Acun",
      "Lingjiao Chen",
      "Mehul Smriti Raje",
      "Max Bartolo",
      "Sabri Eyuboglu",
      "Amirata Ghorbani",
      "Emmett Goodman",
      "Oana Inel",
      "Tariq Kane",
      "Christine R. Kirkpatrick",
      "Tzu-Sheng Kuo",
      "Jonas Mueller",
      "Tristan Thrush",
      "Joaquin Vanschoren",
      "Margaret Warren",
      "Adina Williams",
      "Serena Yeung",
      "Newsha Ardalani",
      "Praveen Paritosh",
      "Ce Zhang",
      "James Zou",
      "Carole-Jean Wu",
      "Cody Coleman",
      "Andrew Ng",
      "Peter Mattson",
      "Vijay Janapa Reddi"
    ],
    "githubLink": "https://github.com/facebookresearch/dataperf",
    "itemCount": "5 benchmarks",
    "source": "arXiv",
    "specs": "Benchmark Suite (Vision, Speech, text-to-image, etc.)",
    "year": "2022",
    "id": "saved-1769661346478-5lht3",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfbDvcVFzFe-ZkVLpWStSmWWzptN8EImFKMLxFQE9JDj8JJnYYq7vaQLNrAuffNBv6XYSqatJboXdQF8UQ9ZCdi1VdSt3fxQrMZG-accfhXCg1cFqtLwphUWHo0rJTjINbLa4ArA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE148WIAug-UdwHcrGvIPNFfwIDuE_x4e-yo-zH-PeyT9T4HfVCBwU17ziQ568TGyd6dfPqdaE7pXdD9BqVG9pCTFFxwyooUwaqFvGxHdmTkUqo3o_EpxzB8wS4zbZeuY5qD4F8TNAlOxNPgxUa2MuyqJuTjHre3_X5tvIlFjqb5K30kUjSHlfbidh-Dw==",
        "title": "dmlr.ai"
      }
    ]
  },
  {
    "title": "The People's Speech Dataset",
    "paperLink": "https://arxiv.org/abs/2111.09344",
    "description": "A large-scale diverse English speech recognition dataset, supervised and licensed for academic and commercial usage (CC-BY-SA and CC-BY). It focuses on conversational speech and diverse acoustic environments.",
    "authors": [
      "Daniel Galvez",
      "Greg Diamos",
      "Juan Ciro",
      "Juan Felipe Cerón",
      "Keith Achorn",
      "Anjali Gopi",
      "David Kanter",
      "Maximilian Lam",
      "Mark Mazumder",
      "Vijay Janapa Reddi"
    ],
    "githubLink": "https://github.com/mlcommons/peoples_speech",
    "itemCount": "30,000+ hours",
    "source": "arXiv",
    "specs": "Audio (speech), English, Supervised, CC-BY-SA/CC-BY",
    "year": "2021",
    "id": "saved-1769661346478-zotti",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfbDvcVFzFe-ZkVLpWStSmWWzptN8EImFKMLxFQE9JDj8JJnYYq7vaQLNrAuffNBv6XYSqatJboXdQF8UQ9ZCdi1VdSt3fxQrMZG-accfhXCg1cFqtLwphUWHo0rJTjINbLa4ArA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE148WIAug-UdwHcrGvIPNFfwIDuE_x4e-yo-zH-PeyT9T4HfVCBwU17ziQ568TGyd6dfPqdaE7pXdD9BqVG9pCTFFxwyooUwaqFvGxHdmTkUqo3o_EpxzB8wS4zbZeuY5qD4F8TNAlOxNPgxUa2MuyqJuTjHre3_X5tvIlFjqb5K30kUjSHlfbidh-Dw==",
        "title": "dmlr.ai"
      }
    ]
  },
  {
    "title": "Multilingual Spoken Words Corpus (MSWC)",
    "paperLink": "https://openreview.net/forum?id=c20jWJthARl",
    "description": "A large audio dataset of spoken words in 50 languages, containing over 340,000 keywords and 23.4 million examples. It is designed for keyword spotting and spoken term search research.",
    "authors": [
      "Mark Mazumder",
      "Sharad Chitlangia",
      "Colby Banbury",
      "Yiping Kang",
      "Juan Manuel Ciro",
      "Keith Achorn",
      "Daniel Galvez",
      "Mark Sabini",
      "Peter Mattson",
      "David Kanter",
      "Greg Diamos",
      "Pete Warden",
      "Josh Meyer",
      "Vijay Janapa Reddi"
    ],
    "githubLink": "https://huggingface.co/datasets/MLCommons/ml_spoken_words",
    "itemCount": "23.4 million examples (6,000+ hours)",
    "source": "Hugging Face",
    "specs": "Audio (keywords), 50 languages, CC-BY 4.0",
    "year": "2021",
    "id": "saved-1769661346478-80iyi",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfbDvcVFzFe-ZkVLpWStSmWWzptN8EImFKMLxFQE9JDj8JJnYYq7vaQLNrAuffNBv6XYSqatJboXdQF8UQ9ZCdi1VdSt3fxQrMZG-accfhXCg1cFqtLwphUWHo0rJTjINbLa4ArA==",
        "title": "openreview.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE148WIAug-UdwHcrGvIPNFfwIDuE_x4e-yo-zH-PeyT9T4HfVCBwU17ziQ568TGyd6dfPqdaE7pXdD9BqVG9pCTFFxwyooUwaqFvGxHdmTkUqo3o_EpxzB8wS4zbZeuY5qD4F8TNAlOxNPgxUa2MuyqJuTjHre3_X5tvIlFjqb5K30kUjSHlfbidh-Dw==",
        "title": "dmlr.ai"
      }
    ]
  },
  {
    "title": "Objaverse++",
    "paperLink": "https://arxiv.org/abs/2504.07334",
    "description": "A curated subset of the Objaverse dataset enhanced with high-quality manual and automated annotations to address low-quality model prevalence. It focuses on improving image-to-3D generation performance.",
    "authors": [
      "Minseop Kwak",
      "et al."
    ],
    "githubLink": "https://github.com/Rank3D/ObjaversePP",
    "itemCount": "~500,000 curated models",
    "source": "arXiv",
    "specs": "3D models, Aesthetic quality scores, Detailed attribute tags",
    "year": "2025",
    "id": "saved-1769661403582-zivhm",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFB2VqGYXAhk1-QcaWz_zic51E-rzKs2unv5hU1br1CwXtB2clsnNDJ6v89GXH-bMS1aAZpVmeNgE23jz_wnK_4NcgOe4RAQ-eKfxpLRHtFGYjEeaos1ZOU4oNY_4JCnrGcToM=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHoNOT5nV394pxYFVXiWF_oeDx8hiN_NCFIM8WBiopyi9FjBePoP2fmisWlcJSK1duc2vFgrkdkpx3KXlgBwiEU2vMrOtxv0XF0RaeOUknYCR3npbKPOlSRnEeo30bL",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpPVrrsYDxql4AlV6rS8LNX4d05T18g3wdlLGufDOLfURgSUJvS3aJ2Dp1AcZ5lwr_bhohWvnji66pxit60oFHZpx9Ttpfe-_RkPZlfU-c7G6qnaMC4UeQmpXgBhelT9mFZ40N-qvjehrkQot9KpvCfC2cQYUj6h8Kv9eXP31IYkJ-Yo-ALLVfQYGaUAE5iqUao5AP_tZUkq6n",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjn64_RrHs_xLA9IaQC7i8wSzVQkf2duHSomhABB-r4qdp4_8WcgJ48R7-29WG0M5uwsVI-eYfbQUmaCZIIiDM3EjVnXy49y3kBu1iJ-dD1tLz-J5pZemTg08J",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVQx-CiXk-WpceL0XFeyJYP2YGLpAfHjPIrMLYIxilwdBQYpxv7olIFj1IEdte4wlITLgdZ7l8wAUbWPkg9AEXnz2R9lhCv_zE_f4HlRLhMy6JFJUG0fx3XKFu",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "T3Bench",
    "paperLink": "https://arxiv.org/abs/2310.02977",
    "description": "A comprehensive benchmark specifically designed for text-to-3D generation. It includes diverse text prompts of varying complexity and automatic metrics to assess quality and text alignment.",
    "authors": [
      "Yuze He",
      "Yushi Bai",
      "Matthieu Lin",
      "Wang Zhao",
      "Yubin Hu",
      "Jenny Sheng",
      "Ran Yi",
      "Juanzi Li",
      "Yong-Jin Liu"
    ],
    "githubLink": "https://github.com/THU-LYJ-Lab/T3Bench",
    "itemCount": "Benchmark suite (prompts + metrics)",
    "source": "arXiv",
    "specs": "Text prompts, Evaluation metrics (Quality, Alignment)",
    "year": "2023",
    "id": "saved-1769661403582-8prf8",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFB2VqGYXAhk1-QcaWz_zic51E-rzKs2unv5hU1br1CwXtB2clsnNDJ6v89GXH-bMS1aAZpVmeNgE23jz_wnK_4NcgOe4RAQ-eKfxpLRHtFGYjEeaos1ZOU4oNY_4JCnrGcToM=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHoNOT5nV394pxYFVXiWF_oeDx8hiN_NCFIM8WBiopyi9FjBePoP2fmisWlcJSK1duc2vFgrkdkpx3KXlgBwiEU2vMrOtxv0XF0RaeOUknYCR3npbKPOlSRnEeo30bL",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpPVrrsYDxql4AlV6rS8LNX4d05T18g3wdlLGufDOLfURgSUJvS3aJ2Dp1AcZ5lwr_bhohWvnji66pxit60oFHZpx9Ttpfe-_RkPZlfU-c7G6qnaMC4UeQmpXgBhelT9mFZ40N-qvjehrkQot9KpvCfC2cQYUj6h8Kv9eXP31IYkJ-Yo-ALLVfQYGaUAE5iqUao5AP_tZUkq6n",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjn64_RrHs_xLA9IaQC7i8wSzVQkf2duHSomhABB-r4qdp4_8WcgJ48R7-29WG0M5uwsVI-eYfbQUmaCZIIiDM3EjVnXy49y3kBu1iJ-dD1tLz-J5pZemTg08J",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVQx-CiXk-WpceL0XFeyJYP2YGLpAfHjPIrMLYIxilwdBQYpxv7olIFj1IEdte4wlITLgdZ7l8wAUbWPkg9AEXnz2R9lhCv_zE_f4HlRLhMy6JFJUG0fx3XKFu",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Google Scanned Objects (GSO)",
    "paperLink": "https://arxiv.org/abs/2204.11918",
    "description": "A high-quality collection of over 1,000 3D-scanned household items. The models are realistic, reflecting real-world properties, and are preprocessed for simulation platforms like Gazebo.",
    "authors": [
      "Laura Downs",
      "Anthony Francis",
      "Nate Koenig",
      "Brandon Kinman",
      "Ryan Hickman",
      "Krista Reymann",
      "Thomas B. McHugh",
      "Vincent Vanhoucke"
    ],
    "githubLink": "https://github.com/GoogleResearch/google-scanned-objects",
    "itemCount": "1,030 scanned objects",
    "source": "arXiv",
    "specs": "Scanned 3D models, SDF collision models, Metadata",
    "year": "2022",
    "id": "saved-1769661403583-8d00r",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFB2VqGYXAhk1-QcaWz_zic51E-rzKs2unv5hU1br1CwXtB2clsnNDJ6v89GXH-bMS1aAZpVmeNgE23jz_wnK_4NcgOe4RAQ-eKfxpLRHtFGYjEeaos1ZOU4oNY_4JCnrGcToM=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHoNOT5nV394pxYFVXiWF_oeDx8hiN_NCFIM8WBiopyi9FjBePoP2fmisWlcJSK1duc2vFgrkdkpx3KXlgBwiEU2vMrOtxv0XF0RaeOUknYCR3npbKPOlSRnEeo30bL",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpPVrrsYDxql4AlV6rS8LNX4d05T18g3wdlLGufDOLfURgSUJvS3aJ2Dp1AcZ5lwr_bhohWvnji66pxit60oFHZpx9Ttpfe-_RkPZlfU-c7G6qnaMC4UeQmpXgBhelT9mFZ40N-qvjehrkQot9KpvCfC2cQYUj6h8Kv9eXP31IYkJ-Yo-ALLVfQYGaUAE5iqUao5AP_tZUkq6n",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjn64_RrHs_xLA9IaQC7i8wSzVQkf2duHSomhABB-r4qdp4_8WcgJ48R7-29WG0M5uwsVI-eYfbQUmaCZIIiDM3EjVnXy49y3kBu1iJ-dD1tLz-J5pZemTg08J",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVQx-CiXk-WpceL0XFeyJYP2YGLpAfHjPIrMLYIxilwdBQYpxv7olIFj1IEdte4wlITLgdZ7l8wAUbWPkg9AEXnz2R9lhCv_zE_f4HlRLhMy6JFJUG0fx3XKFu",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Amazon Berkeley Objects (ABO)",
    "paperLink": "https://arxiv.org/abs/2110.06199",
    "description": "A large-scale dataset of product images and 3D models corresponding to real household objects. It is used to bridge the gap between real and virtual 3D worlds, suitable for single-view reconstruction and material estimation.",
    "authors": [
      "Jasmine Collins",
      "Shubham Goel",
      "Kenan Deng",
      "Achleshwar Luthra",
      "Leon Xu",
      "Erhan Gundogdu",
      "Xi Zhang",
      "Tomas F. Yago Vicente",
      "Thomas Dideriksen",
      "Himanshu Arora",
      "Matthieu Guillaumin",
      "Jitendra Malik"
    ],
    "githubLink": "https://github.com/JasmineCollins/abo",
    "itemCount": "7,953 3D models, 147,702 product listings",
    "source": "arXiv",
    "specs": "3D meshes, Physically-based materials (PBR), Multi-view images, Metadata",
    "year": "2021",
    "id": "saved-1769661403583-avp60",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFB2VqGYXAhk1-QcaWz_zic51E-rzKs2unv5hU1br1CwXtB2clsnNDJ6v89GXH-bMS1aAZpVmeNgE23jz_wnK_4NcgOe4RAQ-eKfxpLRHtFGYjEeaos1ZOU4oNY_4JCnrGcToM=",
        "title": "huggingface.co"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHoNOT5nV394pxYFVXiWF_oeDx8hiN_NCFIM8WBiopyi9FjBePoP2fmisWlcJSK1duc2vFgrkdkpx3KXlgBwiEU2vMrOtxv0XF0RaeOUknYCR3npbKPOlSRnEeo30bL",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHpPVrrsYDxql4AlV6rS8LNX4d05T18g3wdlLGufDOLfURgSUJvS3aJ2Dp1AcZ5lwr_bhohWvnji66pxit60oFHZpx9Ttpfe-_RkPZlfU-c7G6qnaMC4UeQmpXgBhelT9mFZ40N-qvjehrkQot9KpvCfC2cQYUj6h8Kv9eXP31IYkJ-Yo-ALLVfQYGaUAE5iqUao5AP_tZUkq6n",
        "title": "researchgate.net"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjn64_RrHs_xLA9IaQC7i8wSzVQkf2duHSomhABB-r4qdp4_8WcgJ48R7-29WG0M5uwsVI-eYfbQUmaCZIIiDM3EjVnXy49y3kBu1iJ-dD1tLz-J5pZemTg08J",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGVQx-CiXk-WpceL0XFeyJYP2YGLpAfHjPIrMLYIxilwdBQYpxv7olIFj1IEdte4wlITLgdZ7l8wAUbWPkg9AEXnz2R9lhCv_zE_f4HlRLhMy6JFJUG0fx3XKFu",
        "title": "arxiv.org"
      }
    ]
  },
  {
    "title": "Segment Any Text (SaT)",
    "paperLink": "https://arxiv.org/abs/2406.16678",
    "description": "A universal sentence segmentation benchmark and tool designed to be robust to missing punctuation and adaptable to various domains (e.g., legal, lyrics, code-switching).",
    "authors": [
      "Markus Frohmann",
      "Igor Sterner",
      "Ivan Vulić",
      "Benjamin Minixhofer",
      "Markus Schedl"
    ],
    "githubLink": "https://github.com/segment-any-text/wtpsplit",
    "itemCount": "Multiple corpora",
    "source": "Hugging Face",
    "specs": "Multilingual; Diverse domains; Sentence-level segmentation; Text",
    "year": "2024",
    "id": "saved-1769661476719-0t6hy",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkTBwXa0ryLb8TvV-PAW3aRzkxsq0A5hP85SByeHHoKx_OiIdpt9Qgur6cj2zQVVA9h02esPc0bY_xofWDf0Nfpy-WAOIRU2AdHrHvwya5ERyQjknwwKVs88CC73BtrvKs1PvIKeMUEUGeWEsg3buTNXCfYeYylpOvnVsLDzzvuq5k2zV6wckCIw06t7FXi79u",
        "title": "iitb.ac.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiaJC9P18qfHW4LHjUZ5W_S-16q3hc60tZqe3IojJF8tfG3q7AlvzmGwn5ZSDA2Vt56HAv4iOwkgP-wvSJRiwp-xopx_bEWqbwOKWmy1w2ZWZUwC2nvBjW2E4=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEqrpO12nN4XEcKIDAOSXNWNDlbE3GInoiPU3o_zsC6M8fGi-tffTN2LPQiPx6Bneope1Znw10kn61_BX9F1V1EynfMzPC9HPh7uxLaqqT1BfCIHrDXAIcGSXTbW_dArtt_RBBM",
        "title": "mdpi.com"
      }
    ]
  },
  {
    "title": "Wiki-Section",
    "paperLink": "https://aclanthology.org/D19-1178/",
    "description": "A dataset focusing on domain-specific topic segmentation, specifically covering the domains of 'Cities' and 'Diseases' from Wikipedia. It provides labeled section boundaries.",
    "authors": [
      "Sebastian Arnold",
      "Rudolf Schneider",
      "Philippe Cudré-Mauroux",
      "Felix A. Gers",
      "Alexander Löser"
    ],
    "githubLink": "https://github.com/sebastianarnold/SECTOR",
    "itemCount": "~20,000 documents",
    "source": "Scholar",
    "specs": "English and German Wikipedia articles; Domain-specific (Cities, Diseases); Text",
    "year": "2019",
    "id": "saved-1769661476719-lkhbd",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkTBwXa0ryLb8TvV-PAW3aRzkxsq0A5hP85SByeHHoKx_OiIdpt9Qgur6cj2zQVVA9h02esPc0bY_xofWDf0Nfpy-WAOIRU2AdHrHvwya5ERyQjknwwKVs88CC73BtrvKs1PvIKeMUEUGeWEsg3buTNXCfYeYylpOvnVsLDzzvuq5k2zV6wckCIw06t7FXi79u",
        "title": "iitb.ac.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiaJC9P18qfHW4LHjUZ5W_S-16q3hc60tZqe3IojJF8tfG3q7AlvzmGwn5ZSDA2Vt56HAv4iOwkgP-wvSJRiwp-xopx_bEWqbwOKWmy1w2ZWZUwC2nvBjW2E4=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEqrpO12nN4XEcKIDAOSXNWNDlbE3GInoiPU3o_zsC6M8fGi-tffTN2LPQiPx6Bneope1Znw10kn61_BX9F1V1EynfMzPC9HPh7uxLaqqT1BfCIHrDXAIcGSXTbW_dArtt_RBBM",
        "title": "mdpi.com"
      }
    ]
  },
  {
    "title": "Wiki-727K",
    "paperLink": "https://arxiv.org/abs/1803.09337",
    "description": "A large-scale dataset for text segmentation automatically extracted from English Wikipedia, using the table of contents to define segment boundaries. It is designed for supervised learning of topic segmentation.",
    "authors": [
      "Omri Koshorek",
      "Adir Cohen",
      "Noam Mor",
      "Michael Rotman",
      "Jonathan Berant"
    ],
    "githubLink": "https://github.com/koomri/text-segmentation",
    "itemCount": "727,746 documents",
    "source": "arXiv",
    "specs": "English Wikipedia articles; Hierarchical segmentation (sections, subsections); Text",
    "year": "2018",
    "id": "saved-1769661476719-tojyz",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkTBwXa0ryLb8TvV-PAW3aRzkxsq0A5hP85SByeHHoKx_OiIdpt9Qgur6cj2zQVVA9h02esPc0bY_xofWDf0Nfpy-WAOIRU2AdHrHvwya5ERyQjknwwKVs88CC73BtrvKs1PvIKeMUEUGeWEsg3buTNXCfYeYylpOvnVsLDzzvuq5k2zV6wckCIw06t7FXi79u",
        "title": "iitb.ac.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiaJC9P18qfHW4LHjUZ5W_S-16q3hc60tZqe3IojJF8tfG3q7AlvzmGwn5ZSDA2Vt56HAv4iOwkgP-wvSJRiwp-xopx_bEWqbwOKWmy1w2ZWZUwC2nvBjW2E4=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEqrpO12nN4XEcKIDAOSXNWNDlbE3GInoiPU3o_zsC6M8fGi-tffTN2LPQiPx6Bneope1Znw10kn61_BX9F1V1EynfMzPC9HPh7uxLaqqT1BfCIHrDXAIcGSXTbW_dArtt_RBBM",
        "title": "mdpi.com"
      }
    ]
  },
  {
    "title": "Cities and Elements",
    "paperLink": "https://dl.acm.org/doi/10.3115/1620754.1620811",
    "description": "Two small datasets created from Wikipedia articles about cities and chemical elements, used as standard benchmarks for evaluating topic segmentation algorithms.",
    "authors": [
      "Harr Chen",
      "S.R.K. Branavan",
      "Regina Barzilay",
      "David R. Karger"
    ],
    "githubLink": "https://github.com/charr/GlobalSeg",
    "itemCount": "100 documents (Cities), 95 documents (Elements)",
    "source": "Scholar",
    "specs": "Wikipedia articles; Text",
    "year": "2009",
    "id": "saved-1769661476719-bqqwr",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkTBwXa0ryLb8TvV-PAW3aRzkxsq0A5hP85SByeHHoKx_OiIdpt9Qgur6cj2zQVVA9h02esPc0bY_xofWDf0Nfpy-WAOIRU2AdHrHvwya5ERyQjknwwKVs88CC73BtrvKs1PvIKeMUEUGeWEsg3buTNXCfYeYylpOvnVsLDzzvuq5k2zV6wckCIw06t7FXi79u",
        "title": "iitb.ac.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiaJC9P18qfHW4LHjUZ5W_S-16q3hc60tZqe3IojJF8tfG3q7AlvzmGwn5ZSDA2Vt56HAv4iOwkgP-wvSJRiwp-xopx_bEWqbwOKWmy1w2ZWZUwC2nvBjW2E4=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEqrpO12nN4XEcKIDAOSXNWNDlbE3GInoiPU3o_zsC6M8fGi-tffTN2LPQiPx6Bneope1Znw10kn61_BX9F1V1EynfMzPC9HPh7uxLaqqT1BfCIHrDXAIcGSXTbW_dArtt_RBBM",
        "title": "mdpi.com"
      }
    ]
  },
  {
    "title": "Choi Dataset",
    "paperLink": "https://aclanthology.org/A00-2004/",
    "description": "A widely used synthetic benchmark for linear text segmentation. It consists of artificial documents generated by concatenating random segments (sets of sentences) from the Brown Corpus.",
    "authors": [
      "Freddy Y. Y. Choi"
    ],
    "githubLink": "https://github.com/koomri/text-segmentation",
    "itemCount": "922 documents",
    "source": "Scholar",
    "specs": "Synthetic documents; Concatenated Brown Corpus segments; Text",
    "year": "2000",
    "id": "saved-1769661476719-cresp",
    "groundingSources": [
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkTBwXa0ryLb8TvV-PAW3aRzkxsq0A5hP85SByeHHoKx_OiIdpt9Qgur6cj2zQVVA9h02esPc0bY_xofWDf0Nfpy-WAOIRU2AdHrHvwya5ERyQjknwwKVs88CC73BtrvKs1PvIKeMUEUGeWEsg3buTNXCfYeYylpOvnVsLDzzvuq5k2zV6wckCIw06t7FXi79u",
        "title": "iitb.ac.in"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEiaJC9P18qfHW4LHjUZ5W_S-16q3hc60tZqe3IojJF8tfG3q7AlvzmGwn5ZSDA2Vt56HAv4iOwkgP-wvSJRiwp-xopx_bEWqbwOKWmy1w2ZWZUwC2nvBjW2E4=",
        "title": "arxiv.org"
      },
      {
        "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEqrpO12nN4XEcKIDAOSXNWNDlbE3GInoiPU3o_zsC6M8fGi-tffTN2LPQiPx6Bneope1Znw10kn61_BX9F1V1EynfMzPC9HPh7uxLaqqT1BfCIHrDXAIcGSXTbW_dArtt_RBBM",
        "title": "mdpi.com"
      }
    ]
  }
]